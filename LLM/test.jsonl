{"doc_id": "192546007", "sentence": "Specifically , we investigate the attention and feature extraction mechanisms of state - of - the - art recurrent neural networks and self - attentive architectures for sentiment analysis , entailment and machine translation under adversarial attacks .", "ner": [["attention", "Method"], ["feature extraction mechanisms", "Method"], ["recurrent neural networks", "Method"], ["self - attentive architectures", "Method"], ["sentiment analysis", "Task"], ["entailment", "Task"], ["machine translation", "Task"]], "rel": [["attention", "Part-Of", "recurrent neural networks"], ["feature extraction mechanisms", "Part-Of", "recurrent neural networks"], ["attention", "Part-Of", "self - attentive architectures"], ["feature extraction mechanisms", "Part-Of", "self - attentive architectures"], ["recurrent neural networks", "Used-For", "sentiment analysis"], ["self - attentive architectures", "Used-For", "sentiment analysis"], ["recurrent neural networks", "Used-For", "entailment"], ["self - attentive architectures", "Used-For", "entailment"], ["recurrent neural networks", "Used-For", "machine translation"], ["self - attentive architectures", "Used-For", "machine translation"]], "rel_plus": [["attention:Method", "Part-Of", "recurrent neural networks:Method"], ["feature extraction mechanisms:Method", "Part-Of", "recurrent neural networks:Method"], ["attention:Method", "Part-Of", "self - attentive architectures:Method"], ["feature extraction mechanisms:Method", "Part-Of", "self - attentive architectures:Method"], ["recurrent neural networks:Method", "Used-For", "sentiment analysis:Task"], ["self - attentive architectures:Method", "Used-For", "sentiment analysis:Task"], ["recurrent neural networks:Method", "Used-For", "entailment:Task"], ["self - attentive architectures:Method", "Used-For", "entailment:Task"], ["recurrent neural networks:Method", "Used-For", "machine translation:Task"], ["self - attentive architectures:Method", "Used-For", "machine translation:Task"]]}
{"doc_id": "192546007", "sentence": "Self - attentive neural models have recently become a prominent component that achieves state - of - theart performances on many natural language processing ( NLP ) tasks such as text classification and machine translation ( MT ) .", "ner": [["Self - attentive neural models", "Method"], ["natural language processing", "Task"], ["NLP", "Task"], ["text classification", "Task"], ["machine translation", "Task"], ["MT", "Task"]], "rel": [["NLP", "Synonym-Of", "natural language processing"], ["text classification", "SubTask-Of", "natural language processing"], ["machine translation", "SubTask-Of", "natural language processing"], ["Self - attentive neural models", "Used-For", "natural language processing"], ["Self - attentive neural models", "Used-For", "text classification"], ["MT", "Synonym-Of", "machine translation"], ["Self - attentive neural models", "Used-For", "machine translation"]], "rel_plus": [["NLP:Task", "Synonym-Of", "natural language processing:Task"], ["text classification:Task", "SubTask-Of", "natural language processing:Task"], ["machine translation:Task", "SubTask-Of", "natural language processing:Task"], ["Self - attentive neural models:Method", "Used-For", "natural language processing:Task"], ["Self - attentive neural models:Method", "Used-For", "text classification:Task"], ["MT:Task", "Synonym-Of", "machine translation:Task"], ["Self - attentive neural models:Method", "Used-For", "machine translation:Task"]]}
{"doc_id": "192546007", "sentence": "This type of models , including Transformer ( Vaswani et al. , 2 0 1 7 ) and \" Bidirectional Encoder Representations from Transformers , \" shortened as BERT ( Devlin et al. , 2 0 1 9 ) , rely on the attention mechanism ( Luong et al. , 2 0 1 5 ) to learn a context - dependent representation ; compared to recurrent neural networks ( RNN ) , these self - attention - based models have faster encoding speed and the capacity of modeling a wider context .", "ner": [["Transformer", "Method"], ["Bidirectional Encoder Representations from Transformers", "Method"], ["BERT", "Method"], ["attention mechanism", "Method"], ["recurrent neural networks", "Method"], ["RNN", "Method"], ["self - attention - based models", "Method"]], "rel": [["attention mechanism", "Part-Of", "Transformer"], ["BERT", "Synonym-Of", "Bidirectional Encoder Representations from Transformers"], ["attention mechanism", "Part-Of", "Bidirectional Encoder Representations from Transformers"], ["RNN", "Synonym-Of", "recurrent neural networks"], ["Bidirectional Encoder Representations from Transformers", "Compare-With", "recurrent neural networks"], ["Transformer", "Compare-With", "recurrent neural networks"], ["self - attention - based models", "Compare-With", "recurrent neural networks"], ["Bidirectional Encoder Representations from Transformers", "SubClass-Of", "self - attention - based models"], ["Transformer", "SubClass-Of", "self - attention - based models"], ["BERT", "SubClass-Of", "self - attention - based models"]], "rel_plus": [["attention mechanism:Method", "Part-Of", "Transformer:Method"], ["BERT:Method", "Synonym-Of", "Bidirectional Encoder Representations from Transformers:Method"], ["attention mechanism:Method", "Part-Of", "Bidirectional Encoder Representations from Transformers:Method"], ["RNN:Method", "Synonym-Of", "recurrent neural networks:Method"], ["Bidirectional Encoder Representations from Transformers:Method", "Compare-With", "recurrent neural networks:Method"], ["Transformer:Method", "Compare-With", "recurrent neural networks:Method"], ["self - attention - based models:Method", "Compare-With", "recurrent neural networks:Method"], ["Bidirectional Encoder Representations from Transformers:Method", "SubClass-Of", "self - attention - based models:Method"], ["Transformer:Method", "SubClass-Of", "self - attention - based models:Method"], ["BERT:Method", "SubClass-Of", "self - attention - based models:Method"]]}
{"doc_id": "192546007", "sentence": "Particularly , BERT is recently proposed to extend the directionality of the Transformer model , and \" pre - trained \" using multiple objectives to strengthen its encoding capability .", "ner": [["BERT", "Method"], ["Transformer", "Method"]], "rel": [["BERT", "SubClass-Of", "Transformer"]], "rel_plus": [["BERT:Method", "SubClass-Of", "Transformer:Method"]]}
{"doc_id": "192546007", "sentence": "BERT achieves state - of - the - art performance on several NLP tasks including classification and sequence - to - sequence problems , often outperforming task - specific feature engineering or model architecture ; therefore , BERT is poised to be a key component in almost every neural model for NLP tasks .", "ner": [["BERT", "Method"], ["NLP", "Task"], ["classification", "Task"], ["sequence - to - sequence problems", "Task"], ["task - specific feature engineering", "Method"], ["BERT", "Method"], ["neural model", "Method"], ["NLP", "Task"]], "rel": [["BERT", "Used-For", "NLP"], ["sequence - to - sequence problems", "SubTask-Of", "NLP"], ["classification", "SubTask-Of", "NLP"], ["BERT", "Used-For", "classification"], ["BERT", "Used-For", "sequence - to - sequence problems"], ["BERT", "Compare-With", "task - specific feature engineering"], ["BERT", "Part-Of", "neural model"], ["neural model", "Used-For", "NLP"]], "rel_plus": [["BERT:Method", "Used-For", "NLP:Task"], ["sequence - to - sequence problems:Task", "SubTask-Of", "NLP:Task"], ["classification:Task", "SubTask-Of", "NLP:Task"], ["BERT:Method", "Used-For", "classification:Task"], ["BERT:Method", "Used-For", "sequence - to - sequence problems:Task"], ["BERT:Method", "Compare-With", "task - specific feature engineering:Method"], ["BERT:Method", "Part-Of", "neural model:Method"], ["neural model:Method", "Used-For", "NLP:Task"]]}
{"doc_id": "192546007", "sentence": "Despite the superior performance , it remains unclear whether the self - attentive structure deployed by Transformer or BERT is robust to adversarial attacks compared with other neural networks .", "ner": [["self - attentive structure", "Method"], ["Transformer", "Method"], ["BERT", "Method"], ["neural networks", "Method"]], "rel": [["Transformer", "Part-Of", "self - attentive structure"], ["BERT", "Part-Of", "self - attentive structure"], ["self - attentive structure", "Compare-With", "neural networks"]], "rel_plus": [["Transformer:Method", "Part-Of", "self - attentive structure:Method"], ["BERT:Method", "Part-Of", "self - attentive structure:Method"], ["self - attentive structure:Method", "Compare-With", "neural networks:Method"]]}
{"doc_id": "192546007", "sentence": "We conduct experiments on two mainstream self - attentive models : ( a ) Transformer for neural machine translation , and ( b ) BERT for sentiment and entailment classification .", "ner": [["mainstream self - attentive models", "Method"], ["Transformer", "Method"], ["neural machine translation", "Task"], ["BERT", "Method"], ["sentiment and entailment classification", "Task"]], "rel": [["Transformer", "SubClass-Of", "mainstream self - attentive models"], ["BERT", "SubClass-Of", "mainstream self - attentive models"], ["Transformer", "Used-For", "neural machine translation"], ["BERT", "Used-For", "sentiment and entailment classification"]], "rel_plus": [["Transformer:Method", "SubClass-Of", "mainstream self - attentive models:Method"], ["BERT:Method", "SubClass-Of", "mainstream self - attentive models:Method"], ["Transformer:Method", "Used-For", "neural machine translation:Task"], ["BERT:Method", "Used-For", "sentiment and entailment classification:Task"]]}
{"doc_id": "192546007", "sentence": "To the best of our knowledge , this paper brings the following contributions . \u2022 We propose novel algorithms to generate more natural adversarial examples that both preserve the semantics and mislead the classifiers . \u2022 We conduct comprehensive experiments to examine the robustness of RNN , Transformer , and BERT .", "ner": [["RNN", "Method"], ["Transformer", "Method"], ["BERT", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "192546007", "sentence": "This section describes the target neural architectures , LSTM and self - attentive models , and how to adapt these models for the downstream tasks : sentiment analysis , entailment and translation .", "ner": [["LSTM", "Method"], ["self - attentive models", "Method"], ["sentiment analysis", "Task"], ["entailment", "Task"], ["translation", "Task"]], "rel": [["self - attentive models", "Used-For", "sentiment analysis"], ["LSTM", "Used-For", "sentiment analysis"], ["self - attentive models", "Used-For", "entailment"], ["LSTM", "Used-For", "entailment"], ["self - attentive models", "Used-For", "translation"], ["LSTM", "Used-For", "translation"]], "rel_plus": [["self - attentive models:Method", "Used-For", "sentiment analysis:Task"], ["LSTM:Method", "Used-For", "sentiment analysis:Task"], ["self - attentive models:Method", "Used-For", "entailment:Task"], ["LSTM:Method", "Used-For", "entailment:Task"], ["self - attentive models:Method", "Used-For", "translation:Task"], ["LSTM:Method", "Used-For", "translation:Task"]]}
{"doc_id": "192546007", "sentence": "For classification tasks including sentiment analysis and entailment detection , we use a Bidirectional LSTM with an attention ( Hochreiter and Schmidhuber , 1 9 9 7 ; Bahdanau et al. , 2 0 1 4 ) layer as the sentence encoder , and a fully connected layer for classification problems .", "ner": [["classification", "Task"], ["sentiment analysis", "Task"], ["entailment detection", "Task"], ["Bidirectional LSTM", "Method"], ["attention", "Method"], ["fully connected layer", "Method"], ["classification", "Task"]], "rel": [["sentiment analysis", "SubTask-Of", "classification"], ["entailment detection", "SubTask-Of", "classification"], ["entailment detection", "Used-For", "classification"], ["Bidirectional LSTM", "Used-For", "sentiment analysis"], ["Bidirectional LSTM", "Used-For", "entailment detection"], ["attention", "Part-Of", "Bidirectional LSTM"], ["fully connected layer", "Used-For", "classification"]], "rel_plus": [["sentiment analysis:Task", "SubTask-Of", "classification:Task"], ["entailment detection:Task", "SubTask-Of", "classification:Task"], ["entailment detection:Task", "Used-For", "classification:Task"], ["Bidirectional LSTM:Method", "Used-For", "sentiment analysis:Task"], ["Bidirectional LSTM:Method", "Used-For", "entailment detection:Task"], ["attention:Method", "Part-Of", "Bidirectional LSTM:Method"], ["fully connected layer:Method", "Used-For", "classification:Task"]]}
{"doc_id": "192546007", "sentence": "For machine translation , we employ a common seq 2 seq model ( Sutskever et al. , 2 0 1 4 ) , in which both the encoder and decoder are a 2 - layer stacked Bi - LSTM with 5 1 2 hidden units .", "ner": [["machine translation", "Task"], ["seq 2 seq", "Method"], ["2 - layer stacked Bi - LSTM", "Method"]], "rel": [["seq 2 seq", "Used-For", "machine translation"], ["2 - layer stacked Bi - LSTM", "Part-Of", "seq 2 seq"]], "rel_plus": [["seq 2 seq:Method", "Used-For", "machine translation:Task"], ["2 - layer stacked Bi - LSTM:Method", "Part-Of", "seq 2 seq:Method"]]}
{"doc_id": "192546007", "sentence": "Self - attentive models are further distinguished into BERT and Transformers .", "ner": [["Self - attentive models", "Method"], ["BERT", "Method"], ["Transformers", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "192546007", "sentence": "The classification problems adopt the BERT model with an identical setup to the original paper ( Devlin et al. , 2 0 1 9 ) , in which BERT is used as an encoder that represents a sentence as a vector .", "ner": [["classification", "Task"], ["BERT", "Method"], ["BERT", "Method"]], "rel": [["BERT", "Used-For", "classification"]], "rel_plus": [["BERT:Method", "Used-For", "classification:Task"]]}
{"doc_id": "192546007", "sentence": "We also experiment with a smaller BERT model without pre - training , denoted as BERT NOPT , in order to isolate the impact of pre - training .", "ner": [["BERT", "Method"], ["BERT NOPT", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "192546007", "sentence": "To the best of our knowledge , there is no prior work that uses pre - trained BERT for machine translation .", "ner": [["BERT", "Method"], ["machine translation", "Task"]], "rel": [["BERT", "Used-For", "machine translation"]], "rel_plus": [["BERT:Method", "Used-For", "machine translation:Task"]]}
{"doc_id": "192546007", "sentence": "Thus , the Transformer model is employed for neural machine translation task .", "ner": [["Transformer", "Method"], ["neural machine translation", "Task"]], "rel": [["Transformer", "Used-For", "neural machine translation"]], "rel_plus": [["Transformer:Method", "Used-For", "neural machine translation:Task"]]}
{"doc_id": "192546007", "sentence": "Although the GS - GR method potentially achieves a high success rate , the adversarial examples formed by GS - GR are usually unnatural ; sometimes GS - GR completely changes the semantics of the original sentence by replacing the most important word with its antonym , for example : changing \" this is a good restaurant \" into \" this is a bad restaurant . \" This can not be treated as a successful attack , since humans will notice the change and agree with the model 's output .", "ner": [["GS - GR", "Method"], ["GS - GR", "Method"], ["GS - GR", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "192546007", "sentence": "In the experimental results , we show that the GS - EC method achieves a similar success rate as GS - GR in misleading the model , while being able to generate more natural and semanticallyconsistent adversarial sentences .", "ner": [["GS - EC", "Method"], ["GS - GR", "Method"]], "rel": [["GS - EC", "Compare-With", "GS - GR"]], "rel_plus": [["GS - EC:Method", "Compare-With", "GS - GR:Method"]]}
{"doc_id": "192546007", "sentence": "These methods are denoted as AS MIN -GR that replaces the word with the lowest score , and AS MAX -GR with the highest score .", "ner": [["AS MIN -GR", "Method"], ["AS MAX -GR", "Method"]], "rel": [["AS MIN -GR", "Compare-With", "AS MAX -GR"]], "rel_plus": [["AS MIN -GR:Method", "Compare-With", "AS MAX -GR:Method"]]}
{"doc_id": "192546007", "sentence": "We evaluate the robustness of the classification models ( for sentiment analysis and entailment ) by the following three criteria : ( a ) the success rate of the attacks misleading the model , ( b ) readability , and ( c ) human accuracy .", "ner": [["classification", "Task"], ["sentiment analysis", "Task"], ["entailment", "Task"]], "rel": [["sentiment analysis", "SubTask-Of", "classification"], ["entailment", "SubTask-Of", "classification"]], "rel_plus": [["sentiment analysis:Task", "SubTask-Of", "classification:Task"], ["entailment:Task", "SubTask-Of", "classification:Task"]]}
{"doc_id": "192546007", "sentence": "For the experiments on machine translation task , we evaluate the attack success rate and BLEU scores ( Papineni et al. , 2 0 0 2 ) for 2 0 0 sentence pairs in the WMT 1 7 Task ( Bojar et al. , 2 0 1 7 ) .", "ner": [["machine translation", "Task"], ["WMT 1 7 Task", "Dataset"]], "rel": [["WMT 1 7 Task", "Benchmark-For", "machine translation"]], "rel_plus": [["WMT 1 7 Task:Dataset", "Benchmark-For", "machine translation:Task"]]}
{"doc_id": "192546007", "sentence": "We first evaluate the robustness of LSTM , BERT , and BERT NOPT on binary sentiment analysis using the Yelp dataset ( Zhang et al. , 2 0 1 5 ) .", "ner": [["LSTM", "Method"], ["BERT", "Method"], ["BERT NOPT", "Method"], ["binary sentiment analysis", "Task"], ["Yelp", "Dataset"]], "rel": [["LSTM", "Used-For", "binary sentiment analysis"], ["BERT", "Used-For", "binary sentiment analysis"], ["BERT NOPT", "Used-For", "binary sentiment analysis"], ["Yelp", "Benchmark-For", "binary sentiment analysis"], ["LSTM", "Evaluated-With", "Yelp"], ["BERT", "Evaluated-With", "Yelp"], ["BERT NOPT", "Evaluated-With", "Yelp"]], "rel_plus": [["LSTM:Method", "Used-For", "binary sentiment analysis:Task"], ["BERT:Method", "Used-For", "binary sentiment analysis:Task"], ["BERT NOPT:Method", "Used-For", "binary sentiment analysis:Task"], ["Yelp:Dataset", "Benchmark-For", "binary sentiment analysis:Task"], ["LSTM:Method", "Evaluated-With", "Yelp:Dataset"], ["BERT:Method", "Evaluated-With", "Yelp:Dataset"], ["BERT NOPT:Method", "Evaluated-With", "Yelp:Dataset"]]}
{"doc_id": "192546007", "sentence": "Models under attack have accuracies of 9 3 . 7 % , 8 7 . 3 % and 9 0 . 7 % for fine - tuned BERT model , BERT NOPT and LSTM , respectively , on the test set .", "ner": [["BERT", "Method"], ["BERT NOPT", "Method"], ["LSTM", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "192546007", "sentence": "Note that for attention - based attacks ( i.e. , AS MIN -GR , AS MAX -GR , AS MIN -EC , and AS MAX -EC ) , the average of the first ( i.e. , the one that is closest to the model input ) attention layer from all 1 2 heads in BERT and BERT NOPT are used for our attacks . 1 To illustrate how adversarial attacks work , Fig. 1 shows the results from AS MAX -EC and AS MIN -EC methods that select a word to change based on the attention scores of the original sentence .", "ner": [["attention - based attacks", "Method"], ["AS MIN -GR", "Method"], ["AS MAX -GR", "Method"], ["AS MIN -EC", "Method"], ["AS MAX -EC", "Method"], ["attention layer", "Method"], ["1 2 heads", "Method"], ["BERT", "Method"], ["BERT NOPT", "Method"], ["AS MAX -EC", "Method"], ["AS MIN -EC", "Method"]], "rel": [["AS MIN -GR", "SubClass-Of", "attention - based attacks"], ["AS MAX -GR", "SubClass-Of", "attention - based attacks"], ["AS MIN -EC", "SubClass-Of", "attention - based attacks"], ["AS MAX -EC", "SubClass-Of", "attention - based attacks"], ["attention layer", "Part-Of", "1 2 heads"], ["1 2 heads", "Part-Of", "BERT"], ["1 2 heads", "Part-Of", "BERT NOPT"]], "rel_plus": [["AS MIN -GR:Method", "SubClass-Of", "attention - based attacks:Method"], ["AS MAX -GR:Method", "SubClass-Of", "attention - based attacks:Method"], ["AS MIN -EC:Method", "SubClass-Of", "attention - based attacks:Method"], ["AS MAX -EC:Method", "SubClass-Of", "attention - based attacks:Method"], ["attention layer:Method", "Part-Of", "1 2 heads:Method"], ["1 2 heads:Method", "Part-Of", "BERT:Method"], ["1 2 heads:Method", "Part-Of", "BERT NOPT:Method"]]}
{"doc_id": "192546007", "sentence": "The proposed GS - EC method can achieve almost identical success rates with GS - GR while restricting the search space based on the embedding distances .", "ner": [["GS - EC", "Method"], ["GS - GR", "Method"]], "rel": [["GS - EC", "Compare-With", "GS - GR"]], "rel_plus": [["GS - EC:Method", "Compare-With", "GS - GR:Method"]]}
{"doc_id": "192546007", "sentence": "GS - EC leads to higher quality adversarial examples in Section 4. 2 . \u2022 We found that using attention , especially AS MAX methods , can easily break the LSTM model .", "ner": [["GS - EC", "Method"], ["attention", "Method"], ["AS MAX", "Method"], ["LSTM", "Method"]], "rel": [["attention", "Part-Of", "AS MAX"]], "rel_plus": [["attention:Method", "Part-Of", "AS MAX:Method"]]}
{"doc_id": "192546007", "sentence": "However , the same vulnerability does not exist in BERT or BERT NOPT models .", "ner": [["BERT", "Method"], ["BERT NOPT", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "192546007", "sentence": "Since different types of attention - based attacks are suitable for different models , we summarize the best attention - based attack performance as A * in the table , which takes the maximum over four different types of attention - based attacks . \u2022 Self - attentive models ( BERT and BERT NOPT ) consistently lead to lower attack successful rates compared with the LSTM model , under RANDOM , LIST , attention - based attacks and greedy - based attacks .", "ner": [["attention - based attacks", "Method"], ["attention - based attack", "Method"], ["attention - based attacks", "Method"], ["Self - attentive models", "Method"], ["BERT", "Method"], ["BERT NOPT", "Method"], ["LSTM", "Method"], ["attention - based attacks", "Method"], ["greedy - based attacks", "Method"]], "rel": [["BERT", "SubClass-Of", "Self - attentive models"], ["BERT NOPT", "SubClass-Of", "Self - attentive models"], ["Self - attentive models", "Compare-With", "LSTM"]], "rel_plus": [["BERT:Method", "SubClass-Of", "Self - attentive models:Method"], ["BERT NOPT:Method", "SubClass-Of", "Self - attentive models:Method"], ["Self - attentive models:Method", "Compare-With", "LSTM:Method"]]}
{"doc_id": "192546007", "sentence": "We demonstrate the robustness of BERT model under GS - EC attack in Fig 2 .", "ner": [["BERT", "Method"], ["GS - EC attack", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "192546007", "sentence": "We can see that , GS - EC caused a substantial shift in the LSTM 's attention map while that of BERT remain stable .", "ner": [["GS - EC", "Method"], ["LSTM", "Method"], ["attention map", "Method"], ["BERT", "Method"]], "rel": [["attention map", "Part-Of", "LSTM"]], "rel_plus": [["attention map:Method", "Part-Of", "LSTM:Method"]]}
{"doc_id": "192546007", "sentence": "Table 2 : Adversarial examples for the BERT sentiment analysis model generated by GS - GR and GS - EC methods .", "ner": [["BERT sentiment analysis model", "Method"], ["GS - GR", "Method"], ["GS - EC", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "192546007", "sentence": "First , Table 2 compares the quality of the results generated by GS - GR and GS - EC attacks on a BERT model .", "ner": [["GS - GR", "Method"], ["GS - EC attacks", "Method"], ["BERT", "Method"]], "rel": [["GS - GR", "Compare-With", "GS - EC attacks"], ["GS - EC attacks", "Used-For", "BERT"], ["GS - GR", "Used-For", "BERT"]], "rel_plus": [["GS - GR:Method", "Compare-With", "GS - EC attacks:Method"], ["GS - EC attacks:Method", "Used-For", "BERT:Method"], ["GS - GR:Method", "Used-For", "BERT:Method"]]}
{"doc_id": "192546007", "sentence": "Here we see that constraints imposed by GS - EC make it superior than GS - GR in terms of retrieving words that are coherent with the context .", "ner": [["GS - EC", "Method"], ["GS - GR", "Method"]], "rel": [["GS - EC", "Compare-With", "GS - GR"]], "rel_plus": [["GS - EC:Method", "Compare-With", "GS - GR:Method"]]}
{"doc_id": "192546007", "sentence": "Table 3 is a comparison of LSTM and BERT models using the GS - EC attack .", "ner": [["LSTM", "Method"], ["BERT", "Method"], ["GS - EC attack", "Method"]], "rel": [["GS - EC attack", "Used-For", "LSTM"], ["LSTM", "Compare-With", "BERT"], ["GS - EC attack", "Used-For", "BERT"]], "rel_plus": [["GS - EC attack:Method", "Used-For", "LSTM:Method"], ["LSTM:Method", "Compare-With", "BERT:Method"], ["GS - EC attack:Method", "Used-For", "BERT:Method"]]}
{"doc_id": "192546007", "sentence": "It shows that the distance in embeddings space of BERT can better reflect semantic similarity and contribute to more natural adversarial examples .", "ner": [["BERT", "Method"], ["semantic similarity", "Task"]], "rel": [["BERT", "Used-For", "semantic similarity"]], "rel_plus": [["BERT:Method", "Used-For", "semantic similarity:Task"]]}
{"doc_id": "192546007", "sentence": "And , in Table 4 , we compare using GS - GR and GS - EC method on BERT model .", "ner": [["GS - GR", "Method"], ["GS - EC", "Method"], ["BERT", "Method"]], "rel": [["GS - GR", "Compare-With", "GS - EC"], ["GS - GR", "Used-For", "BERT"], ["GS - EC", "Used-For", "BERT"]], "rel_plus": [["GS - GR:Method", "Compare-With", "GS - EC:Method"], ["GS - GR:Method", "Used-For", "BERT:Method"], ["GS - EC:Method", "Used-For", "BERT:Method"]]}
{"doc_id": "192546007", "sentence": "Again , we see that the GS - EC method , which restricts the distance between sentence embeddings of original and adversarial inputs , can produce superior adversaries .", "ner": [["GS - EC", "Method"], ["sentence embeddings", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "192546007", "sentence": "Model Readability Human Accuracy LSTM 0. 6 5 2 . 1 % BERT 1. 0 6 8 . 8 % Table 3 : Comparison of LSTM and BERT models under human evaluations against GS - EC attack .", "ner": [["LSTM", "Method"], ["BERT", "Method"], ["LSTM", "Method"], ["BERT", "Method"], ["GS - EC attack", "Method"]], "rel": [["LSTM", "Compare-With", "BERT"]], "rel_plus": [["LSTM:Method", "Compare-With", "BERT:Method"]]}
{"doc_id": "192546007", "sentence": "GS - GR 0. 5 5 6 4 . 6 % GS - EC 1. 0 6 8 . 8 % Table 4 : Comparison of GS - GR and GS - EC attacks on BERT model for sentiment analysis .", "ner": [["GS - GR", "Method"], ["GS - EC", "Method"], ["GS - GR", "Method"], ["GS - EC attacks", "Method"], ["BERT", "Method"], ["sentiment analysis", "Task"]], "rel": [["GS - GR", "Compare-With", "GS - EC attacks"], ["GS - EC attacks", "Used-For", "BERT"], ["GS - GR", "Used-For", "BERT"], ["BERT", "Used-For", "sentiment analysis"]], "rel_plus": [["GS - GR:Method", "Compare-With", "GS - EC attacks:Method"], ["GS - EC attacks:Method", "Used-For", "BERT:Method"], ["GS - GR:Method", "Used-For", "BERT:Method"], ["BERT:Method", "Used-For", "sentiment analysis:Task"]]}
{"doc_id": "192546007", "sentence": "MultiNLI is one of the many datasets that see major improvements by BERT .", "ner": [["MultiNLI", "Dataset"], ["BERT", "Method"]], "rel": [["BERT", "Evaluated-With", "MultiNLI"]], "rel_plus": [["BERT:Method", "Evaluated-With", "MultiNLI:Dataset"]]}
{"doc_id": "192546007", "sentence": "The BERT model is trained to achieve 8 3 . 5 % accuracy and LSTM 7 6 % .", "ner": [["BERT", "Method"], ["LSTM", "Method"]], "rel": [["BERT", "Compare-With", "LSTM"]], "rel_plus": [["BERT:Method", "Compare-With", "LSTM:Method"]]}
{"doc_id": "192546007", "sentence": "Our findings are summarized as follows : \u2022 The entailment task is more difficult than single - sentence classification , as evidenced by the higher success rates of attacks among all models and attacks . \u2022 The greedy - based attacks consistently achieve higher success rates .   Samples illustrated in Table 6 show that the GS - EC method can find more coherent words for the attack , as opposed to GS - GR .", "ner": [["entailment", "Task"], ["single - sentence classification", "Task"], ["greedy - based attacks", "Method"], ["GS - EC", "Method"], ["GS - GR", "Method"]], "rel": [["entailment", "Compare-With", "single - sentence classification"], ["GS - EC", "Compare-With", "GS - GR"]], "rel_plus": [["entailment:Task", "Compare-With", "single - sentence classification:Task"], ["GS - EC:Method", "Compare-With", "GS - GR:Method"]]}
{"doc_id": "192546007", "sentence": "We implement LSTM and Transformer machine translation models using OpenNMT - py 2 .", "ner": [["LSTM", "Method"], ["Transformer machine translation", "Method"], ["OpenNMT - py 2", "Method"]], "rel": [["OpenNMT - py 2", "Used-For", "LSTM"], ["OpenNMT - py 2", "Used-For", "Transformer machine translation"]], "rel_plus": [["OpenNMT - py 2:Method", "Used-For", "LSTM:Method"], ["OpenNMT - py 2:Method", "Used-For", "Transformer machine translation:Method"]]}
{"doc_id": "192546007", "sentence": "Specifically , for the LSTM model , we train it with 4 5 3 thousand pairs from the Europarl corpus of German - English WMT 1 5 Task 3 , common crawl , and news - commentary .", "ner": [["LSTM", "Method"], ["German - English WMT 1 5 Task 3", "Dataset"], ["common crawl", "Dataset"], ["news - commentary", "Dataset"]], "rel": [["LSTM", "Trained-With", "German - English WMT 1 5 Task 3"], ["LSTM", "Trained-With", "common crawl"], ["LSTM", "Trained-With", "news - commentary"]], "rel_plus": [["LSTM:Method", "Trained-With", "German - English WMT 1 5 Task 3:Dataset"], ["LSTM:Method", "Trained-With", "common crawl:Dataset"], ["LSTM:Method", "Trained-With", "news - commentary:Dataset"]]}
{"doc_id": "192546007", "sentence": "Unlike the classification tasks , in machine translation the attack goal is harder to define .", "ner": [["classification", "Task"], ["machine translation", "Task"]], "rel": [["machine translation", "Compare-With", "classification"]], "rel_plus": [["machine translation:Task", "Compare-With", "classification:Task"]]}
{"doc_id": "192546007", "sentence": "First , we notice that the success rate of the attacks are below 3 0 % , presumably because translation is substantially more complex compared with the aforementioned text classification tasks .", "ner": [["translation", "Task"], ["text classification", "Task"]], "rel": [["translation", "Compare-With", "text classification"]], "rel_plus": [["translation:Task", "Compare-With", "text classification:Task"]]}
{"doc_id": "192546007", "sentence": "Nevertheless , the attacks on the Transformer model is significantly less successful than the LSTM - based one .", "ner": [["Transformer", "Method"], ["LSTM - based one", "Method"]], "rel": [["Transformer", "Compare-With", "LSTM - based one"]], "rel_plus": [["Transformer:Method", "Compare-With", "LSTM - based one:Method"]]}
{"doc_id": "192546007", "sentence": "We observe that the Transformer - based model always achieves a higher BLEU score over LSTM - based model , i.e. , have a better translation performance whether the sentences contain typos or not .", "ner": [["Transformer - based model", "Method"], ["LSTM - based model", "Method"], ["translation", "Task"]], "rel": [["Transformer - based model", "Compare-With", "LSTM - based model"], ["LSTM - based model", "Used-For", "translation"], ["Transformer - based model", "Used-For", "translation"]], "rel_plus": [["Transformer - based model:Method", "Compare-With", "LSTM - based model:Method"], ["LSTM - based model:Method", "Used-For", "translation:Task"], ["Transformer - based model:Method", "Used-For", "translation:Task"]]}
{"doc_id": "192546007", "sentence": "We conclude that Transformer - based model exhibits a greater robustness over LSTM - based model in the case of machine translation .", "ner": [["Transformer - based model", "Method"], ["LSTM - based model", "Method"], ["machine translation", "Task"]], "rel": [["Transformer - based model", "Compare-With", "LSTM - based model"], ["Transformer - based model", "Used-For", "machine translation"], ["LSTM - based model", "Used-For", "machine translation"]], "rel_plus": [["Transformer - based model:Method", "Compare-With", "LSTM - based model:Method"], ["Transformer - based model:Method", "Used-For", "machine translation:Task"], ["LSTM - based model:Method", "Used-For", "machine translation:Task"]]}
{"doc_id": "192546007", "sentence": "In addition , we present some successful adversarial examples in Table 9 , and see that the greedy attack can indeed generate natural examples for both models .   2 7 . 5 % 1 0 . 5 % Table 8 : BLEU scores using typo - based attack on LSTM and Transformer translation models .", "ner": [["LSTM", "Method"], ["Transformer translation", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "192546007", "sentence": "This is somewhat counter - intuitive - at the first glance one may assume that the self - attention layer is not robust   Original input And in this vein , he passed the prize money of 2 5, 0 0 0 euros on straight away Adv input And as this vein , he passed the prize money of 2 5, 0 0 0 euros on straight away Original output Und in diesem Sinne hat er sofort das Preis geld von 2 5. 0 0 0 Euro \u00fcber wiesen Adv output Und als diese Art , ging er sofort das Preis geld von 2 5. 0 0 0 Euro weiter Table 9 : Adversarial examples for LSTM and Transformer ( shortened as TF ) models with the target keyword \" Art . \" in the output . since perturbation in one word can affect all the attention scores .", "ner": [["LSTM", "Method"], ["Transformer", "Method"], ["TF", "Method"]], "rel": [["TF", "Synonym-Of", "Transformer"]], "rel_plus": [["TF:Method", "Synonym-Of", "Transformer:Method"]]}
{"doc_id": "192546007", "sentence": "For the GS - EC attack in the sentiment analysis task , embeddings from the LSTM model has an average R e of 0. 8 3 whereas for the BERT model it is 0. 5 6 under the same attack by changing one word .", "ner": [["GS - EC", "Method"], ["sentiment analysis", "Task"], ["LSTM", "Method"], ["BERT", "Method"]], "rel": [["GS - EC", "Used-For", "sentiment analysis"], ["BERT", "Used-For", "sentiment analysis"], ["LSTM", "Used-For", "sentiment analysis"], ["LSTM", "Compare-With", "BERT"]], "rel_plus": [["GS - EC:Method", "Used-For", "sentiment analysis:Task"], ["BERT:Method", "Used-For", "sentiment analysis:Task"], ["LSTM:Method", "Used-For", "sentiment analysis:Task"], ["LSTM:Method", "Compare-With", "BERT:Method"]]}
{"doc_id": "192546007", "sentence": "This supports our claim that the impact of an adversarial example is more severe on the LSTM model than BERT , which presumably plays an important role in the robustness of self - attentive models .   Robustness of neural network models has been a prominent research topic since Szegedy et al. ( 2 0 1 3 ) discovered that CNN - based image classification models are vulnerable to adversarial examples .", "ner": [["LSTM", "Method"], ["BERT", "Method"], ["self - attentive models", "Method"], ["neural network", "Method"], ["CNN", "Method"], ["image classification", "Task"]], "rel": [["LSTM", "Compare-With", "BERT"], ["BERT", "SubClass-Of", "self - attentive models"], ["LSTM", "SubClass-Of", "self - attentive models"], ["CNN", "Used-For", "image classification"]], "rel_plus": [["LSTM:Method", "Compare-With", "BERT:Method"], ["BERT:Method", "SubClass-Of", "self - attentive models:Method"], ["LSTM:Method", "SubClass-Of", "self - attentive models:Method"], ["CNN:Method", "Used-For", "image classification:Task"]]}
{"doc_id": "192546007", "sentence": "Previous work on attacking neural NLP models include using Fast Gradient Sign Method ( Goodfellow et al. , 2 0 1 5 ) to perturb the embedding of RNN - based classifiers ( Papernot et al. , 2 0 1 6 ; , but they have difficulties mapping from continuous embedding space to discrete input space .", "ner": [["NLP", "Task"], ["Fast Gradient Sign", "Method"], ["RNN - based classifiers", "Method"]], "rel": [["Fast Gradient Sign", "Used-For", "NLP"]], "rel_plus": [["Fast Gradient Sign:Method", "Used-For", "NLP:Task"]]}
{"doc_id": "192546007", "sentence": "Zhao et al. ( 2 0 1 8) utilize generative adversarial networks ( GAN ) to generate adversarial attacks against black - box models for applications including image classification , textual entailment , and machine translation .", "ner": [["generative adversarial networks", "Method"], ["GAN", "Method"], ["image classification", "Task"], ["textual entailment", "Task"], ["machine translation", "Task"]], "rel": [["GAN", "Synonym-Of", "generative adversarial networks"], ["generative adversarial networks", "Used-For", "image classification"], ["generative adversarial networks", "Used-For", "textual entailment"], ["generative adversarial networks", "Used-For", "machine translation"]], "rel_plus": [["GAN:Method", "Synonym-Of", "generative adversarial networks:Method"], ["generative adversarial networks:Method", "Used-For", "image classification:Task"], ["generative adversarial networks:Method", "Used-For", "textual entailment:Task"], ["generative adversarial networks:Method", "Used-For", "machine translation:Task"]]}
{"doc_id": "192546007", "sentence": "In terms of comparisons between LSTM and Transformers , Tang et al. ( 2 0 1 8) show that multiheaded attention is a critical factor in Transformer when learning long distance linguistic relations .", "ner": [["LSTM", "Method"], ["Transformers", "Method"], ["multiheaded attention", "Method"], ["Transformer", "Method"]], "rel": [["LSTM", "Compare-With", "Transformers"], ["multiheaded attention", "Part-Of", "Transformer"]], "rel_plus": [["LSTM:Method", "Compare-With", "Transformers:Method"], ["multiheaded attention:Method", "Part-Of", "Transformer:Method"]]}
{"doc_id": "192546007", "sentence": "We show that self - attentive models are more robust to adversarial attacks than recurrent networks under small input perturbations on three NLP tasks , i.e. , sentiment analysis , entailment , and translation .", "ner": [["self - attentive models", "Method"], ["recurrent networks", "Method"], ["NLP", "Task"], ["sentiment analysis", "Task"], ["entailment", "Task"], ["translation", "Task"]], "rel": [["self - attentive models", "Compare-With", "recurrent networks"], ["sentiment analysis", "SubTask-Of", "NLP"], ["entailment", "SubTask-Of", "NLP"], ["translation", "SubTask-Of", "NLP"], ["self - attentive models", "Used-For", "NLP"], ["recurrent networks", "Used-For", "NLP"], ["self - attentive models", "Used-For", "sentiment analysis"], ["recurrent networks", "Used-For", "sentiment analysis"], ["self - attentive models", "Used-For", "entailment"], ["recurrent networks", "Used-For", "entailment"], ["self - attentive models", "Used-For", "translation"], ["recurrent networks", "Used-For", "translation"]], "rel_plus": [["self - attentive models:Method", "Compare-With", "recurrent networks:Method"], ["sentiment analysis:Task", "SubTask-Of", "NLP:Task"], ["entailment:Task", "SubTask-Of", "NLP:Task"], ["translation:Task", "SubTask-Of", "NLP:Task"], ["self - attentive models:Method", "Used-For", "NLP:Task"], ["recurrent networks:Method", "Used-For", "NLP:Task"], ["self - attentive models:Method", "Used-For", "sentiment analysis:Task"], ["recurrent networks:Method", "Used-For", "sentiment analysis:Task"], ["self - attentive models:Method", "Used-For", "entailment:Task"], ["recurrent networks:Method", "Used-For", "entailment:Task"], ["self - attentive models:Method", "Used-For", "translation:Task"], ["recurrent networks:Method", "Used-For", "translation:Task"]]}
{"doc_id": "52169846", "sentence": "However , the effectiveness of such techniques has not been assessed for the hierarchical text classification ( HTC ) yet .", "ner": [["hierarchical text classification", "Task"], ["HTC", "Task"]], "rel": [["HTC", "Synonym-Of", "hierarchical text classification"]], "rel_plus": [["HTC:Task", "Synonym-Of", "hierarchical text classification:Task"]]}
{"doc_id": "52169846", "sentence": "We trained classification models with prominent machine learning algorithm implementations --- fastText , XGBoost , SVM , and Keras ' CNN --- and noticeable word embeddings generation methods --- GloVe , word 2 vec , and fastText --- with publicly available data and evaluated them with measures specifically appropriate for the hierarchical context .", "ner": [["classification", "Task"], ["machine learning", "Method"], ["fastText", "Method"], ["XGBoost", "Method"], ["SVM", "Method"], ["CNN", "Method"], ["word embeddings generation methods", "Method"], ["GloVe", "Method"], ["word 2 vec", "Method"], ["fastText", "Method"]], "rel": [["machine learning", "Used-For", "classification"], ["word embeddings generation methods", "Used-For", "classification"], ["fastText", "Used-For", "classification"], ["XGBoost", "Used-For", "classification"], ["SVM", "Used-For", "classification"], ["CNN", "Used-For", "classification"], ["GloVe", "Used-For", "classification"], ["word 2 vec", "Used-For", "classification"], ["fastText", "Used-For", "classification"], ["fastText", "SubClass-Of", "machine learning"], ["XGBoost", "SubClass-Of", "machine learning"], ["SVM", "SubClass-Of", "machine learning"], ["CNN", "SubClass-Of", "machine learning"], ["GloVe", "SubClass-Of", "word embeddings generation methods"], ["word 2 vec", "SubClass-Of", "word embeddings generation methods"], ["fastText", "SubClass-Of", "word embeddings generation methods"]], "rel_plus": [["machine learning:Method", "Used-For", "classification:Task"], ["word embeddings generation methods:Method", "Used-For", "classification:Task"], ["fastText:Method", "Used-For", "classification:Task"], ["XGBoost:Method", "Used-For", "classification:Task"], ["SVM:Method", "Used-For", "classification:Task"], ["CNN:Method", "Used-For", "classification:Task"], ["GloVe:Method", "Used-For", "classification:Task"], ["word 2 vec:Method", "Used-For", "classification:Task"], ["fastText:Method", "Used-For", "classification:Task"], ["fastText:Method", "SubClass-Of", "machine learning:Method"], ["XGBoost:Method", "SubClass-Of", "machine learning:Method"], ["SVM:Method", "SubClass-Of", "machine learning:Method"], ["CNN:Method", "SubClass-Of", "machine learning:Method"], ["GloVe:Method", "SubClass-Of", "word embeddings generation methods:Method"], ["word 2 vec:Method", "SubClass-Of", "word embeddings generation methods:Method"], ["fastText:Method", "SubClass-Of", "word embeddings generation methods:Method"]]}
{"doc_id": "52169846", "sentence": "FastText achieved an $ { } _ {LCA}F_ 1 $ of 0. 8 9 3 on a single - labeled version of the RCV 1 dataset .", "ner": [["FastText", "Method"], ["RCV 1", "Dataset"]], "rel": [["FastText", "Evaluated-With", "RCV 1"]], "rel_plus": [["FastText:Method", "Evaluated-With", "RCV 1:Dataset"]]}
{"doc_id": "52169846", "sentence": "Text classification (TC) - a.k.a . text categorization , topic classification - is the field that studies solutions for this problem , and uses a combination of knowledge areas such as Information Retrieval , Artificial Intelligence , Natural Language Processing ( NLP ) , Data Mining , Machine Learning , and Statistics .", "ner": [["Text classification", "Task"], ["(TC)", "Task"], ["text categorization", "Task"], ["topic classification", "Task"], ["Information Retrieval", "Task"], ["Artificial Intelligence", "Task"], ["Natural Language Processing", "Task"], ["NLP", "Task"], ["Data Mining", "Task"], ["Machine Learning", "Method"], ["Statistics", "Method"]], "rel": [["(TC)", "Synonym-Of", "Text classification"], ["text categorization", "SubTask-Of", "Text classification"], ["topic classification", "SubTask-Of", "Text classification"], ["NLP", "Synonym-Of", "Natural Language Processing"]], "rel_plus": [["(TC):Task", "Synonym-Of", "Text classification:Task"], ["text categorization:Task", "SubTask-Of", "Text classification:Task"], ["topic classification:Task", "SubTask-Of", "Text classification:Task"], ["NLP:Task", "Synonym-Of", "Natural Language Processing:Task"]]}
{"doc_id": "52169846", "sentence": "TC tasks usually have two or a just few classes , for example , automatic email categorization , spam detection , customer request routing , etc .", "ner": [["TC", "Task"], ["automatic email categorization", "Task"], ["spam detection", "Task"], ["customer request routing", "Task"]], "rel": [["automatic email categorization", "SubTask-Of", "TC"], ["spam detection", "SubTask-Of", "TC"], ["customer request routing", "SubTask-Of", "TC"]], "rel_plus": [["automatic email categorization:Task", "SubTask-Of", "TC:Task"], ["spam detection:Task", "SubTask-Of", "TC:Task"], ["customer request routing:Task", "SubTask-Of", "TC:Task"]]}
{"doc_id": "52169846", "sentence": "This is where the hierarchical classification ( HC ) arises : it is a particular type of structured classification problem , where the output of the classification algorithm must correspond to one or more nodes of a taxonomic hierarchy [ 3 8 ] .", "ner": [["hierarchical classification", "Task"], ["HC", "Task"], ["classification", "Task"], ["classification", "Task"]], "rel": [["HC", "Synonym-Of", "hierarchical classification"], ["hierarchical classification", "SubTask-Of", "classification"]], "rel_plus": [["HC:Task", "Synonym-Of", "hierarchical classification:Task"], ["hierarchical classification:Task", "SubTask-Of", "classification:Task"]]}
{"doc_id": "52169846", "sentence": "When applied to textual data , HC then obviously becomes hierarchical text classification ( HTC ) .", "ner": [["HC", "Task"], ["hierarchical text classification", "Task"], ["HTC", "Task"]], "rel": [["HTC", "Synonym-Of", "hierarchical text classification"]], "rel_plus": [["HTC:Task", "Synonym-Of", "hierarchical text classification:Task"]]}
{"doc_id": "52169846", "sentence": "Some examples of large hierarchical text repositories are web directories ( e.g. Best of the Web 1 , DMOZ 2 , Wikipedia topic classifications 3 ) , library and patent classification schemes ( e.g. Library of Congress Classification 4 , United States Patent Classification 5 ) , or the classification schemes used in medical applications ( e.g. Medical Subject Headings ( MeSH ) 6 ) .", "ner": [["Best of the Web", "Dataset"], ["DMOZ", "Dataset"], ["Wikipedia topic classifications", "Dataset"], ["library and patent classification", "Task"], ["Library of Congress Classification", "Dataset"], ["United States Patent Classification", "Dataset"], ["classification schemes used in medical applications", "Task"], ["Medical Subject Headings", "Dataset"], ["MeSH", "Dataset"]], "rel": [["Library of Congress Classification", "Benchmark-For", "library and patent classification"], ["United States Patent Classification", "Benchmark-For", "library and patent classification"], ["Medical Subject Headings", "Benchmark-For", "classification schemes used in medical applications"], ["MeSH", "Synonym-Of", "Medical Subject Headings"]], "rel_plus": [["Library of Congress Classification:Dataset", "Benchmark-For", "library and patent classification:Task"], ["United States Patent Classification:Dataset", "Benchmark-For", "library and patent classification:Task"], ["Medical Subject Headings:Dataset", "Benchmark-For", "classification schemes used in medical applications:Task"], ["MeSH:Dataset", "Synonym-Of", "Medical Subject Headings:Dataset"]]}
{"doc_id": "52169846", "sentence": "The HTC problem poses some particular challenges , and while many classification algorithms are likely to work well in problems with only two or a small number of well - separated categories , accurate classification over large sets of closely related classes is inherently difficult [ 2 7 ] .", "ner": [["HTC", "Task"], ["classification algorithms", "Method"]], "rel": [["classification algorithms", "Used-For", "HTC"]], "rel_plus": [["classification algorithms:Method", "Used-For", "HTC:Task"]]}
{"doc_id": "52169846", "sentence": "Moreover , in the recent years , some breakthroughs have been achieved in the machine learning and NLP fields , which have been improving the effectiveness of many TC systems .", "ner": [["machine learning", "Task"], ["NLP", "Task"], ["TC", "Task"]], "rel": [["NLP", "Used-For", "TC"], ["machine learning", "Used-For", "TC"]], "rel_plus": [["NLP:Task", "Used-For", "TC:Task"], ["machine learning:Task", "Used-For", "TC:Task"]]}
{"doc_id": "52169846", "sentence": "Such progress include two main topics : ( 1 ) efficient text representation in vector space models such as word embeddings [ 2 8 , 3 2 ] and ( 2 ) efficient classification algorithms implementations , e.g. softmax - based linear classifiers [ 1 5 ] , scalable tree boosting systems [ 3 ] , and neural network variations [ 2 3 ] .", "ner": [["word embeddings", "Method"], ["classification algorithms", "Method"], ["softmax - based linear classifiers", "Method"], ["scalable tree boosting systems", "Method"], ["neural network variations", "Method"]], "rel": [["softmax - based linear classifiers", "SubClass-Of", "classification algorithms"], ["scalable tree boosting systems", "SubClass-Of", "classification algorithms"], ["neural network variations", "SubClass-Of", "classification algorithms"]], "rel_plus": [["softmax - based linear classifiers:Method", "SubClass-Of", "classification algorithms:Method"], ["scalable tree boosting systems:Method", "SubClass-Of", "classification algorithms:Method"], ["neural network variations:Method", "SubClass-Of", "classification algorithms:Method"]]}
{"doc_id": "52169846", "sentence": "However , to the best of our knowledge , and despite the close relationship between TC and HTC , the impact of those recent advancements have not been fully explored with regards to HTC yet .", "ner": [["TC", "Task"], ["HTC", "Task"], ["HTC", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "52169846", "sentence": "The present work investigated whether and how some techniques that have recently shown to improve the results of TC tasks can be extended to have a positive impact on the HTC problem through empirical experimentation and analysis .", "ner": [["TC", "Task"], ["HTC", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "52169846", "sentence": "More specifically , we have attempted to at least partially address the following main questions : \u2022 How do recently developed text representation methods - GloVe , word 2 vec , and fastText - and efficient classification algorithms implementations - fastText , XGBoost , and Keras ' CNN - that have recently boosted the flat text classification results improve the effectiveness of HTC ? \u2022 What are the classification models effectiveness difference when comparing traditional classification measures - e.g. flat F 1 -against measures created specifically for hierarchical classification - e.g. hF 1 and lcaF 1 ?", "ner": [["text representation methods", "Method"], ["GloVe", "Method"], ["word 2 vec", "Method"], ["fastText", "Method"], ["classification algorithms", "Method"], ["fastText", "Method"], ["XGBoost", "Method"], ["CNN", "Method"], ["flat text classification", "Task"], ["HTC", "Task"], ["classification models", "Method"], ["hierarchical classification", "Task"]], "rel": [["GloVe", "SubClass-Of", "text representation methods"], ["word 2 vec", "SubClass-Of", "text representation methods"], ["fastText", "SubClass-Of", "text representation methods"], ["fastText", "SubClass-Of", "classification algorithms"], ["XGBoost", "SubClass-Of", "classification algorithms"], ["CNN", "Used-For", "flat text classification"], ["XGBoost", "Used-For", "flat text classification"], ["fastText", "Used-For", "flat text classification"], ["fastText", "Used-For", "flat text classification"], ["word 2 vec", "Used-For", "flat text classification"], ["GloVe", "Used-For", "flat text classification"]], "rel_plus": [["GloVe:Method", "SubClass-Of", "text representation methods:Method"], ["word 2 vec:Method", "SubClass-Of", "text representation methods:Method"], ["fastText:Method", "SubClass-Of", "text representation methods:Method"], ["fastText:Method", "SubClass-Of", "classification algorithms:Method"], ["XGBoost:Method", "SubClass-Of", "classification algorithms:Method"], ["CNN:Method", "Used-For", "flat text classification:Task"], ["XGBoost:Method", "Used-For", "flat text classification:Task"], ["fastText:Method", "Used-For", "flat text classification:Task"], ["fastText:Method", "Used-For", "flat text classification:Task"], ["word 2 vec:Method", "Used-For", "flat text classification:Task"], ["GloVe:Method", "Used-For", "flat text classification:Task"]]}
{"doc_id": "52169846", "sentence": "The following three sections provide descriptions of formal HTC definitions ( section 2 ) , text representation schemes ( section 3 ) , and classification algorithms ( section 4 ) that we will use for experimentation .", "ner": [["HTC", "Task"], ["text representation schemes", "Task"], ["classification", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "52169846", "sentence": "Section 5 reviews relevant advancements within the HTC research , and the impact of recent techniques onto similar classification tasks .", "ner": [["HTC", "Task"], ["classification", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "52169846", "sentence": "If the classification problem allows for classes that are not mutually exclusive , i.e. if a text piece can belong to one , more than one , or no class at all , it is called an any - of , multi - value , or multi - label classification ; on the other hand , if the classes are mutually exclusive , i.e. each document belongs to exactly one class , it as then called an one - of , single - label , multinomial , polytomous , or multi - class classification [ 2 7 ] .", "ner": [["multi - label classification", "Task"], ["multi - class classification", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "52169846", "sentence": "As hierarchies were becoming ever more popular for the organization of text documents , researchers from the Institute of Informatics and Telecommunications -NCSR Demokritos in Athens , Greece and from the Laboratoire d'Informatique de Grenoble , France organized the Large Scale HTC ( LSHTC ) Challenge .", "ner": [["Large Scale HTC", "Dataset"], ["LSHTC", "Dataset"]], "rel": [["LSHTC", "Synonym-Of", "Large Scale HTC"]], "rel_plus": [["LSHTC:Dataset", "Synonym-Of", "Large Scale HTC:Dataset"]]}
{"doc_id": "52169846", "sentence": "LSHTC became a series of competitions to assess the effectiveness of classification systems in large - scale classification in a large number of classes , which occurred in four occasions ( 2 0 0 9 , 2 0 1 1 , 2 0 1 2 , and 2 0 1 4 ) , and set some benchmarks for the task [ 3 1 ] .", "ner": [["LSHTC", "Dataset"], ["classification systems", "Method"], ["classification", "Task"]], "rel": [["classification systems", "Evaluated-With", "LSHTC"], ["classification systems", "Used-For", "classification"], ["LSHTC", "Benchmark-For", "classification"]], "rel_plus": [["classification systems:Method", "Evaluated-With", "LSHTC:Dataset"], ["classification systems:Method", "Used-For", "classification:Task"], ["LSHTC:Dataset", "Benchmark-For", "classification:Task"]]}
{"doc_id": "52169846", "sentence": "The ( 2 ) task objective determines whether the classifier must always choose a leaf node - mandatory leaf node prediction (MLNP) - or can choose any node in any level - non - mandatory leaf node prediction ( NMLNP ) [ 3 8 ] .", "ner": [["mandatory leaf node prediction", "Method"], ["(MLNP)", "Method"], ["non - mandatory leaf node prediction", "Method"], ["NMLNP", "Method"]], "rel": [["(MLNP)", "Synonym-Of", "mandatory leaf node prediction"], ["NMLNP", "Synonym-Of", "non - mandatory leaf node prediction"]], "rel_plus": [["(MLNP):Method", "Synonym-Of", "mandatory leaf node prediction:Method"], ["NMLNP:Method", "Synonym-Of", "non - mandatory leaf node prediction:Method"]]}
{"doc_id": "52169846", "sentence": "Many approaches have been proposed to exploit the hierarchical structure of the target categories during the classification processes , and Silla Jr. & Freitas [ 3 8 ] summarized them into three main clusters , as follows : 3.a flat : ignores the hierarchy by \" flattening \" it to the leaf nodes level and works any usual multi - class classification algorithm during training and testing phases , 3.b global ( a.k.a . big - bang approach ): trains a single classifier while taking the hierarchy into account and may use a top - down strategy at the testing phase 3.c local approaches : sometimes incorrectly referred as \" top - down \" approach , uses the hierarchy structure to build classifiers using local information , i.e. only the data that belongs to a particular node is considered to learn one or many classification models per each node .", "ner": [["multi - class classification", "Task"], ["classification", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "52169846", "sentence": "Silla Jr. & Freitas [ 3 8 ] subdivide the local classification approach further into three subgroups depending on the way local information is used at the training phase : 3.c.i local classifier per node ( LCN ) trains a binary classifier for each child node 3.c.ii local classifier per parent node ( LCPN ) trains a multi - class classifier for each parent node 3.c.iii local classifier per level ( LCL ) trains a multi - class classifier for the entire hierarchy level During the test phase , all systems built using this local classification approach use a top - down strategy , i.e. they predict a class at an uppermost level and then use that information to predict further under the candidates nodes from the previous step only in recursive manner until a leaf node is reached or the blocking criteria for a NMLNP is met .", "ner": [["classification", "Task"], ["local classifier per node", "Method"], ["LCN", "Method"], ["local classifier per parent node", "Method"], ["LCPN", "Method"], ["local classifier per level", "Method"], ["LCL", "Method"], ["classification approach", "Method"], ["NMLNP", "Method"]], "rel": [["LCN", "Synonym-Of", "local classifier per node"], ["LCPN", "Synonym-Of", "local classifier per parent node"], ["LCL", "Synonym-Of", "local classifier per level"]], "rel_plus": [["LCN:Method", "Synonym-Of", "local classifier per node:Method"], ["LCPN:Method", "Synonym-Of", "local classifier per parent node:Method"], ["LCL:Method", "Synonym-Of", "local classifier per level:Method"]]}
{"doc_id": "52169846", "sentence": "Their lcaF 1 measure was studied empirically on datasets used by LSHTC and BioASQ 8 HTC competitions to conclude that \" flat \" measures are indeed not adequate to evaluate HC systems .", "ner": [["LSHTC", "Dataset"], ["BioASQ", "Dataset"], ["HTC", "Task"], ["HC", "Task"]], "rel": [["BioASQ", "Benchmark-For", "HTC"], ["LSHTC", "Benchmark-For", "HTC"]], "rel_plus": [["BioASQ:Dataset", "Benchmark-For", "HTC:Task"], ["LSHTC:Dataset", "Benchmark-For", "HTC:Task"]]}
{"doc_id": "52169846", "sentence": "In most approaches , TC takes advantage of the techniques developed by the Information Retrieval community to address the document indexing problem in order to build such representation models .", "ner": [["TC", "Task"], ["Information Retrieval", "Task"]], "rel": [["Information Retrieval", "Used-For", "TC"]], "rel_plus": [["Information Retrieval:Task", "Used-For", "TC:Task"]]}
{"doc_id": "52169846", "sentence": "Some of the most popular schemes in this approach are the latent semantic indexing ( LSI ) and latent Dirichlet allocation ( LDA ) .", "ner": [["latent semantic indexing", "Method"], ["LSI", "Method"], ["latent Dirichlet allocation", "Method"], ["LDA", "Method"]], "rel": [["LSI", "Synonym-Of", "latent semantic indexing"], ["LDA", "Synonym-Of", "latent Dirichlet allocation"]], "rel_plus": [["LSI:Method", "Synonym-Of", "latent semantic indexing:Method"], ["LDA:Method", "Synonym-Of", "latent Dirichlet allocation:Method"]]}
{"doc_id": "52169846", "sentence": "LSI consists of a low - rank approximation of the document - term matrix built from it using singular value decomposition ( SVD ) , and can arguably capture some aspects of basic linguistic notions such as synonymy and polysemy;LDA is a generative probabilistic model in which each document is modeled as a finite mixture of latent topics with Dirichlet distribution [ 2 7 ] .", "ner": [["LSI", "Method"], ["singular value decomposition", "Method"], ["SVD", "Method"], ["polysemy;LDA", "Method"], ["generative probabilistic model", "Method"]], "rel": [["SVD", "Synonym-Of", "singular value decomposition"], ["polysemy;LDA", "SubClass-Of", "generative probabilistic model"]], "rel_plus": [["SVD:Method", "Synonym-Of", "singular value decomposition:Method"], ["polysemy;LDA:Method", "SubClass-Of", "generative probabilistic model:Method"]]}
{"doc_id": "52169846", "sentence": "Such methods include the continuous bag of words ( CBoW ) model , the continuous skip - gram model (CSG) - a.k.a . word 2 vec models 9 [ 2 8 ] - and the global vectors model ( GloVe ) [ 3 2 ] .", "ner": [["continuous bag of words", "Method"], ["CBoW", "Method"], ["continuous skip - gram model", "Method"], ["(CSG)", "Method"], ["word 2 vec", "Method"], ["global vectors model", "Method"], ["GloVe", "Method"]], "rel": [["CBoW", "Synonym-Of", "continuous bag of words"], ["(CSG)", "Synonym-Of", "continuous skip - gram model"], ["continuous skip - gram model", "SubClass-Of", "word 2 vec"], ["GloVe", "Synonym-Of", "global vectors model"]], "rel_plus": [["CBoW:Method", "Synonym-Of", "continuous bag of words:Method"], ["(CSG):Method", "Synonym-Of", "continuous skip - gram model:Method"], ["continuous skip - gram model:Method", "SubClass-Of", "word 2 vec:Method"], ["GloVe:Method", "Synonym-Of", "global vectors model:Method"]]}
{"doc_id": "52169846", "sentence": "While word 2 vec is based on predictive models , GloVe is based on count - based models [ 2 ] .", "ner": [["word 2 vec", "Method"], ["predictive models", "Method"], ["GloVe", "Method"], ["count - based models", "Method"]], "rel": [["word 2 vec", "SubClass-Of", "predictive models"], ["GloVe", "SubClass-Of", "count - based models"]], "rel_plus": [["word 2 vec:Method", "SubClass-Of", "predictive models:Method"], ["GloVe:Method", "SubClass-Of", "count - based models:Method"]]}
{"doc_id": "52169846", "sentence": "Word 2 vec models are trained using a shallow feedforward neural network that aims to predict a word based on the context regardless of its position ( CBoW ) or predict the words that surround a given single word ( CSG ) [ 2 8 ] .", "ner": [["Word 2 vec", "Method"], ["feedforward neural network", "Method"], ["CBoW", "Method"], ["CSG", "Method"]], "rel": [["feedforward neural network", "Used-For", "Word 2 vec"]], "rel_plus": [["feedforward neural network:Method", "Used-For", "Word 2 vec:Method"]]}
{"doc_id": "52169846", "sentence": "GloVe is a log - bilinear regression model that combines global co - occurrence matrix factorization ( somehow similar to LSI ) and local context window methods [ 3 2 ] .", "ner": [["GloVe", "Method"], ["log - bilinear regression model", "Method"], ["global co - occurrence matrix factorization", "Method"], ["LSI", "Method"], ["local context window methods", "Method"]], "rel": [["local context window methods", "Part-Of", "GloVe"], ["global co - occurrence matrix factorization", "Part-Of", "GloVe"], ["GloVe", "SubClass-Of", "log - bilinear regression model"]], "rel_plus": [["local context window methods:Method", "Part-Of", "GloVe:Method"], ["global co - occurrence matrix factorization:Method", "Part-Of", "GloVe:Method"], ["GloVe:Method", "SubClass-Of", "log - bilinear regression model:Method"]]}
{"doc_id": "52169846", "sentence": "FastText can be used as a word embedding generator as well ; as such , it is similar to the CBoW model with a few particularities , which we described with more details in subsection 4. 1 .", "ner": [["FastText", "Method"], ["word embedding generator", "Method"], ["CBoW", "Method"]], "rel": [["FastText", "SubClass-Of", "word embedding generator"], ["FastText", "Compare-With", "CBoW"]], "rel_plus": [["FastText:Method", "SubClass-Of", "word embedding generator:Method"], ["FastText:Method", "Compare-With", "CBoW:Method"]]}
{"doc_id": "52169846", "sentence": "Moreover , some recently proposed classification algorithms incorporate the principles used to compute those word vectors into the classification task itself [ 2 2 , 1 5 ] .", "ner": [["classification algorithms", "Method"], ["classification", "Task"]], "rel": [["classification algorithms", "Used-For", "classification"]], "rel_plus": [["classification algorithms:Method", "Used-For", "classification:Task"]]}
{"doc_id": "52169846", "sentence": "The following sub - subsections provide an overview about some of them , namely , linear classifier , gradient tree boosting , and convolutional neural networks ( CNN ) , for future reference in section 6 .", "ner": [["linear classifier", "Method"], ["gradient tree boosting", "Method"], ["convolutional neural networks", "Method"], ["CNN", "Method"]], "rel": [["CNN", "Synonym-Of", "convolutional neural networks"]], "rel_plus": [["CNN:Method", "Synonym-Of", "convolutional neural networks:Method"]]}
{"doc_id": "52169846", "sentence": "Rocchio 1 0 , Na\u00efve Bayes 1 1 , and SVM 1 2 are examples of linear classifiers .", "ner": [["Rocchio", "Method"], ["Na\u00efve Bayes", "Method"], ["SVM", "Method"], ["linear classifiers", "Method"]], "rel": [["Rocchio", "SubClass-Of", "linear classifiers"], ["Na\u00efve Bayes", "SubClass-Of", "linear classifiers"], ["SVM", "SubClass-Of", "linear classifiers"]], "rel_plus": [["Rocchio:Method", "SubClass-Of", "linear classifiers:Method"], ["Na\u00efve Bayes:Method", "SubClass-Of", "linear classifiers:Method"], ["SVM:Method", "SubClass-Of", "linear classifiers:Method"]]}
{"doc_id": "52169846", "sentence": "On the other hand , in unsupervised mode , fastText simply generates word embeddings for general purposes , then not taking classes into account .", "ner": [["fastText", "Method"], ["generates word embeddings", "Task"]], "rel": [["fastText", "Used-For", "generates word embeddings"]], "rel_plus": [["fastText:Method", "Used-For", "generates word embeddings:Task"]]}
{"doc_id": "52169846", "sentence": "In data mining , decision trees can be used as classification and regression models , and induced from labeled training tuples by recursively partitioning the data into smaller , purer subsets given a certain splitting criteria until either all remaining tuples belong to the same class , there are no remaining splitting attributes , or there are no remaining tuples for a given branch [ 1 1 ] . 1 0 Rocchio classification model uses centroids to calculate decision boundaries and classify a tuple according to the region it belongs to [ 2 7 ] .", "ner": [["data mining", "Task"], ["decision trees", "Method"], ["classification", "Task"], ["regression", "Task"], ["Rocchio classification model", "Method"]], "rel": [["decision trees", "Used-For", "classification"], ["decision trees", "Used-For", "regression"]], "rel_plus": [["decision trees:Method", "Used-For", "classification:Task"], ["decision trees:Method", "Used-For", "regression:Task"]]}
{"doc_id": "52169846", "sentence": "1 1 Na\u00efve Bayes is a statistical model based on Bayes ' theorem that makes predictions based on the probability that a tuple belongs to a class given its feature values . [ 1 1 ] 1 2 Support Vector Machine is a classification model that tries to find the hypothesis that minimizes the classification error based on the structural risk minimization principle by looking for the decision boundary that maximizes the distance between itself and the tuples that belong to either class . [ 1 3 ] There are many ways to improve the tree induction algorithm by , for example , using different splitting criteria ( gain ratio , information gain , Gini index , \u03c7 2 , etc . ) , pruning too specific branches , or using tree ensembles .", "ner": [["Na\u00efve Bayes", "Method"], ["statistical model", "Method"], ["Bayes ' theorem", "Method"], ["Support Vector Machine", "Method"], ["classification", "Task"], ["classification", "Task"]], "rel": [["Bayes ' theorem", "Part-Of", "Na\u00efve Bayes"], ["Na\u00efve Bayes", "SubClass-Of", "statistical model"], ["Support Vector Machine", "Used-For", "classification"]], "rel_plus": [["Bayes ' theorem:Method", "Part-Of", "Na\u00efve Bayes:Method"], ["Na\u00efve Bayes:Method", "SubClass-Of", "statistical model:Method"], ["Support Vector Machine:Method", "Used-For", "classification:Task"]]}
{"doc_id": "52169846", "sentence": "This allowed DL to produce extremely promising results for various tasks in natural language understanding , particularly topic classification [ 2 3 ] .", "ner": [["DL", "Method"], ["natural language understanding", "Task"], ["topic classification", "Task"]], "rel": [["DL", "Used-For", "natural language understanding"], ["topic classification", "SubTask-Of", "natural language understanding"], ["DL", "Used-For", "topic classification"]], "rel_plus": [["DL:Method", "Used-For", "natural language understanding:Task"], ["topic classification:Task", "SubTask-Of", "natural language understanding:Task"], ["DL:Method", "Used-For", "topic classification:Task"]]}
{"doc_id": "52169846", "sentence": "Besides the general feed - forward neural network ( FNN ) , a few specialized architectures are already used heavily in industry , including CNN and recurrent neural networks ( RNN ) , which can scale to , for example , high resolution images and long temporal sequences [ 8 ] .", "ner": [["feed - forward neural network", "Method"], ["FNN", "Method"], ["CNN", "Method"], ["recurrent neural networks", "Method"], ["RNN", "Method"]], "rel": [["FNN", "Synonym-Of", "feed - forward neural network"], ["RNN", "Synonym-Of", "recurrent neural networks"]], "rel_plus": [["FNN:Method", "Synonym-Of", "feed - forward neural network:Method"], ["RNN:Method", "Synonym-Of", "recurrent neural networks:Method"]]}
{"doc_id": "52169846", "sentence": "CNN is a specialization of FNN that employs convolution - a specialized kind of linear operation - rather then a matrix multiplication with connection weights .", "ner": [["CNN", "Method"], ["FNN", "Method"], ["convolution", "Method"], ["linear operation", "Method"]], "rel": [["convolution", "Part-Of", "CNN"], ["CNN", "SubClass-Of", "FNN"], ["convolution", "SubClass-Of", "linear operation"]], "rel_plus": [["convolution:Method", "Part-Of", "CNN:Method"], ["CNN:Method", "SubClass-Of", "FNN:Method"], ["convolution:Method", "SubClass-Of", "linear operation:Method"]]}
{"doc_id": "52169846", "sentence": "Its architecture usually consists of layers with three stages , namely , convolution , detection , and pooling .", "ner": [["convolution", "Method"], ["detection", "Task"], ["pooling", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "52169846", "sentence": "Finally , the pooling stage replaces the detection output with a summary statistic of nearby outputs , which might be used to make a final prediction or to connect to the next convolution layer [ 8 ] .", "ner": [["pooling", "Method"], ["detection", "Task"], ["convolution layer", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "52169846", "sentence": "At the same time , recent investigation on problems that bear some similarity with the HTC , such as binary TC ( sentiment analysis , spam detection , etc ) have experienced some rapid development with the usage of the representation and classification methods presented in sections 3 and 4 .", "ner": [["HTC", "Task"], ["binary TC", "Task"], ["sentiment analysis", "Task"], ["spam detection", "Task"]], "rel": [["sentiment analysis", "SubTask-Of", "binary TC"], ["spam detection", "SubTask-Of", "binary TC"]], "rel_plus": [["sentiment analysis:Task", "SubTask-Of", "binary TC:Task"], ["spam detection:Task", "SubTask-Of", "binary TC:Task"]]}
{"doc_id": "52169846", "sentence": "This section aims to present past and current research status regarding HTC and some techniques used in related areas that can have an impact on this field as well , considering the similarity that it holds with other TC problems .", "ner": [["HTC", "Task"], ["TC", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "52169846", "sentence": "Subsection 5. 1 provides an overview about recent HTC research , sections 5. 2 and 5. 3 describe the advancements that other TC problems have seen in recent years , and finally subsection 5. 4 critically analyzes all those studies and their relation to HTC .", "ner": [["HTC", "Task"], ["TC", "Task"], ["HTC", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "52169846", "sentence": "Experimental results using the Reuters - 2 2 1 7 3 1 5 showed a significantly higher then previous models due to ( 1 ) mainly the selection of features and ( 2 ) marginally the hierarchical disposition of the individual classifiers , as long as they are complex ones - i.e. the benefit was inconclusive while using Na\u00efve Bayes model , but substantial with a more elaborated algorithm from the Bayesian family .", "ner": [["Reuters - 2 2 1 7 3", "Dataset"], ["Na\u00efve Bayes", "Method"]], "rel": [["Na\u00efve Bayes", "Evaluated-With", "Reuters - 2 2 1 7 3"]], "rel_plus": [["Na\u00efve Bayes:Method", "Evaluated-With", "Reuters - 2 2 1 7 3:Dataset"]]}
{"doc_id": "52169846", "sentence": "Soon after that , Dumais & Chen [ 5 ] used SVM to build an HTC model with two levels .", "ner": [["SVM", "Method"], ["HTC", "Task"]], "rel": [["SVM", "Used-For", "HTC"]], "rel_plus": [["SVM:Method", "Used-For", "HTC:Task"]]}
{"doc_id": "52169846", "sentence": "The classification is based on threshold , and considers parent and child nodes either in a Boolean decision function ( LCN approach ) or multiplying them ( LCPN approach ) .", "ner": [["classification", "Task"], ["Boolean decision function", "Method"], ["LCN", "Method"], ["LCPN", "Method"]], "rel": [["Boolean decision function", "Used-For", "classification"], ["LCN", "Used-For", "classification"], ["LCPN", "Used-For", "classification"], ["LCN", "SubClass-Of", "Boolean decision function"]], "rel_plus": [["Boolean decision function:Method", "Used-For", "classification:Task"], ["LCN:Method", "Used-For", "classification:Task"], ["LCPN:Method", "Used-For", "classification:Task"], ["LCN:Method", "SubClass-Of", "Boolean decision function:Method"]]}
{"doc_id": "52169846", "sentence": "The authors used SVM because it was considered an effective , efficient algorithm for TC , and experimented on a web content data set with 3 5 0 K records , which was a considerable amount for the time .", "ner": [["SVM", "Method"], ["TC", "Task"]], "rel": [["SVM", "Used-For", "TC"]], "rel_plus": [["SVM:Method", "Used-For", "TC:Task"]]}
{"doc_id": "52169846", "sentence": "The researchers experimented the method on Reuters - 2 1 5 7 8 ( 9 0 classes , \u223c 1 1 K records ) , the RCV 1 1 7 ( 1 0 3 classes , \u223c 8 0 0 K records ) , and the ICCCFT 1 8 ( 7 9 classes , \u223c 1 K records ) , and considered the same F 1 function variation as Ruiz & Srinivasan [ 3 4 ] did for an effectiveness measure .", "ner": [["Reuters - 2 1 5 7 8", "Dataset"], ["RCV 1", "Dataset"], ["ICCCFT 1 8", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "52169846", "sentence": "The editions of the LSHTC Challenge brought many diverse approaches into the HTC area .", "ner": [["LSHTC", "Dataset"], ["HTC", "Task"]], "rel": [["LSHTC", "Benchmark-For", "HTC"]], "rel_plus": [["LSHTC:Dataset", "Benchmark-For", "HTC:Task"]]}
{"doc_id": "52169846", "sentence": "The report summarizes the results by saying that 1 4 Zipf 's Law is an empirical rulethat states that the collection frequency cf i of the ith most common term is proportional to 1/i [ 2 7 , flat classification approaches were competitive with the hierarchical ones , and highlights only a few that seem noteworthy , such as some models built upon k -Nearest Neighbor ( k NN ) 2 0 and Rocchio improvements .", "ner": [["flat classification approaches", "Method"], ["k -Nearest Neighbor", "Method"], ["k NN", "Method"], ["Rocchio", "Method"]], "rel": [["k NN", "Synonym-Of", "k -Nearest Neighbor"], ["flat classification approaches", "Compare-With", "k -Nearest Neighbor"], ["flat classification approaches", "Compare-With", "Rocchio"]], "rel_plus": [["k NN:Method", "Synonym-Of", "k -Nearest Neighbor:Method"], ["flat classification approaches:Method", "Compare-With", "k -Nearest Neighbor:Method"], ["flat classification approaches:Method", "Compare-With", "Rocchio:Method"]]}
{"doc_id": "52169846", "sentence": "The 4th edition winning submission , in particular , consisted of an ensemble of sparse generative models extending Multinomial Na\u00efve Bayes that combined document , label , and hierarchy level multinomials with feature pre - processing using variants of TF - IDF and BM 2 5 [ 3 3 ] .", "ner": [["sparse generative models", "Method"], ["Multinomial Na\u00efve Bayes", "Method"], ["TF - IDF", "Method"], ["BM 2 5", "Method"]], "rel": [["TF - IDF", "Used-For", "sparse generative models"], ["BM 2 5", "Used-For", "sparse generative models"], ["sparse generative models", "SubClass-Of", "Multinomial Na\u00efve Bayes"]], "rel_plus": [["TF - IDF:Method", "Used-For", "sparse generative models:Method"], ["BM 2 5:Method", "Used-For", "sparse generative models:Method"], ["sparse generative models:Method", "SubClass-Of", "Multinomial Na\u00efve Bayes:Method"]]}
{"doc_id": "52169846", "sentence": "Balikas & Amini [ 1 ] elaborated an empirical study that employed word embeddings as features for large scale TC .", "ner": [["word embeddings", "Method"], ["TC", "Task"]], "rel": [["word embeddings", "Used-For", "TC"]], "rel_plus": [["word embeddings:Method", "Used-For", "TC:Task"]]}
{"doc_id": "52169846", "sentence": "The word embeddings were generated using the skip - gram model of word 2 vec based on 1 0 million PubMed 2 1 abstracts plus 2. 5 M Wikipedia documents in four sizes : 5 0 , 1 0 0 , 2 0 0 , and 4 0 0 elements .", "ner": [["word embeddings", "Method"], ["skip - gram model", "Method"], ["word 2 vec", "Method"]], "rel": [["skip - gram model", "Used-For", "word embeddings"], ["skip - gram model", "Part-Of", "word 2 vec"]], "rel_plus": [["skip - gram model:Method", "Used-For", "word embeddings:Method"], ["skip - gram model:Method", "Part-Of", "word 2 vec:Method"]]}
{"doc_id": "52169846", "sentence": "Besides the way the word embeddings are combined , the vector size has proportional effect on the classification effectiveness as well .", "ner": [["word embeddings", "Method"], ["classification", "Task"]], "rel": [["word embeddings", "Used-For", "classification"]], "rel_plus": [["word embeddings:Method", "Used-For", "classification:Task"]]}
{"doc_id": "52169846", "sentence": "The results , however , do not reach the baseline model , which is TF - IDF based SVM model .", "ner": [["TF - IDF", "Method"], ["SVM", "Method"]], "rel": [["TF - IDF", "Part-Of", "SVM"]], "rel_plus": [["TF - IDF:Method", "Part-Of", "SVM:Method"]]}
{"doc_id": "52169846", "sentence": "Nevertheless , when combining TF - IDF to the concatenated document distributed representations , the results are better than the TF - IDF alone by a small , but statistically significant margin .", "ner": [["TF - IDF", "Method"], ["TF - IDF", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "52169846", "sentence": "While no breakthrough has occurred with the HTC task over the last years , other TC problems on the other hand have benefited from the recent great improvements on text representation using distributed vector space models .", "ner": [["HTC", "Task"], ["TC", "Task"], ["distributed vector space models", "Method"]], "rel": [["distributed vector space models", "Used-For", "HTC"], ["distributed vector space models", "Used-For", "TC"]], "rel_plus": [["distributed vector space models:Method", "Used-For", "HTC:Task"], ["distributed vector space models:Method", "Used-For", "TC:Task"]]}
{"doc_id": "52169846", "sentence": "Over the recent years , many researchers used methods based on word embeddings to improve the accuracy of classification tasks such as sentiment analysis and topic classification .", "ner": [["word embeddings", "Method"], ["classification", "Task"], ["sentiment analysis", "Task"], ["topic classification", "Task"]], "rel": [["sentiment analysis", "SubTask-Of", "classification"], ["topic classification", "SubTask-Of", "classification"], ["word embeddings", "Used-For", "classification"], ["word embeddings", "Used-For", "sentiment analysis"], ["word embeddings", "Used-For", "topic classification"]], "rel_plus": [["sentiment analysis:Task", "SubTask-Of", "classification:Task"], ["topic classification:Task", "SubTask-Of", "classification:Task"], ["word embeddings:Method", "Used-For", "classification:Task"], ["word embeddings:Method", "Used-For", "sentiment analysis:Task"], ["word embeddings:Method", "Used-For", "topic classification:Task"]]}
{"doc_id": "52169846", "sentence": "This section provides some examples from these other TC tasks that are somehow similar to HTC and took advantage from those advancements .", "ner": [["TC", "Task"], ["HTC", "Task"]], "rel": [["TC", "Compare-With", "HTC"]], "rel_plus": [["TC:Task", "Compare-With", "HTC:Task"]]}
{"doc_id": "52169846", "sentence": "They collected a dataset 2 2 with 1 0 0 , 0 0 0 movie reviews from the Internet Movie Database (IMDb) - 2 5 , 0 0 0 labeled reviews for the classification task , 2 5 , 0 0 0 for the classification test , and 5 0 , 0 0 0 of unlabeled ones as additional data to build the semantic component of their model .", "ner": [["Internet Movie Database", "Dataset"], ["(IMDb)", "Dataset"], ["classification", "Task"], ["classification", "Task"]], "rel": [["(IMDb)", "Synonym-Of", "Internet Movie Database"], ["Internet Movie Database", "Benchmark-For", "classification"], ["Internet Movie Database", "Benchmark-For", "classification"]], "rel_plus": [["(IMDb):Dataset", "Synonym-Of", "Internet Movie Database:Dataset"], ["Internet Movie Database:Dataset", "Benchmark-For", "classification:Task"], ["Internet Movie Database:Dataset", "Benchmark-For", "classification:Task"]]}
{"doc_id": "52169846", "sentence": "Such paragraph vectors can be used as features for conventional machine learning techniques , so the authors took the data collected by Maas et al. [ 2 6 ] to calculate paragraph vectors , used them as inputs to a neural network to predict the sentiment , and compared the results against other approaches that used the same 2 0 Nearest Neighbors classifiers are labor intensive classification methods that are based on comparing a given test tuple with training tuples that are similar to it [ 1 1 , p. 4 2 3 ] 2 1 https://www.ncbi.nlm.nih.gov/pubmed/ 2 2 The so - called Large Movie Review Dataset v 1 . 0 has been widely used as a benchmark dataset for binary sentiment TC and is publicly available at http://ai.stanford.edu/~amaas/data/sentiment/ 2 3 A dataset with 2, 0 0 0 balanced , processed reviews from the IMDb archive publicly available at http://www.cs.cornell.edu/people/pabo/movie - review - data/ dataset .", "ner": [["machine learning techniques", "Method"], ["Nearest Neighbors classifiers", "Method"], ["classification", "Task"], ["Large Movie Review Dataset v 1 . 0", "Dataset"], ["binary sentiment TC", "Method"], ["IMDb", "Dataset"]], "rel": [["Nearest Neighbors classifiers", "Used-For", "classification"], ["Large Movie Review Dataset v 1 . 0", "Benchmark-For", "binary sentiment TC"]], "rel_plus": [["Nearest Neighbors classifiers:Method", "Used-For", "classification:Task"], ["Large Movie Review Dataset v 1 . 0:Dataset", "Benchmark-For", "binary sentiment TC:Method"]]}
{"doc_id": "52169846", "sentence": "Still on the sentiment analysis topic , however on a slightly different scenario , Tang et al. [ 4 1 ] proposed the learning of Sentiment Specific Word Embedding ( SSWE ) by integrating the sentiment information into the loss function of the model and its application in a supervised learning framework for Twitter sentiment classification task .", "ner": [["sentiment analysis", "Task"], ["Sentiment Specific Word Embedding", "Method"], ["SSWE", "Method"], ["Twitter sentiment classification", "Task"]], "rel": [["Sentiment Specific Word Embedding", "Used-For", "sentiment analysis"], ["SSWE", "Synonym-Of", "Sentiment Specific Word Embedding"], ["Sentiment Specific Word Embedding", "Used-For", "Twitter sentiment classification"]], "rel_plus": [["Sentiment Specific Word Embedding:Method", "Used-For", "sentiment analysis:Task"], ["SSWE:Method", "Synonym-Of", "Sentiment Specific Word Embedding:Method"], ["Sentiment Specific Word Embedding:Method", "Used-For", "Twitter sentiment classification:Task"]]}
{"doc_id": "52169846", "sentence": "The authors used a partial version of a benchmark dataset used on SemEval 2 4 2 0 1 3 with 6, 2 5 1 positive/negative unbalanced records , and found that the SVM classification model built upon their SSWE has an effectiveness ( macro - F 1 ) comparable with models created from state - of - the - art , manually designed features .", "ner": [["SemEval 2 4 2 0 1 3", "Dataset"], ["SVM classification", "Method"], ["SSWE", "Method"]], "rel": [["SVM classification", "Evaluated-With", "SemEval 2 4 2 0 1 3"], ["SSWE", "Part-Of", "SVM classification"]], "rel_plus": [["SVM classification:Method", "Evaluated-With", "SemEval 2 4 2 0 1 3:Dataset"], ["SSWE:Method", "Part-Of", "SVM classification:Method"]]}
{"doc_id": "52169846", "sentence": "Furthermore , they compared their SSWE with three other word embeddings - C&W 2 5 [ 4 ] , word 2 vec [ 2 8 ] , and WVSA ( Word Vectors for Sentiment Analysis ) [ 2 6 ] -to conclude that the effectiveness of word embeddings that do not directly take advantage of the sentiment information in the text - C&W and word 2 vec - are considerably lower than the others .", "ner": [["SSWE", "Method"], ["word embeddings", "Method"], ["C&W", "Method"], ["word 2 vec", "Method"], ["WVSA", "Method"], ["Word Vectors for Sentiment Analysis", "Method"], ["word embeddings", "Method"], ["C&W", "Method"], ["word 2 vec", "Method"]], "rel": [["C&W", "SubClass-Of", "word embeddings"], ["word 2 vec", "SubClass-Of", "word embeddings"], ["WVSA", "SubClass-Of", "word embeddings"], ["SSWE", "Compare-With", "word embeddings"], ["SSWE", "Compare-With", "C&W"], ["SSWE", "Compare-With", "word 2 vec"], ["Word Vectors for Sentiment Analysis", "Synonym-Of", "WVSA"], ["SSWE", "Compare-With", "WVSA"]], "rel_plus": [["C&W:Method", "SubClass-Of", "word embeddings:Method"], ["word 2 vec:Method", "SubClass-Of", "word embeddings:Method"], ["WVSA:Method", "SubClass-Of", "word embeddings:Method"], ["SSWE:Method", "Compare-With", "word embeddings:Method"], ["SSWE:Method", "Compare-With", "C&W:Method"], ["SSWE:Method", "Compare-With", "word 2 vec:Method"], ["Word Vectors for Sentiment Analysis:Method", "Synonym-Of", "WVSA:Method"], ["SSWE:Method", "Compare-With", "WVSA:Method"]]}
{"doc_id": "52169846", "sentence": "Their study is just the beginning of a clear strategy trend in this topic : 7 out of the 1 0 top - ranked solutions for the SemEval - 2 0 1 6 Sentiment Analysis in Twitter Task incorporated either general - purpose or task - specific word embeddings in their participating systems [ 3 0 ] .", "ner": [["SemEval - 2 0 1 6 Sentiment Analysis in Twitter", "Dataset"], ["word embeddings", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "52169846", "sentence": "As an exponent of this trend , Vosoughi et al. [ 4 5 ] created a method to compute distributed representations for short texts using a long short - term memory ( LSTM ) 2 6 NN at a characterlevel .", "ner": [["compute distributed representations for short texts", "Task"], ["long short - term memory", "Method"], ["LSTM", "Method"]], "rel": [["long short - term memory", "Used-For", "compute distributed representations for short texts"], ["LSTM", "Synonym-Of", "long short - term memory"]], "rel_plus": [["long short - term memory:Method", "Used-For", "compute distributed representations for short texts:Task"], ["LSTM:Method", "Synonym-Of", "long short - term memory:Method"]]}
{"doc_id": "52169846", "sentence": "To evaluate the quality of the resulting vectors , the authors used them to perform a polarity classification on the dataset provided on SemEval - 2 0 1 5 Task 1 0 subtask B competition 2 7 .", "ner": [["polarity classification", "Task"], ["SemEval - 2 0 1 5 Task 1 0 subtask B competition 2 7", "Dataset"]], "rel": [["SemEval - 2 0 1 5 Task 1 0 subtask B competition 2 7", "Benchmark-For", "polarity classification"]], "rel_plus": [["SemEval - 2 0 1 5 Task 1 0 subtask B competition 2 7:Dataset", "Benchmark-For", "polarity classification:Task"]]}
{"doc_id": "52169846", "sentence": "On the other hand , there are also examples of the usage of word embeddings on more general , multi category TC tasks .", "ner": [["word embeddings", "Method"], ["multi category TC", "Task"]], "rel": [["word embeddings", "Used-For", "multi category TC"]], "rel_plus": [["word embeddings:Method", "Used-For", "multi category TC:Task"]]}
{"doc_id": "52169846", "sentence": "Huang et al. [ 1 2 ] propose a method to learn so called document embeddings directly in TC task , that aims to represent a document as a combination of the word embeddings of its words , which is learned using a neural network architecture .", "ner": [["document embeddings", "Method"], ["TC", "Task"], ["word embeddings", "Method"], ["neural network architecture", "Method"]], "rel": [["document embeddings", "Used-For", "TC"], ["neural network architecture", "Used-For", "word embeddings"]], "rel_plus": [["document embeddings:Method", "Used-For", "TC:Task"], ["neural network architecture:Method", "Used-For", "word embeddings:Method"]]}
{"doc_id": "52169846", "sentence": "The authors use resulting network in two ways during the classification phase : the network itself as a classification model or the weights from one of its last hidden layers as the input for an SVM classifier .", "ner": [["classification", "Task"], ["classification", "Task"], ["SVM", "Method"]], "rel": [["SVM", "Used-For", "classification"]], "rel_plus": [["SVM:Method", "Used-For", "classification:Task"]]}
{"doc_id": "52169846", "sentence": "Ma et al. [ 2 5 ] use a Gaussian process approach to model the distribution of word embeddings according to their respective themes .", "ner": [["Gaussian process", "Method"], ["word embeddings", "Method"]], "rel": [["Gaussian process", "Used-For", "word embeddings"]], "rel_plus": [["Gaussian process:Method", "Used-For", "word embeddings:Method"]]}
{"doc_id": "52169846", "sentence": "Their results show that the proposed method has a 3. 3 % accuracy gain over two other approaches that used ( 1 ) classical TF - IDF and ( 2 ) topic models estimated using latent Dirichlet allocation ( LDA ) as representation methods connected to MaxEnt classifiers , and suggest the accuracy increase occurs because \" It is clear that using word embeddings 2 4 SemEval ( Semantic Evaluation ) is an ongoing series of evaluations of computational semantic analysis systems organized by the Association for Computational Linguistics ( ACL ) https://aclweb.org/aclwiki/SemEval_Portal 2 5", "ner": [["TF - IDF", "Method"], ["topic models", "Task"], ["latent Dirichlet allocation", "Method"], ["LDA", "Method"], ["MaxEnt classifiers", "Method"], ["word embeddings", "Method"], ["SemEval", "Dataset"], ["Semantic Evaluation", "Dataset"]], "rel": [["latent Dirichlet allocation", "Used-For", "topic models"], ["LDA", "Synonym-Of", "latent Dirichlet allocation"], ["SemEval", "Synonym-Of", "Semantic Evaluation"]], "rel_plus": [["latent Dirichlet allocation:Method", "Used-For", "topic models:Task"], ["LDA:Method", "Synonym-Of", "latent Dirichlet allocation:Method"], ["SemEval:Dataset", "Synonym-Of", "Semantic Evaluation:Dataset"]]}
{"doc_id": "52169846", "sentence": "C&W is a short that Tang et al. [ 4 1 ] used for the method reportedly introduced by Collobert et al. [ 4 ] , which was not formally named by the authors . 2 6 The long short - term memory ( LSTM ) uses \" a memory cell which can maintain its state over time and non - linear gating units which regulate the information flow into and out of the cell \" [ 1 0 ] , and is usually associated with the deep learning algorithms family [ 2 3 ] . 2 7 http://alt.qcri.org/semeval 2 0 1 5 /task 1 0 / which were trained from universal dataset mitigated the problem of unseen words . \" On another approach , Kusner et al. [ 2 0 ] take advantage of the word embeddings to create a distance metric between text documents .", "ner": [["C&W", "Method"], ["long short - term memory", "Method"], ["LSTM", "Method"], ["word embeddings", "Method"]], "rel": [["LSTM", "Synonym-Of", "long short - term memory"]], "rel_plus": [["LSTM:Method", "Synonym-Of", "long short - term memory:Method"]]}
{"doc_id": "52169846", "sentence": "Their proposed metric aims to incorporate the semantic similarity between word pairs - the lowest \" traveling cost \" ( Euclidean distance ) from a word to another within the word 2 vec embedding space - into a document distance function .", "ner": [["semantic similarity", "Task"], ["Euclidean distance", "Method"], ["word 2 vec", "Method"]], "rel": [["Euclidean distance", "Used-For", "semantic similarity"]], "rel_plus": [["Euclidean distance:Method", "Used-For", "semantic similarity:Task"]]}
{"doc_id": "52169846", "sentence": "The minimum cumulative cost to move from a document to another - the so - called Word Mover 's Distance (WMD) - is then used to perform k NN document classification on eight real world document classification data sets .", "ner": [["Word Mover 's Distance", "Method"], ["(WMD)", "Method"], ["k NN document classification", "Task"], ["document classification", "Task"]], "rel": [["(WMD)", "Synonym-Of", "Word Mover 's Distance"], ["Word Mover 's Distance", "Used-For", "k NN document classification"]], "rel_plus": [["(WMD):Method", "Synonym-Of", "Word Mover 's Distance:Method"], ["Word Mover 's Distance:Method", "Used-For", "k NN document classification:Task"]]}
{"doc_id": "52169846", "sentence": "The resulting k NN classification model using WMD yields unprecedented low classification error rates when compared to other well established methods such as latent semantic indexing ( LSI ) and LDA .", "ner": [["k NN classification", "Task"], ["WMD", "Method"], ["classification", "Task"], ["latent semantic indexing", "Method"], ["LSI", "Method"], ["LDA", "Method"]], "rel": [["WMD", "Used-For", "k NN classification"], ["WMD", "Used-For", "classification"], ["LSI", "Synonym-Of", "latent semantic indexing"], ["WMD", "Compare-With", "latent semantic indexing"], ["WMD", "Compare-With", "LDA"]], "rel_plus": [["WMD:Method", "Used-For", "k NN classification:Task"], ["WMD:Method", "Used-For", "classification:Task"], ["LSI:Method", "Synonym-Of", "latent semantic indexing:Method"], ["WMD:Method", "Compare-With", "latent semantic indexing:Method"], ["WMD:Method", "Compare-With", "LDA:Method"]]}
{"doc_id": "52169846", "sentence": "Joulin et al. [ 1 5 ] built a classification model called fastText , already presented in section 4 .", "ner": [["classification", "Task"], ["fastText", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "52169846", "sentence": "For sentiment analysis comparison , they used the same eight datasets and evaluation protocol as Zhang et al. [ 5 0 ] , and found that fastText ( using 1 0 hidden units , trained 5 epochs , with bigram information ) accuracy is competitive with complex models , but needed only a fraction of time to process - the faster competitor took 2 4 minutes to train ( some took up to 7 hours ) , while the worst case for fastText took only 1 0 seconds .", "ner": [["sentiment analysis", "Task"], ["fastText", "Method"], ["fastText", "Method"]], "rel": [["fastText", "Used-For", "sentiment analysis"]], "rel_plus": [["fastText:Method", "Used-For", "sentiment analysis:Task"]]}
{"doc_id": "52169846", "sentence": "Nevertheless , at the same time that Sebastiani [ 3 7 ] reports that some NN - based models using logistic regression provide some good results , he observes that they have a relative effectiveness slightly worse than many other models known at the time , e.g. SVM .", "ner": [["NN - based models", "Method"], ["logistic regression", "Method"], ["SVM", "Method"]], "rel": [["logistic regression", "Part-Of", "NN - based models"], ["logistic regression", "Compare-With", "SVM"]], "rel_plus": [["logistic regression:Method", "Part-Of", "NN - based models:Method"], ["logistic regression:Method", "Compare-With", "SVM:Method"]]}
{"doc_id": "52169846", "sentence": "In the authors ' point of view , part - of - speech tagging ( POS ) , chunking , named entity recognition ( NER ) , and semantic role labeling ( SRL ) problems can be roughly seen as assigning labels to words , so they build an architecture capable of capturing feature vectors from words and higher level features through CNN to do that from raw text .", "ner": [["part - of - speech tagging", "Task"], ["POS", "Task"], ["chunking", "Task"], ["named entity recognition", "Task"], ["NER", "Task"], ["semantic role labeling", "Task"], ["SRL", "Task"], ["CNN", "Method"]], "rel": [["POS", "Synonym-Of", "part - of - speech tagging"], ["CNN", "Used-For", "part - of - speech tagging"], ["NER", "Synonym-Of", "named entity recognition"], ["CNN", "Used-For", "named entity recognition"], ["SRL", "Synonym-Of", "semantic role labeling"], ["CNN", "Used-For", "semantic role labeling"]], "rel_plus": [["POS:Task", "Synonym-Of", "part - of - speech tagging:Task"], ["CNN:Method", "Used-For", "part - of - speech tagging:Task"], ["NER:Task", "Synonym-Of", "named entity recognition:Task"], ["CNN:Method", "Used-For", "named entity recognition:Task"], ["SRL:Task", "Synonym-Of", "semantic role labeling:Task"], ["CNN:Method", "Used-For", "semantic role labeling:Task"]]}
{"doc_id": "52169846", "sentence": "Still on sentiment classification studies , Kim [ 1 6 ] reports on experiments with CNN trained upon distributed text representations .", "ner": [["sentiment classification", "Task"], ["CNN", "Method"]], "rel": [["CNN", "Used-For", "sentiment classification"]], "rel_plus": [["CNN:Method", "Used-For", "sentiment classification:Task"]]}
{"doc_id": "52169846", "sentence": "The experiment results show that the authors ' simple CNN with one convolution layer only performs remarkably well despite little tuning of hyperparameters , surpassing the state - of - the - art methods in 4 out of the 7 analyzed tasks/datasets .", "ner": [["CNN", "Method"], ["convolution", "Method"]], "rel": [["convolution", "Part-Of", "CNN"]], "rel_plus": [["convolution:Method", "Part-Of", "CNN:Method"]]}
{"doc_id": "52169846", "sentence": "Johnson & Zhang [ 1 4 ] also use CNN architecture , but instead of word embeddings , their model works on high - dimensional one - hot encoding vectors , i.e. each document is seen as sequence of dictionary - sized , ordered vectors with a single true bit each that corresponds to a given word .", "ner": [["CNN", "Method"], ["word embeddings", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "52169846", "sentence": "For evaluation purposes , the authors executed experiments on sentiment classification - IMDb dataset [ 2 6 ] -and topic categorization - RCV 1 dataset [ 2 4 ] , disregarding the hierarchical structure - , and compared the results against SVM - based classifiers .", "ner": [["sentiment classification", "Task"], ["IMDb", "Dataset"], ["topic categorization", "Task"], ["RCV 1", "Dataset"], ["SVM - based classifiers", "Method"]], "rel": [["IMDb", "Benchmark-For", "sentiment classification"], ["RCV 1", "Benchmark-For", "topic categorization"]], "rel_plus": [["IMDb:Dataset", "Benchmark-For", "sentiment classification:Task"], ["RCV 1:Dataset", "Benchmark-For", "topic categorization:Task"]]}
{"doc_id": "52169846", "sentence": "Since such kind of model requires very large datasets , in order to perform meaningful experiments , the authors had to build 8 of them , which had from 2 to 1 4 classes , number of records ranging from 1 2 0 , 0 0 0 to 3. 6 million , and related to two main task , namely sentiment analysis and topic classification .", "ner": [["sentiment analysis", "Task"], ["topic classification", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "52169846", "sentence": "To compare the models effectiveness , besides training their own new model , the authors also did so with models using ( 1 ) a multinomial logistic regression method built upon traditional text representation techniques and ( 2 ) a recurrent neural network using pre - trained word 2 vec word embeddings as input .", "ner": [["multinomial logistic regression", "Method"], ["word 2 vec word embeddings", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "52169846", "sentence": "At the model first level , a bi - directional recurrent structure captures the contextual information ; at its second level , a max - pooling layer finds the best features to execute the classification .", "ner": [["max - pooling", "Method"], ["classification", "Task"]], "rel": [["max - pooling", "Used-For", "classification"]], "rel_plus": [["max - pooling:Method", "Used-For", "classification:Task"]]}
{"doc_id": "52169846", "sentence": "The model input consists of word embeddings that were pre - trained using the word 2 vec skip - gram method on Wikipedia dumps .", "ner": [["word embeddings", "Method"], ["word 2 vec skip - gram method", "Method"], ["Wikipedia dumps", "Dataset"]], "rel": [["word 2 vec skip - gram method", "Part-Of", "word embeddings"], ["word embeddings", "Trained-With", "Wikipedia dumps"]], "rel_plus": [["word 2 vec skip - gram method:Method", "Part-Of", "word embeddings:Method"], ["word embeddings:Method", "Trained-With", "Wikipedia dumps:Dataset"]]}
{"doc_id": "52169846", "sentence": "This is in part because of the variations within the HTC task itself ( single - or multi - label ) , but also in part because it seems it took a long time for the community to evolve to a point when a thorough study about hierarchical classification evaluation could have been done .", "ner": [["HTC", "Task"], ["hierarchical classification", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "52169846", "sentence": "In other words , comparing the effectiveness of hierarchical classification against flat classification is not only inadequate , but also inaccurate , as the problem is different in its own nature [ 1 9 ] .", "ner": [["hierarchical classification", "Task"], ["flat classification", "Task"]], "rel": [["hierarchical classification", "Compare-With", "flat classification"]], "rel_plus": [["hierarchical classification:Task", "Compare-With", "flat classification:Task"]]}
{"doc_id": "52169846", "sentence": "Many different methods have been applied to HTC , and the most representative ones have been referred , namely Bayesian models [ 1 8 , 3 3 ] , SVM [ 5 , 9 ] , NN [ 3 4 ] , boosting methods [ 6 ] , Rocchio , and k NN [ 3 1 ] .", "ner": [["HTC", "Task"], ["Bayesian models", "Method"], ["SVM", "Method"], ["NN", "Method"], ["boosting methods", "Method"], ["Rocchio", "Method"], ["k NN", "Method"]], "rel": [["SVM", "Used-For", "HTC"], ["Bayesian models", "Used-For", "HTC"], ["NN", "Used-For", "HTC"], ["boosting methods", "Used-For", "HTC"], ["Rocchio", "Used-For", "HTC"], ["k NN", "Used-For", "HTC"]], "rel_plus": [["SVM:Method", "Used-For", "HTC:Task"], ["Bayesian models:Method", "Used-For", "HTC:Task"], ["NN:Method", "Used-For", "HTC:Task"], ["boosting methods:Method", "Used-For", "HTC:Task"], ["Rocchio:Method", "Used-For", "HTC:Task"], ["k NN:Method", "Used-For", "HTC:Task"]]}
{"doc_id": "52169846", "sentence": "Such comparison , on the other hand , makes the fact that HTC researchers seem to have been paying little or no attention to the recent classification effectiveness improvements achieved using advances in other TC tasks also surprising .", "ner": [["HTC", "Task"], ["classification", "Task"], ["TC", "Task"]], "rel": [["classification", "Used-For", "TC"]], "rel_plus": [["classification:Task", "Used-For", "TC:Task"]]}
{"doc_id": "52169846", "sentence": "For example , considering the reports on TC competitions , while Partalas et al. [ 3 1 ] do not even refer to the usage of word embeddings in LSHTC Challenges , Nakov et al. [ 3 0 ] mention that most of the best systems performing sentiment analysis on Twitter used them in some way .", "ner": [["TC", "Task"], ["word embeddings", "Method"], ["LSHTC", "Dataset"], ["sentiment analysis", "Task"]], "rel": [["word embeddings", "Used-For", "TC"], ["word embeddings", "Used-For", "LSHTC"], ["LSHTC", "Benchmark-For", "sentiment analysis"], ["word embeddings", "Used-For", "sentiment analysis"]], "rel_plus": [["word embeddings:Method", "Used-For", "TC:Task"], ["word embeddings:Method", "Used-For", "LSHTC:Dataset"], ["LSHTC:Dataset", "Benchmark-For", "sentiment analysis:Task"], ["word embeddings:Method", "Used-For", "sentiment analysis:Task"]]}
{"doc_id": "52169846", "sentence": "It seems clear that word embeddings and other vector space models improve some TC schemes considerably .", "ner": [["word embeddings", "Method"], ["TC", "Task"]], "rel": [["word embeddings", "Used-For", "TC"]], "rel_plus": [["word embeddings:Method", "Used-For", "TC:Task"]]}
{"doc_id": "52169846", "sentence": "Nevertheless , the ideas behind word embeddings are undoubtedly advantageous for TC in many different ways , from calculating a distance metric for k -NN classifier [ 2 0 ] to transforming a word embedding learner into a classifier itself [ 1 5 ] .", "ner": [["word embeddings", "Method"], ["TC", "Task"], ["k -NN classifier", "Method"], ["word embedding", "Method"]], "rel": [["word embeddings", "Used-For", "TC"], ["k -NN classifier", "Used-For", "TC"]], "rel_plus": [["word embeddings:Method", "Used-For", "TC:Task"], ["k -NN classifier:Method", "Used-For", "TC:Task"]]}
{"doc_id": "52169846", "sentence": "All in all , despite its promising results , the effect of using word embeddings in HTC remains a great unknown , as no empirical evidence has been reported on it .", "ner": [["word embeddings", "Method"], ["HTC", "Task"]], "rel": [["word embeddings", "Used-For", "HTC"]], "rel_plus": [["word embeddings:Method", "Used-For", "HTC:Task"]]}
{"doc_id": "52169846", "sentence": "Although Collobert et al. [ 4 ] show that the use of CNN provides competitive results in more than one NLP classification task , the concepts that have been preached by them and others influenced many following researchers who later on started to reconsider NN models and find promising results [ 4 1 , 1 6 ] .", "ner": [["CNN", "Method"], ["NLP classification", "Task"], ["NN models", "Method"]], "rel": [["CNN", "Used-For", "NLP classification"], ["NN models", "Used-For", "NLP classification"]], "rel_plus": [["CNN:Method", "Used-For", "NLP classification:Task"], ["NN models:Method", "Used-For", "NLP classification:Task"]]}
{"doc_id": "52169846", "sentence": "Their most important contribution to the present investigation is the indication that adequate word embeddings combined with appropriate classification NN 's provide promising results .", "ner": [["word embeddings", "Method"], ["classification NN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "52169846", "sentence": "Nevertheless , this assumption lacks of empirical evidence when it comes to the hierarchical text context , as no report has been found specifically about it and it is doubtful that simple TC problems are adequate to evaluate deep neural networks representations , which in theory have power expected to provide much better final classification results [ 1 5 ] .", "ner": [["TC", "Task"], ["deep neural networks", "Method"], ["classification", "Task"]], "rel": [["deep neural networks", "Used-For", "TC"], ["deep neural networks", "Used-For", "classification"]], "rel_plus": [["deep neural networks:Method", "Used-For", "TC:Task"], ["deep neural networks:Method", "Used-For", "classification:Task"]]}
{"doc_id": "52169846", "sentence": "We have designed and implemented experiments to analyze the effectiveness of combining word embeddings as the text representation layer with modern classification algorithms applied to the HTC problem .", "ner": [["modern classification algorithms", "Method"], ["HTC", "Task"]], "rel": [["modern classification algorithms", "Used-For", "HTC"]], "rel_plus": [["modern classification algorithms:Method", "Used-For", "HTC:Task"]]}
{"doc_id": "52169846", "sentence": "Since no dataset is widely used in the HTC research , choosing an appropriate dataset to perform HTC experiments becomes a somewhat hard task .", "ner": [["HTC", "Task"], ["HTC", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "52169846", "sentence": "Therefore , we have mainly considered corpora provided by Reuters ( Reuters - 2 2 1 7 3 and RCV 1 ) and PubMed ( from BioASQ ) .", "ner": [["Reuters - 2 2 1 7 3", "Dataset"], ["RCV 1", "Dataset"], ["BioASQ", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "52169846", "sentence": "We consider this as a downside for two reasons : ( 1 ) the results obtained within such a specific corpus might not generalize to other HTC tasks and ( 2 ) GloVe and word 2 vec pretrained word vectors are general , which makes them inadequate for such a specific classification task 3 0 .", "ner": [["HTC", "Task"], ["GloVe", "Method"], ["word 2 vec", "Method"], ["classification", "Task"]], "rel": [["GloVe", "Used-For", "classification"], ["word 2 vec", "Used-For", "classification"]], "rel_plus": [["GloVe:Method", "Used-For", "classification:Task"], ["word 2 vec:Method", "Used-For", "classification:Task"]]}
{"doc_id": "52169846", "sentence": "This approach is based on the assumption that 2 9 Those two papers combined had more than 3, 5 0 0 citations as counted by Google Scholar by Jan 2 0 1 7 3 0 BioASQ has recently provided word embeddings pre - trained using word 2 vec , which could be potentially useful for this analysis .", "ner": [["BioASQ", "Dataset"], ["word embeddings", "Method"], ["word 2 vec", "Method"]], "rel": [["word embeddings", "Trained-With", "BioASQ"], ["word 2 vec", "Part-Of", "word embeddings"]], "rel_plus": [["word embeddings:Method", "Trained-With", "BioASQ:Dataset"], ["word 2 vec:Method", "Part-Of", "word embeddings:Method"]]}
{"doc_id": "52169846", "sentence": "Nevertheless , as we intend to compare GloVe and word 2 vec results , having word embeddings trained with a single method only is not enough for our purposes . 3 1 http://trec.nist.gov/data/reuters/reuters.html < ? xml version=\" 1 . 0 \" encoding=\"iso - 8 8 5 9 - 1 \" ? > ... < headline > Tylan stock jumps ; weighs sale of company.</headline > ... < text><p > The stock of Tylan General Inc. jumped Tuesday after the maker of process - management equipment said it is exploring the sale of the company and added that it has already received some inquiries from potential buyers.</p>( ... )</text > ...", "ner": [["GloVe", "Method"], ["word 2 vec", "Method"], ["word embeddings", "Method"]], "rel": [["GloVe", "Compare-With", "word 2 vec"]], "rel_plus": [["GloVe:Method", "Compare-With", "word 2 vec:Method"]]}
{"doc_id": "52169846", "sentence": "< metadata > ... < codes class=\"bip : topics: 1 . 0 \" > < code code=\"C 1 5 \" > < /code > < code code=\"C 1 5 2 \" > < /code > < code code=\"C 1 8 \" > < /code > < code code=\"C 1 8 1 \" > < /code > < code code=\"CCAT \" > < /code > < /codes > ... < /metadata > Figure 1 : An excerpt from a random XML file of the RCV 1 dataset . [ 2 4 ] the least common label is the one that more specifically identifies the document .", "ner": [["RCV 1", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "52169846", "sentence": "In order to compare a flat classification against hierarchical LCPN approach , we have created two data groups : ( 1 ) a train/test split by applying a holdout method [ 1 1 ] to randomly reserve 1 0 % of all RCV 1 tuples for test purposes and ( 2 ) a so - called \" hierarchical split \" by recursively stratifying subsets of the train subset based on the parent nodes .", "ner": [["flat classification", "Method"], ["hierarchical LCPN approach", "Method"], ["RCV 1", "Dataset"]], "rel": [["flat classification", "Compare-With", "hierarchical LCPN approach"]], "rel_plus": [["flat classification:Method", "Compare-With", "hierarchical LCPN approach:Method"]]}
{"doc_id": "52169846", "sentence": "Initial experiments with these datasets indicated the already expected incorrect classifications that occur with NMLNP in deeper hierarchy levels due to the models ' inability to stop the classification before reaching a leaf node or recovering from it [ 3 8 ] .", "ner": [["NMLNP", "Method"], ["classification", "Task"]], "rel": [["NMLNP", "Used-For", "classification"]], "rel_plus": [["NMLNP:Method", "Used-For", "classification:Task"]]}
{"doc_id": "52169846", "sentence": "Besides using RCV 1 and its hierarchy as the main elements for experimentation , we also employed general - purpose pre - trained word embeddings .", "ner": [["RCV 1", "Dataset"], ["word embeddings", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "52169846", "sentence": "The group responsible for word 2 vec published a dataset with around 3 million word vectors with 3 0 0 elements in length that were trained on about 1 0 0 billion words read from Google News dataset 3 2 .", "ner": [["word 2 vec", "Method"], ["Google News dataset", "Dataset"]], "rel": [["word 2 vec", "Trained-With", "Google News dataset"]], "rel_plus": [["word 2 vec:Method", "Trained-With", "Google News dataset:Dataset"]]}
{"doc_id": "52169846", "sentence": "The authors of GloVe also published pre - trained versions of word vectors ; for these experiments , we will use a   Out of the many possible classification models mentioned in section 5 , we concentrated our efforts on three of them , namely fastText , XGBoost , and CNN .", "ner": [["GloVe", "Method"], ["classification", "Task"], ["fastText", "Method"], ["XGBoost", "Method"], ["CNN", "Method"]], "rel": [["fastText", "Used-For", "classification"], ["XGBoost", "Used-For", "classification"], ["CNN", "Used-For", "classification"]], "rel_plus": [["fastText:Method", "Used-For", "classification:Task"], ["XGBoost:Method", "Used-For", "classification:Task"], ["CNN:Method", "Used-For", "classification:Task"]]}
{"doc_id": "52169846", "sentence": "These classifiers were trained using the two aforementioned pre - trained word embeddings as well as word embeddings obtained from the fastText supervised algorithm - more details in the upcoming paragraphs - and the following hierarchical strategies : \u2022 Flat : A single model was trained as in a general multi - class task with 1 0 3 classes while ignoring the hierarchical information - see subsection 2. 1 for more details . \u2022 LCPN + VC : An individual classification model was created for each one of the 2 2 datasets generated by the \" hierarchical split \" described in subsection 6. 1 .", "ner": [["word embeddings", "Method"], ["word embeddings", "Method"], ["fastText", "Method"], ["LCPN + VC", "Method"], ["classification", "Task"]], "rel": [["fastText", "Part-Of", "word embeddings"], ["LCPN + VC", "Used-For", "classification"]], "rel_plus": [["fastText:Method", "Part-Of", "word embeddings:Method"], ["LCPN + VC:Method", "Used-For", "classification:Task"]]}
{"doc_id": "52169846", "sentence": "The following list describes the learning algorithms we used with details about parameters , specific data preprocessing , and variations : \u2022 fastText : We used this algorithm in two ways - as a classification learner and as a word embedding generator .", "ner": [["fastText", "Method"], ["classification learner", "Method"], ["word embedding generator", "Method"]], "rel": [["fastText", "Used-For", "classification learner"], ["fastText", "Used-For", "word embedding generator"]], "rel_plus": [["fastText:Method", "Used-For", "classification learner:Method"], ["fastText:Method", "Used-For", "word embedding generator:Method"]]}
{"doc_id": "52169846", "sentence": "We explored word embeddings with 5 different vector sizes to investigate how expanding the numerical distribution affects the final classification effectiveness in a flat strategy .", "ner": [["word embeddings", "Method"], ["classification", "Task"]], "rel": [["word embeddings", "Used-For", "classification"]], "rel_plus": [["word embeddings:Method", "Used-For", "classification:Task"]]}
{"doc_id": "52169846", "sentence": "During that step , we learned that fastText was able to improve its classification effectives as we increased the vector size up to a certain point only .", "ner": [["fastText", "Method"], ["classification", "Task"]], "rel": [["fastText", "Used-For", "classification"]], "rel_plus": [["fastText:Method", "Used-For", "classification:Task"]]}
{"doc_id": "52169846", "sentence": "Other training parameters : softmax loss function , bigrams , learning rate 0. 1 , and 5 epochs . \u2022 XGBoost : In order to accommodate the distributed text representation in a format suitable to this algorithm , we decided to combine word embeddings to compose a document representation from the corresponding word embeddings average .", "ner": [["softmax loss function", "Method"], ["bigrams", "Method"], ["XGBoost", "Method"], ["word embeddings", "Method"], ["word embeddings", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "52169846", "sentence": "Besides these architectures that use word embeddings , we implemented one with TF - IDF representation , created from an all - lowercase , stemmed , puntuation - and stopword - free version of the RVC 1 dataset .", "ner": [["word embeddings", "Method"], ["TF - IDF representation", "Method"], ["RVC 1", "Dataset"]], "rel": [["word embeddings", "Part-Of", "TF - IDF representation"], ["TF - IDF representation", "Trained-With", "RVC 1"]], "rel_plus": [["word embeddings:Method", "Part-Of", "TF - IDF representation:Method"], ["TF - IDF representation:Method", "Trained-With", "RVC 1:Dataset"]]}
{"doc_id": "52169846", "sentence": "For all experiments , we set the XGBoost algorithm to use a softmax objective and run it for 3 0 rounds , which seemed to be enough to converge to minimum loss .", "ner": [["XGBoost", "Method"], ["softmax", "Method"]], "rel": [["softmax", "Part-Of", "XGBoost"]], "rel_plus": [["softmax:Method", "Part-Of", "XGBoost:Method"]]}
{"doc_id": "52169846", "sentence": "We then used the Keras API 3 5 to built a neural network with the following architecture : a frozen embedding layer that uses the fastText vectors , two convolution layers with rectified linear units ( ReLU with kernel size 5 and 1 2 8 output filters ) and max pooling ( pool size 5 ) each [ 8 ] , a densely - connected layer with ReLU activation and finally another densely - connected layer with softmax function .", "ner": [["frozen embedding layer", "Method"], ["fastText", "Method"], ["convolution", "Method"], ["rectified linear units", "Method"], ["ReLU", "Method"], ["max pooling", "Method"], ["densely - connected layer", "Method"], ["ReLU activation", "Method"], ["densely - connected layer", "Method"], ["softmax", "Method"]], "rel": [["fastText", "Part-Of", "frozen embedding layer"], ["convolution", "Part-Of", "frozen embedding layer"], ["max pooling", "Part-Of", "frozen embedding layer"], ["densely - connected layer", "Part-Of", "frozen embedding layer"], ["densely - connected layer", "Part-Of", "frozen embedding layer"], ["rectified linear units", "Part-Of", "convolution"], ["ReLU", "Synonym-Of", "rectified linear units"], ["ReLU activation", "Part-Of", "densely - connected layer"], ["softmax", "Part-Of", "densely - connected layer"]], "rel_plus": [["fastText:Method", "Part-Of", "frozen embedding layer:Method"], ["convolution:Method", "Part-Of", "frozen embedding layer:Method"], ["max pooling:Method", "Part-Of", "frozen embedding layer:Method"], ["densely - connected layer:Method", "Part-Of", "frozen embedding layer:Method"], ["densely - connected layer:Method", "Part-Of", "frozen embedding layer:Method"], ["rectified linear units:Method", "Part-Of", "convolution:Method"], ["ReLU:Method", "Synonym-Of", "rectified linear units:Method"], ["ReLU activation:Method", "Part-Of", "densely - connected layer:Method"], ["softmax:Method", "Part-Of", "densely - connected layer:Method"]]}
{"doc_id": "52169846", "sentence": "Other training parameters : 1 0 epochs , batch size 1 2 8 [ 2 1 ] . \u2022 SVM : We used the same document representation resulting from the word embeddings combination created for XGBoost as input attributes for an SVM classifier [ 1 3 ] .", "ner": [["SVM", "Method"], ["word embeddings", "Method"], ["XGBoost", "Method"], ["SVM", "Method"]], "rel": [["word embeddings", "Part-Of", "SVM"], ["XGBoost", "Part-Of", "SVM"]], "rel_plus": [["word embeddings:Method", "Part-Of", "SVM:Method"], ["XGBoost:Method", "Part-Of", "SVM:Method"]]}
{"doc_id": "52169846", "sentence": "Moreover , despite a somewhat high Pearson correlation of 0. 7 5 6 between flat F 1 and lcaF 1 , the former is potentially misleading to assess the model effectiveness - for example , it indicates XGBoost with word 2 vec 3 4 Our pre - processed RCV 1 training subset had an average document length of 2 6 1 . 5 7 words with 9 0 as the mode .", "ner": [["XGBoost", "Method"], ["word 2 vec", "Method"], ["RCV 1", "Dataset"]], "rel": [["word 2 vec", "Part-Of", "XGBoost"]], "rel_plus": [["word 2 vec:Method", "Part-Of", "XGBoost:Method"]]}
{"doc_id": "52169846", "sentence": "We suspect this comes from the fact that , when used in supervised mode , fastText uses the class label as learning objective , which results in word embeddings that specifically reflect the concepts behind the classes distribution .", "ner": [["fastText", "Method"], ["word embeddings", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "52169846", "sentence": "This hypothesis is supported by the fact that fastText word embeddings created in supervised mode with a relatively small amount of data yielded effectiveness on par with pre - trained word embeddings generated from much more data in an unsupervised manner .", "ner": [["fastText word embeddings", "Method"], ["word embeddings", "Method"]], "rel": [["fastText word embeddings", "Compare-With", "word embeddings"]], "rel_plus": [["fastText word embeddings:Method", "Compare-With", "word embeddings:Method"]]}
{"doc_id": "52169846", "sentence": "Considering the XGBoost algorithm , flat models using any pre - trained word embeddings are surpassed by SVM counterpart .", "ner": [["XGBoost", "Method"], ["word embeddings", "Method"], ["SVM", "Method"]], "rel": [["word embeddings", "Part-Of", "XGBoost"], ["XGBoost", "Compare-With", "SVM"]], "rel_plus": [["word embeddings:Method", "Part-Of", "XGBoost:Method"], ["XGBoost:Method", "Compare-With", "SVM:Method"]]}
{"doc_id": "52169846", "sentence": "Moreover , these XGBoost flat models had worse results than the traditional TF - IDF representation .", "ner": [["XGBoost", "Method"], ["TF - IDF", "Method"]], "rel": [["XGBoost", "Compare-With", "TF - IDF"]], "rel_plus": [["XGBoost:Method", "Compare-With", "TF - IDF:Method"]]}
{"doc_id": "52169846", "sentence": "Nevertheless , XGBoost achieved reasonable classification results and exceeded the baseline classifier in all contexts where it could take advantage of LCPN+VC strategy .", "ner": [["XGBoost", "Method"], ["classification", "Task"], ["LCPN+VC", "Method"]], "rel": [["XGBoost", "Used-For", "classification"]], "rel_plus": [["XGBoost:Method", "Used-For", "classification:Task"]]}
{"doc_id": "52169846", "sentence": "Training this CNN - which is a rather simple implementation considering the complexity that top - notch CNN can reach - took around 6 hours in our computer ( Intel R Core TM i 5 - 4 3 0 0 U CPU at 1. 9 GHz with 8GB RAM ) , while typical training time for fastText and XGBoost ranged from 3 to 8 minutes for the former and 0. 2 to 2. 2 hours for the latter .", "ner": [["CNN", "Method"], ["CNN", "Method"], ["fastText", "Method"], ["XGBoost", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "52169846", "sentence": "Our results finally suggest that word embedding systems depend on the word embeddings quality to some extent , as we noticed that word 2 vec embeddings have a slight advantage over GloVe 's .", "ner": [["word embedding", "Method"], ["word embeddings", "Method"], ["word 2 vec", "Method"], ["GloVe", "Method"]], "rel": [["word 2 vec", "Compare-With", "GloVe"]], "rel_plus": [["word 2 vec:Method", "Compare-With", "GloVe:Method"]]}
{"doc_id": "52169846", "sentence": "At the same time , both SVM and XGBoost achieved fair results when using supervised fastText word embeddings generated from a relatively small amount of data .", "ner": [["SVM", "Method"], ["XGBoost", "Method"], ["fastText word embeddings", "Method"]], "rel": [["fastText word embeddings", "Part-Of", "SVM"], ["fastText word embeddings", "Part-Of", "XGBoost"], ["SVM", "Compare-With", "XGBoost"]], "rel_plus": [["fastText word embeddings:Method", "Part-Of", "SVM:Method"], ["fastText word embeddings:Method", "Part-Of", "XGBoost:Method"], ["SVM:Method", "Compare-With", "XGBoost:Method"]]}
{"doc_id": "52169846", "sentence": "This contributes to the understanding word embeddings specifically generated during the classification task , even when short , are well appropriate representations for this problem .", "ner": [["word embeddings", "Method"], ["classification", "Task"]], "rel": [["word embeddings", "Used-For", "classification"]], "rel_plus": [["word embeddings:Method", "Used-For", "classification:Task"]]}
{"doc_id": "52169846", "sentence": "Throughout this work , we have analyzed the application of distributed text representations combined with modern classification algorithms implementations to the HTC task .", "ner": [["classification", "Task"], ["HTC", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "52169846", "sentence": "After an observant literature research and careful examination of related works , we identified three noticeable word embeddings generation methods - GloVe , word 2 vec , and fastText - and three prominent classification models - fastText , XGBoost , and CNN - that recently improved the results for the typical text classification and could potentially provide similar advancements for the hierarchical specialization .", "ner": [["word embeddings", "Method"], ["GloVe", "Method"], ["word 2 vec", "Method"], ["fastText", "Method"], ["classification models", "Method"], ["fastText", "Method"], ["XGBoost", "Method"], ["CNN", "Method"], ["text classification", "Task"]], "rel": [["GloVe", "SubClass-Of", "word embeddings"], ["word 2 vec", "SubClass-Of", "word embeddings"], ["fastText", "SubClass-Of", "word embeddings"], ["CNN", "SubClass-Of", "classification models"], ["XGBoost", "SubClass-Of", "classification models"], ["fastText", "SubClass-Of", "classification models"], ["classification models", "Used-For", "text classification"], ["word embeddings", "Used-For", "text classification"], ["GloVe", "Used-For", "text classification"], ["word 2 vec", "Used-For", "text classification"], ["fastText", "Used-For", "text classification"], ["fastText", "Used-For", "text classification"], ["XGBoost", "Used-For", "text classification"], ["CNN", "Used-For", "text classification"]], "rel_plus": [["GloVe:Method", "SubClass-Of", "word embeddings:Method"], ["word 2 vec:Method", "SubClass-Of", "word embeddings:Method"], ["fastText:Method", "SubClass-Of", "word embeddings:Method"], ["CNN:Method", "SubClass-Of", "classification models:Method"], ["XGBoost:Method", "SubClass-Of", "classification models:Method"], ["fastText:Method", "SubClass-Of", "classification models:Method"], ["classification models:Method", "Used-For", "text classification:Task"], ["word embeddings:Method", "Used-For", "text classification:Task"], ["GloVe:Method", "Used-For", "text classification:Task"], ["word 2 vec:Method", "Used-For", "text classification:Task"], ["fastText:Method", "Used-For", "text classification:Task"], ["fastText:Method", "Used-For", "text classification:Task"], ["XGBoost:Method", "Used-For", "text classification:Task"], ["CNN:Method", "Used-For", "text classification:Task"]]}
{"doc_id": "52169846", "sentence": "We also noticed we could exploit the hierarchical structure to build classification models using LCPN strategy and virtual categories .", "ner": [["classification", "Task"], ["LCPN", "Method"]], "rel": [["LCPN", "Used-For", "classification"]], "rel_plus": [["LCPN:Method", "Used-For", "classification:Task"]]}
{"doc_id": "52169846", "sentence": "In order to assess the feasibility and effectiveness of these representations , models , and strategies to the HTC task , we performed experiments using the RCV 1 dataset .", "ner": [["HTC", "Task"], ["RCV 1", "Dataset"]], "rel": [["RCV 1", "Benchmark-For", "HTC"]], "rel_plus": [["RCV 1:Dataset", "Benchmark-For", "HTC:Task"]]}
{"doc_id": "52169846", "sentence": "The results indicate that classification models created using a hierarchical strategy ( LCPN with \" virtual category \" ) surpasses the flat approach in all experimented equivalent comparisons .", "ner": [["classification", "Task"], ["LCPN", "Method"]], "rel": [["LCPN", "Used-For", "classification"]], "rel_plus": [["LCPN:Method", "Used-For", "classification:Task"]]}
{"doc_id": "52169846", "sentence": "FastText was the outstanding method as a classifier and provided very good results as a word embedding generator , despite the relatively small amount of data provided for this second task .", "ner": [["FastText", "Method"], ["word embedding generator", "Method"]], "rel": [["FastText", "SubClass-Of", "word embedding generator"]], "rel_plus": [["FastText:Method", "SubClass-Of", "word embedding generator:Method"]]}
{"doc_id": "52169846", "sentence": "These findings support the increasing understanding that combining task - specific word embeddings provides the best results for text classification [ 1 6 , 4 1 ] , to which now we include its hierarchical specialization .", "ner": [["word embeddings", "Method"], ["text classification", "Task"]], "rel": [["word embeddings", "Used-For", "text classification"]], "rel_plus": [["word embeddings:Method", "Used-For", "text classification:Task"]]}
{"doc_id": "52169846", "sentence": "We plan to apply these methods to the PubMed data to check how such an approach extents to the medical text context - in particular the usage of fastText .", "ner": [["PubMed data", "Dataset"], ["fastText", "Method"]], "rel": [["fastText", "Evaluated-With", "PubMed data"]], "rel_plus": [["fastText:Method", "Evaluated-With", "PubMed data:Dataset"]]}
{"doc_id": "52169846", "sentence": "As BioASQ provides pre - trained word embeddings generated with word 2 vec using a considerable amount of medical texts , comparing them with those that fastText creates in supervised mode should provides us with evidence for a more general understanding on how their quality affects the final classification results .", "ner": [["BioASQ", "Dataset"], ["word embeddings", "Method"], ["word 2 vec", "Method"], ["fastText", "Method"], ["classification", "Task"]], "rel": [["word embeddings", "Trained-With", "BioASQ"], ["word 2 vec", "Part-Of", "word embeddings"], ["word embeddings", "Compare-With", "fastText"], ["word embeddings", "Used-For", "classification"], ["fastText", "Used-For", "classification"]], "rel_plus": [["word embeddings:Method", "Trained-With", "BioASQ:Dataset"], ["word 2 vec:Method", "Part-Of", "word embeddings:Method"], ["word embeddings:Method", "Compare-With", "fastText:Method"], ["word embeddings:Method", "Used-For", "classification:Task"], ["fastText:Method", "Used-For", "classification:Task"]]}
{"doc_id": "52169846", "sentence": "We would also like to study the behavior of fastText when applied to task with more classes , such as the BioASQ , to check whether word embeddings with more than 3 0 or 4 0 elements would still allow for classification effectiveness improvement .", "ner": [["fastText", "Method"], ["BioASQ", "Dataset"], ["word embeddings", "Method"], ["classification", "Task"]], "rel": [["fastText", "Evaluated-With", "BioASQ"], ["word embeddings", "Used-For", "classification"]], "rel_plus": [["fastText:Method", "Evaluated-With", "BioASQ:Dataset"], ["word embeddings:Method", "Used-For", "classification:Task"]]}
{"doc_id": "52169846", "sentence": "Additionally , the exploration of other text representation extensions of word 2 vec , like paragraph 2 vec and doc 2 vec , could complement this investigation .", "ner": [["word 2 vec", "Method"], ["paragraph 2 vec", "Method"], ["doc 2 vec", "Method"]], "rel": [["paragraph 2 vec", "SubClass-Of", "word 2 vec"], ["doc 2 vec", "SubClass-Of", "word 2 vec"]], "rel_plus": [["paragraph 2 vec:Method", "SubClass-Of", "word 2 vec:Method"], ["doc 2 vec:Method", "SubClass-Of", "word 2 vec:Method"]]}
{"doc_id": "52169846", "sentence": "However , both XGBoost and CNN ( through the Keras API ) allow for the loss function customization .", "ner": [["XGBoost", "Method"], ["CNN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "52169846", "sentence": "We believe that finding a differentiable function that approximates either hF 1 or lcaF 1 and using it as the loss function rather then the softmax could finally bring together state - of - the - art algorithms with hierarchical information to create a method that implements a global HTC approach .", "ner": [["softmax", "Method"], ["HTC", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "210157153", "sentence": "In this paper , we propose Matrix - LSTM , a grid of Long Short - Term Memory ( LSTM ) cells that efficiently process events and learn end - to - end task - dependent event - surfaces .", "ner": [["Matrix - LSTM", "Method"], ["Long Short - Term Memory", "Method"], ["LSTM", "Method"]], "rel": [["LSTM", "Synonym-Of", "Long Short - Term Memory"], ["Matrix - LSTM", "SubClass-Of", "Long Short - Term Memory"]], "rel_plus": [["LSTM:Method", "Synonym-Of", "Long Short - Term Memory:Method"], ["Matrix - LSTM:Method", "SubClass-Of", "Long Short - Term Memory:Method"]]}
{"doc_id": "210157153", "sentence": "Compared to existing reconstruction approaches , our learned event - surface shows good flexibility and expressiveness on optical flow estimation on the MVSEC benchmark and it improves the state - of - the - art of event - based object classification on the N - Cars dataset .", "ner": [["reconstruction", "Task"], ["optical flow estimation", "Task"], ["MVSEC", "Dataset"], ["object classification", "Task"], ["N - Cars", "Dataset"]], "rel": [["MVSEC", "Benchmark-For", "optical flow estimation"], ["N - Cars", "Benchmark-For", "object classification"]], "rel_plus": [["MVSEC:Dataset", "Benchmark-For", "optical flow estimation:Task"], ["N - Cars:Dataset", "Benchmark-For", "object classification:Task"]]}
{"doc_id": "210157153", "sentence": "Event - cameras only provide a timed sequence of changes that is not directly compatible with computer vision LSTM Neural Network End - to - End Training Figure 1 .", "ner": [["computer vision LSTM Neural Network", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210157153", "sentence": "The Matrix - LSTM end - to - end differentiable surface applied on an N - MNIST [ 3 1 ] sample .", "ner": [["Matrix - LSTM", "Method"], ["N - MNIST", "Dataset"]], "rel": [["Matrix - LSTM", "Evaluated-With", "N - MNIST"]], "rel_plus": [["Matrix - LSTM:Method", "Evaluated-With", "N - MNIST:Dataset"]]}
{"doc_id": "210157153", "sentence": "In this paper , we focus on this recent trend in event - based processing , and propose to apply a Long Short - Term Memory ( LSTM ) network [ 1 6 ] as a convolutional filter over the 2D stream of events to accumulate pixel information through time and extract pixel values for building 2D event representations .", "ner": [["Long Short - Term Memory", "Method"], ["LSTM", "Method"]], "rel": [["LSTM", "Synonym-Of", "Long Short - Term Memory"]], "rel_plus": [["LSTM:Method", "Synonym-Of", "Long Short - Term Memory:Method"]]}
{"doc_id": "210157153", "sentence": "The framework is end - to - end differentiable , it can be used as input of any existing frame - based state - of - the - art architecture and jointly trained to extract the best representation from the events . \u2022 Replacing input representations with a Matrix - LSTM layer in existing architectures , we show that it improves the state - of - the - art on event - based object classification on N - CARS [ 4 0 ] by 3. 3 % and performs better than hand - crafted features on N - Caltech 1 0 1 [ 3 1 ] .", "ner": [["Matrix - LSTM", "Method"], ["object classification", "Task"], ["N - CARS", "Dataset"], ["N - Caltech 1 0 1", "Dataset"]], "rel": [["Matrix - LSTM", "Used-For", "object classification"], ["N - CARS", "Benchmark-For", "object classification"], ["Matrix - LSTM", "Evaluated-With", "N - CARS"], ["Matrix - LSTM", "Evaluated-With", "N - Caltech 1 0 1"]], "rel_plus": [["Matrix - LSTM:Method", "Used-For", "object classification:Task"], ["N - CARS:Dataset", "Benchmark-For", "object classification:Task"], ["Matrix - LSTM:Method", "Evaluated-With", "N - CARS:Dataset"], ["Matrix - LSTM:Method", "Evaluated-With", "N - Caltech 1 0 1:Dataset"]]}
{"doc_id": "210157153", "sentence": "Finally , it improves optical flow estimation on the MVSEC benchmark [ 4 9 ] up to 3 0 . 7 6 % over handcrafted features [ 4 9 ] and up to 2 3 . 0 7 % over end - to - end differentiable ones [ 1 3 ] . \u2022 We developed custom CUDA kernels to efficiently aggregate events by position and perform a convolutionlike operation on the stream of events using an LSTM as a convolutional filter .", "ner": [["optical flow estimation", "Task"], ["MVSEC", "Dataset"], ["LSTM", "Method"], ["convolutional filter", "Method"]], "rel": [["MVSEC", "Benchmark-For", "optical flow estimation"], ["LSTM", "SubClass-Of", "convolutional filter"]], "rel_plus": [["MVSEC:Dataset", "Benchmark-For", "optical flow estimation:Task"], ["LSTM:Method", "SubClass-Of", "convolutional filter:Method"]]}
{"doc_id": "210157153", "sentence": "To this end , convolutional neural networks ( CNNs ) are by far the most widespread method in frame - based architectures for image classification [ 1 5 , 1 9 , 4 2 ] , object detection [ 1 4 , 2 4 , 3 5 ] , semantic segmentation [ 7 , 2 5 , 4 6 ] , and many others .", "ner": [["convolutional neural networks", "Method"], ["CNNs", "Method"], ["image classification", "Task"], ["object detection", "Task"], ["semantic segmentation", "Task"]], "rel": [["CNNs", "Synonym-Of", "convolutional neural networks"], ["convolutional neural networks", "Used-For", "image classification"], ["convolutional neural networks", "Used-For", "object detection"], ["convolutional neural networks", "Used-For", "semantic segmentation"]], "rel_plus": [["CNNs:Method", "Synonym-Of", "convolutional neural networks:Method"], ["convolutional neural networks:Method", "Used-For", "image classification:Task"], ["convolutional neural networks:Method", "Used-For", "object detection:Task"], ["convolutional neural networks:Method", "Used-For", "semantic segmentation:Task"]]}
{"doc_id": "210157153", "sentence": "Initially , neural systems designed to perform spike - based computation , such as Spiking Neural Networks ( SNNs ) [ 2 6 ] , have been applied to event - based processing in several tasks , e.g. , edge detection [ 4 4 , 2 8 ] , object classification [ 2 1 , 1 0 ] and hand - gestures recognition [ 4 ] .", "ner": [["Spiking Neural Networks", "Method"], ["SNNs", "Method"], ["edge detection", "Task"], ["object classification", "Task"], ["hand - gestures recognition", "Task"]], "rel": [["SNNs", "Synonym-Of", "Spiking Neural Networks"], ["Spiking Neural Networks", "Used-For", "edge detection"], ["Spiking Neural Networks", "Used-For", "object classification"], ["Spiking Neural Networks", "Used-For", "hand - gestures recognition"]], "rel_plus": [["SNNs:Method", "Synonym-Of", "Spiking Neural Networks:Method"], ["Spiking Neural Networks:Method", "Used-For", "edge detection:Task"], ["Spiking Neural Networks:Method", "Used-For", "object classification:Task"], ["Spiking Neural Networks:Method", "Used-For", "hand - gestures recognition:Task"]]}
{"doc_id": "210157153", "sentence": "Recent works [ 3 6 ] tried to overcome these limitations by first training a frame - based conventional neural network , and then convert its weights into SNNs parameters , managing to deal with complex structures such as GoogLeNet Inception - V 3 [ 4 2 ] .", "ner": [["frame - based conventional neural network", "Method"], ["SNNs", "Method"], ["GoogLeNet Inception - V 3", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210157153", "sentence": "Crucially , the accumulation procedure employed in HATS is hand - crafted , while our work is end - toend trainable thanks to a grid of LSTM cells [ 1 6 ] that learn the accumulation step .", "ner": [["HATS", "Method"], ["LSTM", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210157153", "sentence": "In [ 4 8 ] , the authors propose the EV - FlowNet network for optical flow estimation together with a new time - surface variant .", "ner": [["EV - FlowNet", "Method"], ["optical flow estimation", "Task"]], "rel": [["EV - FlowNet", "Used-For", "optical flow estimation"]], "rel_plus": [["EV - FlowNet:Method", "Used-For", "optical flow estimation:Task"]]}
{"doc_id": "210157153", "sentence": "A multi - layer perceptron ( MLP ) is used to implement a trilinear filter that produces a voxelgrid of temporal features .", "ner": [["multi - layer perceptron", "Method"], ["MLP", "Method"]], "rel": [["MLP", "Synonym-Of", "multi - layer perceptron"]], "rel_plus": [["MLP:Method", "Synonym-Of", "multi - layer perceptron:Method"]]}
{"doc_id": "210157153", "sentence": "Among these , [ 3 0 ] uses a variant of the LSTM network , called Phas - edLSTM , to learn the precise timings of events .", "ner": [["LSTM", "Method"], ["Phas - edLSTM", "Method"]], "rel": [["Phas - edLSTM", "SubClass-Of", "LSTM"]], "rel_plus": [["Phas - edLSTM:Method", "SubClass-Of", "LSTM:Method"]]}
{"doc_id": "210157153", "sentence": "In contrast , in this paper we use the LSTM as a convolutional filter , thus , we obtain a translation - invariant module that integrates local temporal features independently while retaining spatial structures .", "ner": [["LSTM", "Method"], ["convolutional filter", "Method"], ["translation - invariant module", "Method"]], "rel": [["LSTM", "SubClass-Of", "convolutional filter"]], "rel_plus": [["LSTM:Method", "SubClass-Of", "convolutional filter:Method"]]}
{"doc_id": "210157153", "sentence": "Although LSTMs should be able to retain memory over very long periods , we found that discretizing time into intervals helps the Matrix - LSTM layer in maintaining event information , especially in tasks requiring rich and precise time information such as optical flow estimation ( see Section 4. 2 ) .", "ner": [["LSTMs", "Method"], ["Matrix - LSTM", "Method"], ["optical flow estimation", "Task"]], "rel": [["Matrix - LSTM", "Used-For", "optical flow estimation"]], "rel_plus": [["Matrix - LSTM:Method", "Used-For", "optical flow estimation:Task"]]}
{"doc_id": "210157153", "sentence": "Inspired by the convolution operation defined on images , we designed the Matrix - LSTM layer to enjoy translation invariance by performing a local mapping of events into features .", "ner": [["convolution operation", "Method"], ["Matrix - LSTM", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210157153", "sentence": "By sharing the parameters across the LSTM cells , we can consider Matrix - LSTM as a convolution operator over the 2D - grid streams of events applied with a 1 \u00d7 1 receptive field .", "ner": [["LSTM", "Method"], ["Matrix - LSTM", "Method"], ["convolution operator", "Method"], ["1 \u00d7 1 receptive field", "Method"]], "rel": [["1 \u00d7 1 receptive field", "Part-Of", "Matrix - LSTM"], ["Matrix - LSTM", "SubClass-Of", "convolution operator"]], "rel_plus": [["1 \u00d7 1 receptive field:Method", "Part-Of", "Matrix - LSTM:Method"], ["Matrix - LSTM:Method", "SubClass-Of", "convolution operator:Method"]]}
{"doc_id": "210157153", "sentence": "To showcase the flexibility of the proposed mechanism , we tested it on two different tasks : a high - level task , i.e. , object classification ( see Section A ) and a low - level one , i.e. , optical flow estimation ( see Section 4. 2 ) where the network is required to extract effective temporal features .", "ner": [["object classification", "Task"], ["optical flow estimation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "210157153", "sentence": "We evaluated the model on the classification task using two publicly available event - based collections , namely the N - Cars [ 4 0 ] and the N - Caltech 1 0 1 [ 3 1 ] datasets .", "ner": [["classification", "Task"], ["N - Cars", "Dataset"], ["N - Caltech 1 0 1", "Dataset"]], "rel": [["N - Cars", "Benchmark-For", "classification"], ["N - Caltech 1 0 1", "Benchmark-For", "classification"]], "rel_plus": [["N - Cars:Dataset", "Benchmark-For", "classification:Task"], ["N - Caltech 1 0 1:Dataset", "Benchmark-For", "classification:Task"]]}
{"doc_id": "210157153", "sentence": "The N - Caltech 1 0 1 collection is an event - based conversion of the popular Caltech - 1 0 1 [ 2 2 ] dataset obtained by moving an event - based camera in front of a still monitor showing one of the original RGB images .", "ner": [["N - Caltech 1 0 1", "Dataset"], ["Caltech - 1 0 1", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "210157153", "sentence": "We used two network configurations to test Matrix - LSTM on both datasets , namely the classifier used in Events - to - Video [ 3 4 ] , and the one used to evaluate the EST [ 1 3 ] reconstruction .", "ner": [["Matrix - LSTM", "Method"], ["Events - to - Video", "Method"], ["EST", "Method"], ["reconstruction", "Task"]], "rel": [["Matrix - LSTM", "Part-Of", "Events - to - Video"], ["EST", "Used-For", "reconstruction"]], "rel_plus": [["Matrix - LSTM:Method", "Part-Of", "Events - to - Video:Method"], ["EST:Method", "Used-For", "reconstruction:Task"]]}
{"doc_id": "210157153", "sentence": "Both are based on ResNet [ 1 5 ] backbones and pre - trained on ImageNet [ 9 ] .", "ner": [["ResNet", "Method"], ["ImageNet", "Dataset"]], "rel": [["ResNet", "Trained-With", "ImageNet"]], "rel_plus": [["ResNet:Method", "Trained-With", "ImageNet:Dataset"]]}
{"doc_id": "210157153", "sentence": "Events - to - Video [ 3 4 ] uses a ResNet 1 8 configuration maintaining the first 3 channels convolution ( since reconstructed images are RGB ) while adding an extra fully - connected layer to account for the different number of classes in both N - Calthec 1 0 1 and N - Cars ( we refer to this configuration as ResNet - Ev 2 Vid ) .", "ner": [["Events - to - Video", "Method"], ["ResNet 1 8", "Method"], ["3 channels convolution", "Method"], ["fully - connected layer", "Method"], ["N - Calthec 1 0 1", "Dataset"], ["N - Cars", "Dataset"], ["ResNet - Ev 2 Vid", "Method"]], "rel": [["ResNet 1 8", "Part-Of", "Events - to - Video"], ["fully - connected layer", "Part-Of", "Events - to - Video"], ["3 channels convolution", "Part-Of", "ResNet 1 8"], ["Events - to - Video", "Evaluated-With", "N - Calthec 1 0 1"], ["ResNet - Ev 2 Vid", "Evaluated-With", "N - Calthec 1 0 1"], ["Events - to - Video", "Evaluated-With", "N - Cars"], ["ResNet - Ev 2 Vid", "Evaluated-With", "N - Cars"], ["ResNet 1 8", "Part-Of", "ResNet - Ev 2 Vid"], ["3 channels convolution", "Part-Of", "ResNet - Ev 2 Vid"], ["fully - connected layer", "Part-Of", "ResNet - Ev 2 Vid"]], "rel_plus": [["ResNet 1 8:Method", "Part-Of", "Events - to - Video:Method"], ["fully - connected layer:Method", "Part-Of", "Events - to - Video:Method"], ["3 channels convolution:Method", "Part-Of", "ResNet 1 8:Method"], ["Events - to - Video:Method", "Evaluated-With", "N - Calthec 1 0 1:Dataset"], ["ResNet - Ev 2 Vid:Method", "Evaluated-With", "N - Calthec 1 0 1:Dataset"], ["Events - to - Video:Method", "Evaluated-With", "N - Cars:Dataset"], ["ResNet - Ev 2 Vid:Method", "Evaluated-With", "N - Cars:Dataset"], ["ResNet 1 8:Method", "Part-Of", "ResNet - Ev 2 Vid:Method"], ["3 channels convolution:Method", "Part-Of", "ResNet - Ev 2 Vid:Method"], ["fully - connected layer:Method", "Part-Of", "ResNet - Ev 2 Vid:Method"]]}
{"doc_id": "210157153", "sentence": "EST [ 1 3 ] instead uses a ResNet 3 4 backbone and replaces both the first and last layers respectively , with a convolution matching the input features , and a fullyconnected layer with the proper number of neurons ( we refer to this configuration as ResNet - EST ) .", "ner": [["EST", "Method"], ["ResNet 3 4", "Method"], ["fullyconnected layer", "Method"], ["ResNet - EST", "Method"]], "rel": [["fullyconnected layer", "Part-Of", "ResNet - EST"], ["EST", "Part-Of", "ResNet - EST"]], "rel_plus": [["fullyconnected layer:Method", "Part-Of", "ResNet - EST:Method"], ["EST:Method", "Part-Of", "ResNet - EST:Method"]]}
{"doc_id": "210157153", "sentence": "We perform early stopping on a validation set in all experiments , using 2 0 % of the training on N - Cars and using the splits provided by the EST official code repository [ 1 2 ] for N - Caltech 1 0 1 .", "ner": [["early stopping", "Method"], ["N - Cars", "Dataset"], ["EST", "Method"], ["N - Caltech 1 0 1", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "210157153", "sentence": "On N - Caltech 1 0 1 , instead , we use a batch size of 1 6 while decaying the learning rate by a factor of 0. 8 after each epoch when testing on ResNet - Ev 2 Vid , and a batch size of 1 0 0 with no decay with the ResNet - EST setup .", "ner": [["N - Caltech 1 0 1", "Dataset"], ["ResNet - Ev 2 Vid", "Method"], ["ResNet - EST", "Method"]], "rel": [["ResNet - Ev 2 Vid", "Evaluated-With", "N - Caltech 1 0 1"], ["ResNet - EST", "Evaluated-With", "N - Caltech 1 0 1"]], "rel_plus": [["ResNet - Ev 2 Vid:Method", "Evaluated-With", "N - Caltech 1 0 1:Dataset"], ["ResNet - EST:Method", "Evaluated-With", "N - Caltech 1 0 1:Dataset"]]}
{"doc_id": "210157153", "sentence": "The empirical evaluation is organized as it follow for both ResNet - Ev 2 Vid and ResNet - EST configurations .", "ner": [["ResNet - Ev 2 Vid", "Method"], ["ResNet - EST", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210157153", "sentence": "We al - ways perform hyper - parameters search using a ResNet 1 8 backbone on N - Cars , since it is faster to train and thus it allows us to explore a larger combination of parameters .", "ner": [["ResNet 1 8", "Method"], ["N - Cars", "Dataset"]], "rel": [["ResNet 1 8", "Evaluated-With", "N - Cars"]], "rel_plus": [["ResNet 1 8:Method", "Evaluated-With", "N - Cars:Dataset"]]}
{"doc_id": "210157153", "sentence": "We then select the best configuration to train the remaining architectures , namely , ResNet 3 4 on N - Cars and both variants on N - Caltech 1 0 1 .", "ner": [["ResNet 3 4", "Method"], ["N - Cars", "Dataset"], ["N - Caltech 1 0 1", "Dataset"]], "rel": [["ResNet 3 4", "Evaluated-With", "N - Cars"], ["ResNet 3 4", "Evaluated-With", "N - Caltech 1 0 1"]], "rel_plus": [["ResNet 3 4:Method", "Evaluated-With", "N - Cars:Dataset"], ["ResNet 3 4:Method", "Evaluated-With", "N - Caltech 1 0 1:Dataset"]]}
{"doc_id": "210157153", "sentence": "Matrix - LSTM + ResNet - Ev 2 Vid .", "ner": [["Matrix - LSTM", "Method"], ["ResNet - Ev 2 Vid", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210157153", "sentence": "We start out with the ResNet - Ev 2 Vid baseline ( setting up the Matrix - LSTM to output 3 channels ) by identifying the optimal time feature to provide as input to the LSTM , as reported in Table 1 .", "ner": [["ResNet - Ev 2 Vid", "Method"], ["Matrix - LSTM", "Method"], ["LSTM", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210157153", "sentence": "In Table 1 we also show how the network performance changes when the frame normalization used while training the ResNet backbone on ImageNet is applied to the Matrix - LSTM output .", "ner": [["ResNet", "Method"], ["ImageNet", "Dataset"], ["Matrix - LSTM", "Method"]], "rel": [["ResNet", "Trained-With", "ImageNet"], ["ResNet", "Part-Of", "Matrix - LSTM"]], "rel_plus": [["ResNet:Method", "Trained-With", "ImageNet:Dataset"], ["ResNet:Method", "Part-Of", "Matrix - LSTM:Method"]]}
{"doc_id": "210157153", "sentence": "Matrix - LSTM + ResNet - EST .", "ner": [["Matrix - LSTM", "Method"], ["ResNet - EST", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210157153", "sentence": "We continue the experiments on N - Cars by considering the ResNet - EST configuration as the baseline , where we explore the effects of using bins , i.e. , intervals , on the quality of the Matrix - LSTM reconstruction .", "ner": [["N - Cars", "Dataset"], ["ResNet - EST", "Method"], ["Matrix - LSTM", "Method"], ["reconstruction", "Task"]], "rel": [["ResNet - EST", "Evaluated-With", "N - Cars"], ["Matrix - LSTM", "Used-For", "reconstruction"]], "rel_plus": [["ResNet - EST:Method", "Evaluated-With", "N - Cars:Dataset"], ["Matrix - LSTM:Method", "Used-For", "reconstruction:Task"]]}
{"doc_id": "210157153", "sentence": "Having found the bin setup performing the best ( namely B = 2 , 4 ) , and since our Matrix - LSTM module can produce a variable number of output features , we perform the last set of experiments to select the Matrix - LSTM hidden size ( a parameter that also controls the number of output channels ) .", "ner": [["Matrix - LSTM", "Method"], ["Matrix - LSTM", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210157153", "sentence": "Results of the top performing configurations for both ResNet - Ev 2 Vid and ResNet - EST variants on both N - Cars and N - Caltech 1 0 1 are reported in Table 5 .", "ner": [["ResNet - Ev 2 Vid", "Method"], ["ResNet - EST", "Method"], ["N - Cars", "Dataset"], ["N - Caltech 1 0 1", "Dataset"]], "rel": [["ResNet - Ev 2 Vid", "Evaluated-With", "N - Cars"], ["ResNet - EST", "Evaluated-With", "N - Cars"], ["ResNet - Ev 2 Vid", "Evaluated-With", "N - Caltech 1 0 1"], ["ResNet - EST", "Evaluated-With", "N - Caltech 1 0 1"]], "rel_plus": [["ResNet - Ev 2 Vid:Method", "Evaluated-With", "N - Cars:Dataset"], ["ResNet - EST:Method", "Evaluated-With", "N - Cars:Dataset"], ["ResNet - Ev 2 Vid:Method", "Evaluated-With", "N - Caltech 1 0 1:Dataset"], ["ResNet - EST:Method", "Evaluated-With", "N - Caltech 1 0 1:Dataset"]]}
{"doc_id": "210157153", "sentence": "We use relative delay with ResNet - Ev 2 Vid and global ts + local ts with ResNet - EST .", "ner": [["ResNet - Ev 2 Vid", "Method"], ["global ts + local ts", "Method"], ["ResNet - EST", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210157153", "sentence": "Indeed , using the ResNet 3 4 - Ev 2 Vid setup , our solution sets a new state - of - the - art on N - Cars , even surpassing the Events - to - Video model that was trained to extract realistic reconstructions , and that could , therefore , take full advantage of the ResNet pre - training .", "ner": [["ResNet 3 4 - Ev 2 Vid", "Method"], ["N - Cars", "Dataset"], ["Events - to - Video", "Method"], ["ResNet", "Method"]], "rel": [["ResNet 3 4 - Ev 2 Vid", "Evaluated-With", "N - Cars"], ["ResNet 3 4 - Ev 2 Vid", "Compare-With", "Events - to - Video"], ["ResNet", "Part-Of", "Events - to - Video"]], "rel_plus": [["ResNet 3 4 - Ev 2 Vid:Method", "Evaluated-With", "N - Cars:Dataset"], ["ResNet 3 4 - Ev 2 Vid:Method", "Compare-With", "Events - to - Video:Method"], ["ResNet:Method", "Part-Of", "Events - to - Video:Method"]]}
{"doc_id": "210157153", "sentence": "On the ResNet - EST configuration , the model performs consistently better on N - Cars , while slightly worse on N - Caltech 1 0 1 on most configurations .", "ner": [["ResNet - EST", "Method"], ["N - Cars", "Dataset"], ["N - Caltech 1 0 1", "Dataset"]], "rel": [["ResNet - EST", "Evaluated-With", "N - Cars"], ["ResNet - EST", "Evaluated-With", "N - Caltech 1 0 1"]], "rel_plus": [["ResNet - EST:Method", "Evaluated-With", "N - Cars:Dataset"], ["ResNet - EST:Method", "Evaluated-With", "N - Caltech 1 0 1:Dataset"]]}
{"doc_id": "210157153", "sentence": "However , we remark that search for the best configuration was indeed performed on N - Cars , while a hyper - parameter search directly performed on N - Caltech 1 0 1 would have probably lead to better results .", "ner": [["N - Cars", "Dataset"], ["N - Caltech 1 0 1", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "210157153", "sentence": "Fusing event - data with lidar , IMU , motion capture and GPS sources , MVSEC is the first eventbased dataset to provide a solid benchmark for event camera in real urban conditions .", "ner": [["MVSEC", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "210157153", "sentence": "We replaced this input representation with the Matrix - LSTM reconstruction mechanism , setting the output channels to 4 as well .", "ner": [["Matrix - LSTM", "Method"], ["reconstruction", "Task"]], "rel": [["Matrix - LSTM", "Used-For", "reconstruction"]], "rel_plus": [["Matrix - LSTM:Method", "Used-For", "reconstruction:Task"]]}
{"doc_id": "210157153", "sentence": "To perform a fair comparison with Ev - FlowNet , that uses a 4 - channels eventsurface , we fix the hidden state of our LSTM to 4 , and use the same training hyper - parameters as well .", "ner": [["Ev - FlowNet", "Method"], ["LSTM", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210157153", "sentence": "We noticed that EV - FlowNet is quite unstable at higher learning rates , while Matrix - LSTM could benefit from larger rates , so we multiply its learning rate , i.e. , the Matrix - LSTM gradients , by a factor of 1 0 during training .", "ner": [["EV - FlowNet", "Method"], ["Matrix - LSTM", "Method"], ["Matrix - LSTM", "Method"]], "rel": [["EV - FlowNet", "Compare-With", "Matrix - LSTM"]], "rel_plus": [["EV - FlowNet:Method", "Compare-With", "Matrix - LSTM:Method"]]}
{"doc_id": "210157153", "sentence": "We compared the time performance of Matrix - LSTM with other event representations following EST [ 1 3 ] and HATS [ 4 0 ] evaluation procedure .", "ner": [["Matrix - LSTM", "Method"], ["EST", "Method"], ["HATS", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210157153", "sentence": "The average forward pass time of Matrix - LSTM has been computed on both ResNet - Ev 2 Vid and ResNet - EST configurations using a GeForce GTX 1 0 8 0 Ti GPU .", "ner": [["Matrix - LSTM", "Method"], ["ResNet - Ev 2 Vid", "Method"], ["ResNet - EST", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210157153", "sentence": "Our surface achieves similar time performance than both HATS and EST , performing only \u2248 2 ms slower than EST on the same configuration ( 9 bins and 2 channels ) .", "ner": [["HATS", "Method"], ["EST", "Method"], ["EST", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210157153", "sentence": "However , while EST can exploit parallel batch computation of events within the same sample , since each event feature is processed independently , Matrix - LSTM relies on sequential computation to reconstruct the surface .", "ner": [["EST", "Method"], ["Matrix - LSTM", "Method"]], "rel": [["EST", "Compare-With", "Matrix - LSTM"]], "rel_plus": [["EST:Method", "Compare-With", "Matrix - LSTM:Method"]]}
{"doc_id": "210157153", "sentence": "In Figure 4 we analyze the accuracy - vs - latency tradeoff on the N - Cars dataset , as proposed in [ 4 0 ] , using the ResNet 1 8 - Ev 2 Vid configuration .", "ner": [["N - Cars", "Dataset"], ["ResNet 1 8 - Ev 2 Vid", "Method"]], "rel": [["ResNet 1 8 - Ev 2 Vid", "Evaluated-With", "N - Cars"]], "rel_plus": [["ResNet 1 8 - Ev 2 Vid:Method", "Evaluated-With", "N - Cars:Dataset"]]}
{"doc_id": "210157153", "sentence": "Gabor - SNN [ 4 0 ] HOTS [ 2 0 ] Matrix - LSTM delay Matrix - LSTM delay + aug .", "ner": [["Gabor - SNN", "Method"], ["HOTS", "Method"], ["Matrix - LSTM", "Method"], ["Matrix - LSTM", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210157153", "sentence": "By modeling the reconstruction with a spatially shared LSTM we obtained a fully differentiable procedure that can be trained end - to - end to extract the event representation that best fits the task at hand .", "ner": [["reconstruction", "Task"], ["LSTM", "Method"]], "rel": [["LSTM", "Used-For", "reconstruction"]], "rel_plus": [["LSTM:Method", "Used-For", "reconstruction:Task"]]}
{"doc_id": "210157153", "sentence": "We proposed an efficient implementation of the method that exploits parallel batchwise computation and demonstrates the effectiveness of the Matrix - LSTM layer on multiple tasks , improving the stateof - the - art of object classification on N - Cars by 3. 3 % and the performance on optical flow prediction on MVSEC by up to 2 3 . 0 7 % over previous differentiable techniques [ 1 3 ] .", "ner": [["Matrix - LSTM", "Method"], ["object classification", "Task"], ["N - Cars", "Dataset"], ["optical flow prediction", "Task"], ["MVSEC", "Dataset"]], "rel": [["Matrix - LSTM", "Used-For", "object classification"], ["N - Cars", "Benchmark-For", "object classification"], ["Matrix - LSTM", "Evaluated-With", "N - Cars"], ["Matrix - LSTM", "Used-For", "optical flow prediction"], ["MVSEC", "Evaluated-With", "optical flow prediction"], ["Matrix - LSTM", "Evaluated-With", "MVSEC"]], "rel_plus": [["Matrix - LSTM:Method", "Used-For", "object classification:Task"], ["N - Cars:Dataset", "Benchmark-For", "object classification:Task"], ["Matrix - LSTM:Method", "Evaluated-With", "N - Cars:Dataset"], ["Matrix - LSTM:Method", "Used-For", "optical flow prediction:Task"], ["MVSEC:Dataset", "Evaluated-With", "optical flow prediction:Task"], ["Matrix - LSTM:Method", "Evaluated-With", "MVSEC:Dataset"]]}
{"doc_id": "210157153", "sentence": "As a future line of research , we plan to explore the use of Matrix - LSTM for more complex tasks such as gray - scale frame reconstruction [ 3 4 ] , ego - motion and depth estimation [ 5 0 , 4 5 ] .", "ner": [["Matrix - LSTM", "Method"], ["gray - scale frame reconstruction", "Task"], ["ego - motion", "Task"], ["depth estimation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "210157153", "sentence": "In Table 8 we show how different choices of receptive field size impact classification accuracy on the N - Cars [ 4 0 ] datasets using the ResNet 1 8 - Ev 2 Vid configuration with relative delay encoding .", "ner": [["classification", "Task"], ["N - Cars", "Dataset"], ["ResNet 1 8 - Ev 2 Vid", "Method"]], "rel": [["ResNet 1 8 - Ev 2 Vid", "Used-For", "classification"], ["N - Cars", "Benchmark-For", "classification"], ["ResNet 1 8 - Ev 2 Vid", "Evaluated-With", "N - Cars"]], "rel_plus": [["ResNet 1 8 - Ev 2 Vid:Method", "Used-For", "classification:Task"], ["N - Cars:Dataset", "Benchmark-For", "classification:Task"], ["ResNet 1 8 - Ev 2 Vid:Method", "Evaluated-With", "N - Cars:Dataset"]]}
{"doc_id": "210157153", "sentence": "Event surfaces produced by the Matrix - LSTM layer are indeed more blurry and this may prevent the subsequent ResNet backbone from extracting effective features .", "ner": [["Matrix - LSTM", "Method"], ["ResNet", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210157153", "sentence": "In both configurations , finally , increasing the batch size reduces the mean processing time .   We performed optical flow experiments starting from the publicly available Ev - FlowNet codebase [ 4 7 ] and replacing the original hand - crafted features with the proposed Matrix - LSTM layer .", "ner": [["Ev - FlowNet", "Method"], ["Matrix - LSTM", "Method"]], "rel": [["Matrix - LSTM", "Part-Of", "Ev - FlowNet"]], "rel_plus": [["Matrix - LSTM:Method", "Part-Of", "Ev - FlowNet:Method"]]}
{"doc_id": "210157153", "sentence": "Despite this discrepancy , which prevents the Matrix - LSTM performance on dt= 4 settings to be directly compared with the results reported on the Ev - FlowNet paper , we can still evaluate the benefits of our surface on larger flow magnitudes .", "ner": [["Matrix - LSTM", "Method"], ["Ev - FlowNet", "Method"]], "rel": [["Matrix - LSTM", "Compare-With", "Ev - FlowNet"]], "rel_plus": [["Matrix - LSTM:Method", "Compare-With", "Ev - FlowNet:Method"]]}
{"doc_id": "210157153", "sentence": "Using our Ev - FlowNet results as baseline , we show that Matrix - LSTM is able to improve the optical flow quality even on the dt= 4 setting , highlighting the capability of the layer to adapt to different sequence lengths and movement conditions .", "ner": [["Ev - FlowNet", "Method"], ["Matrix - LSTM", "Method"]], "rel": [["Ev - FlowNet", "Compare-With", "Matrix - LSTM"]], "rel_plus": [["Ev - FlowNet:Method", "Compare-With", "Matrix - LSTM:Method"]]}
{"doc_id": "210157153", "sentence": "We propose to visualize the Matrix - LSTM surface as an RGB image by using the ResNet 1 8 - Ev 2 Vid configuration and interpreting the 3 output channels as RGB color .", "ner": [["Matrix - LSTM", "Method"], ["ResNet 1 8 - Ev 2 Vid", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210157153", "sentence": "A video of such visualization showing the incremental frame reconstruction on N - Caltech 1 0 1 samples is provided at this url : https://drive.google.com/open ? id= 1 KzhKKwJGXvMnhbg 1 l 6 gEArgYote 7 WW 8 V .", "ner": [["reconstruction", "Task"], ["N - Caltech 1 0 1", "Dataset"]], "rel": [["N - Caltech 1 0 1", "Benchmark-For", "reconstruction"]], "rel_plus": [["N - Caltech 1 0 1:Dataset", "Benchmark-For", "reconstruction:Task"]]}
{"doc_id": "210157153", "sentence": "The same visualization technique is also used to display N - Cars Matrix - LSTM surfaces in Table 8 .", "ner": [["N - Cars", "Dataset"], ["Matrix - LSTM", "Method"]], "rel": [["Matrix - LSTM", "Evaluated-With", "N - Cars"]], "rel_plus": [["Matrix - LSTM:Method", "Evaluated-With", "N - Cars:Dataset"]]}
{"doc_id": "3627225", "sentence": "By learning to learn in the concept space rather than in the complicated instance space , deep meta - learning can substantially improve vanilla meta - learning , which is demonstrated on various few - shot image recognition problems .", "ner": [["deep meta - learning", "Method"], ["vanilla meta - learning", "Method"], ["few - shot image recognition", "Task"]], "rel": [["deep meta - learning", "Compare-With", "vanilla meta - learning"], ["deep meta - learning", "Used-For", "few - shot image recognition"]], "rel_plus": [["deep meta - learning:Method", "Compare-With", "vanilla meta - learning:Method"], ["deep meta - learning:Method", "Used-For", "few - shot image recognition:Task"]]}
{"doc_id": "3627225", "sentence": "For example , on 5 - way - 1 - shot image recognition on CIFAR - 1 0 0 and CUB - 2 0 0 , it improves Matching Nets from 5 0 . 5 3 % and 5 6 . 5 3 % to 5 8 . 1 8 % and 6 3 . 4 7 % , improves MAML from 4 9 . 2 8 % and 5 0 . 4 5 % to 5 6 . 6 5 % and 6 4 . 6 3 % , and improves Meta - SGD from 5 3 . 8 3 % and 5 3 . 3 4 % to 6 1 . 6 2 % and 6 6 . 9 5 % , respectively .", "ner": [["5 - way - 1 - shot image recognition", "Task"], ["CIFAR - 1 0 0", "Dataset"], ["CUB - 2 0 0", "Dataset"], ["Matching Nets", "Method"], ["MAML", "Method"], ["Meta - SGD", "Method"]], "rel": [["CIFAR - 1 0 0", "Benchmark-For", "5 - way - 1 - shot image recognition"], ["CUB - 2 0 0", "Benchmark-For", "5 - way - 1 - shot image recognition"], ["Matching Nets", "Used-For", "5 - way - 1 - shot image recognition"], ["MAML", "Used-For", "5 - way - 1 - shot image recognition"], ["Meta - SGD", "Used-For", "5 - way - 1 - shot image recognition"], ["Matching Nets", "Evaluated-With", "CIFAR - 1 0 0"], ["MAML", "Evaluated-With", "CIFAR - 1 0 0"], ["Meta - SGD", "Evaluated-With", "CIFAR - 1 0 0"], ["Matching Nets", "Evaluated-With", "CUB - 2 0 0"], ["MAML", "Evaluated-With", "CUB - 2 0 0"], ["Meta - SGD", "Evaluated-With", "CUB - 2 0 0"]], "rel_plus": [["CIFAR - 1 0 0:Dataset", "Benchmark-For", "5 - way - 1 - shot image recognition:Task"], ["CUB - 2 0 0:Dataset", "Benchmark-For", "5 - way - 1 - shot image recognition:Task"], ["Matching Nets:Method", "Used-For", "5 - way - 1 - shot image recognition:Task"], ["MAML:Method", "Used-For", "5 - way - 1 - shot image recognition:Task"], ["Meta - SGD:Method", "Used-For", "5 - way - 1 - shot image recognition:Task"], ["Matching Nets:Method", "Evaluated-With", "CIFAR - 1 0 0:Dataset"], ["MAML:Method", "Evaluated-With", "CIFAR - 1 0 0:Dataset"], ["Meta - SGD:Method", "Evaluated-With", "CIFAR - 1 0 0:Dataset"], ["Matching Nets:Method", "Evaluated-With", "CUB - 2 0 0:Dataset"], ["MAML:Method", "Evaluated-With", "CUB - 2 0 0:Dataset"], ["Meta - SGD:Method", "Evaluated-With", "CUB - 2 0 0:Dataset"]]}
{"doc_id": "3627225", "sentence": "Recently , meta - learning , pioneered by ( Schmidhuber , 1 9 8 7 ) , draws renewed interest which learns on the level of tasks instead of instances , and learns task - agnostic learning algorithms ( e.g. SGD ) instead of task - specific models ( e.g. CNN ) .", "ner": [["meta - learning", "Method"], ["task - agnostic learning algorithms", "Method"], ["SGD", "Method"], ["task - specific models", "Method"], ["CNN", "Method"]], "rel": [["SGD", "SubClass-Of", "task - agnostic learning algorithms"], ["CNN", "SubClass-Of", "task - specific models"]], "rel_plus": [["SGD:Method", "SubClass-Of", "task - agnostic learning algorithms:Method"], ["CNN:Method", "SubClass-Of", "task - specific models:Method"]]}
{"doc_id": "3627225", "sentence": "Meta - learning has been shown to significantly outperform conventional learning on various few - shot learning problems , ranging from classification ( Santoro et al. , 2 0 1 6 ; Vinyals et al. , 2 0 1 6 ; Ravi & Larochelle , 2 0 1 7 ; Finn et al. , 2 0 1 7 ; Li et al. , 2 0 1 7 ; Snell et al. , 2 0 1 7 ) , reinforcement learning ( Wang et al. , 2 0 1 6 ; Duan et al. , 2 0 1 6 ; Nikhil Mishra , 2 0 1 8 ; Kevin Frans , 2 0 1 8) , regression ( Santoro et al. , 2 0 1 6 ; Finn et al. , 2 0 1 7 ) , machine translation ( Kaiser et al. , 2 0 1 7 ) , to object tracking ( Park & Berg , 2 0 1 8) .", "ner": [["Meta - learning", "Method"], ["few - shot learning problems", "Task"], ["classification", "Task"], ["reinforcement learning", "Task"], ["regression", "Task"], ["machine translation", "Task"], ["object tracking", "Task"]], "rel": [["Meta - learning", "Used-For", "few - shot learning problems"], ["classification", "SubTask-Of", "few - shot learning problems"], ["reinforcement learning", "SubTask-Of", "few - shot learning problems"], ["regression", "SubTask-Of", "few - shot learning problems"], ["machine translation", "SubTask-Of", "few - shot learning problems"], ["object tracking", "SubTask-Of", "few - shot learning problems"], ["Meta - learning", "Used-For", "classification"], ["Meta - learning", "Used-For", "reinforcement learning"], ["Meta - learning", "Used-For", "regression"], ["Meta - learning", "Used-For", "machine translation"], ["Meta - learning", "Used-For", "object tracking"]], "rel_plus": [["Meta - learning:Method", "Used-For", "few - shot learning problems:Task"], ["classification:Task", "SubTask-Of", "few - shot learning problems:Task"], ["reinforcement learning:Task", "SubTask-Of", "few - shot learning problems:Task"], ["regression:Task", "SubTask-Of", "few - shot learning problems:Task"], ["machine translation:Task", "SubTask-Of", "few - shot learning problems:Task"], ["object tracking:Task", "SubTask-Of", "few - shot learning problems:Task"], ["Meta - learning:Method", "Used-For", "classification:Task"], ["Meta - learning:Method", "Used-For", "reinforcement learning:Task"], ["Meta - learning:Method", "Used-For", "regression:Task"], ["Meta - learning:Method", "Used-For", "machine translation:Task"], ["Meta - learning:Method", "Used-For", "object tracking:Task"]]}
{"doc_id": "3627225", "sentence": "In our deep meta - learning framework , the key idea is to train a concept generator together with meta - learning tasks and large - scale image recognition tasks , which will finally improve the performance of vanilla meta learning methods .", "ner": [["deep meta - learning", "Method"], ["concept generator", "Method"], ["meta - learning tasks", "Task"], ["image recognition", "Task"], ["vanilla meta learning", "Method"]], "rel": [["concept generator", "Part-Of", "deep meta - learning"], ["meta - learning tasks", "Used-For", "concept generator"], ["image recognition", "Used-For", "concept generator"], ["deep meta - learning", "Compare-With", "vanilla meta learning"]], "rel_plus": [["concept generator:Method", "Part-Of", "deep meta - learning:Method"], ["meta - learning tasks:Task", "Used-For", "concept generator:Method"], ["image recognition:Task", "Used-For", "concept generator:Method"], ["deep meta - learning:Method", "Compare-With", "vanilla meta learning:Method"]]}
{"doc_id": "3627225", "sentence": "Our main contributions can be summarized as follows : \u2022 We propose deep meta - learning to integrate the power of deep learning into meta - learning , and show it improves vanilla meta - learning significantly on the problem of few - shot image recognition ( see Figure 1 ) .", "ner": [["deep meta - learning", "Method"], ["deep learning", "Method"], ["meta - learning", "Method"], ["vanilla meta - learning", "Method"], ["few - shot image recognition", "Task"]], "rel": [["meta - learning", "Part-Of", "deep meta - learning"], ["deep learning", "Part-Of", "deep meta - learning"], ["deep meta - learning", "Compare-With", "vanilla meta - learning"], ["deep meta - learning", "Used-For", "few - shot image recognition"], ["vanilla meta - learning", "Used-For", "few - shot image recognition"]], "rel_plus": [["meta - learning:Method", "Part-Of", "deep meta - learning:Method"], ["deep learning:Method", "Part-Of", "deep meta - learning:Method"], ["deep meta - learning:Method", "Compare-With", "vanilla meta - learning:Method"], ["deep meta - learning:Method", "Used-For", "few - shot image recognition:Task"], ["vanilla meta - learning:Method", "Used-For", "few - shot image recognition:Task"]]}
{"doc_id": "3627225", "sentence": "Since the concept generator will continue to evolve with coming labeled data , this framework could literally be implemented as a life - long learning system . \u2022 We instantiate the deep meta - learning framework on top of three state - of - the - art meta - learners including Matching Nets ( Vinyals et al. , 2 0 1 6 ) , MAML ( Finn et al. , 2 0 1 7 ) , and Meta - SGD ( Li et al. , 2 0 1 7 ) , and conduct extensive experiments to show that deep metalearning utilizes data more efficiently than all existing methods , and provides significantly better results on few - shot image recognition .", "ner": [["deep meta - learning", "Method"], ["meta - learners", "Method"], ["Matching Nets", "Method"], ["MAML", "Method"], ["Meta - SGD", "Method"], ["deep metalearning", "Method"], ["few - shot image recognition", "Task"]], "rel": [["Matching Nets", "SubClass-Of", "meta - learners"], ["MAML", "SubClass-Of", "meta - learners"], ["Meta - SGD", "SubClass-Of", "meta - learners"], ["deep metalearning", "Used-For", "few - shot image recognition"]], "rel_plus": [["Matching Nets:Method", "SubClass-Of", "meta - learners:Method"], ["MAML:Method", "SubClass-Of", "meta - learners:Method"], ["Meta - SGD:Method", "SubClass-Of", "meta - learners:Method"], ["deep metalearning:Method", "Used-For", "few - shot image recognition:Task"]]}
{"doc_id": "3627225", "sentence": "In ( Ravi & Larochelle , 2 0 1 7 ) , an LSTM is learned to train a learner such as CNN as it rolls out . ( Finn et al. , 2 0 1 7 ) learn how to initialize SGD while ( Li et al. , 2 0 1 7 ) learn a full - stack SGD , including initialization , update direction , and learning rate .", "ner": [["LSTM", "Method"], ["CNN", "Method"], ["SGD", "Method"], ["SGD", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "3627225", "sentence": "Memory - augmented neural networks show high capacity for meta - learning . ( Santoro et al. , 2 0 1 6 ) train an LSTM as a controller for accessing ( read and write ) an additional memory module , which is an extension of the internal memory in LSTM .", "ner": [["Memory - augmented neural networks", "Method"], ["meta - learning", "Method"], ["LSTM", "Method"], ["LSTM", "Method"]], "rel": [["Memory - augmented neural networks", "Used-For", "meta - learning"]], "rel_plus": [["Memory - augmented neural networks:Method", "Used-For", "meta - learning:Method"]]}
{"doc_id": "3627225", "sentence": "For example , Fast R - CNN ( Girshick , 2 0 1 5 ) trains image classifiers and bounding - box regressors simultaneously to perform object detection .", "ner": [["Fast R - CNN", "Method"], ["image classifiers", "Task"], ["bounding - box regressors", "Task"], ["object detection", "Task"]], "rel": [["Fast R - CNN", "Used-For", "image classifiers"], ["Fast R - CNN", "Used-For", "bounding - box regressors"], ["Fast R - CNN", "Used-For", "object detection"]], "rel_plus": [["Fast R - CNN:Method", "Used-For", "image classifiers:Task"], ["Fast R - CNN:Method", "Used-For", "bounding - box regressors:Task"], ["Fast R - CNN:Method", "Used-For", "object detection:Task"]]}
{"doc_id": "3627225", "sentence": "Quite a few methods have been proposed for few - shot image recognition . ( Li et al. , 2 0 0 6 ) present a Bayesian model for learning categories from a few examples per category . ( Salakhutdinov et al. , 2 0 1 2 ) organize seen categories into super - categories to derive hierarchically structured priors for new categories using a hierarchical Bayesian model . ( Lake et al. , 2 0 1 1 ) develop a generative model that composes pen strokes into characters for handwritten character recognition . ( Wong & Yuille , 2 0 1 5 ) extend this idea to natural images without relying on domain knowledge .", "ner": [["few - shot image recognition", "Task"], ["Bayesian model", "Method"], ["hierarchical Bayesian model", "Method"], ["generative model", "Method"], ["handwritten character recognition", "Task"]], "rel": [["generative model", "Used-For", "handwritten character recognition"]], "rel_plus": [["generative model:Method", "Used-For", "handwritten character recognition:Task"]]}
{"doc_id": "3627225", "sentence": "In this section , we propose a new meta - learning framework , called deep meta - learning ( DEML ) , which integrates the representation power of deep learning into meta - learning , and enables learning to learn in the concept space .", "ner": [["meta - learning framework", "Method"], ["deep meta - learning", "Method"], ["DEML", "Method"], ["deep learning", "Method"], ["meta - learning", "Method"]], "rel": [["deep meta - learning", "SubClass-Of", "meta - learning framework"], ["DEML", "Synonym-Of", "deep meta - learning"], ["deep learning", "Part-Of", "deep meta - learning"], ["meta - learning", "Part-Of", "deep meta - learning"]], "rel_plus": [["deep meta - learning:Method", "SubClass-Of", "meta - learning framework:Method"], ["DEML:Method", "Synonym-Of", "deep meta - learning:Method"], ["deep learning:Method", "Part-Of", "deep meta - learning:Method"], ["meta - learning:Method", "Part-Of", "deep meta - learning:Method"]]}
{"doc_id": "3627225", "sentence": "Meta - learning tasks T follow a distribution p(T ) in a task space , and ( x , y ) represents a labeled instance sampled from an external dataset D. The objective is to minimize the expectation of the joint , denoted by J , of two losses : the loss L T ( \u03b8 M , \u03b8 G ) on the meta - learning tasks and the loss L ( x , y ) ( \u03b8 D , \u03b8 G ) on the concept discrimination tasks .", "ner": [["Meta - learning tasks", "Task"], ["meta - learning tasks", "Task"], ["concept discrimination", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "3627225", "sentence": "The concept generator G , parameterized by \u03b8 G , is a deep neural network that could be any popular convolutional neural network such as AlexNet ( Krizhevsky et al. , 2 0 1 2 ) , Inception ( Szegedy et al. , 2 0 1 5 ) , VGG ( Simonyan & Zisserman , 2 0 1 4 ) , or ResNet ( He et al. , 2 0 1 6 ) .", "ner": [["concept generator", "Method"], ["deep neural network", "Method"], ["convolutional neural network", "Method"], ["AlexNet", "Method"], ["Inception", "Method"], ["VGG", "Method"], ["ResNet", "Method"]], "rel": [["convolutional neural network", "SubClass-Of", "deep neural network"], ["concept generator", "SubClass-Of", "deep neural network"], ["AlexNet", "SubClass-Of", "convolutional neural network"], ["Inception", "SubClass-Of", "convolutional neural network"], ["VGG", "SubClass-Of", "convolutional neural network"], ["ResNet", "SubClass-Of", "convolutional neural network"]], "rel_plus": [["convolutional neural network:Method", "SubClass-Of", "deep neural network:Method"], ["concept generator:Method", "SubClass-Of", "deep neural network:Method"], ["AlexNet:Method", "SubClass-Of", "convolutional neural network:Method"], ["Inception:Method", "SubClass-Of", "convolutional neural network:Method"], ["VGG:Method", "SubClass-Of", "convolutional neural network:Method"], ["ResNet:Method", "SubClass-Of", "convolutional neural network:Method"]]}
{"doc_id": "3627225", "sentence": "The concept discriminator D , parameterized by \u03b8 D , is designed to predict labels for concepts generated by G. It could be implemented with any supervised learning method , such as support vector machines , nearest neighbor classifiers , or neural networks .", "ner": [["concept discriminator", "Method"], ["predict labels for concepts generated by G.", "Task"], ["supervised learning method", "Method"], ["support vector machines", "Method"], ["nearest neighbor classifiers", "Method"], ["neural networks", "Method"]], "rel": [["concept discriminator", "Used-For", "predict labels for concepts generated by G."], ["support vector machines", "SubClass-Of", "supervised learning method"], ["nearest neighbor classifiers", "SubClass-Of", "supervised learning method"], ["neural networks", "SubClass-Of", "supervised learning method"]], "rel_plus": [["concept discriminator:Method", "Used-For", "predict labels for concepts generated by G.:Task"], ["support vector machines:Method", "SubClass-Of", "supervised learning method:Method"], ["nearest neighbor classifiers:Method", "SubClass-Of", "supervised learning method:Method"], ["neural networks:Method", "SubClass-Of", "supervised learning method:Method"]]}
{"doc_id": "3627225", "sentence": "The goal is to minimize the expected loss on the concept discrimination tasks : where could be any loss function suitable for concept discrimination .", "ner": [["concept discrimination", "Task"], ["concept discrimination", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "3627225", "sentence": "For example , if we choose Matching Nets ( Vinyals et al. , 2 0 1 6 ) as our meta - learner , then f T \u2022 G(x ) would be formalized as follows : is the softmax over the cosine distance c and the embedding function g. If MAML ( Finn et al. , 2 0 1 7 ) is chosen as the meta - learner , then where and \u03b1 is a fixed learning rate .", "ner": [["Matching Nets", "Method"], ["meta - learner", "Method"], ["softmax", "Method"], ["MAML", "Method"], ["meta - learner", "Method"]], "rel": [["Matching Nets", "Part-Of", "meta - learner"], ["MAML", "Part-Of", "meta - learner"]], "rel_plus": [["Matching Nets:Method", "Part-Of", "meta - learner:Method"], ["MAML:Method", "Part-Of", "meta - learner:Method"]]}
{"doc_id": "3627225", "sentence": "After introducing the framework , modules , and criterion of deep meta - learning , we are ready to describe a complete algorithm of DEML .", "ner": [["deep meta - learning", "Method"], ["DEML", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "3627225", "sentence": "In this case , our deep meta - learning can be formulated as the following optimization problem : and \u03bb is a hyperparameter balancing meta - learning and concept discrimination .", "ner": [["deep meta - learning", "Method"], ["meta - learning", "Task"], ["concept discrimination", "Task"]], "rel": [["deep meta - learning", "Used-For", "meta - learning"], ["deep meta - learning", "Used-For", "concept discrimination"]], "rel_plus": [["deep meta - learning:Method", "Used-For", "meta - learning:Task"], ["deep meta - learning:Method", "Used-For", "concept discrimination:Task"]]}
{"doc_id": "3627225", "sentence": "The stochastic gradient descent ( SGD ) algorithm can be applied to optimize the above objective .", "ner": [["stochastic gradient descent", "Method"], ["SGD", "Method"]], "rel": [["SGD", "Synonym-Of", "stochastic gradient descent"]], "rel_plus": [["SGD:Method", "Synonym-Of", "stochastic gradient descent:Method"]]}
{"doc_id": "3627225", "sentence": "In our implementation , we use the Adam ( Kingma & Ba , 2 0 1 4 ) method , a variant of SGD .", "ner": [["Adam", "Method"], ["SGD", "Method"]], "rel": [["Adam", "SubClass-Of", "SGD"]], "rel_plus": [["Adam:Method", "SubClass-Of", "SGD:Method"]]}
{"doc_id": "3627225", "sentence": "In this section , we evaluate the proposed deep meta - learning ( DEML ) on a number of few - shot image recognition problems , but note that it is applicable to classification , reinforcement learning , and regression in general .", "ner": [["deep meta - learning", "Method"], ["DEML", "Method"], ["few - shot image recognition", "Task"], ["classification", "Task"], ["reinforcement learning", "Task"], ["regression", "Task"]], "rel": [["DEML", "Synonym-Of", "deep meta - learning"], ["deep meta - learning", "Used-For", "few - shot image recognition"], ["deep meta - learning", "Used-For", "classification"], ["deep meta - learning", "Used-For", "reinforcement learning"], ["deep meta - learning", "Used-For", "regression"]], "rel_plus": [["DEML:Method", "Synonym-Of", "deep meta - learning:Method"], ["deep meta - learning:Method", "Used-For", "few - shot image recognition:Task"], ["deep meta - learning:Method", "Used-For", "classification:Task"], ["deep meta - learning:Method", "Used-For", "reinforcement learning:Task"], ["deep meta - learning:Method", "Used-For", "regression:Task"]]}
{"doc_id": "3627225", "sentence": "For concept discrimination tasks , we perform experiments on a subset of ImageNet ( Deng et al. , 2 0 0 9 ) .", "ner": [["concept discrimination", "Task"], ["ImageNet", "Dataset"]], "rel": [["ImageNet", "Benchmark-For", "concept discrimination"]], "rel_plus": [["ImageNet:Dataset", "Benchmark-For", "concept discrimination:Task"]]}
{"doc_id": "3627225", "sentence": "For meta - learning tasks , we perform experiments on MiniImagenet ( Vinyals et al. , 2 0 1 6 ) , Caltech - 2 5 6 ( Griffin et al. , 2 0 0 7 ) , CIFAR - 1 0 0 ( Krizhevsky , 2 0 0 9 ) , and CUB - 2 0 0 ( Wah et al. , 2 0 1 1 ) .", "ner": [["meta - learning tasks", "Task"], ["MiniImagenet", "Dataset"], ["Caltech - 2 5 6", "Dataset"], ["CIFAR - 1 0 0", "Dataset"], ["CUB - 2 0 0", "Dataset"]], "rel": [["MiniImagenet", "Benchmark-For", "meta - learning tasks"], ["Caltech - 2 5 6", "Benchmark-For", "meta - learning tasks"], ["CIFAR - 1 0 0", "Benchmark-For", "meta - learning tasks"], ["CUB - 2 0 0", "Benchmark-For", "meta - learning tasks"]], "rel_plus": [["MiniImagenet:Dataset", "Benchmark-For", "meta - learning tasks:Task"], ["Caltech - 2 5 6:Dataset", "Benchmark-For", "meta - learning tasks:Task"], ["CIFAR - 1 0 0:Dataset", "Benchmark-For", "meta - learning tasks:Task"], ["CUB - 2 0 0:Dataset", "Benchmark-For", "meta - learning tasks:Task"]]}
{"doc_id": "3627225", "sentence": "The whole dataset is too large , and so in our experiments we use a subset ImageNet - 2 0 0 with 2 0 0 classes sampled from 9 0 0 classes ( excluding the 1 0 0 classes used in MiniImagenet ( Vinyals et al. , 2 0 1 6 ) ) .", "ner": [["ImageNet - 2 0 0", "Dataset"], ["MiniImagenet", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "3627225", "sentence": "Particularly , MiniImagenet and ImageNet - 2 0 0 are mutually exclusive at class level .", "ner": [["MiniImagenet", "Dataset"], ["ImageNet - 2 0 0", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "3627225", "sentence": "The Caltech - 2 5 6 dataset ( Griffin et al. , 2 0 0 7 ) is a successor to the well - known dataset Caltech - 1 0 1 .", "ner": [["Caltech - 2 5 6", "Dataset"], ["Caltech - 1 0 1", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "3627225", "sentence": "Sample n tasks T i \u223c p(T ) and m instances ( x j , y j ) \u223c D 6 : 8 : end for 1 1 : 1 2 : end while To compare DEML with existing meta - learning methods , we evaluate existing meta - learning methods ( Matching Nets ( Vinyals et al. , 2 0 1 6 ) , MAML ( Finn et al. , 2 0 1 7 ) , and Meta - SGD ( Li et al. , 2 0 1 7 ) ) on meta - learning datasets as our baselines .", "ner": [["DEML", "Method"], ["meta - learning methods", "Method"], ["meta - learning methods", "Method"], ["Matching Nets", "Method"], ["MAML", "Method"], ["Meta - SGD", "Method"], ["meta - learning", "Task"]], "rel": [["DEML", "Compare-With", "meta - learning methods"], ["Matching Nets", "SubClass-Of", "meta - learning methods"], ["MAML", "SubClass-Of", "meta - learning methods"], ["Meta - SGD", "Used-For", "meta - learning"], ["Matching Nets", "Used-For", "meta - learning"], ["MAML", "Used-For", "meta - learning"], ["meta - learning methods", "Used-For", "meta - learning"], ["DEML", "Used-For", "meta - learning"]], "rel_plus": [["DEML:Method", "Compare-With", "meta - learning methods:Method"], ["Matching Nets:Method", "SubClass-Of", "meta - learning methods:Method"], ["MAML:Method", "SubClass-Of", "meta - learning methods:Method"], ["Meta - SGD:Method", "Used-For", "meta - learning:Task"], ["Matching Nets:Method", "Used-For", "meta - learning:Task"], ["MAML:Method", "Used-For", "meta - learning:Task"], ["meta - learning methods:Method", "Used-For", "meta - learning:Task"], ["DEML:Method", "Used-For", "meta - learning:Task"]]}
{"doc_id": "3627225", "sentence": "To show that the improvements of DEML are not solely attributed to the deeper neural network and rescaled images , we also evaluate the previous approaches with exactly the same architecture ( excluding the concept discriminator ) and inputs as DEML .", "ner": [["DEML", "Method"], ["deeper neural network", "Method"], ["DEML", "Method"]], "rel": [["deeper neural network", "Part-Of", "DEML"]], "rel_plus": [["deeper neural network:Method", "Part-Of", "DEML:Method"]]}
{"doc_id": "3627225", "sentence": "Accordingly , their deep version implementations are denoted by Deep Matching Nets , Deep MAML , and Deep Meta - SGD , respectively .", "ner": [["Deep Matching Nets", "Method"], ["Deep MAML", "Method"], ["Deep Meta - SGD", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "3627225", "sentence": "Since DEML is a meta - learneragnostic framework for meta - learning , we re - implement Matching Nets , MAML and Meta - SGD on DEML with the following implementation configurations .", "ner": [["DEML", "Method"], ["meta - learneragnostic framework", "Method"], ["meta - learning", "Task"], ["Matching Nets", "Method"], ["MAML", "Method"], ["Meta - SGD", "Method"], ["DEML", "Method"]], "rel": [["DEML", "SubClass-Of", "meta - learneragnostic framework"], ["DEML", "Used-For", "meta - learning"], ["Meta - SGD", "Part-Of", "DEML"], ["MAML", "Part-Of", "DEML"], ["Matching Nets", "Part-Of", "DEML"]], "rel_plus": [["DEML:Method", "SubClass-Of", "meta - learneragnostic framework:Method"], ["DEML:Method", "Used-For", "meta - learning:Task"], ["Meta - SGD:Method", "Part-Of", "DEML:Method"], ["MAML:Method", "Part-Of", "DEML:Method"], ["Matching Nets:Method", "Part-Of", "DEML:Method"]]}
{"doc_id": "3627225", "sentence": "When choosing Matching Nets as the meta - learner , the learner is a neural network with an input layer of size 2 0 4 8 , followed by one hidden layer of size 1 0 2 4 with ReLU nonlinearities , and then an output layer of size 5 1 2 .", "ner": [["Matching Nets", "Method"], ["meta - learner", "Method"], ["neural network", "Method"], ["ReLU", "Method"]], "rel": [["Matching Nets", "Part-Of", "meta - learner"], ["neural network", "Part-Of", "meta - learner"], ["ReLU", "Part-Of", "neural network"]], "rel_plus": [["Matching Nets:Method", "Part-Of", "meta - learner:Method"], ["neural network:Method", "Part-Of", "meta - learner:Method"], ["ReLU:Method", "Part-Of", "neural network:Method"]]}
{"doc_id": "3627225", "sentence": "When choosing MAML or Meta - SGD as the meta - learner , the learner is a neural network with the same input layer , followed by two hidden layers of size 1 0 2 4 and 5 1 2 with ReLU nonlinearities , and then an output layer of size 5 .", "ner": [["MAML", "Method"], ["Meta - SGD", "Method"], ["meta - learner", "Method"], ["neural network", "Method"], ["ReLU", "Method"]], "rel": [["Meta - SGD", "Part-Of", "meta - learner"], ["MAML", "Part-Of", "meta - learner"], ["neural network", "Part-Of", "meta - learner"], ["ReLU", "Part-Of", "neural network"]], "rel_plus": [["Meta - SGD:Method", "Part-Of", "meta - learner:Method"], ["MAML:Method", "Part-Of", "meta - learner:Method"], ["neural network:Method", "Part-Of", "meta - learner:Method"], ["ReLU:Method", "Part-Of", "neural network:Method"]]}
{"doc_id": "3627225", "sentence": "ImageNet - 2 0 0 is sampled for the image recognition pipeline D \u2022 G. The prediction loss is measured by the mean of crossentropy over all the examples in this batch .", "ner": [["ImageNet - 2 0 0", "Dataset"], ["image recognition", "Task"]], "rel": [["ImageNet - 2 0 0", "Benchmark-For", "image recognition"]], "rel_plus": [["ImageNet - 2 0 0:Dataset", "Benchmark-For", "image recognition:Task"]]}
{"doc_id": "3627225", "sentence": "The number of iterations is 6 0 , 0 0 0 in the experiments on MiniImagenet , Caltech - 2 5 6 , Meta - testing .", "ner": [["MiniImagenet", "Dataset"], ["Caltech - 2 5 6", "Dataset"], ["Meta - testing", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "3627225", "sentence": "For both MAML and Meta - SGD , the meta - learner uses onestep adaptation during meta - training and meta - testing for fair , and the learning rate \u03b1 for MAML is set to 0.0 1 in all experiments .", "ner": [["MAML", "Method"], ["Meta - SGD", "Method"], ["meta - learner", "Method"], ["MAML", "Method"]], "rel": [["Meta - SGD", "Part-Of", "meta - learner"], ["MAML", "Part-Of", "meta - learner"]], "rel_plus": [["Meta - SGD:Method", "Part-Of", "meta - learner:Method"], ["MAML:Method", "Part-Of", "meta - learner:Method"]]}
{"doc_id": "3627225", "sentence": "The comparison results between DEML versions and vanilla versions of Matching Nets , MAML , and Meta - SGD are summarized in Table   1 .", "ner": [["DEML", "Method"], ["Matching Nets", "Method"], ["MAML", "Method"], ["Meta - SGD", "Method"]], "rel": [["DEML", "Compare-With", "Matching Nets"], ["DEML", "Compare-With", "MAML"], ["DEML", "Compare-With", "Meta - SGD"]], "rel_plus": [["DEML:Method", "Compare-With", "Matching Nets:Method"], ["DEML:Method", "Compare-With", "MAML:Method"], ["DEML:Method", "Compare-With", "Meta - SGD:Method"]]}
{"doc_id": "3627225", "sentence": "To validate that the improvements of DEML are not merely because of the deeper neural network and rescaled images , we also evaluate the deep versions of the previous approaches on MiniImagenet as mentioned in Section 4. 2 .", "ner": [["DEML", "Method"], ["deeper neural network", "Method"], ["MiniImagenet", "Dataset"]], "rel": [["deeper neural network", "Part-Of", "DEML"], ["DEML", "Evaluated-With", "MiniImagenet"]], "rel_plus": [["deeper neural network:Method", "Part-Of", "DEML:Method"], ["DEML:Method", "Evaluated-With", "MiniImagenet:Dataset"]]}
{"doc_id": "3627225", "sentence": "We enlarge the metatraining dataset by merging together the original 6 4 classes of MiniImagenet and the 2 0 0 classes of ImageNet - 2 0 0 .", "ner": [["MiniImagenet", "Dataset"], ["ImageNet - 2 0 0", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "3627225", "sentence": "To compare deep metalearning with transfer learning , we also evaluate some vari - Another simplified version of DEML is Decaf+Meta - SGD , where one pretrained generator G is attached to the metalearner to execute meta - training process .", "ner": [["deep metalearning", "Method"], ["transfer learning", "Method"], ["DEML", "Method"], ["Decaf+Meta - SGD", "Method"]], "rel": [["deep metalearning", "Compare-With", "transfer learning"], ["Decaf+Meta - SGD", "SubClass-Of", "DEML"]], "rel_plus": [["deep metalearning:Method", "Compare-With", "transfer learning:Method"], ["Decaf+Meta - SGD:Method", "SubClass-Of", "DEML:Method"]]}
{"doc_id": "3627225", "sentence": "It is interesting to note that the baseline Decaf+kNN achieves the best performance on MiniImagenet and Caltech - 2 5 6 .", "ner": [["Decaf+kNN", "Method"], ["MiniImagenet", "Dataset"], ["Caltech - 2 5 6", "Dataset"]], "rel": [["Decaf+kNN", "Evaluated-With", "MiniImagenet"], ["Decaf+kNN", "Evaluated-With", "Caltech - 2 5 6"]], "rel_plus": [["Decaf+kNN:Method", "Evaluated-With", "MiniImagenet:Dataset"], ["Decaf+kNN:Method", "Evaluated-With", "Caltech - 2 5 6:Dataset"]]}
{"doc_id": "3627225", "sentence": "Since the concept generator G is trained on ImageNet - 2 0 0 , which is quite similar to MiniImagenet and Caltech - 2 5 6 ( Tommasi et al. , 2 0 1 7 ) , representations provided by it are so effective that the naive nearest - neighbor baseline achieves the best performance .", "ner": [["concept generator", "Method"], ["ImageNet - 2 0 0", "Dataset"], ["MiniImagenet", "Dataset"], ["Caltech - 2 5 6", "Dataset"]], "rel": [["concept generator", "Trained-With", "ImageNet - 2 0 0"], ["ImageNet - 2 0 0", "Compare-With", "MiniImagenet"], ["ImageNet - 2 0 0", "Compare-With", "Caltech - 2 5 6"]], "rel_plus": [["concept generator:Method", "Trained-With", "ImageNet - 2 0 0:Dataset"], ["ImageNet - 2 0 0:Dataset", "Compare-With", "MiniImagenet:Dataset"], ["ImageNet - 2 0 0:Dataset", "Compare-With", "Caltech - 2 5 6:Dataset"]]}
{"doc_id": "3627225", "sentence": "On the contrary , the performance of Decaf+kNN drops a lot on CIFAR - 1 0 0 and CUB - 2 0 0 , since these two datasets are quite different from ImageNet - 2 0 0 .", "ner": [["Decaf+kNN", "Method"], ["CIFAR - 1 0 0", "Dataset"], ["CUB - 2 0 0", "Dataset"], ["ImageNet - 2 0 0", "Dataset"]], "rel": [["Decaf+kNN", "Evaluated-With", "CIFAR - 1 0 0"], ["Decaf+kNN", "Evaluated-With", "CUB - 2 0 0"], ["CIFAR - 1 0 0", "Compare-With", "ImageNet - 2 0 0"], ["CUB - 2 0 0", "Compare-With", "ImageNet - 2 0 0"]], "rel_plus": [["Decaf+kNN:Method", "Evaluated-With", "CIFAR - 1 0 0:Dataset"], ["Decaf+kNN:Method", "Evaluated-With", "CUB - 2 0 0:Dataset"], ["CIFAR - 1 0 0:Dataset", "Compare-With", "ImageNet - 2 0 0:Dataset"], ["CUB - 2 0 0:Dataset", "Compare-With", "ImageNet - 2 0 0:Dataset"]]}
{"doc_id": "3627225", "sentence": "To emphasize the necessity of our joint learning process , we propose the third version Decaf+Fine - Tune+Meta - SGD which is the same as Decaf+Meta - SGD except that the concept generator and the meta - learner are trained together during meta - training process .", "ner": [["Decaf+Fine - Tune+Meta - SGD", "Method"], ["Decaf+Meta - SGD", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "3627225", "sentence": "The models are trained for 6 0 , 0 0 0 and 2 0 , 0 0 0 iterations on CIFAR - 1 0 0 and CUB - 2 0 0 , respectively , and the results are shown in Figure 3 , together with the results of Decaf+Meta - SGD and DEML+Meta - SGD .", "ner": [["CIFAR - 1 0 0", "Dataset"], ["CUB - 2 0 0", "Dataset"], ["Decaf+Meta - SGD", "Method"], ["DEML+Meta - SGD", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "3627225", "sentence": "DEML+Meta - SGD performs consistently better than Decaf+Meta - SGD and Decaf+Fine - Tune+Meta - SGD on all cases by a wide margin .", "ner": [["DEML+Meta - SGD", "Method"], ["Decaf+Meta - SGD", "Method"], ["Decaf+Fine - Tune+Meta - SGD", "Method"]], "rel": [["DEML+Meta - SGD", "Compare-With", "Decaf+Meta - SGD"], ["DEML+Meta - SGD", "Compare-With", "Decaf+Fine - Tune+Meta - SGD"]], "rel_plus": [["DEML+Meta - SGD:Method", "Compare-With", "Decaf+Meta - SGD:Method"], ["DEML+Meta - SGD:Method", "Compare-With", "Decaf+Fine - Tune+Meta - SGD:Method"]]}
{"doc_id": "3627225", "sentence": "We verify it on CIFAR - 1 0 0 with the 5 - way - 5 - shot case with DEML+Meta - SGD ( Figure 4 ) .", "ner": [["CIFAR - 1 0 0", "Dataset"], ["DEML+Meta - SGD", "Method"]], "rel": [["DEML+Meta - SGD", "Evaluated-With", "CIFAR - 1 0 0"]], "rel_plus": [["DEML+Meta - SGD:Method", "Evaluated-With", "CIFAR - 1 0 0:Dataset"]]}
{"doc_id": "3627225", "sentence": "A balance between the external knowledge and the internal meta - level knowledge is useful in DEML .   In this paper , we propose deep meta - learning that integrates the representation power of deep learning into meta - learning , and enables learning to learn in the concept space .", "ner": [["DEML", "Method"], ["deep meta - learning", "Method"], ["deep learning", "Method"], ["meta - learning", "Method"]], "rel": [["deep learning", "Part-Of", "deep meta - learning"], ["meta - learning", "Part-Of", "deep meta - learning"]], "rel_plus": [["deep learning:Method", "Part-Of", "deep meta - learning:Method"], ["meta - learning:Method", "Part-Of", "deep meta - learning:Method"]]}
{"doc_id": "3627225", "sentence": "Extensive experiments on few - shot image recognition show that this new framework improves the vanilla meta - learning greatly .", "ner": [["few - shot image recognition", "Task"], ["vanilla meta - learning", "Method"]], "rel": [["vanilla meta - learning", "Used-For", "few - shot image recognition"]], "rel_plus": [["vanilla meta - learning:Method", "Used-For", "few - shot image recognition:Task"]]}
{"doc_id": "202719492", "sentence": "Most Named Entity Recognition ( NER ) systems use additional features like part - of - speech ( POS ) tags , shallow parsing , gazetteers , etc .", "ner": [["Named Entity Recognition", "Task"], ["NER", "Task"], ["part - of - speech", "Method"], ["POS", "Method"], ["shallow parsing", "Method"], ["gazetteers", "Method"]], "rel": [["NER", "Synonym-Of", "Named Entity Recognition"], ["part - of - speech", "Used-For", "Named Entity Recognition"], ["shallow parsing", "Used-For", "Named Entity Recognition"], ["gazetteers", "Used-For", "Named Entity Recognition"], ["POS", "Synonym-Of", "part - of - speech"]], "rel_plus": [["NER:Task", "Synonym-Of", "Named Entity Recognition:Task"], ["part - of - speech:Method", "Used-For", "Named Entity Recognition:Task"], ["shallow parsing:Method", "Used-For", "Named Entity Recognition:Task"], ["gazetteers:Method", "Used-For", "Named Entity Recognition:Task"], ["POS:Method", "Synonym-Of", "part - of - speech:Method"]]}
{"doc_id": "202719492", "sentence": "We propose CNN based models that incorporate this semantic information and use them for NER .", "ner": [["CNN", "Method"], ["NER", "Task"]], "rel": [["CNN", "Used-For", "NER"]], "rel_plus": [["CNN:Method", "Used-For", "NER:Task"]]}
{"doc_id": "202719492", "sentence": "Our models show an improvement over the baseline BERT - BiLSTM - CRF model .", "ner": [["BERT - BiLSTM - CRF", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202719492", "sentence": "We present a state - of - the - art F 1 score on Weibo dataset of 7 1 . 8 1 and show a competitive improvement of + 0. 7 2 over baseline on ResumeNER dataset .", "ner": [["Weibo", "Dataset"], ["ResumeNER", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "202719492", "sentence": "Augmenting named entity recognition ( NER ) systems with additional features like gazetteers , bag of words or character level information has been commonplace like Sang and Meulder ( 2 0 0 3 ) , Collobert et al. ( 2 0 1 1 ) and Chiu and Nichols ( 2 0 1 6 ) .", "ner": [["named entity recognition", "Task"], ["NER", "Task"], ["gazetteers", "Method"], ["bag of words", "Method"], ["character level information", "Method"]], "rel": [["NER", "Synonym-Of", "named entity recognition"], ["character level information", "Used-For", "named entity recognition"], ["bag of words", "Used-For", "named entity recognition"], ["gazetteers", "Used-For", "named entity recognition"]], "rel_plus": [["NER:Task", "Synonym-Of", "named entity recognition:Task"], ["character level information:Method", "Used-For", "named entity recognition:Task"], ["bag of words:Method", "Used-For", "named entity recognition:Task"], ["gazetteers:Method", "Used-For", "named entity recognition:Task"]]}
{"doc_id": "202719492", "sentence": "Several authors Shi et al. ( 2 0 1 5 ) , Li et al. ( 2 0 1 5 ) , Sun et al. ( 2 0 1 4 ) and Meng et al. ( 2 0 1 9 ) managed to use the radical representations successfully in a wide range of natural language understanding ( NLU ) tasks .", "ner": [["natural language understanding", "Task"], ["NLU", "Task"]], "rel": [["NLU", "Synonym-Of", "natural language understanding"]], "rel_plus": [["NLU:Task", "Synonym-Of", "natural language understanding:Task"]]}
{"doc_id": "202719492", "sentence": "Only recently , Meng et al. ( 2 0 1 9 ) presented a complex Glyph reinforced model that concatenates glyph embeddings with BERT ( Devlin et al. 2 0 1 8 ) embeddings .", "ner": [["Glyph reinforced model", "Method"], ["glyph embeddings", "Method"], ["BERT", "Method"]], "rel": [["glyph embeddings", "Part-Of", "Glyph reinforced model"], ["BERT", "Part-Of", "Glyph reinforced model"]], "rel_plus": [["glyph embeddings:Method", "Part-Of", "Glyph reinforced model:Method"], ["BERT:Method", "Part-Of", "Glyph reinforced model:Method"]]}
{"doc_id": "202719492", "sentence": "We show that our models have a significant improvement over our baseline on two datasets , Chinese OntoNotes v 5 . 0 ( Pradhan and Ramshaw 2 0 1 7 ) and Weibo ( Peng and Dredze 2 0 1 5 ) .", "ner": [["Chinese OntoNotes v 5 . 0", "Dataset"], ["Weibo", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "202719492", "sentence": "We approach the problem of incorporating Chinese glyphs as an image classification problem and present two CNNs which we call \" strided \" and \" GLYNN \" inspired from computer vision .", "ner": [["image classification", "Task"], ["CNNs", "Method"], ["strided", "Method"], ["GLYNN", "Method"], ["computer vision", "Task"]], "rel": [["CNNs", "Used-For", "image classification"], ["strided", "Used-For", "image classification"], ["GLYNN", "Used-For", "image classification"], ["strided", "SubClass-Of", "CNNs"], ["GLYNN", "SubClass-Of", "CNNs"], ["strided", "Used-For", "computer vision"], ["GLYNN", "Used-For", "computer vision"]], "rel_plus": [["CNNs:Method", "Used-For", "image classification:Task"], ["strided:Method", "Used-For", "image classification:Task"], ["GLYNN:Method", "Used-For", "image classification:Task"], ["strided:Method", "SubClass-Of", "CNNs:Method"], ["GLYNN:Method", "SubClass-Of", "CNNs:Method"], ["strided:Method", "Used-For", "computer vision:Task"], ["GLYNN:Method", "Used-For", "computer vision:Task"]]}
{"doc_id": "202719492", "sentence": "We treat this encoding problem purely in terms of computer vision , i.e. to extract \" meaningful \" features from the image , instead of a specialized CNN that encapsulates the subtle radicals .", "ner": [["computer vision", "Task"], ["CNN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202719492", "sentence": "Both CNNs are used to encode the glyphs and these encoded images are then used as an added feature for our NER system .", "ner": [["CNNs", "Method"], ["NER", "Task"]], "rel": [["CNNs", "Used-For", "NER"]], "rel_plus": [["CNNs:Method", "Used-For", "NER:Task"]]}
{"doc_id": "202719492", "sentence": "We also present an autoencoder architecture to pretrain GLYNN and compare the results .", "ner": [["autoencoder", "Method"], ["GLYNN", "Method"]], "rel": [["autoencoder", "Used-For", "GLYNN"]], "rel_plus": [["autoencoder:Method", "Used-For", "GLYNN:Method"]]}
{"doc_id": "202719492", "sentence": "Since the OntoNotes v 5 . 0 and Weibo datasets have a significant number of non - Chinese characters in them , we also show our model is robust by conducting a robustness test of our systems by throwing in the pictures of the non - Chinese characters as well pictures of English alphabets and show that it still beats the baseline model .", "ner": [["OntoNotes v 5 . 0", "Dataset"], ["Weibo datasets", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "202719492", "sentence": "The main strength of our model is as follows : \u2022 Easy to implement and train \u2022 Robust to non - Chinese languages in the dataset \u2022 Requires less amount of glyph data to train 2 Architecture of our GLYPH models BERT is the state of the art language model introduced by Devlin et al. ( 2 0 1 8) .", "ner": [["GLYPH models", "Method"], ["BERT", "Method"], ["language model", "Method"]], "rel": [["BERT", "SubClass-Of", "language model"]], "rel_plus": [["BERT:Method", "SubClass-Of", "language model:Method"]]}
{"doc_id": "202719492", "sentence": "BERT is a transformer based model which is trained on masked word prediction and next sentence prediction tasks and is trained on Wikipedia data and a large book corpus .", "ner": [["BERT", "Method"], ["transformer based model", "Method"], ["masked word prediction", "Task"], ["next sentence prediction", "Task"], ["Wikipedia", "Dataset"], ["large book corpus", "Dataset"]], "rel": [["BERT", "SubClass-Of", "transformer based model"], ["BERT", "Trained-With", "masked word prediction"], ["BERT", "Trained-With", "next sentence prediction"], ["BERT", "Trained-With", "Wikipedia"], ["BERT", "Trained-With", "large book corpus"]], "rel_plus": [["BERT:Method", "SubClass-Of", "transformer based model:Method"], ["BERT:Method", "Trained-With", "masked word prediction:Task"], ["BERT:Method", "Trained-With", "next sentence prediction:Task"], ["BERT:Method", "Trained-With", "Wikipedia:Dataset"], ["BERT:Method", "Trained-With", "large book corpus:Dataset"]]}
{"doc_id": "202719492", "sentence": "We then combine BERT with a popular used architecture in NER , BiLSTM - CRF as in Huang , Xu and Yu ( 2 0 1 5 ) , Ma and Hovy ( 2 0 1 6 ) , Chiu and Nichols ( 2 0 1 6 ) and Zhang and Yang ( 2 0 1 8) .", "ner": [["BERT", "Method"], ["NER", "Task"], ["BiLSTM - CRF", "Method"]], "rel": [["BERT", "Used-For", "NER"]], "rel_plus": [["BERT:Method", "Used-For", "NER:Task"]]}
{"doc_id": "202719492", "sentence": "Our model ( figure 1 ) consists of the following parts : pretrained BERT embeddings and the ( pretrained ) CNN embeddings .", "ner": [["BERT", "Method"], ["CNN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202719492", "sentence": "We concatenate the last four layers of BERT and the CNN vectors which are our new \" character \" embeddings .", "ner": [["BERT", "Method"], ["CNN", "Method"], ["\" character \" embeddings", "Method"]], "rel": [["CNN", "Part-Of", "\" character \" embeddings"], ["BERT", "Part-Of", "\" character \" embeddings"]], "rel_plus": [["CNN:Method", "Part-Of", "\" character \" embeddings:Method"], ["BERT:Method", "Part-Of", "\" character \" embeddings:Method"]]}
{"doc_id": "202719492", "sentence": "We then feed these character embeddings to the BiLSTM layer which are finally decoded via a CRF layer .", "ner": [["character embeddings", "Method"], ["BiLSTM", "Method"], ["CRF", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202719492", "sentence": "The CNN - LSTM - CRF is then being trained end - to - end while we keep BERT frozen .", "ner": [["CNN - LSTM - CRF", "Method"], ["BERT", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202719492", "sentence": "Thus we try to take advantage of BERT 's large pre - scale training and the information from Chinese glyphs encoded by our CNN 's .", "ner": [["BERT", "Method"], ["CNN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202719492", "sentence": "The selection of the CNN 's are primarily motivated by problems in computer vision .", "ner": [["CNN", "Method"], ["computer vision", "Task"]], "rel": [["CNN", "Used-For", "computer vision"]], "rel_plus": [["CNN:Method", "Used-For", "computer vision:Task"]]}
{"doc_id": "202719492", "sentence": "To vindicate our choice of these CNN 's we applied them to \" Fashion MNIST \" dataset and we got around 9 3 % accuracy .", "ner": [["CNN", "Method"], ["Fashion MNIST", "Dataset"]], "rel": [["CNN", "Evaluated-With", "Fashion MNIST"]], "rel_plus": [["CNN:Method", "Evaluated-With", "Fashion MNIST:Dataset"]]}
{"doc_id": "202719492", "sentence": "This consists of 4 2D convolution layers with strides 2 , filter size of 6 4 , kernel size of 3 and activation leaky ReLU .", "ner": [["2D convolution", "Method"], ["activation leaky ReLU", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202719492", "sentence": "Furthermore we normalize the final output by using layer normalization as introduced by Ba , Kiros , and Hinton ( 2 0 1 6 ) .   In this subsection we describe another CNN which we call \" Glynn \" to encode the glyphs .", "ner": [["layer normalization", "Method"], ["CNN", "Method"], ["Glynn", "Method"]], "rel": [["Glynn", "SubClass-Of", "CNN"]], "rel_plus": [["Glynn:Method", "SubClass-Of", "CNN:Method"]]}
{"doc_id": "202719492", "sentence": "The idea for this CNN comes purely from image classification tasks .", "ner": [["CNN", "Method"], ["image classification", "Task"]], "rel": [["CNN", "Used-For", "image classification"]], "rel_plus": [["CNN:Method", "Used-For", "image classification:Task"]]}
{"doc_id": "202719492", "sentence": "Batch normalization ( Ioffe and Szegedy 2 0 1 5 ) is used to speed up training and the dropout layers are used to prevent overfitting and the maxpooling layers are used to reduce the computational complexity of the network .", "ner": [["Batch normalization", "Method"], ["dropout", "Method"], ["maxpooling", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202719492", "sentence": "We use filter size of 3 2 , kernel size of 3 and padding='same ' in both the convolution layers , while we use sigmoid activation and strides=( 2 , 2 ) for our first convolution layer and ReLU activation and strides=( 1 , 1 ) for the second convolution layer .", "ner": [["convolution layers", "Method"], ["sigmoid activation", "Method"], ["convolution layer", "Method"], ["ReLU", "Method"], ["convolution layer", "Method"]], "rel": [["sigmoid activation", "Part-Of", "convolution layer"], ["ReLU", "Part-Of", "convolution layer"]], "rel_plus": [["sigmoid activation:Method", "Part-Of", "convolution layer:Method"], ["ReLU:Method", "Part-Of", "convolution layer:Method"]]}
{"doc_id": "202719492", "sentence": "We pretrain the CNN using an autoencoder shown in figure 4 .", "ner": [["CNN", "Method"], ["autoencoder", "Method"]], "rel": [["autoencoder", "Part-Of", "CNN"]], "rel_plus": [["autoencoder:Method", "Part-Of", "CNN:Method"]]}
{"doc_id": "202719492", "sentence": "We also employ autoencoder ( Zhou et al. 2 0 1 5 ) to pretrain the CNN .", "ner": [["autoencoder", "Method"], ["CNN", "Method"]], "rel": [["autoencoder", "Part-Of", "CNN"]], "rel_plus": [["autoencoder:Method", "Part-Of", "CNN:Method"]]}
{"doc_id": "202719492", "sentence": "The main purpose of the autoencoder is dimensionality reduction , i.e. the autoencoder encourages the CNN to extract high level features without employing multiple convolution layers like the strided CNN .", "ner": [["autoencoder", "Method"], ["dimensionality reduction", "Task"], ["autoencoder", "Method"], ["CNN", "Method"], ["convolution layers", "Method"], ["CNN", "Method"]], "rel": [["autoencoder", "Used-For", "dimensionality reduction"], ["autoencoder", "Part-Of", "CNN"], ["convolution layers", "Part-Of", "CNN"]], "rel_plus": [["autoencoder:Method", "Used-For", "dimensionality reduction:Task"], ["autoencoder:Method", "Part-Of", "CNN:Method"], ["convolution layers:Method", "Part-Of", "CNN:Method"]]}
{"doc_id": "202719492", "sentence": "We train the autoencoder for 2 0 0 epochs and we use RM - Sprop as our optimizer .", "ner": [["autoencoder", "Method"], ["RM - Sprop", "Method"]], "rel": [["RM - Sprop", "Part-Of", "autoencoder"]], "rel_plus": [["RM - Sprop:Method", "Part-Of", "autoencoder:Method"]]}
{"doc_id": "202719492", "sentence": "Mainstream neural approach predates to 2 0 0 3 when Hammerton ( 2 0 0 3 ) used Long Short - Term Memory for NER , achieving just above average for English F 1 scores and improvement for German NER .", "ner": [["Long Short - Term Memory", "Method"], ["NER", "Task"], ["German NER", "Task"]], "rel": [["Long Short - Term Memory", "Used-For", "NER"], ["Long Short - Term Memory", "Used-For", "German NER"]], "rel_plus": [["Long Short - Term Memory:Method", "Used-For", "NER:Task"], ["Long Short - Term Memory:Method", "Used-For", "German NER:Task"]]}
{"doc_id": "202719492", "sentence": "Hochreiter and Schmidhuber ( 1 9 9 7 ) presented Long Short - Term Memory ( LSTM ) , and it was expanded by Gers , Schmidhuber and Cummings ( 2 0 0 0 ) , and reached its current form by Graves and Schmidhuber ( 2 0 0 5 ) .", "ner": [["Long Short - Term Memory", "Method"], ["LSTM", "Method"]], "rel": [["LSTM", "Synonym-Of", "Long Short - Term Memory"]], "rel_plus": [["LSTM:Method", "Synonym-Of", "Long Short - Term Memory:Method"]]}
{"doc_id": "202719492", "sentence": "LSTM is increasing in its use with NER problems over the past 2 decades .", "ner": [["LSTM", "Method"], ["NER", "Task"]], "rel": [["LSTM", "Used-For", "NER"]], "rel_plus": [["LSTM:Method", "Used-For", "NER:Task"]]}
{"doc_id": "202719492", "sentence": "Recent works in NER follows this approach , mainly using BiLSTM - CRF architecture .", "ner": [["NER", "Task"], ["BiLSTM - CRF", "Method"]], "rel": [["BiLSTM - CRF", "Used-For", "NER"]], "rel_plus": [["BiLSTM - CRF:Method", "Used-For", "NER:Task"]]}
{"doc_id": "202719492", "sentence": "Bi - LSTM - CRF architecture was first proposed by Huang , Xu and Yu ( 2 0 1 5 ) , and has been widely studied and augmented .", "ner": [["Bi - LSTM - CRF", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202719492", "sentence": "Chiu and Nichols ( 2 0 1 6 ) and Ma and Hovy ( 2 0 1 6 ) augmented LSTM - CRF architecture with character - level convolutional neural network to add an additional features to the architecture .", "ner": [["LSTM - CRF", "Method"], ["character - level convolutional neural network", "Method"]], "rel": [["character - level convolutional neural network", "Part-Of", "LSTM - CRF"]], "rel_plus": [["character - level convolutional neural network:Method", "Part-Of", "LSTM - CRF:Method"]]}
{"doc_id": "202719492", "sentence": "Instead of applying convolutional neural network to the text , we apply it to the glyphs to augment our Bi - LSTM - CRF .", "ner": [["convolutional neural network", "Method"], ["Bi - LSTM - CRF", "Method"]], "rel": [["convolutional neural network", "Used-For", "Bi - LSTM - CRF"]], "rel_plus": [["convolutional neural network:Method", "Used-For", "Bi - LSTM - CRF:Method"]]}
{"doc_id": "202719492", "sentence": "Recently , transfer learning architectures has shown significant improvement in various natural language processing tasks such as question answering , natural language understanding , machine translation and natural language inference .", "ner": [["transfer learning architectures", "Method"], ["natural language processing", "Task"], ["question answering", "Task"], ["natural language understanding", "Task"], ["machine translation", "Task"], ["natural language inference", "Task"]], "rel": [["transfer learning architectures", "Used-For", "natural language processing"], ["question answering", "SubTask-Of", "natural language processing"], ["natural language understanding", "SubTask-Of", "natural language processing"], ["machine translation", "SubTask-Of", "natural language processing"], ["natural language inference", "SubTask-Of", "natural language processing"], ["transfer learning architectures", "Used-For", "question answering"], ["transfer learning architectures", "Used-For", "natural language understanding"], ["transfer learning architectures", "Used-For", "machine translation"], ["transfer learning architectures", "Used-For", "natural language inference"]], "rel_plus": [["transfer learning architectures:Method", "Used-For", "natural language processing:Task"], ["question answering:Task", "SubTask-Of", "natural language processing:Task"], ["natural language understanding:Task", "SubTask-Of", "natural language processing:Task"], ["machine translation:Task", "SubTask-Of", "natural language processing:Task"], ["natural language inference:Task", "SubTask-Of", "natural language processing:Task"], ["transfer learning architectures:Method", "Used-For", "question answering:Task"], ["transfer learning architectures:Method", "Used-For", "natural language understanding:Task"], ["transfer learning architectures:Method", "Used-For", "machine translation:Task"], ["transfer learning architectures:Method", "Used-For", "natural language inference:Task"]]}
{"doc_id": "202719492", "sentence": "Devlin et al. ( 2 0 1 8) uses stacked bi - directional transformer layers called BERT that is trained on masked word prediction and next sentence prediction tasks .", "ner": [["stacked bi - directional transformer layers", "Method"], ["BERT", "Method"], ["masked word prediction", "Task"], ["next sentence prediction", "Task"]], "rel": [["BERT", "SubClass-Of", "stacked bi - directional transformer layers"], ["BERT", "Trained-With", "masked word prediction"], ["BERT", "Trained-With", "next sentence prediction"]], "rel_plus": [["BERT:Method", "SubClass-Of", "stacked bi - directional transformer layers:Method"], ["BERT:Method", "Trained-With", "masked word prediction:Task"], ["BERT:Method", "Trained-With", "next sentence prediction:Task"]]}
{"doc_id": "202719492", "sentence": "By employing a task - specific final output layer , BERT can be tuned to many different natural language processing tasks .", "ner": [["task - specific final output layer", "Method"], ["BERT", "Method"], ["natural language processing", "Task"]], "rel": [["task - specific final output layer", "Part-Of", "BERT"], ["BERT", "Used-For", "natural language processing"]], "rel_plus": [["task - specific final output layer:Method", "Part-Of", "BERT:Method"], ["BERT:Method", "Used-For", "natural language processing:Task"]]}
{"doc_id": "202719492", "sentence": "In this work , we apply BERT to NER and use BiLSTM - CRF as the output layer of BERT - CNN .", "ner": [["BERT", "Method"], ["NER", "Task"], ["BiLSTM - CRF", "Method"], ["BERT - CNN", "Method"]], "rel": [["BERT", "Used-For", "NER"], ["BiLSTM - CRF", "Part-Of", "BERT - CNN"]], "rel_plus": [["BERT:Method", "Used-For", "NER:Task"], ["BiLSTM - CRF:Method", "Part-Of", "BERT - CNN:Method"]]}
{"doc_id": "202719492", "sentence": "Even though there are over 2 0 , 0 0 0 CJK characters , we only have about a hundred of out - of - vocabulary characters in OntoNotes v 5 . 0 and Weibo .", "ner": [["OntoNotes v 5 . 0", "Dataset"], ["Weibo", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "202719492", "sentence": "Another major difference between our approach and all the aforementioned authors in the introduction is that our CNN 's are agnostic to the subtleties of Chinese characters and we treat this encoding problem with computer vision ideas and extract \" meaningful \" features from the image , instead of having a specialized CNN that encapsulates the subtle radicals .", "ner": [["CNN", "Method"], ["computer vision", "Task"], ["CNN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202719492", "sentence": "Both Su and Lee ( 2 0 1 7 ) and Meng et al ( 2 0 1 9 ) use autoencoders to pretrain their CNN .", "ner": [["autoencoders", "Method"], ["CNN", "Method"]], "rel": [["autoencoders", "Part-Of", "CNN"]], "rel_plus": [["autoencoders:Method", "Part-Of", "CNN:Method"]]}
{"doc_id": "202719492", "sentence": "Su and Lee ( 2 0 1 7 ) pretrain the CNN by freezing some layers while Meng et al ( 2 0 1 9 ) pretrain the CNN with the objective of recovering an \" image i d \" while we follow the approach of jointly training all layers of the CNN ( Zhou et al. 2 0 1 5 ) with the global objective of reconstructing the image .", "ner": [["CNN", "Method"], ["CNN", "Method"], ["CNN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202719492", "sentence": "Another difference between our architecture and the GLYCE model ( Meng et al. 2 0 1 9 ) is that we use a BiLSTM instead of a transformer to encode the BERT + Glyph Embeddings .", "ner": [["GLYCE", "Method"], ["BiLSTM", "Method"], ["transformer", "Method"], ["BERT + Glyph Embeddings", "Method"]], "rel": [["transformer", "Part-Of", "GLYCE"], ["BERT + Glyph Embeddings", "Part-Of", "BiLSTM"], ["BERT + Glyph Embeddings", "Part-Of", "transformer"]], "rel_plus": [["transformer:Method", "Part-Of", "GLYCE:Method"], ["BERT + Glyph Embeddings:Method", "Part-Of", "BiLSTM:Method"], ["BERT + Glyph Embeddings:Method", "Part-Of", "transformer:Method"]]}
{"doc_id": "202719492", "sentence": "We ran 1 0 trials for 3 0 epochs for each of Glynn CNN ( default and higher dropout ) and strided CNN on Chinese OntoNotes v 5 . 0 .", "ner": [["Glynn CNN", "Method"], ["dropout", "Method"], ["strided CNN", "Method"], ["Chinese OntoNotes v 5 . 0", "Dataset"]], "rel": [["dropout", "Part-Of", "Glynn CNN"], ["Glynn CNN", "Trained-With", "Chinese OntoNotes v 5 . 0"], ["strided CNN", "Trained-With", "Chinese OntoNotes v 5 . 0"]], "rel_plus": [["dropout:Method", "Part-Of", "Glynn CNN:Method"], ["Glynn CNN:Method", "Trained-With", "Chinese OntoNotes v 5 . 0:Dataset"], ["strided CNN:Method", "Trained-With", "Chinese OntoNotes v 5 . 0:Dataset"]]}
{"doc_id": "202719492", "sentence": "We report our scores in Table 3 Using the F 1 scores obtained by the Glynn CNN ( dropout . 5 ) and the strided CNN , we perform the 2sample t - test and obtain a p - value of < . 0 0 1 .", "ner": [["Glynn CNN", "Method"], ["dropout", "Method"], ["strided CNN", "Method"]], "rel": [["dropout", "Part-Of", "Glynn CNN"]], "rel_plus": [["dropout:Method", "Part-Of", "Glynn CNN:Method"]]}
{"doc_id": "202719492", "sentence": "Thus we see that strided CNN is a significant improvement over both the BERT baseline and the GLYNN .", "ner": [["strided CNN", "Method"], ["BERT", "Method"], ["GLYNN", "Method"]], "rel": [["strided CNN", "Compare-With", "BERT"], ["strided CNN", "Compare-With", "GLYNN"]], "rel_plus": [["strided CNN:Method", "Compare-With", "BERT:Method"], ["strided CNN:Method", "Compare-With", "GLYNN:Method"]]}
{"doc_id": "202719492", "sentence": "Since the Weibo dataset is smaller and is noisier than OntoNotes v 5 . 0 , we ran vanilla BERT - BiLSTM - CRF 2 0 times and 4 0 times respectively on Weibo NAM and the Weibo dataset to establish a baseline .", "ner": [["Weibo", "Dataset"], ["OntoNotes v 5 . 0", "Dataset"], ["BERT - BiLSTM - CRF", "Method"], ["Weibo NAM", "Dataset"], ["Weibo", "Dataset"]], "rel": [["Weibo", "Compare-With", "OntoNotes v 5 . 0"], ["BERT - BiLSTM - CRF", "Trained-With", "Weibo NAM"], ["BERT - BiLSTM - CRF", "Trained-With", "Weibo"]], "rel_plus": [["Weibo:Dataset", "Compare-With", "OntoNotes v 5 . 0:Dataset"], ["BERT - BiLSTM - CRF:Method", "Trained-With", "Weibo NAM:Dataset"], ["BERT - BiLSTM - CRF:Method", "Trained-With", "Weibo:Dataset"]]}
{"doc_id": "202719492", "sentence": "We ran 2 0 trials for 3 0 epochs for each of Glynn CNN ( default and higher dropout ) and strided CNN on Weibo .", "ner": [["Glynn CNN", "Method"], ["dropout", "Method"], ["strided CNN", "Method"], ["Weibo", "Dataset"]], "rel": [["dropout", "Part-Of", "Glynn CNN"], ["Glynn CNN", "Trained-With", "Weibo"], ["strided CNN", "Trained-With", "Weibo"]], "rel_plus": [["dropout:Method", "Part-Of", "Glynn CNN:Method"], ["Glynn CNN:Method", "Trained-With", "Weibo:Dataset"], ["strided CNN:Method", "Trained-With", "Weibo:Dataset"]]}
{"doc_id": "202719492", "sentence": "We also calculate the p - value between our CNN 's and the baseline BERT to see if our results are statistically significant .", "ner": [["CNN", "Method"], ["BERT", "Method"]], "rel": [["CNN", "Compare-With", "BERT"]], "rel_plus": [["CNN:Method", "Compare-With", "BERT:Method"]]}
{"doc_id": "202719492", "sentence": "Table 4 shows that Weibo experiment results from both CNNs are statistically significant than the vanilla BERT .", "ner": [["Weibo", "Dataset"], ["CNNs", "Method"], ["BERT", "Method"]], "rel": [["CNNs", "Evaluated-With", "Weibo"], ["BERT", "Evaluated-With", "Weibo"], ["CNNs", "Compare-With", "BERT"]], "rel_plus": [["CNNs:Method", "Evaluated-With", "Weibo:Dataset"], ["BERT:Method", "Evaluated-With", "Weibo:Dataset"], ["CNNs:Method", "Compare-With", "BERT:Method"]]}
{"doc_id": "202719492", "sentence": "However pvalue between strided CNN and GLYNN is . 7 1 , which shows that their performance on average is statistically the same .", "ner": [["strided CNN", "Method"], ["GLYNN", "Method"]], "rel": [["strided CNN", "Compare-With", "GLYNN"]], "rel_plus": [["strided CNN:Method", "Compare-With", "GLYNN:Method"]]}
{"doc_id": "202719492", "sentence": "Even though strided CNN did not show any improvement , we made significant gains with the GLYNN CNN over the baseline and set a new SOTA F 1 score .", "ner": [["strided CNN", "Method"], ["GLYNN CNN", "Method"]], "rel": [["strided CNN", "Compare-With", "GLYNN CNN"]], "rel_plus": [["strided CNN:Method", "Compare-With", "GLYNN CNN:Method"]]}
{"doc_id": "202719492", "sentence": "Avg However the difference in performance between GLYNN ( with . 5 dropout ) and strided is statistically insignificant as the p - value is . 7 6 8 .", "ner": [["GLYNN", "Method"], ["dropout", "Method"]], "rel": [["dropout", "Part-Of", "GLYNN"]], "rel_plus": [["dropout:Method", "Part-Of", "GLYNN:Method"]]}
{"doc_id": "202719492", "sentence": "We used Adam optimizer on OntoNotes v 5 . 0 for 5 trials with the learning rates . 0 0 1 , . 0 0 0 5 and . 0 0 0 1 with early stopping if the loss did not decrease over 5 epochs .", "ner": [["Adam optimizer", "Method"], ["OntoNotes v 5 . 0", "Dataset"], ["early stopping", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202719492", "sentence": "But in general , we found Adam performs poorly compared to Adafactor .", "ner": [["Adam", "Method"], ["Adafactor", "Method"]], "rel": [["Adam", "Compare-With", "Adafactor"]], "rel_plus": [["Adam:Method", "Compare-With", "Adafactor:Method"]]}
{"doc_id": "202719492", "sentence": "Early stopping with all the above learning rates with both Adam and Adafactor on Weibo produced erratic results with extremely high standard deviations .", "ner": [["Early stopping", "Method"], ["Adam", "Method"], ["Adafactor", "Method"], ["Weibo", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "202719492", "sentence": "We also used dropouts of . 5 on each the dropout layers of the Glynn CNN and ran multiple trials .", "ner": [["dropout", "Method"], ["Glynn CNN", "Method"]], "rel": [["dropout", "Part-Of", "Glynn CNN"]], "rel_plus": [["dropout:Method", "Part-Of", "Glynn CNN:Method"]]}
{"doc_id": "202719492", "sentence": "Table 3 summarizes our results on OntoNotes v 5 . 0 after 1 0 trials and we also compute the p - value to test the difference in the average performance of GLYNN .", "ner": [["OntoNotes v 5 . 0", "Dataset"], ["GLYNN", "Method"]], "rel": [["GLYNN", "Evaluated-With", "OntoNotes v 5 . 0"]], "rel_plus": [["GLYNN:Method", "Evaluated-With", "OntoNotes v 5 . 0:Dataset"]]}
{"doc_id": "202719492", "sentence": "We also ran 2 0 trials on Weibo with this new dropout .", "ner": [["Weibo", "Dataset"], ["dropout", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202719492", "sentence": "We did 2 sample t - test between GLYNN with . 5 dropouts and the strided CNN ( resp .", "ner": [["GLYNN", "Method"], ["dropouts", "Method"], ["strided CNN", "Method"]], "rel": [["dropouts", "Part-Of", "GLYNN"]], "rel_plus": [["dropouts:Method", "Part-Of", "GLYNN:Method"]]}
{"doc_id": "202719492", "sentence": "GLYNN and GLYNN with . 5 dropouts ) and we found the p - value to be . 8 4 ( resp . 8 3 ) .", "ner": [["GLYNN", "Method"], ["GLYNN", "Method"], ["dropouts", "Method"]], "rel": [["dropouts", "Part-Of", "GLYNN"]], "rel_plus": [["dropouts:Method", "Part-Of", "GLYNN:Method"]]}
{"doc_id": "202719492", "sentence": "So the two CNN ( with or without higher dropout ) behave pretty much the same .", "ner": [["CNN", "Method"], ["dropout", "Method"]], "rel": [["dropout", "Part-Of", "CNN"]], "rel_plus": [["dropout:Method", "Part-Of", "CNN:Method"]]}
{"doc_id": "202719492", "sentence": "Losses tend to increase after 3 0 epochs and so the models start doing worse at 4 0 epochs . 3 0 is also an optimum choice for the full Weibo dataset and OntoNotes .", "ner": [["Weibo", "Dataset"], ["OntoNotes", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "202719492", "sentence": "Figure 6 shows the relation between training epochs of GLYNN and the test and dev F 1 scores on Weibo NAM and OntoNotes .", "ner": [["GLYNN", "Method"], ["Weibo NAM", "Dataset"], ["OntoNotes", "Dataset"]], "rel": [["GLYNN", "Evaluated-With", "Weibo NAM"], ["GLYNN", "Evaluated-With", "OntoNotes"]], "rel_plus": [["GLYNN:Method", "Evaluated-With", "Weibo NAM:Dataset"], ["GLYNN:Method", "Evaluated-With", "OntoNotes:Dataset"]]}
{"doc_id": "202719492", "sentence": "How important is the autoencoder : We ran multiple tests ( 2 0 for Weibo NAM , 1 0 for OntoNotes v 5 . 0 ) with varying learning rates and training epochs .", "ner": [["autoencoder", "Method"], ["Weibo NAM", "Dataset"], ["OntoNotes v 5 . 0", "Dataset"]], "rel": [["autoencoder", "Evaluated-With", "Weibo NAM"], ["autoencoder", "Evaluated-With", "OntoNotes v 5 . 0"]], "rel_plus": [["autoencoder:Method", "Evaluated-With", "Weibo NAM:Dataset"], ["autoencoder:Method", "Evaluated-With", "OntoNotes v 5 . 0:Dataset"]]}
{"doc_id": "202719492", "sentence": "For example , GLYNN with default hyperparameters without the autoencoder got an average test score of 7 0 . 9 8 (\u00b1 1 . 4 7 ) on Weibo NAM .", "ner": [["GLYNN", "Method"], ["autoencoder", "Method"], ["Weibo NAM", "Dataset"]], "rel": [["GLYNN", "Evaluated-With", "Weibo NAM"]], "rel_plus": [["GLYNN:Method", "Evaluated-With", "Weibo NAM:Dataset"]]}
{"doc_id": "202719492", "sentence": "Using two very different CNNs with and without an autoencoder , we have shown gains over the baseline system on the three most commonly used datasets and achieve state of the art F 1 score on the Weibo dataset .", "ner": [["CNNs", "Method"], ["autoencoder", "Method"], ["Weibo", "Dataset"]], "rel": [["CNNs", "Evaluated-With", "Weibo"]], "rel_plus": [["CNNs:Method", "Evaluated-With", "Weibo:Dataset"]]}
{"doc_id": "202719492", "sentence": "We are excited by the future of glyphs in NLP and we would like use glyphs for other NLP tasks .", "ner": [["NLP", "Task"], ["NLP", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "53109398", "sentence": "Face presentation attack detection ( PAD ) has become a thorny problem for biometric systems and numerous countermeasures have been proposed to address it .", "ner": [["Face presentation attack detection", "Task"], ["PAD", "Task"]], "rel": [["PAD", "Synonym-Of", "Face presentation attack detection"]], "rel_plus": [["PAD:Task", "Synonym-Of", "Face presentation attack detection:Task"]]}
{"doc_id": "53109398", "sentence": "Inspired by the generator of generative adversarial network ( GAN ) , the proposed network consists of a space generator and a feature extractor .", "ner": [["generative adversarial network", "Method"], ["GAN", "Method"]], "rel": [["GAN", "Synonym-Of", "generative adversarial network"]], "rel_plus": [["GAN:Method", "Synonym-Of", "generative adversarial network:Method"]]}
{"doc_id": "53109398", "sentence": "Extensive experiments on two standard face PAD databases , i.e. , Relay - Attack and OULU - NPU , indicate that our proposed color - liked space analysis based countermeasure significantly outperforms the state - of - the - art methods and show excellent generalization capability . access that person 's social network data including good quality images that can be used for face PAD .", "ner": [["face PAD", "Task"], ["Relay - Attack", "Dataset"], ["OULU - NPU", "Dataset"], ["face PAD", "Task"]], "rel": [["Relay - Attack", "Benchmark-For", "face PAD"], ["OULU - NPU", "Benchmark-For", "face PAD"]], "rel_plus": [["Relay - Attack:Dataset", "Benchmark-For", "face PAD:Task"], ["OULU - NPU:Dataset", "Benchmark-For", "face PAD:Task"]]}
{"doc_id": "53109398", "sentence": "In the last decade , many face PAD methods have been proposed to detect fake faces [ 3 ] , [ 7 ] , [ 8 ] , [ 9 ] , [ 1 0 ] , [ 1 1 ] , [ 1 2 ] , [ 1 3 ] , [ 1 4 ] , [ 1 5 ] , [ 1 6 ] , [ 1 7 ] , [ 1 8 ] , [ 1 9 ] , [ 2 0 ] .", "ner": [["face PAD methods", "Method"], ["detect fake faces", "Task"]], "rel": [["face PAD methods", "Used-For", "detect fake faces"]], "rel_plus": [["face PAD methods:Method", "Used-For", "detect fake faces:Task"]]}
{"doc_id": "53109398", "sentence": "In the last decade , many face PAD methods have been proposed to detect fake faces [ 3 ] , [ 7 ] , [ 8 ] , [ 9 ] , [ 1 0 ] , [ 1 1 ] , [ 1 2 ] , [ 1 3 ] , [ 1 4 ] , [ 1 5 ] , [ 1 6 ] , [ 1 7 ] , [ 1 8 ] , [ 1 9 ] , [ 2 0 ] .", "ner": [["face PAD methods", "Method"], ["detect fake faces", "Task"]], "rel": [["face PAD methods", "Used-For", "detect fake faces"]], "rel_plus": [["face PAD methods:Method", "Used-For", "detect fake faces:Task"]]}
{"doc_id": "53109398", "sentence": "Inspired by generative adversarial network ( GAN ) [ 2 1 ] , a color - liked space generator is constructed to map existing color spaces .", "ner": [["generative adversarial network", "Method"], ["GAN", "Method"]], "rel": [["GAN", "Synonym-Of", "generative adversarial network"]], "rel_plus": [["GAN:Method", "Synonym-Of", "generative adversarial network:Method"]]}
{"doc_id": "53109398", "sentence": "Finally , the extracted features are fed into a Support Vector Machine ( SVM ) [ 2 2 ] classifier to detect face presentation attack .", "ner": [["Support Vector Machine", "Method"], ["SVM", "Method"], ["detect face presentation attack", "Task"]], "rel": [["SVM", "Synonym-Of", "Support Vector Machine"], ["Support Vector Machine", "Used-For", "detect face presentation attack"]], "rel_plus": [["SVM:Method", "Synonym-Of", "Support Vector Machine:Method"], ["Support Vector Machine:Method", "Used-For", "detect face presentation attack:Task"]]}
{"doc_id": "53109398", "sentence": "We train and test our proposed method on two public available databases : Replay - Attack [ 1 0 ] and OULU - NPU [ 2 3 ] .", "ner": [["Replay - Attack", "Dataset"], ["OULU - NPU", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "53109398", "sentence": "Therefore , multi - scale local binary pattern ( LBP ) fe tures [ 1 2 ] were extracted to describe edge texture difference In another work , Chingovska et al. [ 1 0 ] used different kin of LBP features to improve detection accuracy .", "ner": [["multi - scale local binary pattern", "Method"], ["LBP", "Method"], ["LBP", "Method"], ["detection", "Task"]], "rel": [["LBP", "Synonym-Of", "multi - scale local binary pattern"], ["LBP", "Used-For", "detection"]], "rel_plus": [["LBP:Method", "Synonym-Of", "multi - scale local binary pattern:Method"], ["LBP:Method", "Used-For", "detection:Task"]]}
{"doc_id": "53109398", "sentence": "To capture t color differences in luminance and chrominance , Boulkenaf et al. [ 1 3 ] , [ 2 9 ] proposed a method by computing LBP featur from different color spaces and concatenating all LBP in a single feature vector .", "ner": [["LBP", "Method"], ["LBP", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "53109398", "sentence": "With the recent advanc of deep learning in computer vision [ 3 3 ] , [ 3 4 ] , deep textur have also been applied in face PAD .", "ner": [["deep learning", "Method"], ["computer vision", "Task"], ["face PAD", "Task"]], "rel": [["deep learning", "Used-For", "computer vision"], ["deep learning", "Used-For", "face PAD"]], "rel_plus": [["deep learning:Method", "Used-For", "computer vision:Task"], ["deep learning:Method", "Used-For", "face PAD:Task"]]}
{"doc_id": "53109398", "sentence": "Yang et al. [ 3 5 ] propos an end - to - end convolutional neural network ( CNN ) model f face PAD .", "ner": [["convolutional neural network", "Method"], ["CNN", "Method"], ["face PAD", "Task"]], "rel": [["CNN", "Synonym-Of", "convolutional neural network"], ["convolutional neural network", "Used-For", "face PAD"]], "rel_plus": [["CNN:Method", "Synonym-Of", "convolutional neural network:Method"], ["convolutional neural network:Method", "Used-For", "face PAD:Task"]]}
{"doc_id": "53109398", "sentence": "Instead of using fully - connected layers , Li at a [ 3 6 ] , [ 3 7 ] extracted hand - crafted features from convolution Fig. 2 .", "ner": [["fully - connected layers", "Method"], ["convolution", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "53109398", "sentence": "Inspired by generative adversarial network ( GAN ) [ 2 1 ] , a color - liked space generator is constructed to map existing color spaces .", "ner": [["generative adversarial network", "Method"], ["GAN", "Method"]], "rel": [["GAN", "Synonym-Of", "generative adversarial network"]], "rel_plus": [["GAN:Method", "Synonym-Of", "generative adversarial network:Method"]]}
{"doc_id": "53109398", "sentence": "Finally , the extracted features are fed into a Support Vector Machine ( SVM ) [ 2 2 ] classifier to detect face presentation attack .", "ner": [["Support Vector Machine", "Method"], ["SVM", "Method"], ["detect face presentation attack", "Task"]], "rel": [["SVM", "Synonym-Of", "Support Vector Machine"], ["Support Vector Machine", "Used-For", "detect face presentation attack"]], "rel_plus": [["SVM:Method", "Synonym-Of", "Support Vector Machine:Method"], ["Support Vector Machine:Method", "Used-For", "detect face presentation attack:Task"]]}
{"doc_id": "53109398", "sentence": "We train and test our proposed method on two public available databases : Replay - Attack [ 1 0 ] and OULU - NPU [ 2 3 ] .", "ner": [["Replay - Attack", "Dataset"], ["OULU - NPU", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "53109398", "sentence": "Unlike traditional mechanism , the proposed pointsto - set mechanism can guarantee a stable decline in triplet loss . 3 ) Extensive experimental analysis is conducted on the two latest and challenging face PAD databases using their pre - defined publicly well - defined experimental evaluation protocols ensuring the reproducibility of the results and a fair comparison with the state - of - the - art methods .", "ner": [["pointsto - set mechanism", "Method"], ["triplet loss", "Method"], ["face PAD", "Task"]], "rel": [["pointsto - set mechanism", "Part-Of", "triplet loss"]], "rel_plus": [["pointsto - set mechanism:Method", "Part-Of", "triplet loss:Method"]]}
{"doc_id": "53109398", "sentence": "The remainder of the paper is organized as follows : Section II reviews the existing state - of - the - art methods of face PAD and briefly provides the development of triplet network .", "ner": [["face PAD", "Task"], ["triplet network", "Method"]], "rel": [["triplet network", "Used-For", "face PAD"]], "rel_plus": [["triplet network:Method", "Used-For", "face PAD:Task"]]}
{"doc_id": "53109398", "sentence": "A. State - of - the - art of Face PAD In the past few years , face PAD has received great attention and many detection approaches have been developed [ 1 0 ] , [ 1 1 ] , [ 1 2 ] , [ 1 3 ] , [ 1 4 ] , [ 1 5 ] , [ 1 6 ] , [ 1 7 ] , [ 1 8 ] , [ 1 9 ] , [ 2 0 ] .", "ner": [["Face PAD", "Task"], ["face PAD", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "53109398", "sentence": "Based on different clues , these countermeasures can be further categorized into texture analysis [ 1 0 ] , [ 1 2 ] , [ 1 3 ] , motion analysis [ 1 4 ] , [ 1 5 ] , [ 2 5 ] , image quality analysis [ 1 6 ] , [ 1 7 ] , [ 1 8 ] , [ 1 9 ] , [ 2 0 ] , and hardware based methods [ 1 1 ] , [ 1 9 ] , [ 2 0 ] , [ 2 6 ] , [ 2 7 ] , [ 2 8 ] . 1 ) Texture analysis based methods : Due to the limitations of printers and display devices , there are substantial differences in color distribution and edge texture between real and fake faces .", "ner": [["texture analysis", "Task"], ["motion analysis", "Task"], ["image quality analysis", "Task"], ["hardware based methods", "Method"], ["Texture analysis based methods", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "53109398", "sentence": "Therefore , multi - scale local binary pattern ( LBP ) features [ 1 2 ] were extracted to describe edge texture differences .", "ner": [["multi - scale local binary pattern", "Method"], ["LBP", "Method"]], "rel": [["LBP", "Synonym-Of", "multi - scale local binary pattern"]], "rel_plus": [["LBP:Method", "Synonym-Of", "multi - scale local binary pattern:Method"]]}
{"doc_id": "53109398", "sentence": "To capture the color differences in luminance and chrominance , Boulkenafet et al. [ 1 3 ] , [ 2 9 ] proposed a method by computing LBP features from different color spaces and concatenating all LBP into a single feature vector .", "ner": [["LBP", "Method"], ["LBP", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "53109398", "sentence": "Instead of using LBP features , Akshay et al. [ 3 1 ] extracted Haralick [ 3 2 ] texture features for face PAD .", "ner": [["LBP", "Method"], ["face PAD", "Task"]], "rel": [["LBP", "Used-For", "face PAD"]], "rel_plus": [["LBP:Method", "Used-For", "face PAD:Task"]]}
{"doc_id": "53109398", "sentence": "With the recent advances of deep learning in computer vision [ 3 3 ] , [ 3 4 ] , deep textures have also been applied in face PAD .", "ner": [["deep learning", "Method"], ["computer vision", "Task"], ["face PAD", "Task"]], "rel": [["deep learning", "Used-For", "computer vision"], ["deep learning", "Used-For", "face PAD"]], "rel_plus": [["deep learning:Method", "Used-For", "computer vision:Task"], ["deep learning:Method", "Used-For", "face PAD:Task"]]}
{"doc_id": "53109398", "sentence": "Yang et al. [ 3 5 ] proposed an end - to - end convolutional neural network ( CNN ) model for face PAD .", "ner": [["convolutional neural network", "Method"], ["CNN", "Method"], ["face PAD", "Task"]], "rel": [["CNN", "Synonym-Of", "convolutional neural network"], ["convolutional neural network", "Used-For", "face PAD"]], "rel_plus": [["CNN:Method", "Synonym-Of", "convolutional neural network:Method"], ["convolutional neural network:Method", "Used-For", "face PAD:Task"]]}
{"doc_id": "53109398", "sentence": "For capturing texture variations , Xu et al. [ 3 8 ] proposed a long short memory network ( LSTM ) and Li et al. [ 3 9 ] proposed a 3D CNN to detect face presentation attacks , respectively .", "ner": [["long short memory network", "Method"], ["LSTM", "Method"], ["3D CNN", "Method"], ["detect face presentation attacks", "Task"]], "rel": [["LSTM", "Synonym-Of", "long short memory network"], ["3D CNN", "Used-For", "detect face presentation attacks"]], "rel_plus": [["LSTM:Method", "Synonym-Of", "long short memory network:Method"], ["3D CNN:Method", "Used-For", "detect face presentation attacks:Task"]]}
{"doc_id": "53109398", "sentence": "More recently , a LBP network [ 4 0 ] has been designed for face PAD which simulates the idea of basic LBP .", "ner": [["LBP", "Method"], ["face PAD", "Task"], ["LBP", "Method"]], "rel": [["LBP", "Used-For", "face PAD"]], "rel_plus": [["LBP:Method", "Used-For", "face PAD:Task"]]}
{"doc_id": "53109398", "sentence": "However , with the popularity of high - definition screens , their detection performances tend to decrease drastically . 2 ) Motion analysis based methods : Apart from texture analysis , motion also plays an important role in face PAD .", "ner": [["detection", "Task"], ["Motion analysis based methods", "Method"], ["texture analysis", "Task"], ["face PAD", "Task"]], "rel": [["Motion analysis based methods", "Used-For", "face PAD"]], "rel_plus": [["Motion analysis based methods:Method", "Used-For", "face PAD:Task"]]}
{"doc_id": "53109398", "sentence": "For instance , based on the fact that involuntary eyes blinking often occurs in the interval of 2 to 4 seconds [ 4 1 ] , an undirected conditional random field framework [ 1 4 ] was proposed to detect printed photo attacks .", "ner": [["undirected conditional random field", "Method"], ["detect printed photo attacks", "Task"]], "rel": [["undirected conditional random field", "Used-For", "detect printed photo attacks"]], "rel_plus": [["undirected conditional random field:Method", "Used-For", "detect printed photo attacks:Task"]]}
{"doc_id": "53109398", "sentence": "In [ 4 2 ] and [ 4 3 ] , LBP - TOP [ 4 4 ] and LDP - TOP [ 4 5 ] features were extracted to describe these variations , respectively .", "ner": [["LBP - TOP", "Method"], ["LDP - TOP", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "53109398", "sentence": "In another work , Santosh et al. [ 4 6 ] used dynamic mode decomposition ( DMD ) to capture the dynamics of movements .", "ner": [["dynamic mode decomposition", "Method"], ["DMD", "Method"]], "rel": [["DMD", "Synonym-Of", "dynamic mode decomposition"]], "rel_plus": [["DMD:Method", "Synonym-Of", "dynamic mode decomposition:Method"]]}
{"doc_id": "53109398", "sentence": "Tan et al. [ 1 5 ] used Difference - of - Gaussians ( DoG ) to extract the differences in motion deformation patterns between real and fake faces .", "ner": [["Difference - of - Gaussians", "Method"], ["DoG", "Method"]], "rel": [["DoG", "Synonym-Of", "Difference - of - Gaussians"]], "rel_plus": [["DoG:Method", "Synonym-Of", "Difference - of - Gaussians:Method"]]}
{"doc_id": "53109398", "sentence": "In the past few years , many computer version and multimedia analysis tasks such as face verification and person re - identification ( Re - ID ) have explored the effectiveness of triplet network .", "ner": [["computer version", "Task"], ["multimedia analysis tasks", "Task"], ["face verification", "Task"], ["person re - identification", "Task"], ["Re - ID", "Task"], ["triplet network", "Method"]], "rel": [["person re - identification", "SubTask-Of", "computer version"], ["face verification", "SubTask-Of", "computer version"], ["triplet network", "Used-For", "computer version"], ["face verification", "SubTask-Of", "multimedia analysis tasks"], ["person re - identification", "SubTask-Of", "multimedia analysis tasks"], ["triplet network", "Used-For", "multimedia analysis tasks"], ["triplet network", "Used-For", "face verification"], ["Re - ID", "Synonym-Of", "person re - identification"], ["triplet network", "Used-For", "person re - identification"]], "rel_plus": [["person re - identification:Task", "SubTask-Of", "computer version:Task"], ["face verification:Task", "SubTask-Of", "computer version:Task"], ["triplet network:Method", "Used-For", "computer version:Task"], ["face verification:Task", "SubTask-Of", "multimedia analysis tasks:Task"], ["person re - identification:Task", "SubTask-Of", "multimedia analysis tasks:Task"], ["triplet network:Method", "Used-For", "multimedia analysis tasks:Task"], ["triplet network:Method", "Used-For", "face verification:Task"], ["Re - ID:Task", "Synonym-Of", "person re - identification:Task"], ["triplet network:Method", "Used-For", "person re - identification:Task"]]}
{"doc_id": "53109398", "sentence": "For instance , Ding et al. [ 5 7 ] used the triplet loss to learn a deep neural network for person Re - ID .", "ner": [["triplet loss", "Method"], ["deep neural network", "Method"], ["person Re - ID", "Task"]], "rel": [["triplet loss", "Part-Of", "deep neural network"], ["deep neural network", "Used-For", "person Re - ID"]], "rel_plus": [["triplet loss:Method", "Part-Of", "deep neural network:Method"], ["deep neural network:Method", "Used-For", "person Re - ID:Task"]]}
{"doc_id": "53109398", "sentence": "In [ 5 9 ] , Swami et al. proposed a triplet network for face verification and got promising results .", "ner": [["triplet network", "Method"], ["face verification", "Task"]], "rel": [["triplet network", "Used-For", "face verification"]], "rel_plus": [["triplet network:Method", "Used-For", "face verification:Task"]]}
{"doc_id": "53109398", "sentence": "More specifically , given a color video frame f ( x , y ) , a convolutional layer with filter size of 3 \u00d7 3 \u00d7 3 \u00d7 6 4 and a leaky rectified linear unit ( lReLU ) layer are firstly used to process f ( x , y ) .", "ner": [["convolutional layer", "Method"], ["leaky rectified linear unit", "Method"], ["lReLU", "Method"]], "rel": [["leaky rectified linear unit", "Part-Of", "convolutional layer"], ["lReLU", "Synonym-Of", "leaky rectified linear unit"]], "rel_plus": [["leaky rectified linear unit:Method", "Part-Of", "convolutional layer:Method"], ["lReLU:Method", "Synonym-Of", "leaky rectified linear unit:Method"]]}
{"doc_id": "53109398", "sentence": "Moreover , the batch normalization ( BN ) layer [ 6 3 ] is introduced after convolutional layer .", "ner": [["batch normalization", "Method"], ["BN", "Method"], ["convolutional layer", "Method"]], "rel": [["BN", "Synonym-Of", "batch normalization"]], "rel_plus": [["BN:Method", "Synonym-Of", "batch normalization:Method"]]}
{"doc_id": "53109398", "sentence": "The hierarchic link of color - liked space generator is summarized in Table I .    Layer Conv , lReLU w \u00d7 h \u00d7 6 4 Residual : Conv , BN , lReLU , Conv , BN , Sum w \u00d7 h \u00d7 6 4 Residual : Conv , BN , lReLU , Conv , BN , Sum w \u00d7 h \u00d7 6 4 Residual : Conv , BN , lReLU , Conv , BN , Sum w \u00d7 h \u00d7 6 4 Residual : Conv , BN , lReLU , Conv , BN , Sum w \u00d7 h \u00d7 6 4 Residual : Conv , BN , lReLU , Conv , BN , Sum Conv space into a feature space by a differentiable function \u2205. After feature extraction , the generated f \u2032 ( x , y ) can be written , where W 2 represents the parameters of feature extractor .", "ner": [["color - liked space generator", "Method"], ["Layer Conv", "Method"], ["lReLU", "Method"], ["Residual", "Method"], ["Conv", "Method"], ["BN", "Method"], ["lReLU", "Method"], ["Conv", "Method"], ["BN", "Method"], ["Residual", "Method"], ["Conv", "Method"], ["BN", "Method"], ["lReLU", "Method"], ["Conv", "Method"], ["BN", "Method"], ["Residual", "Method"], ["Conv", "Method"], ["BN", "Method"], ["lReLU", "Method"], ["Conv", "Method"], ["BN", "Method"], ["Residual", "Method"], ["Conv", "Method"], ["BN", "Method"], ["lReLU", "Method"], ["Conv", "Method"], ["BN", "Method"], ["Residual", "Method"], ["Conv", "Method"], ["BN", "Method"], ["lReLU", "Method"], ["Conv", "Method"], ["BN", "Method"], ["Conv", "Method"], ["feature extraction", "Method"], ["feature extractor", "Method"]], "rel": [["lReLU", "Part-Of", "Layer Conv"], ["Residual", "Part-Of", "Layer Conv"], ["Conv", "Part-Of", "Residual"], ["BN", "Part-Of", "Residual"], ["lReLU", "Part-Of", "Residual"], ["Conv", "Part-Of", "Residual"], ["BN", "Part-Of", "Residual"], ["Conv", "Part-Of", "Residual"], ["BN", "Part-Of", "Residual"], ["lReLU", "Part-Of", "Residual"], ["Conv", "Part-Of", "Residual"], ["BN", "Part-Of", "Residual"], ["Conv", "Part-Of", "Residual"], ["BN", "Part-Of", "Residual"], ["lReLU", "Part-Of", "Residual"], ["Conv", "Part-Of", "Residual"], ["BN", "Part-Of", "Residual"], ["Conv", "Part-Of", "Residual"], ["BN", "Part-Of", "Residual"], ["lReLU", "Part-Of", "Residual"], ["Conv", "Part-Of", "Residual"], ["BN", "Part-Of", "Residual"], ["Conv", "Part-Of", "Residual"], ["BN", "Part-Of", "Residual"], ["lReLU", "Part-Of", "Residual"], ["Conv", "Part-Of", "Residual"], ["BN", "Part-Of", "Residual"]], "rel_plus": [["lReLU:Method", "Part-Of", "Layer Conv:Method"], ["Residual:Method", "Part-Of", "Layer Conv:Method"], ["Conv:Method", "Part-Of", "Residual:Method"], ["BN:Method", "Part-Of", "Residual:Method"], ["lReLU:Method", "Part-Of", "Residual:Method"], ["Conv:Method", "Part-Of", "Residual:Method"], ["BN:Method", "Part-Of", "Residual:Method"], ["Conv:Method", "Part-Of", "Residual:Method"], ["BN:Method", "Part-Of", "Residual:Method"], ["lReLU:Method", "Part-Of", "Residual:Method"], ["Conv:Method", "Part-Of", "Residual:Method"], ["BN:Method", "Part-Of", "Residual:Method"], ["Conv:Method", "Part-Of", "Residual:Method"], ["BN:Method", "Part-Of", "Residual:Method"], ["lReLU:Method", "Part-Of", "Residual:Method"], ["Conv:Method", "Part-Of", "Residual:Method"], ["BN:Method", "Part-Of", "Residual:Method"], ["Conv:Method", "Part-Of", "Residual:Method"], ["BN:Method", "Part-Of", "Residual:Method"], ["lReLU:Method", "Part-Of", "Residual:Method"], ["Conv:Method", "Part-Of", "Residual:Method"], ["BN:Method", "Part-Of", "Residual:Method"], ["Conv:Method", "Part-Of", "Residual:Method"], ["BN:Method", "Part-Of", "Residual:Method"], ["lReLU:Method", "Part-Of", "Residual:Method"], ["Conv:Method", "Part-Of", "Residual:Method"], ["BN:Method", "Part-Of", "Residual:Method"]]}
{"doc_id": "53109398", "sentence": "More specifically , given a color video frame f ( x , y ) , a convolutional layer with filter size of 3 \u00d7 3 \u00d7 3 \u00d7 6 4 and a leaky rectified linear unit ( lReLU ) layer are firstly used to process f ( x , y ) .", "ner": [["convolutional layer", "Method"], ["leaky rectified linear unit", "Method"], ["lReLU", "Method"]], "rel": [["leaky rectified linear unit", "Part-Of", "convolutional layer"], ["lReLU", "Synonym-Of", "leaky rectified linear unit"]], "rel_plus": [["leaky rectified linear unit:Method", "Part-Of", "convolutional layer:Method"], ["lReLU:Method", "Synonym-Of", "leaky rectified linear unit:Method"]]}
{"doc_id": "53109398", "sentence": "Moreover , the batch normalization ( BN ) layer [ 6 3 ] is introduced after convolutional layer .", "ner": [["batch normalization", "Method"], ["BN", "Method"], ["convolutional layer", "Method"]], "rel": [["BN", "Synonym-Of", "batch normalization"]], "rel_plus": [["BN:Method", "Synonym-Of", "batch normalization:Method"]]}
{"doc_id": "53109398", "sentence": "Rather than computing triplet loss in generated color - liked space ,   Layer Residual : Conv , BN , lReLU , Conv , BN , Sum w \u00d7 h \u00d7 6 4 Residual : Conv , BN , lReLU , Conv , BN , Sum w \u00d7 h \u00d7 6 4 Residual : Conv , BN , lReLU , Conv , BN , Sum Conv , lReLU w \u00d7 h \u00d7 3 Conv our proposed method first maps the generated color - liked space into a feature space by a differentiable function \u2205. After feature extraction , the generated f ( x , y ) can be written as \u2205(f ( x , y ) , W 2 ) , where W 2 represents the parameters of feature extractor .", "ner": [["triplet loss", "Method"], ["Residual", "Method"], ["Conv", "Method"], ["BN", "Method"], ["lReLU", "Method"], ["Conv", "Method"], ["BN", "Method"], ["Residual", "Method"], ["Conv", "Method"], ["BN", "Method"], ["lReLU", "Method"], ["Conv", "Method"], ["BN", "Method"], ["Residual", "Method"], ["Conv", "Method"], ["BN", "Method"], ["lReLU", "Method"], ["Conv", "Method"], ["BN", "Method"], ["Conv", "Method"], ["lReLU", "Method"], ["Conv", "Method"], ["feature extraction", "Method"], ["feature extractor", "Method"]], "rel": [["Conv", "Part-Of", "Residual"], ["BN", "Part-Of", "Residual"], ["lReLU", "Part-Of", "Residual"], ["Conv", "Part-Of", "Residual"], ["BN", "Part-Of", "Residual"], ["Conv", "Part-Of", "Residual"], ["BN", "Part-Of", "Residual"], ["lReLU", "Part-Of", "Residual"], ["Conv", "Part-Of", "Residual"], ["BN", "Part-Of", "Residual"], ["Conv", "Part-Of", "Residual"], ["BN", "Part-Of", "Residual"], ["lReLU", "Part-Of", "Residual"], ["Conv", "Part-Of", "Residual"], ["BN", "Part-Of", "Residual"], ["Conv", "Part-Of", "Residual"], ["lReLU", "Part-Of", "Residual"], ["Conv", "Part-Of", "Residual"]], "rel_plus": [["Conv:Method", "Part-Of", "Residual:Method"], ["BN:Method", "Part-Of", "Residual:Method"], ["lReLU:Method", "Part-Of", "Residual:Method"], ["Conv:Method", "Part-Of", "Residual:Method"], ["BN:Method", "Part-Of", "Residual:Method"], ["Conv:Method", "Part-Of", "Residual:Method"], ["BN:Method", "Part-Of", "Residual:Method"], ["lReLU:Method", "Part-Of", "Residual:Method"], ["Conv:Method", "Part-Of", "Residual:Method"], ["BN:Method", "Part-Of", "Residual:Method"], ["Conv:Method", "Part-Of", "Residual:Method"], ["BN:Method", "Part-Of", "Residual:Method"], ["lReLU:Method", "Part-Of", "Residual:Method"], ["Conv:Method", "Part-Of", "Residual:Method"], ["BN:Method", "Part-Of", "Residual:Method"], ["Conv:Method", "Part-Of", "Residual:Method"], ["lReLU:Method", "Part-Of", "Residual:Method"], ["Conv:Method", "Part-Of", "Residual:Method"]]}
{"doc_id": "53109398", "sentence": "For the feature mapping \u2205 , we exploit the pre - trained implementation of the popular VGG - 1 9 [ 2 4 ] network , which consists of stacked convolutional layers coupled with pooling operations to gradually decrease the spatial dimension of the image and to extract higher - level features in higher layers .", "ner": [["VGG - 1 9", "Method"], ["convolutional layers", "Method"]], "rel": [["convolutional layers", "Part-Of", "VGG - 1 9"]], "rel_plus": [["convolutional layers:Method", "Part-Of", "VGG - 1 9:Method"]]}
{"doc_id": "53109398", "sentence": "In training stage , the stochastic gradient descent ( SGD ) algorithm [ 6 6 ] is used to learn the network parameters .", "ner": [["stochastic gradient descent", "Method"], ["SGD", "Method"]], "rel": [["SGD", "Synonym-Of", "stochastic gradient descent"]], "rel_plus": [["SGD:Method", "Synonym-Of", "stochastic gradient descent:Method"]]}
{"doc_id": "53109398", "sentence": "In training stage , the stochastic gradient descent ( SGD ) algorithm [ 6 6 ] is used to learn the network parameters .", "ner": [["stochastic gradient descent", "Method"], ["SGD", "Method"]], "rel": [["SGD", "Synonym-Of", "stochastic gradient descent"]], "rel_plus": [["SGD:Method", "Synonym-Of", "stochastic gradient descent:Method"]]}
{"doc_id": "53109398", "sentence": "But for the parameters in color - liked space generator , they are initialized based on [ 6 7 ] as illustrated in Eq. 7 , which can ensure that all convolutional layers in the network initially have the approximately same output distribution and empirically improve the rate of convergence . where rand ( \u00b7 ) samples from a zero mean , unit standard derivation gaussian function , and n l is the channel number of inputs in convolutional layer .", "ner": [["color - liked space generator", "Method"], ["convolutional layers", "Method"], ["convolutional layer", "Method"]], "rel": [["convolutional layers", "Part-Of", "color - liked space generator"]], "rel_plus": [["convolutional layers:Method", "Part-Of", "color - liked space generator:Method"]]}
{"doc_id": "53109398", "sentence": "In training stage , the momentum and weight decay of SGD are set to 0. 9 and 0.0 0 0 5 , respectively .", "ner": [["momentum", "Method"], ["weight decay", "Method"], ["SGD", "Method"]], "rel": [["momentum", "Part-Of", "SGD"], ["weight decay", "Part-Of", "SGD"]], "rel_plus": [["momentum:Method", "Part-Of", "SGD:Method"], ["weight decay:Method", "Part-Of", "SGD:Method"]]}
{"doc_id": "53109398", "sentence": "In our paper , we realize the proposed color - liked space generator and SVM based on the toolbox of MatConvNet with the version 1. 0 - beta 2 0   Our proposed face PAD algorithm is validated on two publicly available face PAD databases : Replay - Attack [ 1 0 ] and OULU - NPU [ 2 3 ] .", "ner": [["color - liked space generator", "Method"], ["SVM", "Method"], ["MatConvNet", "Method"], ["face PAD algorithm", "Method"], ["face PAD", "Task"], ["Replay - Attack", "Dataset"], ["OULU - NPU", "Dataset"]], "rel": [["MatConvNet", "Part-Of", "color - liked space generator"], ["MatConvNet", "Part-Of", "SVM"], ["face PAD algorithm", "Used-For", "face PAD"], ["Replay - Attack", "Benchmark-For", "face PAD"], ["OULU - NPU", "Benchmark-For", "face PAD"], ["face PAD algorithm", "Evaluated-With", "Replay - Attack"], ["face PAD algorithm", "Evaluated-With", "OULU - NPU"]], "rel_plus": [["MatConvNet:Method", "Part-Of", "color - liked space generator:Method"], ["MatConvNet:Method", "Part-Of", "SVM:Method"], ["face PAD algorithm:Method", "Used-For", "face PAD:Task"], ["Replay - Attack:Dataset", "Benchmark-For", "face PAD:Task"], ["OULU - NPU:Dataset", "Benchmark-For", "face PAD:Task"], ["face PAD algorithm:Method", "Evaluated-With", "Replay - Attack:Dataset"], ["face PAD algorithm:Method", "Evaluated-With", "OULU - NPU:Dataset"]]}
{"doc_id": "53109398", "sentence": "Fig. 6 shows some examples of real and fake faces . 2 ) OULU - NPU : The OULU - NPU Database 4 [ 2 3 ] consists of 4 9 5 0 real access and attack videos and attempts 5 5 clients .", "ner": [["OULU - NPU", "Dataset"], ["OULU - NPU", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "53109398", "sentence": "But for the parameters in color - liked space generator , they are initialized based on [ 6 7 ] as illustrated in Eq. 7 , which can ensure that all convolutional layers in the network initially have the approximately same output distribution and empirically improve the rate of convergence . where rand ( \u00b7 ) samples from a zero mean , unit standard derivation gaussian function , and n l is the channel number of inputs in convolutional layer .", "ner": [["color - liked space generator", "Method"], ["convolutional layers", "Method"], ["convolutional layer", "Method"]], "rel": [["convolutional layers", "Part-Of", "color - liked space generator"]], "rel_plus": [["convolutional layers:Method", "Part-Of", "color - liked space generator:Method"]]}
{"doc_id": "53109398", "sentence": "In training stage , the momentum and weight decay of SGD are set to 0. 9 and 0.0 0 0 5 , respectively .", "ner": [["momentum", "Method"], ["weight decay", "Method"], ["SGD", "Method"]], "rel": [["weight decay", "Part-Of", "SGD"], ["momentum", "Part-Of", "SGD"]], "rel_plus": [["weight decay:Method", "Part-Of", "SGD:Method"], ["momentum:Method", "Part-Of", "SGD:Method"]]}
{"doc_id": "53109398", "sentence": "In our paper , we realize the proposed color - liked space generator and SVM based on the toolbox of MatConvNet with the version 1. 0 - beta 2 0 1 and liblinear with the version 1. 9 6 2 [ 6 8 ] , respectively .", "ner": [["color - liked space generator", "Method"], ["SVM", "Method"], ["MatConvNet", "Method"]], "rel": [["MatConvNet", "Part-Of", "color - liked space generator"], ["MatConvNet", "Part-Of", "SVM"]], "rel_plus": [["MatConvNet:Method", "Part-Of", "color - liked space generator:Method"], ["MatConvNet:Method", "Part-Of", "SVM:Method"]]}
{"doc_id": "53109398", "sentence": "Our proposed face PAD algorithm is validated on two publicly available face PAD databases : Replay - Attack [ 1 0 ] and OULU - NPU [ 2 3 ] .", "ner": [["face PAD algorithm", "Method"], ["face PAD", "Task"], ["Replay - Attack", "Dataset"], ["OULU - NPU", "Dataset"]], "rel": [["face PAD algorithm", "Used-For", "face PAD"], ["Replay - Attack", "Benchmark-For", "face PAD"], ["OULU - NPU", "Benchmark-For", "face PAD"], ["face PAD algorithm", "Evaluated-With", "Replay - Attack"], ["face PAD algorithm", "Evaluated-With", "OULU - NPU"]], "rel_plus": [["face PAD algorithm:Method", "Used-For", "face PAD:Task"], ["Replay - Attack:Dataset", "Benchmark-For", "face PAD:Task"], ["OULU - NPU:Dataset", "Benchmark-For", "face PAD:Task"], ["face PAD algorithm:Method", "Evaluated-With", "Replay - Attack:Dataset"], ["face PAD algorithm:Method", "Evaluated-With", "OULU - NPU:Dataset"]]}
{"doc_id": "53109398", "sentence": "Fig. 6 shows some examples of real and fake faces . 2 ) OULU - NPU : The OULU - NPU Database 4 [ 2 3 ] consists of 4 9 5 0 real access and attack videos and attempts 5 5 clients .", "ner": [["OULU - NPU", "Dataset"], ["OULU - NPU", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "53109398", "sentence": "But for the parameters in color - liked space generator , they are initialized based on [ 6 7 ] as illustrated in Eq. 7 , which can ensure that all convolutional layers in the network initially have the approximately same output distribution and empirically improve the rate of convergence . where rand ( \u00b7 ) samples from a zero mean , unit standard derivation gaussian function , and n l is the channel number of inputs in convolutional layer .", "ner": [["color - liked space generator", "Method"], ["convolutional layers", "Method"], ["convolutional layer", "Method"]], "rel": [["convolutional layers", "Part-Of", "color - liked space generator"]], "rel_plus": [["convolutional layers:Method", "Part-Of", "color - liked space generator:Method"]]}
{"doc_id": "53109398", "sentence": "In training stage , the momentum and weight decay of SGD are set to 0. 9 and 0.0 0 0 5 , respectively .", "ner": [["momentum", "Method"], ["weight decay", "Method"], ["SGD", "Method"]], "rel": [["momentum", "Part-Of", "SGD"], ["weight decay", "Part-Of", "SGD"]], "rel_plus": [["momentum:Method", "Part-Of", "SGD:Method"], ["weight decay:Method", "Part-Of", "SGD:Method"]]}
{"doc_id": "53109398", "sentence": "In our paper , we realize the proposed color - liked space generator and SVM based on the toolbox of MatConvNet with the version 1. 0 - beta 2 0 1 and liblinear with the version 1. 9 6 2 [ 6 8 ] , respectively .", "ner": [["color - liked space generator", "Method"], ["SVM", "Method"], ["MatConvNet", "Method"]], "rel": [["MatConvNet", "Part-Of", "color - liked space generator"], ["MatConvNet", "Part-Of", "SVM"]], "rel_plus": [["MatConvNet:Method", "Part-Of", "color - liked space generator:Method"], ["MatConvNet:Method", "Part-Of", "SVM:Method"]]}
{"doc_id": "53109398", "sentence": "Our proposed face PAD algorithm is validated on two publicly available face PAD databases : Replay - Attack [ 1 0 ] and OULU - NPU [ 2 3 ] .", "ner": [["face PAD algorithm", "Method"], ["face PAD", "Task"], ["Replay - Attack", "Dataset"], ["OULU - NPU", "Dataset"]], "rel": [["face PAD algorithm", "Used-For", "face PAD"], ["OULU - NPU", "Benchmark-For", "face PAD"], ["Replay - Attack", "Benchmark-For", "face PAD"], ["face PAD algorithm", "Evaluated-With", "Replay - Attack"], ["face PAD algorithm", "Evaluated-With", "OULU - NPU"]], "rel_plus": [["face PAD algorithm:Method", "Used-For", "face PAD:Task"], ["OULU - NPU:Dataset", "Benchmark-For", "face PAD:Task"], ["Replay - Attack:Dataset", "Benchmark-For", "face PAD:Task"], ["face PAD algorithm:Method", "Evaluated-With", "Replay - Attack:Dataset"], ["face PAD algorithm:Method", "Evaluated-With", "OULU - NPU:Dataset"]]}
{"doc_id": "53109398", "sentence": "But for the parameters in color - liked space generator , they are initialized based on [ 6 7 ] as illustrated in Eq. 7 , which can ensure that all convolutional layers in the network initially have the approximately same output distribution and empirically improve the rate of convergence . where rand ( \u00b7 ) samples from a zero mean , unit standard derivation gaussian function , and n l is the channel number of inputs in convolutional layer .", "ner": [["color - liked space generator", "Method"], ["convolutional layers", "Method"], ["convolutional layer", "Method"]], "rel": [["convolutional layers", "Part-Of", "color - liked space generator"]], "rel_plus": [["convolutional layers:Method", "Part-Of", "color - liked space generator:Method"]]}
{"doc_id": "53109398", "sentence": "In training stage , the momentum and weight decay of SGD are set to 0. 9 and 0.0 0 0 5 , respectively .", "ner": [["momentum", "Method"], ["weight decay", "Method"], ["SGD", "Method"]], "rel": [["weight decay", "Part-Of", "SGD"], ["momentum", "Part-Of", "SGD"]], "rel_plus": [["weight decay:Method", "Part-Of", "SGD:Method"], ["momentum:Method", "Part-Of", "SGD:Method"]]}
{"doc_id": "53109398", "sentence": "In our paper , we realize the proposed color - liked space generator and SVM based on the toolbox of MatConvNet with the version 1. 0 - beta 2 0 1 and liblinear with the version 1. 9 6 2 [ 6 8 ] , respectively .   Our proposed face PAD algorithm is validated on two publicly available face PAD databases : Replay - Attack [ 1 0 ] and OULU - NPU [ 2 3 ] .", "ner": [["color - liked space generator", "Method"], ["SVM", "Method"], ["MatConvNet", "Method"], ["face PAD algorithm", "Method"], ["face PAD", "Task"], ["Replay - Attack", "Dataset"], ["OULU - NPU", "Dataset"]], "rel": [["MatConvNet", "Part-Of", "color - liked space generator"], ["MatConvNet", "Part-Of", "SVM"], ["face PAD algorithm", "Used-For", "face PAD"], ["Replay - Attack", "Benchmark-For", "face PAD"], ["OULU - NPU", "Benchmark-For", "face PAD"], ["face PAD algorithm", "Evaluated-With", "Replay - Attack"], ["face PAD algorithm", "Evaluated-With", "OULU - NPU"]], "rel_plus": [["MatConvNet:Method", "Part-Of", "color - liked space generator:Method"], ["MatConvNet:Method", "Part-Of", "SVM:Method"], ["face PAD algorithm:Method", "Used-For", "face PAD:Task"], ["Replay - Attack:Dataset", "Benchmark-For", "face PAD:Task"], ["OULU - NPU:Dataset", "Benchmark-For", "face PAD:Task"], ["face PAD algorithm:Method", "Evaluated-With", "Replay - Attack:Dataset"], ["face PAD algorithm:Method", "Evaluated-With", "OULU - NPU:Dataset"]]}
{"doc_id": "53109398", "sentence": "Fig. 6 shows some examples of real and fake faces . 2 ) OULU - NPU : The OULU - NPU Database 4 [ 2 3 ] consists of 4 9 5 0 real access and attack videos and attempts 5 5 clients .", "ner": [["OULU - NPU", "Dataset"], ["OULU - NPU", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "53109398", "sentence": "Fig. 6 shows some examples of real and fake faces . 2 ) OULU - NPU : The OULU - NPU Database 4 [ 2 3 ] consists of 4 9 5 0 real access and attack videos and attempts 5 5 clients .", "ner": [["OULU - NPU", "Dataset"], ["OULU - NPU", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "53109398", "sentence": "More specifically , for ReplayAttack database , \u03b3 should be set to 0. 1 with the averaged ACER= 0 . 5 % ; but for OULU - NPU database , \u03b3 should be set to 1 with the averaged ACER= 5 . 4 % .", "ner": [["ReplayAttack", "Dataset"], ["OULU - NPU", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "53109398", "sentence": "While considering the averaged ACER of both Replay - Attack and OULU - NPU databases , we set the \u03b3 to 0. 5 in our proposed method .", "ner": [["Replay - Attack", "Dataset"], ["OULU - NPU", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "53109398", "sentence": "More specifically , for ReplayAttack database , \u03b3 should be set to 0. 1 with the averaged ACER= 0 . 5 % ; but for OULU - NPU database , \u03b3 should be set to 1 with the averaged ACER= 5 . 4 % .", "ner": [["ReplayAttack", "Dataset"], ["OULU - NPU", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "53109398", "sentence": "While considering the averaged ACER of both Replay - Attack and OULU - NPU databases , we set the \u03b3 to 0. 5 in our proposed method .", "ner": [["Replay - Attack", "Dataset"], ["OULU - NPU", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "53109398", "sentence": "The reason may lie in that the distribution of real and fake faces in Replay - Attack database is simpler than that of OULU - NPU database , which limits the superiority of P 2 C .", "ner": [["Replay - Attack", "Dataset"], ["OULU - NPU", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "53109398", "sentence": "For a clearer analysis of the generated color - liked space , the sample distributions of Replay - Attack and OULU - NPU databases are described in Fig. 1 0 and Fig. 1 1 , respectively .", "ner": [["Replay - Attack", "Dataset"], ["OULU - NPU", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "53109398", "sentence": "By comparing Fig. 1 0 and Fig. 1 1 , we can find that the ReplayAttack database is easier to be addressed than the OULU - NPU database , which is consistent with the results shown in Table   II .", "ner": [["ReplayAttack", "Dataset"], ["OULU - NPU", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "53109398", "sentence": "For instance , in RGB color space , the averaged APCER , BPCER and ACER of Replay - Attack and OULU - NPU are 6. 1 % , 3. 1 % , 4. 6 % and 1 7 . 7 % , 1 1 . 6 % 1 4 . 7 % , respectively .", "ner": [["Replay - Attack", "Dataset"], ["OULU - NPU", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "53109398", "sentence": "For hand - crafted feature , the LBP extracted from our learned color - liked space takes on better superiority compared with [ 2 9 ] that extracts LBP features from existing color spaces .", "ner": [["LBP", "Method"], ["LBP", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "53109398", "sentence": "More specifically , the colorliked space generator is trained and tuned on one of the + means the actual value is greater than the value . \u2020 was retested on OULU - NPU database . \u2021 was retested on Replay - Attack and OULU - NPU databases . databases and then tested on another database .", "ner": [["colorliked space generator", "Method"], ["OULU - NPU", "Dataset"], ["Replay - Attack", "Dataset"], ["OULU - NPU", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "53109398", "sentence": "When the generator is trained on OULU - NPU and tested on Replay - Attack , we notice that the averaged ACER of VGG features is 4 0 . 1 % .", "ner": [["generator", "Method"], ["OULU - NPU", "Dataset"], ["Replay - Attack", "Dataset"]], "rel": [["generator", "Trained-With", "OULU - NPU"], ["generator", "Evaluated-With", "Replay - Attack"]], "rel_plus": [["generator:Method", "Trained-With", "OULU - NPU:Dataset"], ["generator:Method", "Evaluated-With", "Replay - Attack:Dataset"]]}
{"doc_id": "53109398", "sentence": "When the generator is trained on ReplayAttack and tested on OULU - NPU , the averaged metric is 4 5 . 1 % .", "ner": [["generator", "Method"], ["ReplayAttack", "Dataset"], ["OULU - NPU", "Dataset"]], "rel": [["generator", "Trained-With", "ReplayAttack"], ["generator", "Evaluated-With", "OULU - NPU"]], "rel_plus": [["generator:Method", "Trained-With", "ReplayAttack:Dataset"], ["generator:Method", "Evaluated-With", "OULU - NPU:Dataset"]]}
{"doc_id": "53109398", "sentence": "From these results , we conclude that the generator trained on Replay - Attack is not able to be generalized as good as trained on OULU - NPU .", "ner": [["generator", "Method"], ["Replay - Attack", "Dataset"], ["OULU - NPU", "Dataset"]], "rel": [["generator", "Trained-With", "Replay - Attack"], ["generator", "Trained-With", "OULU - NPU"]], "rel_plus": [["generator:Method", "Trained-With", "Replay - Attack:Dataset"], ["generator:Method", "Trained-With", "OULU - NPU:Dataset"]]}
{"doc_id": "53109398", "sentence": "It is caused that the OULU - NPU database contains more variations in the collecting environment ( e.g. , light and camera quality ) compared to ReplayAttack .", "ner": [["OULU - NPU", "Dataset"], ["ReplayAttack", "Dataset"]], "rel": [["OULU - NPU", "Compare-With", "ReplayAttack"]], "rel_plus": [["OULU - NPU:Dataset", "Compare-With", "ReplayAttack:Dataset"]]}
{"doc_id": "53109398", "sentence": "Based on triplet training and perceptual similarity measure mechanisms , a new ( a ) Replay - Attack database in the generated color - liked space . ( b ) OULU - NPU database in the generated color - liked space . the detection results obtained based on these two different combination mechanisms .", "ner": [["Replay - Attack", "Dataset"], ["OULU - NPU", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "53109398", "sentence": "The reason may lie in that the distribution of real and fake faces in Replay - Attack database is simpler than that of OULU - NPU database , which limits the superiority of P 2 C .", "ner": [["Replay - Attack", "Dataset"], ["OULU - NPU", "Dataset"]], "rel": [["Replay - Attack", "Compare-With", "OULU - NPU"]], "rel_plus": [["Replay - Attack:Dataset", "Compare-With", "OULU - NPU:Dataset"]]}
{"doc_id": "53109398", "sentence": "For a clearer analysis of the generated color - liked space , the sample distributions of Replay - Attack and OULU - NPU databases are described in Fig. 1 0 and Fig. 1 1 , respectively .", "ner": [["Replay - Attack", "Dataset"], ["OULU - NPU", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "53109398", "sentence": "By comparing Fig. 1 0 and Fig. 1 1 , we can find that the ReplayAttack database is easier to be addressed than the OULU - NPU database , which is consistent with the results shown in Table   II .", "ner": [["ReplayAttack", "Dataset"], ["OULU - NPU", "Dataset"]], "rel": [["ReplayAttack", "Compare-With", "OULU - NPU"]], "rel_plus": [["ReplayAttack:Dataset", "Compare-With", "OULU - NPU:Dataset"]]}
{"doc_id": "53109398", "sentence": "For instance , in RGB color space , the averaged APCER , BPCER and ACER of Replay - Attack and OULU - NPU are 6. 1 % , 3. 1 % , 4. 6 % and 1 7 . 7 % , 1 1 . 6 % 1 4 . 7 % , respectively .", "ner": [["Replay - Attack", "Dataset"], ["OULU - NPU", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "53109398", "sentence": "For hand - crafted feature , the LBP extracted from our learned color - liked space takes on better superiority compared with [ 2 9 ] that extracts LBP features from existing color spaces .", "ner": [["LBP", "Method"], ["LBP", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "53109398", "sentence": "Extensive experiments on two latest and challenging presentation attack databases ( the ReplayAttack and OULU - NPU ) showed excellent results .", "ner": [["ReplayAttack", "Dataset"], ["OULU - NPU", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "53109398", "sentence": "On OULU - NPU database , the proposed color - liked space based method outperformed the baseline , while very competitive results were achieved on Replay - Attack database .", "ner": [["OULU - NPU", "Dataset"], ["Replay - Attack", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "53109398", "sentence": "Overall , from the results of Replay - Attack and OULU - NPU databases , we find that external - environment factors ( e.g. light and camera quality ) limit the effectiveness of our proposed detection method .", "ner": [["Replay - Attack", "Dataset"], ["OULU - NPU", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "53109398", "sentence": "Inspired by generative adversarial network ( GAN ) [ 2 1 ] , a color - liked space generator is constructed to map existing color spaces .", "ner": [["generative adversarial network", "Method"], ["GAN", "Method"]], "rel": [["GAN", "Synonym-Of", "generative adversarial network"]], "rel_plus": [["GAN:Method", "Synonym-Of", "generative adversarial network:Method"]]}
{"doc_id": "53109398", "sentence": "Finally , the extracted features are fed into a Support Vector Machine ( SVM ) [ 2 2 ] classifier to detect face presentation attack .", "ner": [["Support Vector Machine", "Method"], ["SVM", "Method"]], "rel": [["SVM", "Synonym-Of", "Support Vector Machine"]], "rel_plus": [["SVM:Method", "Synonym-Of", "Support Vector Machine:Method"]]}
{"doc_id": "53109398", "sentence": "We train and test our proposed method on two public available databases : Replay - Attack [ 1 0 ] and OULU - NPU [ 2 3 ] .", "ner": [["Replay - Attack", "Dataset"], ["OULU - NPU", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "53109398", "sentence": "Unlike traditional mechanism , the proposed pointsto - set mechanism can guarantee a stable decline in triplet loss . 3 ) Extensive experimental analysis is conducted on the two latest and challenging face PAD databases using their pre - defined publicly well - defined experimental evaluation protocols ensuring the reproducibility of the results and a fair comparison with the state - of - the - art methods .", "ner": [["pointsto - set mechanism", "Method"], ["triplet loss", "Method"], ["face PAD", "Task"]], "rel": [["pointsto - set mechanism", "Part-Of", "triplet loss"]], "rel_plus": [["pointsto - set mechanism:Method", "Part-Of", "triplet loss:Method"]]}
{"doc_id": "53109398", "sentence": "The remainder of the paper is organized as follows : Section II reviews the existing state - of - the - art methods of face PAD and briefly provides the development of triplet network .", "ner": [["face PAD", "Task"], ["triplet network", "Method"]], "rel": [["triplet network", "Used-For", "face PAD"]], "rel_plus": [["triplet network:Method", "Used-For", "face PAD:Task"]]}
{"doc_id": "53109398", "sentence": "A. State - of - the - art of Face PAD In the past few years , face PAD has received great attention and many detection approaches have been developed [ 1 0 ] , [ 1 1 ] , [ 1 2 ] , [ 1 3 ] , [ 1 4 ] , [ 1 5 ] , [ 1 6 ] , [ 1 7 ] , [ 1 8 ] , [ 1 9 ] , [ 2 0 ] .", "ner": [["Face PAD", "Task"], ["face PAD", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "53109398", "sentence": "Based on different clues , these countermeasures can be further categorized into texture analysis [ 1 0 ] , [ 1 2 ] , [ 1 3 ] , motion analysis [ 1 4 ] , [ 1 5 ] , [ 2 5 ] , image quality analysis [ 1 6 ] , [ 1 7 ] , [ 1 8 ] , [ 1 9 ] , [ 2 0 ] , and hardware based methods [ 1 1 ] , [ 1 9 ] , [ 2 0 ] , [ 2 6 ] , [ 2 7 ] , [ 2 8 ] . 1 ) Texture analysis based methods : Due to the limitations of printers and display devices , there are substantial differences in color distribution and edge texture between real and fake faces .", "ner": [["texture analysis", "Task"], ["motion analysis", "Task"], ["image quality analysis", "Task"], ["hardware based methods", "Method"], ["Texture analysis based methods", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "53109398", "sentence": "Therefore , multi - scale local binary pattern ( LBP ) features [ 1 2 ] were extracted to describe edge texture differences .", "ner": [["multi - scale local binary pattern", "Method"], ["LBP", "Method"]], "rel": [["LBP", "Synonym-Of", "multi - scale local binary pattern"]], "rel_plus": [["LBP:Method", "Synonym-Of", "multi - scale local binary pattern:Method"]]}
{"doc_id": "53109398", "sentence": "To capture the color differences in luminance and chrominance , Boulkenafet et al. [ 1 3 ] , [ 2 9 ] proposed a method by computing LBP features from different color spaces and concatenating all LBP into a single feature vector .", "ner": [["LBP", "Method"], ["LBP", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "53109398", "sentence": "Instead of using LBP features , Akshay et al. [ 3 1 ] extracted Haralick [ 3 2 ] texture features for face PAD .", "ner": [["LBP", "Method"], ["face PAD", "Task"]], "rel": [["LBP", "Used-For", "face PAD"]], "rel_plus": [["LBP:Method", "Used-For", "face PAD:Task"]]}
{"doc_id": "53109398", "sentence": "With the recent advances of deep learning in computer vision [ 3 3 ] , [ 3 4 ] , deep textures have also been applied in face PAD .", "ner": [["deep learning", "Method"], ["computer vision", "Task"], ["face PAD", "Task"]], "rel": [["deep learning", "Used-For", "computer vision"], ["deep learning", "Used-For", "face PAD"]], "rel_plus": [["deep learning:Method", "Used-For", "computer vision:Task"], ["deep learning:Method", "Used-For", "face PAD:Task"]]}
{"doc_id": "53109398", "sentence": "Yang et al. [ 3 5 ] proposed an end - to - end convolutional neural network ( CNN ) model for face PAD .", "ner": [["convolutional neural network", "Method"], ["CNN", "Method"], ["face PAD", "Task"]], "rel": [["CNN", "Synonym-Of", "convolutional neural network"], ["convolutional neural network", "Used-For", "face PAD"]], "rel_plus": [["CNN:Method", "Synonym-Of", "convolutional neural network:Method"], ["convolutional neural network:Method", "Used-For", "face PAD:Task"]]}
{"doc_id": "53109398", "sentence": "For capturing texture variations , Xu et al. [ 3 8 ] proposed a long short memory network ( LSTM ) and Li et al. [ 3 9 ] proposed a 3D CNN to detect face presentation attacks , respectively .", "ner": [["long short memory network", "Method"], ["LSTM", "Method"], ["3D CNN", "Method"], ["detect face presentation attacks", "Task"]], "rel": [["LSTM", "Synonym-Of", "long short memory network"], ["3D CNN", "Used-For", "detect face presentation attacks"], ["long short memory network", "Used-For", "detect face presentation attacks"]], "rel_plus": [["LSTM:Method", "Synonym-Of", "long short memory network:Method"], ["3D CNN:Method", "Used-For", "detect face presentation attacks:Task"], ["long short memory network:Method", "Used-For", "detect face presentation attacks:Task"]]}
{"doc_id": "210702798", "sentence": "Image segmentation is a key topic in image processing and computer vision with applications such as scene understanding , medical image analysis , robotic perception , video surveillance , augmented reality , and image compression , among many others .", "ner": [["computer vision", "Task"], ["scene understanding", "Task"], ["medical image analysis", "Task"], ["robotic perception", "Task"], ["video surveillance", "Task"], ["augmented reality", "Task"], ["image compression", "Task"]], "rel": [["scene understanding", "SubTask-Of", "computer vision"], ["medical image analysis", "SubTask-Of", "computer vision"], ["robotic perception", "SubTask-Of", "computer vision"], ["video surveillance", "SubTask-Of", "computer vision"], ["augmented reality", "SubTask-Of", "computer vision"], ["image compression", "SubTask-Of", "computer vision"]], "rel_plus": [["scene understanding:Task", "SubTask-Of", "computer vision:Task"], ["medical image analysis:Task", "SubTask-Of", "computer vision:Task"], ["robotic perception:Task", "SubTask-Of", "computer vision:Task"], ["video surveillance:Task", "SubTask-Of", "computer vision:Task"], ["augmented reality:Task", "SubTask-Of", "computer vision:Task"], ["image compression:Task", "SubTask-Of", "computer vision:Task"]]}
{"doc_id": "210702798", "sentence": "In this survey , we provide a comprehensive review of the literature at the time of this writing , covering a broad spectrum of pioneering works for semantic and instance - level segmentation , including fully convolutional pixel - labeling networks , encoder - decoder architectures , multi - scale and pyramid based approaches , recurrent networks , visual attention models , and generative models in adversarial settings .", "ner": [["semantic and instance - level segmentation", "Task"], ["fully convolutional pixel - labeling networks", "Method"], ["encoder - decoder architectures", "Method"], ["multi - scale and pyramid based approaches", "Method"], ["recurrent networks", "Method"], ["visual attention", "Method"], ["generative models", "Method"]], "rel": [["fully convolutional pixel - labeling networks", "Used-For", "semantic and instance - level segmentation"], ["encoder - decoder architectures", "Used-For", "semantic and instance - level segmentation"], ["multi - scale and pyramid based approaches", "Used-For", "semantic and instance - level segmentation"], ["recurrent networks", "Used-For", "semantic and instance - level segmentation"], ["visual attention", "Used-For", "semantic and instance - level segmentation"], ["generative models", "Used-For", "semantic and instance - level segmentation"]], "rel_plus": [["fully convolutional pixel - labeling networks:Method", "Used-For", "semantic and instance - level segmentation:Task"], ["encoder - decoder architectures:Method", "Used-For", "semantic and instance - level segmentation:Task"], ["multi - scale and pyramid based approaches:Method", "Used-For", "semantic and instance - level segmentation:Task"], ["recurrent networks:Method", "Used-For", "semantic and instance - level segmentation:Task"], ["visual attention:Method", "Used-For", "semantic and instance - level segmentation:Task"], ["generative models:Method", "Used-For", "semantic and instance - level segmentation:Task"]]}
{"doc_id": "210702798", "sentence": "Segmentation plays a central role in a broad range of applications [ 2 ] , including medical image analysis ( e.g. , tumor boundary extraction and measurement of tissue volumes ) , autonomous vehicles ( e.g. , navigable surface and pedestrian detection ) , video surveillance , and augmented reality to count a few .", "ner": [["Segmentation", "Task"], ["medical image analysis", "Task"], ["tumor boundary extraction", "Task"], ["measurement of tissue volumes", "Task"], ["autonomous vehicles", "Task"], ["navigable surface", "Task"], ["pedestrian detection", "Task"], ["video surveillance", "Task"], ["augmented reality", "Task"]], "rel": [["Segmentation", "Used-For", "medical image analysis"], ["tumor boundary extraction", "SubTask-Of", "medical image analysis"], ["measurement of tissue volumes", "SubTask-Of", "medical image analysis"], ["Segmentation", "Used-For", "autonomous vehicles"], ["navigable surface", "SubTask-Of", "autonomous vehicles"], ["pedestrian detection", "SubTask-Of", "autonomous vehicles"], ["Segmentation", "Used-For", "video surveillance"], ["Segmentation", "Used-For", "augmented reality"]], "rel_plus": [["Segmentation:Task", "Used-For", "medical image analysis:Task"], ["tumor boundary extraction:Task", "SubTask-Of", "medical image analysis:Task"], ["measurement of tissue volumes:Task", "SubTask-Of", "medical image analysis:Task"], ["Segmentation:Task", "Used-For", "autonomous vehicles:Task"], ["navigable surface:Task", "SubTask-Of", "autonomous vehicles:Task"], ["pedestrian detection:Task", "SubTask-Of", "autonomous vehicles:Task"], ["Segmentation:Task", "Used-For", "video surveillance:Task"], ["Segmentation:Task", "Used-For", "augmented reality:Task"]]}
{"doc_id": "210702798", "sentence": "Numerous image segmentation algorithms have been developed in the literature , from the earliest methods , such as thresholding [ 3 ] , histogram - based bundling , regiongrowing [ 4 ] , k - means clustering [ 5 ] , watersheds [ 6 ] , to more advanced algorithms such as active contours [ 7 ] , graph cuts [ 8 ] , conditional and Markov random fields [ 9 ] , and sparsitybased [ 1 0 ] - [ 1 1 ] methods .", "ner": [["image segmentation algorithms", "Method"], ["thresholding", "Method"], ["histogram - based bundling", "Method"], ["regiongrowing", "Method"], ["k - means clustering", "Method"], ["watersheds", "Method"], ["active contours", "Method"], ["graph cuts", "Method"], ["conditional and Markov random fields", "Method"]], "rel": [["thresholding", "SubClass-Of", "image segmentation algorithms"], ["histogram - based bundling", "SubClass-Of", "image segmentation algorithms"], ["regiongrowing", "SubClass-Of", "image segmentation algorithms"], ["k - means clustering", "SubClass-Of", "image segmentation algorithms"], ["watersheds", "SubClass-Of", "image segmentation algorithms"], ["active contours", "SubClass-Of", "image segmentation algorithms"], ["graph cuts", "SubClass-Of", "image segmentation algorithms"], ["conditional and Markov random fields", "SubClass-Of", "image segmentation algorithms"]], "rel_plus": [["thresholding:Method", "SubClass-Of", "image segmentation algorithms:Method"], ["histogram - based bundling:Method", "SubClass-Of", "image segmentation algorithms:Method"], ["regiongrowing:Method", "SubClass-Of", "image segmentation algorithms:Method"], ["k - means clustering:Method", "SubClass-Of", "image segmentation algorithms:Method"], ["watersheds:Method", "SubClass-Of", "image segmentation algorithms:Method"], ["active contours:Method", "SubClass-Of", "image segmentation algorithms:Method"], ["graph cuts:Method", "SubClass-Of", "image segmentation algorithms:Method"], ["conditional and Markov random fields:Method", "SubClass-Of", "image segmentation algorithms:Method"]]}
{"doc_id": "210702798", "sentence": "For example , Figure 1 presents sample image segmentation outputs of a prominent deep learning model , DeepLabv 3 [ 1 2 ] .", "ner": [["image segmentation", "Task"], ["DeepLabv 3", "Method"]], "rel": [["DeepLabv 3", "Used-For", "image segmentation"]], "rel_plus": [["DeepLabv 3:Method", "Used-For", "image segmentation:Task"]]}
{"doc_id": "210702798", "sentence": "Image segmentation can be formulated as a classification problem of pixels with semantic labels ( semantic segmentation ) or partitioning of individual objects ( instance segmentation ) .", "ner": [["Image segmentation", "Task"], ["semantic segmentation", "Task"], ["instance segmentation", "Task"]], "rel": [["semantic segmentation", "SubTask-Of", "Image segmentation"], ["instance segmentation", "SubTask-Of", "Image segmentation"]], "rel_plus": [["semantic segmentation:Task", "SubTask-Of", "Image segmentation:Task"], ["instance segmentation:Task", "SubTask-Of", "Image segmentation:Task"]]}
{"doc_id": "210702798", "sentence": "Semantic segmentation performs pixel - level labeling with a set of object categories ( e.g. , human , car , tree , sky ) for all image pixels , thus it is generally a harder undertaking than image classification , which predicts a \u2022 S. Minaee is with Expedia Inc , and New York University . single label for the entire image .", "ner": [["Semantic segmentation", "Task"], ["pixel - level labeling", "Task"], ["image classification", "Task"]], "rel": [["Semantic segmentation", "Used-For", "pixel - level labeling"], ["Semantic segmentation", "Compare-With", "image classification"]], "rel_plus": [["Semantic segmentation:Task", "Used-For", "pixel - level labeling:Task"], ["Semantic segmentation:Task", "Compare-With", "image classification:Task"]]}
{"doc_id": "210702798", "sentence": "Instance segmentation extends semantic segmentation scope further by detecting and delineating each object of interest in the image ( e.g. , partitioning of individual persons ) .", "ner": [["Instance segmentation", "Task"], ["semantic segmentation", "Task"]], "rel": [["Instance segmentation", "SubTask-Of", "semantic segmentation"]], "rel_plus": [["Instance segmentation:Task", "SubTask-Of", "semantic segmentation:Task"]]}
{"doc_id": "210702798", "sentence": "Our survey covers the most recent literature in image segmentation and discusses more than a hundred deep learning - based segmentation methods proposed until 2 0 1 9 .", "ner": [["image segmentation", "Task"], ["deep learning - based segmentation methods", "Method"]], "rel": [["deep learning - based segmentation methods", "Used-For", "image segmentation"]], "rel_plus": [["deep learning - based segmentation methods:Method", "Used-For", "image segmentation:Task"]]}
{"doc_id": "210702798", "sentence": "CV ] 1 8 Jan 2 0 2 0 1 ) Fully convolutional networks 2 ) Convolutional models with graphical models 3 ) Encoder - decoder based models 4 ) Multi - scale and pyramid network based models 5 ) R - CNN based models ( for instance segmentation ) 6 ) Dilated convolutional models and DeepLab family 7 ) Recurrent neural network based models 8) Attention - based models 9 ) Generative models and adversarial training 1 0 ) Convolutional models with active contour models 1 1 ) Other models Some the key contributions of this survey paper can be summarized as follows : \u2022 This survey covers the contemporary literature with respect to segmentation problem , and overviews more than 1 0 0 segmentation algorithms proposed till 2 0 1 9 , grouped into 1 0 categories .", "ner": [["Fully convolutional networks", "Method"], ["Convolutional models with graphical models", "Method"], ["Encoder - decoder based models", "Method"], ["Multi - scale and pyramid network based models", "Method"], ["R - CNN based models", "Method"], ["instance segmentation", "Task"], ["Dilated convolutional models", "Method"], ["DeepLab", "Method"], ["Recurrent neural network based models", "Method"], ["Attention - based models", "Method"], ["Generative models", "Method"], ["Convolutional models", "Method"], ["active contour models", "Method"], ["segmentation", "Task"], ["segmentation", "Task"]], "rel": [["R - CNN based models", "Used-For", "instance segmentation"]], "rel_plus": [["R - CNN based models:Method", "Used-For", "instance segmentation:Task"]]}
{"doc_id": "210702798", "sentence": "\u2022 We provide a comprehensive review and an insightful analysis of different aspects of segmentation algorithms using deep learning , including the training data , the choice of network architectures , loss functions , training strategies , and their key contributions . \u2022 We provide an overview of around 2 0 popular image segmentation datasets , grouped into 2D , 2. 5 D ( RGB - D ) , and 3D images . \u2022 We provide a comparative summary of the properties and performance of the reviewed methods for segmentation purposes , on popular benchmarks . \u2022 We provide several challenges and potential future directions for deep learning - based image segmentation .", "ner": [["segmentation", "Task"], ["deep learning", "Method"], ["image segmentation", "Task"], ["segmentation", "Task"], ["deep learning", "Method"], ["image segmentation", "Task"]], "rel": [["deep learning", "Used-For", "segmentation"], ["deep learning", "Used-For", "image segmentation"]], "rel_plus": [["deep learning:Method", "Used-For", "segmentation:Task"], ["deep learning:Method", "Used-For", "image segmentation:Task"]]}
{"doc_id": "210702798", "sentence": "This section provides an overview of some of the most prominent deep learning architectures used by the computer vision community , including convolutional neural networks ( CNNs ) [ 1 3 ] , recurrent neural networks ( RNNs ) and long short term memory ( LSTM ) [ 1 4 ] , encoder - decoders [ 1 5 ] , and generative adversarial networks ( GANs ) [ 1 6 ] .", "ner": [["computer vision", "Task"], ["convolutional neural networks", "Method"], ["CNNs", "Method"], ["recurrent neural networks", "Method"], ["RNNs", "Method"], ["long short term memory", "Method"], ["LSTM", "Method"], ["encoder - decoders", "Method"], ["generative adversarial networks", "Method"], ["GANs", "Method"]], "rel": [["convolutional neural networks", "Used-For", "computer vision"], ["recurrent neural networks", "Used-For", "computer vision"], ["long short term memory", "Used-For", "computer vision"], ["encoder - decoders", "Used-For", "computer vision"], ["generative adversarial networks", "Used-For", "computer vision"], ["CNNs", "Synonym-Of", "convolutional neural networks"], ["RNNs", "Synonym-Of", "recurrent neural networks"], ["LSTM", "Synonym-Of", "long short term memory"], ["GANs", "Synonym-Of", "generative adversarial networks"]], "rel_plus": [["convolutional neural networks:Method", "Used-For", "computer vision:Task"], ["recurrent neural networks:Method", "Used-For", "computer vision:Task"], ["long short term memory:Method", "Used-For", "computer vision:Task"], ["encoder - decoders:Method", "Used-For", "computer vision:Task"], ["generative adversarial networks:Method", "Used-For", "computer vision:Task"], ["CNNs:Method", "Synonym-Of", "convolutional neural networks:Method"], ["RNNs:Method", "Synonym-Of", "recurrent neural networks:Method"], ["LSTM:Method", "Synonym-Of", "long short term memory:Method"], ["GANs:Method", "Synonym-Of", "generative adversarial networks:Method"]]}
{"doc_id": "210702798", "sentence": "With the popularity of deep learning in recent years , several other deep neural architectures have been proposed , such as transformers , capsule networks , gated recurrent units , spatial transformer networks , etc . , which will not be covered here .", "ner": [["deep learning", "Method"], ["deep neural architectures", "Method"], ["transformers", "Method"], ["capsule networks", "Method"], ["gated recurrent units", "Method"], ["spatial transformer networks", "Method"]], "rel": [["transformers", "SubClass-Of", "deep neural architectures"], ["capsule networks", "SubClass-Of", "deep neural architectures"], ["gated recurrent units", "SubClass-Of", "deep neural architectures"], ["spatial transformer networks", "SubClass-Of", "deep neural architectures"]], "rel_plus": [["transformers:Method", "SubClass-Of", "deep neural architectures:Method"], ["capsule networks:Method", "SubClass-Of", "deep neural architectures:Method"], ["gated recurrent units:Method", "SubClass-Of", "deep neural architectures:Method"], ["spatial transformer networks:Method", "SubClass-Of", "deep neural architectures:Method"]]}
{"doc_id": "210702798", "sentence": "CNNs are among the most successful and widely used architectures in the deep learning community , especially for computer vision tasks .", "ner": [["CNNs", "Method"], ["deep learning", "Method"], ["computer vision", "Task"]], "rel": [["CNNs", "SubClass-Of", "deep learning"], ["CNNs", "Used-For", "computer vision"]], "rel_plus": [["CNNs:Method", "SubClass-Of", "deep learning:Method"], ["CNNs:Method", "Used-For", "computer vision:Task"]]}
{"doc_id": "210702798", "sentence": "Subsequently , Waibel et al. [ 1 8 ] introduced CNNs with weights shared among temporal receptive fields and backpropagation training for phoneme recognition , and LeCun et al. [ 1 3 ] developed a CNN architecture for document recognition ( Figure 2 ) .", "ner": [["CNNs", "Method"], ["backpropagation", "Method"], ["phoneme recognition", "Task"], ["CNN", "Method"], ["document recognition", "Task"]], "rel": [["CNNs", "Used-For", "phoneme recognition"], ["CNN", "Used-For", "document recognition"]], "rel_plus": [["CNNs:Method", "Used-For", "phoneme recognition:Task"], ["CNN:Method", "Used-For", "document recognition:Task"]]}
{"doc_id": "210702798", "sentence": "Some of the most well - known CNN architectures include : AlexNet [ 1 9 ] , VGGNet [ 2 0 ] , ResNet [ 2 1 ] , GoogLeNet [ 2 2 ] , MobileNet [ 2 3 ] , and DenseNet [ 2 4 ] .", "ner": [["CNN", "Method"], ["AlexNet", "Method"], ["VGGNet", "Method"], ["ResNet", "Method"], ["GoogLeNet", "Method"], ["MobileNet", "Method"], ["DenseNet", "Method"]], "rel": [["AlexNet", "SubClass-Of", "CNN"], ["VGGNet", "SubClass-Of", "CNN"], ["ResNet", "SubClass-Of", "CNN"], ["GoogLeNet", "SubClass-Of", "CNN"], ["MobileNet", "SubClass-Of", "CNN"], ["DenseNet", "SubClass-Of", "CNN"]], "rel_plus": [["AlexNet:Method", "SubClass-Of", "CNN:Method"], ["VGGNet:Method", "SubClass-Of", "CNN:Method"], ["ResNet:Method", "SubClass-Of", "CNN:Method"], ["GoogLeNet:Method", "SubClass-Of", "CNN:Method"], ["MobileNet:Method", "SubClass-Of", "CNN:Method"], ["DenseNet:Method", "SubClass-Of", "CNN:Method"]]}
{"doc_id": "210702798", "sentence": "However , a type of RNNs called Long Short Term Memory ( LSTM ) [ 1 4 ] is designed to avoid these issues .", "ner": [["RNNs", "Method"], ["Long Short Term Memory", "Method"], ["LSTM", "Method"]], "rel": [["Long Short Term Memory", "SubClass-Of", "RNNs"], ["LSTM", "Synonym-Of", "Long Short Term Memory"]], "rel_plus": [["Long Short Term Memory:Method", "SubClass-Of", "RNNs:Method"], ["LSTM:Method", "Synonym-Of", "Long Short Term Memory:Method"]]}
{"doc_id": "210702798", "sentence": "The output here could be an enhanced version of the image ( such as in image de - blurring , or super - resolution ) , or a segmentation map .", "ner": [["image de - blurring", "Task"], ["super - resolution", "Task"], ["segmentation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "210702798", "sentence": "One of the most popular is the stacked denoising auto - encoder ( SDAE ) [ 2 6 ] , which stacks several auto - encoders and uses them for image denoising purposes .", "ner": [["stacked denoising auto - encoder", "Method"], ["SDAE", "Method"], ["auto - encoders", "Method"], ["image denoising", "Task"]], "rel": [["SDAE", "Synonym-Of", "stacked denoising auto - encoder"], ["auto - encoders", "Part-Of", "stacked denoising auto - encoder"], ["stacked denoising auto - encoder", "Used-For", "image denoising"]], "rel_plus": [["SDAE:Method", "Synonym-Of", "stacked denoising auto - encoder:Method"], ["auto - encoders:Method", "Part-Of", "stacked denoising auto - encoder:Method"], ["stacked denoising auto - encoder:Method", "Used-For", "image denoising:Task"]]}
{"doc_id": "210702798", "sentence": "Another popular variant is the variational auto - encoder ( VAE ) [ 2 7 ] , which imposes a prior distribution on the latent representation .", "ner": [["variational auto - encoder", "Method"], ["VAE", "Method"]], "rel": [["VAE", "Synonym-Of", "variational auto - encoder"]], "rel_plus": [["VAE:Method", "Synonym-Of", "variational auto - encoder:Method"]]}
{"doc_id": "210702798", "sentence": "The GAN loss function may be written as We can regard the GAN as a minimax game between G and D , where D is trying to minimize its classification error in distinguishing fake samples from real ones , hence maximizing the loss function , and G is trying to maximize the discriminator network 's error , hence minimizing the loss function .", "ner": [["GAN loss function", "Method"], ["GAN", "Method"], ["classification", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "210702798", "sentence": "Since the invention of GANs , researchers have endeavored to improve/modify GANs several ways .", "ner": [["GANs", "Method"], ["GANs", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210702798", "sentence": "For example , Radford et al. [ 2 8 ] proposed a convolutional GAN model , which works better than fully - connected networks when used for image generation .", "ner": [["convolutional GAN", "Method"], ["fully - connected networks", "Method"], ["image generation", "Task"]], "rel": [["convolutional GAN", "Compare-With", "fully - connected networks"], ["convolutional GAN", "Used-For", "image generation"], ["fully - connected networks", "Used-For", "image generation"]], "rel_plus": [["convolutional GAN:Method", "Compare-With", "fully - connected networks:Method"], ["convolutional GAN:Method", "Used-For", "image generation:Task"], ["fully - connected networks:Method", "Used-For", "image generation:Task"]]}
{"doc_id": "210702798", "sentence": "For example , one can imagine adapting an image classification model trained on ImageNet to a different task , such as texture classification , or face recognition .", "ner": [["image classification model", "Method"], ["ImageNet", "Dataset"], ["texture classification", "Task"], ["face recognition", "Task"]], "rel": [["image classification model", "Trained-With", "ImageNet"], ["image classification model", "Used-For", "texture classification"], ["image classification model", "Used-For", "face recognition"]], "rel_plus": [["image classification model:Method", "Trained-With", "ImageNet:Dataset"], ["image classification model:Method", "Used-For", "texture classification:Task"], ["image classification model:Method", "Used-For", "face recognition:Task"]]}
{"doc_id": "210702798", "sentence": "In image segmentation case , many people use a model trained on ImageNet ( a larger dataset than most of image segmentation datasets ) , as the encoder part of the network , and re - train their model from those initial weights .", "ner": [["image segmentation", "Task"], ["ImageNet", "Dataset"], ["image segmentation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "210702798", "sentence": "Long et al. [ 3 2 ] proposed one of the first deep learning works for semantic image segmentation , using a fully convolutional network ( FCN ) .", "ner": [["deep learning", "Method"], ["semantic image segmentation", "Task"], ["fully convolutional network", "Method"], ["FCN", "Method"]], "rel": [["deep learning", "Used-For", "semantic image segmentation"], ["fully convolutional network", "Used-For", "semantic image segmentation"], ["FCN", "Synonym-Of", "fully convolutional network"]], "rel_plus": [["deep learning:Method", "Used-For", "semantic image segmentation:Task"], ["fully convolutional network:Method", "Used-For", "semantic image segmentation:Task"], ["FCN:Method", "Synonym-Of", "fully convolutional network:Method"]]}
{"doc_id": "210702798", "sentence": "An FCN ( Figure 7 ) includes only convolutional layers , which enables it to take an image of arbitrary size and produce a segmentation map of the same size .", "ner": [["FCN", "Method"], ["convolutional layers", "Method"], ["segmentation map", "Task"]], "rel": [["convolutional layers", "Part-Of", "FCN"], ["FCN", "Used-For", "segmentation map"]], "rel_plus": [["convolutional layers:Method", "Part-Of", "FCN:Method"], ["FCN:Method", "Used-For", "segmentation map:Task"]]}
{"doc_id": "210702798", "sentence": "The authors modified existing CNN architectures , such as VGG 1 6 and GoogLeNet , to manage non - fixed sized input and output , by replacing all fully - connected layers with the fully - convolutional layers .", "ner": [["CNN", "Method"], ["VGG 1 6", "Method"], ["GoogLeNet", "Method"]], "rel": [["VGG 1 6", "SubClass-Of", "CNN"], ["GoogLeNet", "SubClass-Of", "CNN"]], "rel_plus": [["VGG 1 6:Method", "SubClass-Of", "CNN:Method"], ["GoogLeNet:Method", "SubClass-Of", "CNN:Method"]]}
{"doc_id": "210702798", "sentence": "As a result , the model outputs a spatial segmentation map instead of classification scores .", "ner": [["spatial segmentation map", "Task"], ["classification", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "210702798", "sentence": "The model was tested on PASCAL VOC , NYUDv 2 , and SIFT Flow , and achieved state - of - the - art segmentation performance .", "ner": [["PASCAL VOC", "Dataset"], ["NYUDv 2", "Dataset"], ["SIFT Flow", "Dataset"], ["segmentation", "Task"]], "rel": [["PASCAL VOC", "Benchmark-For", "segmentation"], ["NYUDv 2", "Benchmark-For", "segmentation"], ["SIFT Flow", "Benchmark-For", "segmentation"]], "rel_plus": [["PASCAL VOC:Dataset", "Benchmark-For", "segmentation:Task"], ["NYUDv 2:Dataset", "Benchmark-For", "segmentation:Task"], ["SIFT Flow:Dataset", "Benchmark-For", "segmentation:Task"]]}
{"doc_id": "210702798", "sentence": "This work is considered a milestone in image segmentation , demonstrating that deep networks can be trained for semantic segmentation in an end - to - end manner on variablesized images .", "ner": [["image segmentation", "Task"], ["semantic segmentation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "210702798", "sentence": "For instance , Liu et al. [ 3 3 ] proposed a model called ParseNet , to address an issue with FCN - ignoring global context information .", "ner": [["ParseNet", "Method"], ["FCN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210702798", "sentence": "ParseNet adds global context to FCNs by using the average feature for a layer to augment the features at each location .", "ner": [["ParseNet", "Method"], ["FCNs", "Method"]], "rel": [["FCNs", "SubClass-Of", "ParseNet"]], "rel_plus": [["FCNs:Method", "SubClass-Of", "ParseNet:Method"]]}
{"doc_id": "210702798", "sentence": "In a nutshell , ParseNet is an FCN with the described module replacing the convolutional layers ( Figure 9 ) .", "ner": [["ParseNet", "Method"], ["FCN", "Method"], ["convolutional layers", "Method"]], "rel": [["convolutional layers", "Part-Of", "ParseNet"], ["ParseNet", "SubClass-Of", "FCN"]], "rel_plus": [["convolutional layers:Method", "Part-Of", "ParseNet:Method"], ["ParseNet:Method", "SubClass-Of", "FCN:Method"]]}
{"doc_id": "210702798", "sentence": "FCNs have been applied to a variety of segmentation problems , such as brain tumor segmentation [ 3 4 ] , instanceaware semantic segmentation [ 3 5 ] , skin lesion segmentation [ 3 6 ] , and iris segmentation [ 3 7 ] .", "ner": [["FCNs", "Method"], ["segmentation", "Task"], ["brain tumor segmentation", "Task"], ["instanceaware semantic segmentation", "Task"], ["skin lesion segmentation", "Task"], ["iris segmentation", "Task"]], "rel": [["FCNs", "Used-For", "segmentation"], ["brain tumor segmentation", "SubTask-Of", "segmentation"], ["instanceaware semantic segmentation", "SubTask-Of", "segmentation"], ["skin lesion segmentation", "SubTask-Of", "segmentation"], ["iris segmentation", "SubTask-Of", "segmentation"], ["FCNs", "Used-For", "brain tumor segmentation"], ["FCNs", "Used-For", "instanceaware semantic segmentation"], ["FCNs", "Used-For", "skin lesion segmentation"], ["FCNs", "Used-For", "iris segmentation"]], "rel_plus": [["FCNs:Method", "Used-For", "segmentation:Task"], ["brain tumor segmentation:Task", "SubTask-Of", "segmentation:Task"], ["instanceaware semantic segmentation:Task", "SubTask-Of", "segmentation:Task"], ["skin lesion segmentation:Task", "SubTask-Of", "segmentation:Task"], ["iris segmentation:Task", "SubTask-Of", "segmentation:Task"], ["FCNs:Method", "Used-For", "brain tumor segmentation:Task"], ["FCNs:Method", "Used-For", "instanceaware semantic segmentation:Task"], ["FCNs:Method", "Used-For", "skin lesion segmentation:Task"], ["FCNs:Method", "Used-For", "iris segmentation:Task"]]}
{"doc_id": "210702798", "sentence": "To integrate more context , several ap - proaches incorporate probabilistic graphical models , such as Conditional Random Fields ( CRFs ) and Markov Random Field ( MRFs ) , into DL architectures .", "ner": [["probabilistic graphical models", "Method"], ["Conditional Random Fields", "Method"], ["CRFs", "Method"], ["Markov Random Field", "Method"], ["MRFs", "Method"], ["DL architectures", "Method"]], "rel": [["Conditional Random Fields", "SubClass-Of", "probabilistic graphical models"], ["Markov Random Field", "SubClass-Of", "probabilistic graphical models"], ["CRFs", "Synonym-Of", "Conditional Random Fields"], ["MRFs", "Synonym-Of", "Markov Random Field"], ["Conditional Random Fields", "Part-Of", "DL architectures"], ["Markov Random Field", "Part-Of", "DL architectures"], ["probabilistic graphical models", "Part-Of", "DL architectures"]], "rel_plus": [["Conditional Random Fields:Method", "SubClass-Of", "probabilistic graphical models:Method"], ["Markov Random Field:Method", "SubClass-Of", "probabilistic graphical models:Method"], ["CRFs:Method", "Synonym-Of", "Conditional Random Fields:Method"], ["MRFs:Method", "Synonym-Of", "Markov Random Field:Method"], ["Conditional Random Fields:Method", "Part-Of", "DL architectures:Method"], ["Markov Random Field:Method", "Part-Of", "DL architectures:Method"], ["probabilistic graphical models:Method", "Part-Of", "DL architectures:Method"]]}
{"doc_id": "210702798", "sentence": "Chen et al. [ 3 8 ] proposed a semantic segmentation algorithm based on the combination of CNNs and fully connected CRFs ( Figure 1 0 ) .", "ner": [["semantic segmentation algorithm", "Method"], ["CNNs", "Method"], ["fully connected CRFs", "Method"]], "rel": [["CNNs", "Part-Of", "semantic segmentation algorithm"], ["fully connected CRFs", "Part-Of", "semantic segmentation algorithm"]], "rel_plus": [["CNNs:Method", "Part-Of", "semantic segmentation algorithm:Method"], ["fully connected CRFs:Method", "Part-Of", "semantic segmentation algorithm:Method"]]}
{"doc_id": "210702798", "sentence": "They showed that responses from the final layer of deep CNNs are not sufficiently localized for accurate object segmentation ( due to the invariance properties that make CNNs good for high level tasks such as classification ) .", "ner": [["CNNs", "Method"], ["object segmentation", "Task"], ["CNNs", "Method"], ["classification", "Task"]], "rel": [["CNNs", "Used-For", "classification"]], "rel_plus": [["CNNs:Method", "Used-For", "classification:Task"]]}
{"doc_id": "210702798", "sentence": "To overcome the poor localization property of deep CNNs , they combined the responses at the final CNN layer with a fully - connected CRF .", "ner": [["CNNs", "Method"], ["CNN", "Method"], ["fully - connected CRF", "Method"]], "rel": [["fully - connected CRF", "Part-Of", "CNN"]], "rel_plus": [["fully - connected CRF:Method", "Part-Of", "CNN:Method"]]}
{"doc_id": "210702798", "sentence": "The coarse score map of a CNN is upsampled via interpolated interpolation , and fed to a fully - connected CRF to refine the segmentation result .", "ner": [["CNN", "Method"], ["interpolated interpolation", "Method"], ["fully - connected CRF", "Method"], ["segmentation", "Task"]], "rel": [["interpolated interpolation", "Used-For", "CNN"], ["fully - connected CRF", "Used-For", "segmentation"]], "rel_plus": [["interpolated interpolation:Method", "Used-For", "CNN:Method"], ["fully - connected CRF:Method", "Used-For", "segmentation:Task"]]}
{"doc_id": "210702798", "sentence": "They presented a method that jointly trains CNNs and fullyconnected CRFs for semantic image segmentation , and achieved encouraging results on the challenging PASCAL VOC 2 0 1 2 dataset .", "ner": [["CNNs", "Method"], ["fullyconnected CRFs", "Method"], ["image segmentation", "Task"], ["PASCAL VOC 2 0 1 2", "Dataset"]], "rel": [["CNNs", "Used-For", "image segmentation"], ["fullyconnected CRFs", "Used-For", "image segmentation"], ["PASCAL VOC 2 0 1 2", "Benchmark-For", "image segmentation"], ["fullyconnected CRFs", "Evaluated-With", "PASCAL VOC 2 0 1 2"], ["CNNs", "Evaluated-With", "PASCAL VOC 2 0 1 2"]], "rel_plus": [["CNNs:Method", "Used-For", "image segmentation:Task"], ["fullyconnected CRFs:Method", "Used-For", "image segmentation:Task"], ["PASCAL VOC 2 0 1 2:Dataset", "Benchmark-For", "image segmentation:Task"], ["fullyconnected CRFs:Method", "Evaluated-With", "PASCAL VOC 2 0 1 2:Dataset"], ["CNNs:Method", "Evaluated-With", "PASCAL VOC 2 0 1 2:Dataset"]]}
{"doc_id": "210702798", "sentence": "In [ 4 0 ] , Zheng et al. proposed a similar semantic segmentation approach integrating CRF with CNN .", "ner": [["semantic segmentation approach", "Method"], ["CRF", "Method"], ["CNN", "Method"]], "rel": [["CRF", "Part-Of", "semantic segmentation approach"], ["CNN", "Part-Of", "semantic segmentation approach"]], "rel_plus": [["CRF:Method", "Part-Of", "semantic segmentation approach:Method"], ["CNN:Method", "Part-Of", "semantic segmentation approach:Method"]]}
{"doc_id": "210702798", "sentence": "In another relevant work , Lin et al. [ 4 1 ] proposed an efficient algorithm for semantic segmentation based on contextual deep CRFs .", "ner": [["semantic segmentation", "Task"], ["contextual deep CRFs", "Method"]], "rel": [["contextual deep CRFs", "Used-For", "semantic segmentation"]], "rel_plus": [["contextual deep CRFs:Method", "Used-For", "semantic segmentation:Task"]]}
{"doc_id": "210702798", "sentence": "Liu et al. [ 4 2 ] proposed a semantic segmentation algorithm that incorporates rich information into MRFs , including highorder relations and mixture of label contexts .", "ner": [["semantic segmentation", "Task"], ["MRFs", "Method"]], "rel": [["MRFs", "Used-For", "semantic segmentation"]], "rel_plus": [["MRFs:Method", "Used-For", "semantic segmentation:Task"]]}
{"doc_id": "210702798", "sentence": "Unlike previous works that optimized MRFs using iterative algorithms , they proposed a CNN model , namely a Parsing Network , which enables deterministic end - to - end computation in a single forward pass .", "ner": [["MRFs", "Method"], ["CNN", "Method"], ["Parsing Network", "Method"]], "rel": [["Parsing Network", "SubClass-Of", "CNN"], ["MRFs", "Compare-With", "Parsing Network"]], "rel_plus": [["Parsing Network:Method", "SubClass-Of", "CNN:Method"], ["MRFs:Method", "Compare-With", "Parsing Network:Method"]]}
{"doc_id": "210702798", "sentence": "Another popular family of deep models for image segmentation is based on the convolutional encoder - decoder architecture .", "ner": [["image segmentation", "Task"], ["encoder - decoder", "Method"]], "rel": [["encoder - decoder", "Used-For", "image segmentation"]], "rel_plus": [["encoder - decoder:Method", "Used-For", "image segmentation:Task"]]}
{"doc_id": "210702798", "sentence": "Most of the DL - based segmentation works use some kind of encoder - decoder models .", "ner": [["DL", "Method"], ["segmentation", "Task"], ["encoder - decoder", "Method"]], "rel": [["DL", "Used-For", "segmentation"], ["encoder - decoder", "Used-For", "segmentation"]], "rel_plus": [["DL:Method", "Used-For", "segmentation:Task"], ["encoder - decoder:Method", "Used-For", "segmentation:Task"]]}
{"doc_id": "210702798", "sentence": "We group these works into two categories , encoder - decoder models for general segmentation , and for medical image segmentation ( to better distinguish between applications ) .", "ner": [["encoder - decoder", "Method"], ["segmentation", "Task"], ["medical image segmentation", "Task"]], "rel": [["encoder - decoder", "Used-For", "segmentation"], ["encoder - decoder", "Used-For", "medical image segmentation"]], "rel_plus": [["encoder - decoder:Method", "Used-For", "segmentation:Task"], ["encoder - decoder:Method", "Used-For", "medical image segmentation:Task"]]}
{"doc_id": "210702798", "sentence": "Noh et al. [ 4 3 ] published an early paper on semantic segmentation based on deconvolution ( a.k.a . transposed convolution ) .", "ner": [["semantic segmentation", "Task"], ["deconvolution", "Method"], ["transposed convolution", "Method"]], "rel": [["deconvolution", "Used-For", "semantic segmentation"], ["transposed convolution", "Synonym-Of", "deconvolution"]], "rel_plus": [["deconvolution:Method", "Used-For", "semantic segmentation:Task"], ["transposed convolution:Method", "Synonym-Of", "deconvolution:Method"]]}
{"doc_id": "210702798", "sentence": "Following a convolution network based on the VGG 1 6 - layer net , is a multi - layer deconvolution network to generate the accurate segmentation map .", "ner": [["convolution network", "Method"], ["VGG 1 6 - layer net", "Method"], ["multi - layer deconvolution network", "Method"], ["segmentation", "Task"]], "rel": [["VGG 1 6 - layer net", "Part-Of", "convolution network"], ["convolution network", "SubClass-Of", "multi - layer deconvolution network"], ["convolution network", "Used-For", "segmentation"]], "rel_plus": [["VGG 1 6 - layer net:Method", "Part-Of", "convolution network:Method"], ["convolution network:Method", "SubClass-Of", "multi - layer deconvolution network:Method"], ["convolution network:Method", "Used-For", "segmentation:Task"]]}
{"doc_id": "210702798", "sentence": "In another promising work known as SegNet , Badrinarayanan et al. [ 4 4 ] proposed a convolutional encoderdecoder architecture for image segmentation ( Figure 1 2 ) .", "ner": [["SegNet", "Method"], ["convolutional encoderdecoder architecture", "Method"], ["image segmentation", "Task"]], "rel": [["convolutional encoderdecoder architecture", "Used-For", "image segmentation"]], "rel_plus": [["convolutional encoderdecoder architecture:Method", "Used-For", "image segmentation:Task"]]}
{"doc_id": "210702798", "sentence": "Similar to the deconvolution network , the core trainable segmentation engine of SegNet consists of an encoder network , which is topologically identical to the 1 3 convolutional layers in the VGG 1 6 network , and a corresponding decoder network followed by a pixel - wise classification layer .", "ner": [["segmentation", "Task"], ["SegNet", "Method"], ["encoder network", "Method"], ["convolutional layers", "Method"], ["VGG 1 6", "Method"], ["decoder network", "Method"], ["pixel - wise classification layer", "Method"]], "rel": [["SegNet", "Used-For", "segmentation"], ["encoder network", "Part-Of", "SegNet"], ["pixel - wise classification layer", "Part-Of", "encoder network"], ["decoder network", "Part-Of", "encoder network"], ["convolutional layers", "Part-Of", "VGG 1 6"]], "rel_plus": [["SegNet:Method", "Used-For", "segmentation:Task"], ["encoder network:Method", "Part-Of", "SegNet:Method"], ["pixel - wise classification layer:Method", "Part-Of", "encoder network:Method"], ["decoder network:Method", "Part-Of", "encoder network:Method"], ["convolutional layers:Method", "Part-Of", "VGG 1 6:Method"]]}
{"doc_id": "210702798", "sentence": "The main novelty of SegNet is in the way the decoder upsamples its lower resolution input feature map(s ) ; specifically , it uses pooling indices computed in the max - pooling step of the corresponding encoder to perform non - linear upsampling .", "ner": [["SegNet", "Method"], ["max - pooling", "Method"], ["non - linear upsampling", "Task"]], "rel": [["max - pooling", "Part-Of", "SegNet"], ["max - pooling", "Used-For", "non - linear upsampling"]], "rel_plus": [["max - pooling:Method", "Part-Of", "SegNet:Method"], ["max - pooling:Method", "Used-For", "non - linear upsampling:Task"]]}
{"doc_id": "210702798", "sentence": "A Bayesian version of SegNet was also proposed by the same authors to model the uncertainty inherent to the convolutional encoder - decoder network for scene segmentation [ 4 5 ] .", "ner": [["Bayesian version of SegNet", "Method"], ["convolutional encoder - decoder network", "Method"], ["scene segmentation", "Task"]], "rel": [["convolutional encoder - decoder network", "Used-For", "scene segmentation"]], "rel_plus": [["convolutional encoder - decoder network:Method", "Used-For", "scene segmentation:Task"]]}
{"doc_id": "210702798", "sentence": "Several other works adopt transposed convolutions , or encoder - decoders for image segmentation , such as Stacked Deconvolutional Network ( SDN ) [ 4 6 ] , Linknet [ 4 7 ] , W - Net [ 4 8 ] , and locality - sensitive deconvolution networks for RGB - D segmentation [ 4 9 ] .   There are several models initially developed for medical/biomedical image segmentation , which are inspired by FCNs and encoder - decoder models .", "ner": [["transposed convolutions", "Method"], ["encoder - decoders", "Method"], ["image segmentation", "Task"], ["Stacked Deconvolutional Network", "Method"], ["SDN", "Method"], ["Linknet", "Method"], ["W - Net", "Method"], ["locality - sensitive deconvolution networks", "Method"], ["RGB - D segmentation", "Task"], ["medical/biomedical image segmentation", "Task"], ["FCNs", "Method"], ["encoder - decoder", "Method"]], "rel": [["encoder - decoders", "Used-For", "image segmentation"], ["transposed convolutions", "Used-For", "image segmentation"], ["SDN", "Synonym-Of", "Stacked Deconvolutional Network"], ["Stacked Deconvolutional Network", "Used-For", "RGB - D segmentation"], ["Linknet", "Used-For", "RGB - D segmentation"], ["W - Net", "Used-For", "RGB - D segmentation"]], "rel_plus": [["encoder - decoders:Method", "Used-For", "image segmentation:Task"], ["transposed convolutions:Method", "Used-For", "image segmentation:Task"], ["SDN:Method", "Synonym-Of", "Stacked Deconvolutional Network:Method"], ["Stacked Deconvolutional Network:Method", "Used-For", "RGB - D segmentation:Task"], ["Linknet:Method", "Used-For", "RGB - D segmentation:Task"], ["W - Net:Method", "Used-For", "RGB - D segmentation:Task"]]}
{"doc_id": "210702798", "sentence": "U - Net [ 5 0 ] , and V - Net [ 5 1 ] , are two well - known such architectures , which are now also being used outside the medical domain .", "ner": [["U - Net", "Method"], ["V - Net", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210702798", "sentence": "Finally , a 1 \u00d7 1 convolution processes the feature maps to generate a segmentation map that categorizes each pixel of the input image .", "ner": [["1 \u00d7 1 convolution", "Method"], ["generate a segmentation map", "Task"]], "rel": [["1 \u00d7 1 convolution", "Used-For", "generate a segmentation map"]], "rel_plus": [["1 \u00d7 1 convolution:Method", "Used-For", "generate a segmentation map:Task"]]}
{"doc_id": "210702798", "sentence": "U - Net was trained on 3 0 transmitted light microscopy images , and it won the ISBI cell tracking challenge 2 0 1 5 by a large margin .", "ner": [["U - Net", "Method"], ["ISBI cell tracking challenge 2 0 1 5", "Dataset"]], "rel": [["U - Net", "Evaluated-With", "ISBI cell tracking challenge 2 0 1 5"]], "rel_plus": [["U - Net:Method", "Evaluated-With", "ISBI cell tracking challenge 2 0 1 5:Dataset"]]}
{"doc_id": "210702798", "sentence": "For example , Zhang et al. [ 5 4 ] developed a road segmentation/extraction algorithm based on U - Net .", "ner": [["road segmentation/extraction", "Task"], ["U - Net", "Method"]], "rel": [["U - Net", "Used-For", "road segmentation/extraction"]], "rel_plus": [["U - Net:Method", "Used-For", "road segmentation/extraction:Task"]]}
{"doc_id": "210702798", "sentence": "V - Net ( Figure 1 4 ) is another well - known , FCN - based model , which was proposed by Milletari et al. [ 5 1 ] for 3D medical image segmentation .", "ner": [["V - Net", "Method"], ["FCN", "Method"], ["3D medical image segmentation", "Task"]], "rel": [["V - Net", "SubClass-Of", "FCN"], ["V - Net", "Used-For", "3D medical image segmentation"]], "rel_plus": [["V - Net:Method", "SubClass-Of", "FCN:Method"], ["V - Net:Method", "Used-For", "3D medical image segmentation:Task"]]}
{"doc_id": "210702798", "sentence": "One of the most prominent models of this sort is the Feature Pyramid Network ( FPN ) proposed by Lin et al. [ 5 6 ] , which was developed mainly for object detection but was then also applied to segmentation .", "ner": [["Feature Pyramid Network", "Method"], ["FPN", "Method"], ["object detection", "Task"], ["segmentation", "Task"]], "rel": [["FPN", "Synonym-Of", "Feature Pyramid Network"], ["Feature Pyramid Network", "Used-For", "object detection"], ["Feature Pyramid Network", "Used-For", "segmentation"]], "rel_plus": [["FPN:Method", "Synonym-Of", "Feature Pyramid Network:Method"], ["Feature Pyramid Network:Method", "Used-For", "object detection:Task"], ["Feature Pyramid Network:Method", "Used-For", "segmentation:Task"]]}
{"doc_id": "210702798", "sentence": "For image segmentation , the authors use two multi - layer perceptrons ( MLPs ) to generate the masks .", "ner": [["image segmentation", "Task"], ["multi - layer perceptrons", "Method"], ["MLPs", "Method"]], "rel": [["multi - layer perceptrons", "Used-For", "image segmentation"], ["MLPs", "Synonym-Of", "multi - layer perceptrons"]], "rel_plus": [["multi - layer perceptrons:Method", "Used-For", "image segmentation:Task"], ["MLPs:Method", "Synonym-Of", "multi - layer perceptrons:Method"]]}
{"doc_id": "210702798", "sentence": "Zhao et al. [ 5 7 ] developed the Pyramid Scene Parsing Network ( PSPN ) , a multi - scale network to better learn the global context representation of a scene ( Figure 1 6 ) .", "ner": [["Pyramid Scene Parsing Network", "Method"], ["PSPN", "Method"], ["multi - scale network", "Method"]], "rel": [["PSPN", "Synonym-Of", "Pyramid Scene Parsing Network"], ["Pyramid Scene Parsing Network", "SubClass-Of", "multi - scale network"]], "rel_plus": [["PSPN:Method", "Synonym-Of", "Pyramid Scene Parsing Network:Method"], ["Pyramid Scene Parsing Network:Method", "SubClass-Of", "multi - scale network:Method"]]}
{"doc_id": "210702798", "sentence": "Different patterns are extracted from the input image using a residual network ( ResNet ) as a feature extractor , with a dilated network .", "ner": [["residual network", "Method"], ["ResNet", "Method"], ["dilated network", "Method"]], "rel": [["ResNet", "Synonym-Of", "residual network"], ["dilated network", "Part-Of", "residual network"]], "rel_plus": [["ResNet:Method", "Synonym-Of", "residual network:Method"], ["dilated network:Method", "Part-Of", "residual network:Method"]]}
{"doc_id": "210702798", "sentence": "There are other models using multi - scale analysis for segmentation , such as DM - Net ( Dynamic Multi - scale Filters Network ) [ 5 9 ] , Context contrasted network and gated multiscale aggregation ( CCN ) [ 6 0 ] , Adaptive Pyramid Context Network ( APC - Net ) [ 6 1 ] , Multi - scale context intertwining ( MSCI ) [ 6 2 ] , and salient object segmentation [ 6 3 ] .", "ner": [["segmentation", "Task"], ["DM - Net", "Method"], ["Dynamic Multi - scale Filters Network", "Method"], ["Context contrasted network", "Method"], ["gated multiscale aggregation", "Method"], ["CCN", "Method"], ["Adaptive Pyramid Context Network", "Method"], ["APC - Net", "Method"], ["Multi - scale context intertwining", "Method"], ["MSCI", "Method"], ["salient object segmentation", "Method"]], "rel": [["DM - Net", "Used-For", "segmentation"], ["Context contrasted network", "Used-For", "segmentation"], ["gated multiscale aggregation", "Used-For", "segmentation"], ["Adaptive Pyramid Context Network", "Used-For", "segmentation"], ["Multi - scale context intertwining", "Used-For", "segmentation"], ["salient object segmentation", "Used-For", "segmentation"], ["DM - Net", "Synonym-Of", "Dynamic Multi - scale Filters Network"], ["CCN", "Synonym-Of", "gated multiscale aggregation"], ["APC - Net", "Synonym-Of", "Adaptive Pyramid Context Network"], ["MSCI", "Synonym-Of", "Multi - scale context intertwining"]], "rel_plus": [["DM - Net:Method", "Used-For", "segmentation:Task"], ["Context contrasted network:Method", "Used-For", "segmentation:Task"], ["gated multiscale aggregation:Method", "Used-For", "segmentation:Task"], ["Adaptive Pyramid Context Network:Method", "Used-For", "segmentation:Task"], ["Multi - scale context intertwining:Method", "Used-For", "segmentation:Task"], ["salient object segmentation:Method", "Used-For", "segmentation:Task"], ["DM - Net:Method", "Synonym-Of", "Dynamic Multi - scale Filters Network:Method"], ["CCN:Method", "Synonym-Of", "gated multiscale aggregation:Method"], ["APC - Net:Method", "Synonym-Of", "Adaptive Pyramid Context Network:Method"], ["MSCI:Method", "Synonym-Of", "Multi - scale context intertwining:Method"]]}
{"doc_id": "210702798", "sentence": "The regional convolutional network ( R - CNN ) and its extensions ( Fast R - CNN , Faster R - CNN , Maksed - RCNN ) have proven successful in object detection applications .", "ner": [["regional convolutional network", "Method"], ["R - CNN", "Method"], ["Fast R - CNN", "Method"], ["Faster R - CNN", "Method"], ["Maksed - RCNN", "Method"], ["object detection", "Task"]], "rel": [["R - CNN", "Synonym-Of", "regional convolutional network"], ["Fast R - CNN", "SubClass-Of", "regional convolutional network"], ["Faster R - CNN", "SubClass-Of", "regional convolutional network"], ["Maksed - RCNN", "SubClass-Of", "regional convolutional network"], ["regional convolutional network", "Used-For", "object detection"], ["Fast R - CNN", "Used-For", "object detection"], ["Faster R - CNN", "Used-For", "object detection"], ["Maksed - RCNN", "Used-For", "object detection"]], "rel_plus": [["R - CNN:Method", "Synonym-Of", "regional convolutional network:Method"], ["Fast R - CNN:Method", "SubClass-Of", "regional convolutional network:Method"], ["Faster R - CNN:Method", "SubClass-Of", "regional convolutional network:Method"], ["Maksed - RCNN:Method", "SubClass-Of", "regional convolutional network:Method"], ["regional convolutional network:Method", "Used-For", "object detection:Task"], ["Fast R - CNN:Method", "Used-For", "object detection:Task"], ["Faster R - CNN:Method", "Used-For", "object detection:Task"], ["Maksed - RCNN:Method", "Used-For", "object detection:Task"]]}
{"doc_id": "210702798", "sentence": "Some of the extensions of R - CNN have been heavily used to address the instance segmentation problem ; i.e. , the task of simultaneously performing object detection and semantic segmentation .", "ner": [["R - CNN", "Method"], ["instance segmentation", "Task"], ["object detection", "Task"], ["semantic segmentation", "Task"]], "rel": [["R - CNN", "Used-For", "instance segmentation"], ["object detection", "SubTask-Of", "instance segmentation"], ["semantic segmentation", "SubTask-Of", "instance segmentation"], ["R - CNN", "Used-For", "object detection"], ["R - CNN", "Used-For", "semantic segmentation"]], "rel_plus": [["R - CNN:Method", "Used-For", "instance segmentation:Task"], ["object detection:Task", "SubTask-Of", "instance segmentation:Task"], ["semantic segmentation:Task", "SubTask-Of", "instance segmentation:Task"], ["R - CNN:Method", "Used-For", "object detection:Task"], ["R - CNN:Method", "Used-For", "semantic segmentation:Task"]]}
{"doc_id": "210702798", "sentence": "In particular , the Faster R - CNN [ 6 4 ] architecture ( Figure 1 7 ) developed for object detection uses a region proposal network ( RPN ) to propose bounding box candidates .", "ner": [["Faster R - CNN", "Method"], ["object detection", "Task"], ["region proposal network", "Method"], ["RPN", "Method"]], "rel": [["region proposal network", "Part-Of", "Faster R - CNN"], ["Faster R - CNN", "Used-For", "object detection"], ["RPN", "Synonym-Of", "region proposal network"]], "rel_plus": [["region proposal network:Method", "Part-Of", "Faster R - CNN:Method"], ["Faster R - CNN:Method", "Used-For", "object detection:Task"], ["RPN:Method", "Synonym-Of", "region proposal network:Method"]]}
{"doc_id": "210702798", "sentence": "The RPN extracts a Region of Interest ( RoI ) , and a RoIPool layer computes features from these proposals in order to infer the bounding box coordinates and the class of the object .", "ner": [["RPN", "Method"], ["Region of Interest", "Task"], ["RoI", "Task"], ["RoIPool layer", "Method"]], "rel": [["RoIPool layer", "Part-Of", "RPN"], ["RoI", "Synonym-Of", "Region of Interest"], ["RPN", "Used-For", "Region of Interest"]], "rel_plus": [["RoIPool layer:Method", "Part-Of", "RPN:Method"], ["RoI:Task", "Synonym-Of", "Region of Interest:Task"], ["RPN:Method", "Used-For", "Region of Interest:Task"]]}
{"doc_id": "210702798", "sentence": "In one extension of this model , He et al. [ 6 5 ] proposed a Mask R - CNN for object instance segmentation , which beat all previous benchmarks on many COCO challenges .", "ner": [["Mask R - CNN", "Method"], ["object instance segmentation", "Task"], ["COCO", "Dataset"]], "rel": [["Mask R - CNN", "Used-For", "object instance segmentation"], ["COCO", "Benchmark-For", "object instance segmentation"], ["Mask R - CNN", "Evaluated-With", "COCO"]], "rel_plus": [["Mask R - CNN:Method", "Used-For", "object instance segmentation:Task"], ["COCO:Dataset", "Benchmark-For", "object instance segmentation:Task"], ["Mask R - CNN:Method", "Evaluated-With", "COCO:Dataset"]]}
{"doc_id": "210702798", "sentence": "Mask R - CNN is essentially a Faster R - CNN with 3 output branches ( Figure 1 8 ) -the first computes the bounding box coordinates , the second computes the associated classes , and the third computes the binary mask to Fig. 1 7 .", "ner": [["Mask R - CNN", "Method"], ["Faster R - CNN", "Method"]], "rel": [["Mask R - CNN", "SubClass-Of", "Faster R - CNN"]], "rel_plus": [["Mask R - CNN:Method", "SubClass-Of", "Faster R - CNN:Method"]]}
{"doc_id": "210702798", "sentence": "Each image is processed by convolutional layers and its features are extracted , a sliding window is used in RPN for each location over the feature map , for each location , k ( k = 9 ) anchor boxes are used ( 3 scales of 1 2 8 , 2 5 6 and 5 1 2 , and 3 aspect ratios of 1: 1 , 1: 2 , 2: 1 ) to generate a region proposal ; A cls layer outputs 2k scores whether there or not there is an object for k boxes ; A reg layer outputs 4k for the coordinates ( box center coordinates , width and height ) of k boxes .", "ner": [["RPN", "Method"], ["region proposal", "Task"]], "rel": [["RPN", "Used-For", "region proposal"]], "rel_plus": [["RPN:Method", "Used-For", "region proposal:Task"]]}
{"doc_id": "210702798", "sentence": "The Mask R - CNN loss function combines the losses of the bounding box coordinates , the predicted class , and the segmentation mask , and trains all of them jointly .", "ner": [["Mask R - CNN loss function", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210702798", "sentence": "As in the Mask R - CNN , the output of the adaptive feature pooling layer feeds three branches .", "ner": [["Mask R - CNN", "Method"], ["adaptive feature pooling", "Method"]], "rel": [["adaptive feature pooling", "Part-Of", "Mask R - CNN"]], "rel_plus": [["adaptive feature pooling:Method", "Part-Of", "Mask R - CNN:Method"]]}
{"doc_id": "210702798", "sentence": "Chen et al. [ 6 9 ] developed an instance segmentation model , MaskLab ( Figure 2 1 ) , by refining object detection with semantic and direction features based on Faster R - CNN .", "ner": [["instance segmentation", "Task"], ["MaskLab", "Method"], ["object detection", "Task"], ["Faster R - CNN", "Method"]], "rel": [["MaskLab", "Used-For", "instance segmentation"], ["MaskLab", "Used-For", "object detection"]], "rel_plus": [["MaskLab:Method", "Used-For", "instance segmentation:Task"], ["MaskLab:Method", "Used-For", "object detection:Task"]]}
{"doc_id": "210702798", "sentence": "This model produces three outputs , box detection , semantic segmentation , and direction prediction .", "ner": [["box detection", "Task"], ["semantic segmentation", "Task"], ["direction prediction", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "210702798", "sentence": "Within each region of interest , MaskLab performs foreground/background segmentation by combining semantic and direction prediction .", "ner": [["MaskLab", "Method"], ["foreground/background segmentation", "Task"], ["semantic and direction prediction", "Task"]], "rel": [["MaskLab", "Used-For", "foreground/background segmentation"], ["MaskLab", "Used-For", "semantic and direction prediction"]], "rel_plus": [["MaskLab:Method", "Used-For", "foreground/background segmentation:Task"], ["MaskLab:Method", "Used-For", "semantic and direction prediction:Task"]]}
{"doc_id": "210702798", "sentence": "Another interesting model is Tensormask , proposed by Chen et al. [ 7 0 ] , which is based on dense sliding window instance segmentation .", "ner": [["Tensormask", "Method"], ["dense sliding window instance segmentation", "Method"]], "rel": [["dense sliding window instance segmentation", "Part-Of", "Tensormask"]], "rel_plus": [["dense sliding window instance segmentation:Method", "Part-Of", "Tensormask:Method"]]}
{"doc_id": "210702798", "sentence": "Many other instance segmentation models have been developed based on R - CNN , such as those developed for mask proposals , including R - FCN [ 7 1 ] , DeepMask [ 7 2 ] , SharpMask [ 7 3 ] , PolarMask [ 7 4 ] , and boundary - aware instance segmentation [ 7 5 ] .", "ner": [["instance segmentation", "Task"], ["R - CNN", "Method"], ["R - FCN", "Method"], ["DeepMask", "Method"], ["SharpMask", "Method"], ["PolarMask", "Method"], ["boundary - aware instance segmentation", "Method"]], "rel": [["R - CNN", "Used-For", "instance segmentation"], ["R - FCN", "SubClass-Of", "R - CNN"], ["DeepMask", "SubClass-Of", "R - CNN"], ["SharpMask", "SubClass-Of", "R - CNN"], ["PolarMask", "SubClass-Of", "R - CNN"], ["boundary - aware instance segmentation", "SubClass-Of", "R - CNN"]], "rel_plus": [["R - CNN:Method", "Used-For", "instance segmentation:Task"], ["R - FCN:Method", "SubClass-Of", "R - CNN:Method"], ["DeepMask:Method", "SubClass-Of", "R - CNN:Method"], ["SharpMask:Method", "SubClass-Of", "R - CNN:Method"], ["PolarMask:Method", "SubClass-Of", "R - CNN:Method"], ["boundary - aware instance segmentation:Method", "SubClass-Of", "R - CNN:Method"]]}
{"doc_id": "210702798", "sentence": "It is worth noting that there is another promising research direction that attempts to solve the instance segmentation problem by learning grouping cues for bottom - up segmentation , such as Deep Watershed Transform [ 7 6 ] , and Semantic Instance Segmentation via Deep Metric Learning [ 7 7 ] .   Dilated convolution ( a.k.a . \" atrous \" convolution ) introduces another parameter to convolutional layers , the dilation rate .", "ner": [["instance segmentation", "Task"], ["bottom - up segmentation", "Task"], ["Deep Watershed Transform", "Method"], ["Semantic Instance Segmentation", "Method"], ["Deep Metric Learning", "Method"], ["Dilated convolution", "Method"], ["\" atrous \" convolution", "Method"], ["convolutional layers", "Method"]], "rel": [["Semantic Instance Segmentation", "Used-For", "instance segmentation"], ["Deep Watershed Transform", "Used-For", "instance segmentation"], ["Deep Watershed Transform", "Used-For", "bottom - up segmentation"], ["Semantic Instance Segmentation", "Used-For", "bottom - up segmentation"], ["Deep Metric Learning", "Used-For", "Semantic Instance Segmentation"], ["\" atrous \" convolution", "Synonym-Of", "Dilated convolution"], ["convolutional layers", "Part-Of", "Dilated convolution"]], "rel_plus": [["Semantic Instance Segmentation:Method", "Used-For", "instance segmentation:Task"], ["Deep Watershed Transform:Method", "Used-For", "instance segmentation:Task"], ["Deep Watershed Transform:Method", "Used-For", "bottom - up segmentation:Task"], ["Semantic Instance Segmentation:Method", "Used-For", "bottom - up segmentation:Task"], ["Deep Metric Learning:Method", "Used-For", "Semantic Instance Segmentation:Method"], ["\" atrous \" convolution:Method", "Synonym-Of", "Dilated convolution:Method"], ["convolutional layers:Method", "Part-Of", "Dilated convolution:Method"]]}
{"doc_id": "210702798", "sentence": "Dilated convolutions have been popular in the field of real - time segmentation , and many recent publications report the use of this technique .", "ner": [["Dilated convolutions", "Method"], ["real - time segmentation", "Task"]], "rel": [["Dilated convolutions", "Used-For", "real - time segmentation"]], "rel_plus": [["Dilated convolutions:Method", "Used-For", "real - time segmentation:Task"]]}
{"doc_id": "210702798", "sentence": "Some of most important include the DeepLab family [ 7 8 ] , multiscale context aggregation [ 7 9 ] , dense upsampling convolution and hybrid dilatedconvolution ( DUC - HDC ) [ 8 0 ] , densely connected Atrous Spatial Pyramid Pooling ( DenseASPP ) [ 8 1 ] , and the efficient neural network ( ENet ) [ 8 2 ] .", "ner": [["DeepLab", "Method"], ["multiscale context aggregation", "Method"], ["dense upsampling convolution", "Method"], ["hybrid dilatedconvolution", "Method"], ["DUC - HDC", "Method"], ["densely connected Atrous Spatial Pyramid Pooling", "Method"], ["DenseASPP", "Method"], ["efficient neural network", "Method"], ["ENet", "Method"]], "rel": [["multiscale context aggregation", "SubClass-Of", "DeepLab"], ["dense upsampling convolution", "SubClass-Of", "DeepLab"], ["hybrid dilatedconvolution", "SubClass-Of", "DeepLab"], ["densely connected Atrous Spatial Pyramid Pooling", "SubClass-Of", "DeepLab"], ["efficient neural network", "SubClass-Of", "DeepLab"], ["DUC - HDC", "Synonym-Of", "hybrid dilatedconvolution"], ["DenseASPP", "Synonym-Of", "densely connected Atrous Spatial Pyramid Pooling"], ["ENet", "Synonym-Of", "efficient neural network"]], "rel_plus": [["multiscale context aggregation:Method", "SubClass-Of", "DeepLab:Method"], ["dense upsampling convolution:Method", "SubClass-Of", "DeepLab:Method"], ["hybrid dilatedconvolution:Method", "SubClass-Of", "DeepLab:Method"], ["densely connected Atrous Spatial Pyramid Pooling:Method", "SubClass-Of", "DeepLab:Method"], ["efficient neural network:Method", "SubClass-Of", "DeepLab:Method"], ["DUC - HDC:Method", "Synonym-Of", "hybrid dilatedconvolution:Method"], ["DenseASPP:Method", "Synonym-Of", "densely connected Atrous Spatial Pyramid Pooling:Method"], ["ENet:Method", "Synonym-Of", "efficient neural network:Method"]]}
{"doc_id": "210702798", "sentence": "DeepLabv 1 [ 3 8 ] and DeepLabv 2 [ 7 8 ] are among some of the most popular image segmentation approaches , developed by Chen et al .. The latter has three key features .", "ner": [["DeepLabv 1", "Method"], ["DeepLabv 2", "Method"], ["image segmentation", "Task"]], "rel": [["DeepLabv 1", "Used-For", "image segmentation"], ["DeepLabv 2", "Used-For", "image segmentation"]], "rel_plus": [["DeepLabv 1:Method", "Used-For", "image segmentation:Task"], ["DeepLabv 2:Method", "Used-For", "image segmentation:Task"]]}
{"doc_id": "210702798", "sentence": "First is the use of dilated convolution to address the decreasing resolution in the network ( caused by max - pooling and striding ) .", "ner": [["dilated convolution", "Method"], ["max - pooling", "Method"], ["striding", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210702798", "sentence": "Second is Atrous Spatial Pyramid Pooling ( ASPP ) , which probes an incoming convolutional feature layer with filters at multiple sampling rates , thus capturing objects as well as image context at multiple scales to robustly segment objects at multiple scales .", "ner": [["Atrous Spatial Pyramid Pooling", "Method"], ["ASPP", "Method"]], "rel": [["ASPP", "Synonym-Of", "Atrous Spatial Pyramid Pooling"]], "rel_plus": [["ASPP:Method", "Synonym-Of", "Atrous Spatial Pyramid Pooling:Method"]]}
{"doc_id": "210702798", "sentence": "The best DeepLab ( using a ResNet - 1 0 1 as backbone ) has reached a 7 9 . 7 % mIoU score on the 2 0 1 2 PASCAL VOC challenge , a 4 5 . 7 % mIoU score on the PASCAL - Context challenge and a 7 0 . 4 % mIoU score on the Cityscapes challenge .", "ner": [["DeepLab", "Method"], ["ResNet - 1 0 1", "Method"], ["2 0 1 2 PASCAL VOC", "Dataset"], ["PASCAL - Context", "Dataset"], ["Cityscapes", "Dataset"]], "rel": [["ResNet - 1 0 1", "Part-Of", "DeepLab"], ["DeepLab", "Evaluated-With", "2 0 1 2 PASCAL VOC"], ["DeepLab", "Evaluated-With", "PASCAL - Context"], ["DeepLab", "Evaluated-With", "Cityscapes"]], "rel_plus": [["ResNet - 1 0 1:Method", "Part-Of", "DeepLab:Method"], ["DeepLab:Method", "Evaluated-With", "2 0 1 2 PASCAL VOC:Dataset"], ["DeepLab:Method", "Evaluated-With", "PASCAL - Context:Dataset"], ["DeepLab:Method", "Evaluated-With", "Cityscapes:Dataset"]]}
{"doc_id": "210702798", "sentence": "Figure 2 4 illustrates the Deeplab model , which is similar to [ 3 8 ] , the main difference being the use of dilated convolution and ASPP .", "ner": [["Deeplab", "Method"], ["dilated convolution", "Method"], ["ASPP", "Method"]], "rel": [["ASPP", "Part-Of", "Deeplab"], ["dilated convolution", "Part-Of", "Deeplab"]], "rel_plus": [["ASPP:Method", "Part-Of", "Deeplab:Method"], ["dilated convolution:Method", "Part-Of", "Deeplab:Method"]]}
{"doc_id": "210702798", "sentence": "Subsequently , Chen et al. [ 1 2 ] proposed DeepLabv 3 , which combines cascaded and parallel modules of dilated convolutions .", "ner": [["DeepLabv 3", "Method"], ["dilated convolutions", "Method"]], "rel": [["dilated convolutions", "Part-Of", "DeepLabv 3"]], "rel_plus": [["dilated convolutions:Method", "Part-Of", "DeepLabv 3:Method"]]}
{"doc_id": "210702798", "sentence": "The parallel convolution modules are grouped in the ASPP .", "ner": [["parallel convolution modules", "Method"], ["ASPP", "Method"]], "rel": [["parallel convolution modules", "Part-Of", "ASPP"]], "rel_plus": [["parallel convolution modules:Method", "Part-Of", "ASPP:Method"]]}
{"doc_id": "210702798", "sentence": "A 1 \u00d7 1 convolution and batch normalisation A bilinear interpolation stage enlarges the feature maps to the original image resolution .", "ner": [["1 \u00d7 1 convolution", "Method"], ["batch normalisation", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210702798", "sentence": "Finally , a fully connected CRF refines the segmentation result to better capture the object boundaries .", "ner": [["fully connected CRF", "Method"], ["segmentation", "Task"]], "rel": [["fully connected CRF", "Used-For", "segmentation"]], "rel_plus": [["fully connected CRF:Method", "Used-For", "segmentation:Task"]]}
{"doc_id": "210702798", "sentence": "In 2 0 1 8 , Chen et al. [ 8 3 ] released Deeplabv 3 + , which uses an encoder - decoder architecture ( Figure 2 5 ) , including atrous separable convolution , composed of a depthwise convolution ( spatial convolution for each channel of the input ) and pointwise convolution ( 1 \u00d7 1 convolution with the depthwise convolution as input ) .", "ner": [["Deeplabv 3 +", "Method"], ["encoder - decoder", "Method"], ["convolution", "Method"], ["depthwise convolution", "Method"], ["convolution", "Method"], ["pointwise convolution", "Method"], ["1 \u00d7 1 convolution", "Method"], ["depthwise convolution", "Method"]], "rel": [["encoder - decoder", "Part-Of", "Deeplabv 3 +"], ["1 \u00d7 1 convolution", "Part-Of", "pointwise convolution"], ["depthwise convolution", "Part-Of", "pointwise convolution"]], "rel_plus": [["encoder - decoder:Method", "Part-Of", "Deeplabv 3 +:Method"], ["1 \u00d7 1 convolution:Method", "Part-Of", "pointwise convolution:Method"], ["depthwise convolution:Method", "Part-Of", "pointwise convolution:Method"]]}
{"doc_id": "210702798", "sentence": "The most relevant model has a modified Xception backbone with more layers , dilated depthwise separable convolutions instead of max pooling and batch normalization .", "ner": [["Xception", "Method"], ["dilated depthwise separable convolutions", "Method"], ["max pooling", "Method"], ["batch normalization", "Method"]], "rel": [["dilated depthwise separable convolutions", "Part-Of", "Xception"]], "rel_plus": [["dilated depthwise separable convolutions:Method", "Part-Of", "Xception:Method"]]}
{"doc_id": "210702798", "sentence": "The best DeepLabv 3 + pretrained on the COCO and the JFT datasets has obtained a 8 9 . 0 % mIoU score on the 2 0 1 2 PASCAL VOC challenge .   While CNNs are a natural fit for computer vision problems , they are not the only possibility .", "ner": [["DeepLabv 3 +", "Method"], ["COCO", "Dataset"], ["JFT", "Dataset"], ["2 0 1 2 PASCAL VOC", "Dataset"], ["CNNs", "Method"], ["computer vision", "Task"]], "rel": [["DeepLabv 3 +", "Trained-With", "COCO"], ["DeepLabv 3 +", "Trained-With", "JFT"], ["DeepLabv 3 +", "Evaluated-With", "2 0 1 2 PASCAL VOC"], ["CNNs", "Used-For", "computer vision"]], "rel_plus": [["DeepLabv 3 +:Method", "Trained-With", "COCO:Dataset"], ["DeepLabv 3 +:Method", "Trained-With", "JFT:Dataset"], ["DeepLabv 3 +:Method", "Evaluated-With", "2 0 1 2 PASCAL VOC:Dataset"], ["CNNs:Method", "Used-For", "computer vision:Task"]]}
{"doc_id": "210702798", "sentence": "RNNs are useful in modeling the short/long term dependencies among pixels to ( potentially ) improve the estimation of the segmentation map .", "ner": [["RNNs", "Method"], ["segmentation map", "Task"]], "rel": [["RNNs", "Used-For", "segmentation map"]], "rel_plus": [["RNNs:Method", "Used-For", "segmentation map:Task"]]}
{"doc_id": "210702798", "sentence": "Using RNNs , pixels may be linked together and processed sequentially to model global contexts and improve semantic segmentation .", "ner": [["RNNs", "Method"], ["semantic segmentation", "Task"]], "rel": [["RNNs", "Used-For", "semantic segmentation"]], "rel_plus": [["RNNs:Method", "Used-For", "semantic segmentation:Task"]]}
{"doc_id": "210702798", "sentence": "Visin et al. [ 8 4 ] proposed an RNN - based model for semantic segmentation called ReSeg .", "ner": [["RNN", "Method"], ["semantic segmentation", "Task"], ["ReSeg", "Method"]], "rel": [["ReSeg", "SubClass-Of", "RNN"], ["RNN", "Used-For", "semantic segmentation"], ["ReSeg", "Used-For", "semantic segmentation"]], "rel_plus": [["ReSeg:Method", "SubClass-Of", "RNN:Method"], ["RNN:Method", "Used-For", "semantic segmentation:Task"], ["ReSeg:Method", "Used-For", "semantic segmentation:Task"]]}
{"doc_id": "210702798", "sentence": "This model is mainly based on another work , ReNet [ 8 5 ] , which was developed for image classification .", "ner": [["ReNet", "Method"], ["image classification", "Task"]], "rel": [["ReNet", "Used-For", "image classification"]], "rel_plus": [["ReNet:Method", "Used-For", "image classification:Task"]]}
{"doc_id": "210702798", "sentence": "Each ReNet layer ( Figure 2 6 ) is composed of four RNNs that sweep the image horizontally and vertically in both directions , encoding patches/activations , and providing relevant global information .", "ner": [["ReNet", "Method"], ["RNNs", "Method"]], "rel": [["RNNs", "Part-Of", "ReNet"]], "rel_plus": [["RNNs:Method", "Part-Of", "ReNet:Method"]]}
{"doc_id": "210702798", "sentence": "To perform image segmentation with the ReSeg model ( Figure 2 7 ) , ReNet layers are stacked on top of pre - trained VGG - 1 6 convolutional layers that extract generic local features .", "ner": [["image segmentation", "Task"], ["ReSeg", "Method"], ["ReNet", "Method"], ["VGG - 1 6 convolutional layers", "Method"]], "rel": [["ReSeg", "Used-For", "image segmentation"], ["VGG - 1 6 convolutional layers", "Part-Of", "ReNet"]], "rel_plus": [["ReSeg:Method", "Used-For", "image segmentation:Task"], ["VGG - 1 6 convolutional layers:Method", "Part-Of", "ReNet:Method"]]}
{"doc_id": "210702798", "sentence": "Gated Recurrent Units ( GRUs ) are used because they provide a good balance between memory usage and computational power .", "ner": [["Gated Recurrent Units", "Method"], ["GRUs", "Method"]], "rel": [["GRUs", "Synonym-Of", "Gated Recurrent Units"]], "rel_plus": [["GRUs:Method", "Synonym-Of", "Gated Recurrent Units:Method"]]}
{"doc_id": "210702798", "sentence": "In another work , Byeon et al. [ 8 6 ] developed a pixellevel segmentation and classification of scene images using long - short - term - memory ( LSTM ) network .", "ner": [["pixellevel segmentation", "Task"], ["classification of scene images", "Task"], ["long - short - term - memory", "Method"], ["LSTM", "Method"]], "rel": [["long - short - term - memory", "Used-For", "pixellevel segmentation"], ["long - short - term - memory", "Used-For", "classification of scene images"], ["LSTM", "Synonym-Of", "long - short - term - memory"]], "rel_plus": [["long - short - term - memory:Method", "Used-For", "pixellevel segmentation:Task"], ["long - short - term - memory:Method", "Used-For", "classification of scene images:Task"], ["LSTM:Method", "Synonym-Of", "long - short - term - memory:Method"]]}
{"doc_id": "210702798", "sentence": "In this work , classification , segmentation , and context integration are all carried out by 2D LSTM networks , allowing texture and spatial model parameters to be learned within a single model .", "ner": [["classification", "Task"], ["segmentation", "Task"], ["context integration", "Task"], ["2D LSTM", "Method"]], "rel": [["2D LSTM", "Used-For", "classification"], ["2D LSTM", "Used-For", "segmentation"], ["2D LSTM", "Used-For", "context integration"]], "rel_plus": [["2D LSTM:Method", "Used-For", "classification:Task"], ["2D LSTM:Method", "Used-For", "segmentation:Task"], ["2D LSTM:Method", "Used-For", "context integration:Task"]]}
{"doc_id": "210702798", "sentence": "The block - diagram of the proposed 2D LSTM network for image segmentation in [ 8 6 ] is shown in Figure 2 8 .", "ner": [["2D LSTM", "Method"], ["image segmentation", "Task"]], "rel": [["2D LSTM", "Used-For", "image segmentation"]], "rel_plus": [["2D LSTM:Method", "Used-For", "image segmentation:Task"]]}
{"doc_id": "210702798", "sentence": "Liang et al. [ 8 7 ] proposed a semantic segmentation model based on the Graph Long Short - Term Memory ( Graph LSTM ) network , a generalization of LSTM from sequential data or multidimensional data to general graph - structured data .", "ner": [["semantic segmentation", "Task"], ["Graph Long Short - Term Memory", "Method"], ["Graph LSTM", "Method"], ["LSTM", "Method"]], "rel": [["Graph Long Short - Term Memory", "Used-For", "semantic segmentation"], ["Graph LSTM", "Synonym-Of", "Graph Long Short - Term Memory"], ["Graph Long Short - Term Memory", "SubClass-Of", "LSTM"]], "rel_plus": [["Graph Long Short - Term Memory:Method", "Used-For", "semantic segmentation:Task"], ["Graph LSTM:Method", "Synonym-Of", "Graph Long Short - Term Memory:Method"], ["Graph Long Short - Term Memory:Method", "SubClass-Of", "LSTM:Method"]]}
{"doc_id": "210702798", "sentence": "Instead of evenly dividing an image to pixels or patches in existing multi - dimensional LSTM structures ( e.g. , row , grid and diagonal LSTMs ) , they take each arbitrary - shaped superpixel as a semantically consistent node , and adaptively construct an undirected graph for the image , where the spatial relations of the superpixels are naturally used as edges .", "ner": [["LSTM", "Method"], ["row , grid and diagonal LSTMs", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210702798", "sentence": "Figure 2 9 presents a visual comparison of the traditional pixel - wise RNN model and graph - LSTM model .", "ner": [["pixel - wise RNN model", "Method"], ["graph - LSTM", "Method"]], "rel": [["pixel - wise RNN model", "Compare-With", "graph - LSTM"]], "rel_plus": [["pixel - wise RNN model:Method", "Compare-With", "graph - LSTM:Method"]]}
{"doc_id": "210702798", "sentence": "To adapt the Graph LSTM model to semantic segmentation ( Figure 3 0 ) , LSTM layers built on a super - pixel map are appended on the convolutional layers to enhance visual features with global structure context .", "ner": [["Graph LSTM", "Method"], ["semantic segmentation", "Task"], ["LSTM layers", "Method"], ["convolutional layers", "Method"]], "rel": [["convolutional layers", "Part-Of", "Graph LSTM"], ["LSTM layers", "Part-Of", "Graph LSTM"], ["Graph LSTM", "Used-For", "semantic segmentation"]], "rel_plus": [["convolutional layers:Method", "Part-Of", "Graph LSTM:Method"], ["LSTM layers:Method", "Part-Of", "Graph LSTM:Method"], ["Graph LSTM:Method", "Used-For", "semantic segmentation:Task"]]}
{"doc_id": "210702798", "sentence": "The node updating sequence for the subsequent Graph LSTM layers is determined by the confidence - drive scheme based on the initial confidence maps , and then the Graph LSTM layers can sequentially update the hidden states of all superpixel nodes .", "ner": [["Graph LSTM layers", "Method"], ["Graph LSTM layers", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210702798", "sentence": "Xiang and Fox [ 8 8 ] proposed Data Associated Recurrent Neural Networks ( DA - RNNs ) , for joint 3D scene mapping and semantic labeling .", "ner": [["Data Associated Recurrent Neural Networks", "Method"], ["DA - RNNs", "Method"], ["3D scene mapping", "Task"], ["semantic labeling", "Task"]], "rel": [["DA - RNNs", "Synonym-Of", "Data Associated Recurrent Neural Networks"], ["Data Associated Recurrent Neural Networks", "Used-For", "3D scene mapping"], ["Data Associated Recurrent Neural Networks", "Used-For", "semantic labeling"]], "rel_plus": [["DA - RNNs:Method", "Synonym-Of", "Data Associated Recurrent Neural Networks:Method"], ["Data Associated Recurrent Neural Networks:Method", "Used-For", "3D scene mapping:Task"], ["Data Associated Recurrent Neural Networks:Method", "Used-For", "semantic labeling:Task"]]}
{"doc_id": "210702798", "sentence": "DA - RNNs use a new recurrent neural network architecture ( Figure 3 1 ) for semantic labeling on RGB - D videos .", "ner": [["DA - RNNs", "Method"], ["recurrent neural network", "Method"], ["semantic labeling", "Task"]], "rel": [["DA - RNNs", "SubClass-Of", "recurrent neural network"], ["DA - RNNs", "Used-For", "semantic labeling"]], "rel_plus": [["DA - RNNs:Method", "SubClass-Of", "recurrent neural network:Method"], ["DA - RNNs:Method", "Used-For", "semantic labeling:Task"]]}
{"doc_id": "210702798", "sentence": "Hu et al. [ 8 9 ] developed a semantic segmentation algorithm based on natural language expression , using a combination of CNN to encode the image and LSTM to encode its natural language description .", "ner": [["semantic segmentation", "Task"], ["CNN", "Method"], ["LSTM", "Method"]], "rel": [["CNN", "Used-For", "semantic segmentation"], ["LSTM", "Used-For", "semantic segmentation"]], "rel_plus": [["CNN:Method", "Used-For", "semantic segmentation:Task"], ["LSTM:Method", "Used-For", "semantic segmentation:Task"]]}
{"doc_id": "210702798", "sentence": "In the considered model , a recurrent LSTM network is used to encode the referential expression into a vector representation , and an FCN is used to extract a spatial feature map from the image and output a spatial response map for the target object .", "ner": [["LSTM", "Method"], ["FCN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210702798", "sentence": "An example segmentation result of this model ( for the query \" people in blue coat \" ) is shown in Figure 3 3 .   Attention mechanisms have been persistently explored in computer vision over the years , and it is therefore not surprising to find publications that apply such mechanisms to semantic segmentation .", "ner": [["segmentation", "Task"], ["Attention mechanisms", "Method"], ["computer vision", "Task"], ["semantic segmentation", "Task"]], "rel": [["Attention mechanisms", "Used-For", "computer vision"], ["Attention mechanisms", "Used-For", "semantic segmentation"]], "rel_plus": [["Attention mechanisms:Method", "Used-For", "computer vision:Task"], ["Attention mechanisms:Method", "Used-For", "semantic segmentation:Task"]]}
{"doc_id": "210702798", "sentence": "Their Reverse Attention Network ( RAN ) architecture ( Figure 3 5 ) trains the model to capture the opposite concept ( i.e. , features that are not associated with a target class ) as well .", "ner": [["Reverse Attention Network", "Method"], ["RAN", "Method"]], "rel": [["RAN", "Synonym-Of", "Reverse Attention Network"]], "rel_plus": [["RAN:Method", "Synonym-Of", "Reverse Attention Network:Method"]]}
{"doc_id": "210702798", "sentence": "Li et al. [ 9 2 ] developed a Pyramid Attention Network for semantic segmentation .", "ner": [["Pyramid Attention Network", "Method"], ["semantic segmentation", "Task"]], "rel": [["Pyramid Attention Network", "Used-For", "semantic segmentation"]], "rel_plus": [["Pyramid Attention Network:Method", "Used-For", "semantic segmentation:Task"]]}
{"doc_id": "210702798", "sentence": "More recently , Fu et al. [ 9 3 ] proposed a dual attention network for scene segmentation , which can capture rich contextual dependencies based on the self - attention mechanism .", "ner": [["dual attention network", "Method"], ["scene segmentation", "Task"], ["self - attention mechanism", "Method"]], "rel": [["self - attention mechanism", "Part-Of", "dual attention network"], ["dual attention network", "Used-For", "scene segmentation"]], "rel_plus": [["self - attention mechanism:Method", "Part-Of", "dual attention network:Method"], ["dual attention network:Method", "Used-For", "scene segmentation:Task"]]}
{"doc_id": "210702798", "sentence": "Various other works explore attention mechanisms for semantic segmentation , such as OCNet [ 9 4 ] which proposed an object context pooling inspired by self - attention mechanism , Expectation - Maximization Attention ( EMANet ) [ 9 5 ] , Criss - Cross Attention Network ( CCNet ) [ 9 6 ] , end - to - end instance segmentation with recurrent attention [ 9 7 ] , a pointwise spatial attention network for scene parsing [ 9 8 ] , and a discriminative feature network ( DFN ) [ 9 9 ] , which comprises two sub - networks : a Smooth Network ( that contains a Channel Attention Block and global average pooling to select the more discriminative features ) and a Border Network ( to make the bilateral features of the boundary distinguishable ) .   Since their introduction , GANs have been applied to a wide range tasks in computer vision , and have been adopted for image segmentation too .", "ner": [["attention mechanisms", "Method"], ["semantic segmentation", "Task"], ["OCNet", "Method"], ["self - attention mechanism", "Method"], ["Expectation - Maximization Attention", "Method"], ["EMANet", "Method"], ["Criss - Cross Attention Network", "Method"], ["CCNet", "Method"], ["end - to - end instance segmentation with recurrent attention", "Method"], ["pointwise spatial attention network", "Method"], ["scene parsing", "Task"], ["discriminative feature network", "Method"], ["DFN", "Method"], ["Smooth Network", "Method"], ["Channel Attention Block", "Method"], ["global average pooling", "Method"], ["Border Network", "Method"], ["GANs", "Method"], ["computer vision", "Task"], ["image segmentation", "Task"]], "rel": [["attention mechanisms", "Used-For", "semantic segmentation"], ["OCNet", "Used-For", "semantic segmentation"], ["EMANet", "Synonym-Of", "Expectation - Maximization Attention"], ["CCNet", "Synonym-Of", "Criss - Cross Attention Network"], ["pointwise spatial attention network", "Used-For", "scene parsing"], ["DFN", "Synonym-Of", "discriminative feature network"], ["Smooth Network", "Part-Of", "discriminative feature network"], ["Border Network", "Part-Of", "discriminative feature network"], ["global average pooling", "Part-Of", "Smooth Network"], ["Channel Attention Block", "Part-Of", "Smooth Network"], ["GANs", "Used-For", "computer vision"], ["image segmentation", "SubTask-Of", "computer vision"], ["GANs", "Used-For", "image segmentation"]], "rel_plus": [["attention mechanisms:Method", "Used-For", "semantic segmentation:Task"], ["OCNet:Method", "Used-For", "semantic segmentation:Task"], ["EMANet:Method", "Synonym-Of", "Expectation - Maximization Attention:Method"], ["CCNet:Method", "Synonym-Of", "Criss - Cross Attention Network:Method"], ["pointwise spatial attention network:Method", "Used-For", "scene parsing:Task"], ["DFN:Method", "Synonym-Of", "discriminative feature network:Method"], ["Smooth Network:Method", "Part-Of", "discriminative feature network:Method"], ["Border Network:Method", "Part-Of", "discriminative feature network:Method"], ["global average pooling:Method", "Part-Of", "Smooth Network:Method"], ["Channel Attention Block:Method", "Part-Of", "Smooth Network:Method"], ["GANs:Method", "Used-For", "computer vision:Task"], ["image segmentation:Task", "SubTask-Of", "computer vision:Task"], ["GANs:Method", "Used-For", "image segmentation:Task"]]}
{"doc_id": "210702798", "sentence": "They trained a convolutional semantic segmentation network ( Figure 3 7 ) , along with an adversarial network that discriminates ground - truth segmentation maps from those generated by the segmentation network .", "ner": [["convolutional semantic segmentation network", "Method"], ["adversarial network", "Method"], ["segmentation maps", "Task"], ["segmentation network", "Method"]], "rel": [["adversarial network", "Part-Of", "convolutional semantic segmentation network"], ["adversarial network", "Used-For", "segmentation maps"]], "rel_plus": [["adversarial network:Method", "Part-Of", "convolutional semantic segmentation network:Method"], ["adversarial network:Method", "Used-For", "segmentation maps:Task"]]}
{"doc_id": "210702798", "sentence": "They showed that the adversarial training approach leads to improved accuracy on the Stanford Background and PASCAL VOC 2 0 1 2 datasets .", "ner": [["Stanford Background", "Dataset"], ["PASCAL VOC 2 0 1 2", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "210702798", "sentence": "Souly et al. [ 1 0 1 ] proposed semi - weakly supervised semantic segmentation using GANs .", "ner": [["semi - weakly supervised semantic segmentation", "Task"], ["GANs", "Method"]], "rel": [["GANs", "Used-For", "semi - weakly supervised semantic segmentation"]], "rel_plus": [["GANs:Method", "Used-For", "semi - weakly supervised semantic segmentation:Task"]]}
{"doc_id": "210702798", "sentence": "They designed an FCN discriminator to differentiate the predicted probability maps from the ground truth segmentation distribution , considering the spatial resolution .", "ner": [["FCN discriminator", "Method"], ["segmentation", "Task"]], "rel": [["FCN discriminator", "Used-For", "segmentation"]], "rel_plus": [["FCN discriminator:Method", "Used-For", "segmentation:Task"]]}
{"doc_id": "210702798", "sentence": "They used an FCN as the segmentor to generate segmentation label maps , and proposed a novel adversarial critic network with a multi - scale L 1 loss function to force the critic and segmentor to learn both global and local features that capture long and short range spatial relationships between pixels .", "ner": [["FCN", "Method"], ["generate segmentation label maps", "Task"]], "rel": [["FCN", "Used-For", "generate segmentation label maps"]], "rel_plus": [["FCN:Method", "Used-For", "generate segmentation label maps:Task"]]}
{"doc_id": "210702798", "sentence": "Various other publications report on segmentation models based on adversarial training , such as Cell Image Segmentation Using GANs [ 1 0 4 ] , and segmentation and generation of the invisible parts of objects [ 1 0 5 ] .", "ner": [["segmentation", "Task"], ["Cell Image Segmentation", "Method"], ["GANs", "Method"], ["segmentation", "Task"]], "rel": [["Cell Image Segmentation", "Used-For", "segmentation"], ["GANs", "Part-Of", "Cell Image Segmentation"]], "rel_plus": [["Cell Image Segmentation:Method", "Used-For", "segmentation:Task"], ["GANs:Method", "Part-Of", "Cell Image Segmentation:Method"]]}
{"doc_id": "210702798", "sentence": "The exploration of synergies between FCNs and Active Contour Models ( ACMs ) [ 7 ] has recently attracted research interest .", "ner": [["FCNs", "Method"], ["Active Contour Models", "Method"], ["ACMs", "Method"]], "rel": [["ACMs", "Synonym-Of", "Active Contour Models"]], "rel_plus": [["ACMs:Method", "Synonym-Of", "Active Contour Models:Method"]]}
{"doc_id": "210702798", "sentence": "For example , inspired by the global energy formulation of [ 1 0 6 ] , Chen et al. [ 1 0 7 ] proposed a supervised loss layer that incorporated area and size information of the predicted masks during training of an FCN and tackled the problem of ventricle segmentation in cardiac MRI .", "ner": [["FCN", "Method"], ["ventricle segmentation in cardiac MRI", "Task"]], "rel": [["FCN", "Used-For", "ventricle segmentation in cardiac MRI"]], "rel_plus": [["FCN:Method", "Used-For", "ventricle segmentation in cardiac MRI:Task"]]}
{"doc_id": "210702798", "sentence": "Similarly , Gur et al. [ 1 0 8 ] presented an unsupervised loss function based on morphological active contours without edges [ 1 0 9 ] for microvascular image segmentation .", "ner": [["morphological active contours", "Method"], ["microvascular image segmentation", "Task"]], "rel": [["morphological active contours", "Used-For", "microvascular image segmentation"]], "rel_plus": [["morphological active contours:Method", "Used-For", "microvascular image segmentation:Task"]]}
{"doc_id": "210702798", "sentence": "A different approach initially sought to utilize the ACM merely as a post - processor of the output of an FCN and several efforts attempted modest co - learning by pre - training the FCN .", "ner": [["ACM", "Method"], ["FCN", "Method"], ["FCN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210702798", "sentence": "One example of an ACM post - processor for the task of semantic segmentation of natural images is the work by Le et al. [ 1 1 0 ] in which level - set ACMs are implemented as RNNs .", "ner": [["ACM", "Method"], ["semantic segmentation", "Task"], ["ACMs", "Method"], ["RNNs", "Method"]], "rel": [["ACM", "Used-For", "semantic segmentation"], ["RNNs", "Used-For", "ACMs"]], "rel_plus": [["ACM:Method", "Used-For", "semantic segmentation:Task"], ["RNNs:Method", "Used-For", "ACMs:Method"]]}
{"doc_id": "210702798", "sentence": "For medical image segmentation , Hatamizadeh et al. [ 1 1 2 ] proposed an integrated Deep Active Lesion Segmentation ( DALS ) model that trains the FCN backbone to predict the parameter functions of a novel , locallyparameterized level - set energy functional .", "ner": [["medical image segmentation", "Task"], ["Deep Active Lesion Segmentation", "Method"], ["DALS", "Method"], ["FCN", "Method"]], "rel": [["Deep Active Lesion Segmentation", "Used-For", "medical image segmentation"], ["DALS", "Synonym-Of", "Deep Active Lesion Segmentation"], ["FCN", "Part-Of", "Deep Active Lesion Segmentation"]], "rel_plus": [["Deep Active Lesion Segmentation:Method", "Used-For", "medical image segmentation:Task"], ["DALS:Method", "Synonym-Of", "Deep Active Lesion Segmentation:Method"], ["FCN:Method", "Part-Of", "Deep Active Lesion Segmentation:Method"]]}
{"doc_id": "210702798", "sentence": "In another relevant effort , Marcos et al. [ 1 1 3 ] proposed Deep Structured Active Contours ( DSAC ) , which combines ACMs and pre - trained FCNs in a structured prediction framework for building instance segmentation ( albeit with manual initialization ) in aerial images .", "ner": [["Deep Structured Active Contours", "Method"], ["DSAC", "Method"], ["ACMs", "Method"], ["FCNs", "Method"], ["structured prediction framework", "Method"], ["building instance segmentation", "Task"]], "rel": [["DSAC", "Synonym-Of", "Deep Structured Active Contours"], ["ACMs", "Part-Of", "Deep Structured Active Contours"], ["structured prediction framework", "Part-Of", "Deep Structured Active Contours"], ["FCNs", "Part-Of", "Deep Structured Active Contours"], ["FCNs", "Part-Of", "structured prediction framework"], ["ACMs", "Part-Of", "structured prediction framework"], ["structured prediction framework", "Used-For", "building instance segmentation"]], "rel_plus": [["DSAC:Method", "Synonym-Of", "Deep Structured Active Contours:Method"], ["ACMs:Method", "Part-Of", "Deep Structured Active Contours:Method"], ["structured prediction framework:Method", "Part-Of", "Deep Structured Active Contours:Method"], ["FCNs:Method", "Part-Of", "Deep Structured Active Contours:Method"], ["FCNs:Method", "Part-Of", "structured prediction framework:Method"], ["ACMs:Method", "Part-Of", "structured prediction framework:Method"], ["structured prediction framework:Method", "Used-For", "building instance segmentation:Task"]]}
{"doc_id": "210702798", "sentence": "For the same application , Cheng et al. [ 1 1 4 ] proposed the Deep Active Ray Network ( DarNet ) , which is similar to DSAC , but with a different explicit ACM formulation based on polar coordinates to prevent contour self - intersection .", "ner": [["Deep Active Ray Network", "Method"], ["DarNet", "Method"], ["DSAC", "Method"], ["ACM", "Method"]], "rel": [["DarNet", "Synonym-Of", "Deep Active Ray Network"], ["ACM", "Part-Of", "Deep Active Ray Network"]], "rel_plus": [["DarNet:Method", "Synonym-Of", "Deep Active Ray Network:Method"], ["ACM:Method", "Part-Of", "Deep Active Ray Network:Method"]]}
{"doc_id": "210702798", "sentence": "A truly end - to - end backpropagation trainable , fully - integrated FCN - ACM combination was recently introduced by Hatamizadeh et al. [ 1 1 5 ] , dubbed Deep Convolutional Active Contours ( DCAC ) .", "ner": [["backpropagation", "Method"], ["FCN - ACM", "Method"], ["Deep Convolutional Active Contours", "Method"], ["DCAC", "Method"]], "rel": [["DCAC", "Synonym-Of", "Deep Convolutional Active Contours"]], "rel_plus": [["DCAC:Method", "Synonym-Of", "Deep Convolutional Active Contours:Method"]]}
{"doc_id": "210702798", "sentence": "In addition to the above models , there are several other popular DL architectures for segmentation , such as the following : Context Encoding Network ( EncNet ) that uses a basic feature extractor and feeds the feature maps into a Context Encoding Module [ 1 1 6 ] .", "ner": [["DL", "Method"], ["segmentation", "Task"], ["Context Encoding Network", "Method"], ["EncNet", "Method"], ["Context Encoding Module", "Method"]], "rel": [["DL", "Used-For", "segmentation"], ["Context Encoding Network", "Used-For", "segmentation"], ["EncNet", "Synonym-Of", "Context Encoding Network"], ["Context Encoding Module", "Part-Of", "Context Encoding Network"]], "rel_plus": [["DL:Method", "Used-For", "segmentation:Task"], ["Context Encoding Network:Method", "Used-For", "segmentation:Task"], ["EncNet:Method", "Synonym-Of", "Context Encoding Network:Method"], ["Context Encoding Module:Method", "Part-Of", "Context Encoding Network:Method"]]}
{"doc_id": "210702798", "sentence": "RefineNet [ 1 1 7 ] , which is a multi - path refinement network that explicitly exploits all the information available along the down - sampling process to enable highresolution prediction using long - range residual connections . \" Object - Contextual Representations \" ( OCR ) [ 1 1 9 ] , which learns object regions under the supervision of the groundtruth , and computes the object region representation , and the relation between each pixel and each object region , and augment the representation pixels with the object - contextual representation .", "ner": [["RefineNet", "Method"], ["multi - path refinement network", "Method"], ["Object - Contextual Representations \"", "Method"], ["OCR", "Method"]], "rel": [["RefineNet", "SubClass-Of", "multi - path refinement network"], ["OCR", "Synonym-Of", "Object - Contextual Representations \""]], "rel_plus": [["RefineNet:Method", "SubClass-Of", "multi - path refinement network:Method"], ["OCR:Method", "Synonym-Of", "Object - Contextual Representations \":Method"]]}
{"doc_id": "210702798", "sentence": "Seednet [ 1 1 8 ] , which introduced an automatic seed generation technique with deep reinforcement learning that learns to solve the interactive segmentation problem , Feedforward - Net [ 1 2 4 ] which maps image super - pixels to rich feature representations extracted from a sequence of nested regions of increasing extent and exploits statistical structures in the image and in the label space without setting up explicit structured prediction mechanisms .", "ner": [["Seednet", "Method"], ["deep reinforcement learning", "Method"], ["interactive segmentation", "Task"], ["Feedforward - Net", "Method"], ["structured prediction mechanisms", "Method"]], "rel": [["deep reinforcement learning", "Used-For", "Seednet"], ["Seednet", "Used-For", "interactive segmentation"], ["deep reinforcement learning", "Used-For", "interactive segmentation"]], "rel_plus": [["deep reinforcement learning:Method", "Used-For", "Seednet:Method"], ["Seednet:Method", "Used-For", "interactive segmentation:Task"], ["deep reinforcement learning:Method", "Used-For", "interactive segmentation:Task"]]}
{"doc_id": "210702798", "sentence": "Yet additional models include BoxSup [ 1 2 0 ] , Graph convolutional networks [ 1 2 1 ] , Wide ResNet [ 1 2 2 ] , Exfuse ( enhancing low - level and high - level features fusion ) [ 1 2 3 ] , dual image segmentation ( DIS ) [ 1 2 5 ] , FoveaNet ( Perspectiveaware scene parsing ) [ 1 2 6 ] , Ladder DenseNet [ 1 2 7 ] , Bilateral segmentation network ( BiSeNet ) [ 1 2 8 ] , Semantic Prediction Guidance for Scene Parsing ( SPGNet ) [ 1 2 9 ] , Gated shape CNNs [ 1 3 0 ] , Adaptive context network ( AC - Net ) [ 1 3 1 ] , Dynamic - structured semantic propagation network ( DSSPN ) [ 1 3 2 ] , symbolic graph reasoning ( SGR ) [ 1 3 3 ] , CascadeNet [ 1 3 4 ] , Scale - adaptive convolutions ( SAC ) [ 1 3 5 ] , Unified perceptual parsing ( UperNet ) [ 1 3 6 ] .", "ner": [["BoxSup", "Method"], ["Graph convolutional networks", "Method"], ["Wide ResNet", "Method"], ["Exfuse", "Method"], ["enhancing low - level and high - level features fusion", "Method"], ["dual image segmentation", "Method"], ["DIS", "Method"], ["FoveaNet", "Method"], ["Perspectiveaware scene parsing", "Method"], ["Ladder DenseNet", "Method"], ["Bilateral segmentation network", "Method"], ["BiSeNet", "Method"], ["Semantic Prediction Guidance for Scene Parsing", "Method"], ["SPGNet", "Method"], ["Gated shape CNNs", "Method"], ["Adaptive context network", "Method"], ["AC - Net", "Method"], ["Dynamic - structured semantic propagation network", "Method"], ["DSSPN", "Method"], ["symbolic graph reasoning", "Method"], ["SGR", "Method"], ["CascadeNet", "Method"], ["Scale - adaptive convolutions", "Method"], ["SAC", "Method"], ["Unified perceptual parsing", "Method"], ["UperNet", "Method"]], "rel": [["enhancing low - level and high - level features fusion", "Synonym-Of", "Exfuse"], ["DIS", "Synonym-Of", "dual image segmentation"], ["Perspectiveaware scene parsing", "Synonym-Of", "FoveaNet"], ["BiSeNet", "Synonym-Of", "Bilateral segmentation network"], ["SPGNet", "Synonym-Of", "Semantic Prediction Guidance for Scene Parsing"], ["AC - Net", "Synonym-Of", "Adaptive context network"], ["DSSPN", "Synonym-Of", "Dynamic - structured semantic propagation network"], ["SGR", "Synonym-Of", "symbolic graph reasoning"], ["SAC", "Synonym-Of", "Scale - adaptive convolutions"], ["UperNet", "Synonym-Of", "Unified perceptual parsing"]], "rel_plus": [["enhancing low - level and high - level features fusion:Method", "Synonym-Of", "Exfuse:Method"], ["DIS:Method", "Synonym-Of", "dual image segmentation:Method"], ["Perspectiveaware scene parsing:Method", "Synonym-Of", "FoveaNet:Method"], ["BiSeNet:Method", "Synonym-Of", "Bilateral segmentation network:Method"], ["SPGNet:Method", "Synonym-Of", "Semantic Prediction Guidance for Scene Parsing:Method"], ["AC - Net:Method", "Synonym-Of", "Adaptive context network:Method"], ["DSSPN:Method", "Synonym-Of", "Dynamic - structured semantic propagation network:Method"], ["SGR:Method", "Synonym-Of", "symbolic graph reasoning:Method"], ["SAC:Method", "Synonym-Of", "Scale - adaptive convolutions:Method"], ["UperNet:Method", "Synonym-Of", "Unified perceptual parsing:Method"]]}
{"doc_id": "210702798", "sentence": "Panoptic segmentation [ 1 3 7 ] is also another interesting ( and newer ) segmentation problem with rising popularity , and there are already several interesting works on this direction , including Panoptic Feature Pyramid Network [ 1 3 8 ] , attention - guided network for Panoptic segmentation [ 1 3 9 ] , and Seamless Scene Segmentation [ 1 4 0 ] .", "ner": [["Panoptic segmentation", "Task"], ["segmentation", "Task"], ["Panoptic Feature Pyramid Network", "Method"], ["attention - guided network", "Method"], ["Panoptic segmentation", "Task"], ["Seamless Scene Segmentation", "Task"]], "rel": [["Panoptic segmentation", "SubTask-Of", "segmentation"], ["Panoptic Feature Pyramid Network", "Used-For", "Panoptic segmentation"], ["attention - guided network", "Used-For", "Panoptic segmentation"], ["attention - guided network", "Used-For", "Seamless Scene Segmentation"], ["Panoptic Feature Pyramid Network", "Used-For", "Seamless Scene Segmentation"]], "rel_plus": [["Panoptic segmentation:Task", "SubTask-Of", "segmentation:Task"], ["Panoptic Feature Pyramid Network:Method", "Used-For", "Panoptic segmentation:Task"], ["attention - guided network:Method", "Used-For", "Panoptic segmentation:Task"], ["attention - guided network:Method", "Used-For", "Seamless Scene Segmentation:Task"], ["Panoptic Feature Pyramid Network:Method", "Used-For", "Seamless Scene Segmentation:Task"]]}
{"doc_id": "210702798", "sentence": "Figure 4 1 illustrates the timeline of popular DL - based works for semantic segmentation , as well as instance segmentation since 2 0 1 4 .", "ner": [["DL", "Method"], ["semantic segmentation", "Task"], ["instance segmentation", "Task"]], "rel": [["DL", "Used-For", "semantic segmentation"], ["DL", "Used-For", "instance segmentation"]], "rel_plus": [["DL:Method", "Used-For", "semantic segmentation:Task"], ["DL:Method", "Used-For", "instance segmentation:Task"]]}
{"doc_id": "210702798", "sentence": "Data augmentation serves to increase the number of training samples by applying a set of transformation ( either in the data space , or feature space , or sometimes both ) to the images ( i.e. , both the input image and the segmentation map ) .", "ner": [["Data augmentation", "Method"], ["segmentation map", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "210702798", "sentence": "Data augmentation has proven to improve the performance of the models , especially when learning from limited datasets , such as those in medical image analysis .", "ner": [["Data augmentation", "Method"], ["medical image analysis", "Task"]], "rel": [["Data augmentation", "Used-For", "medical image analysis"]], "rel_plus": [["Data augmentation:Method", "Used-For", "medical image analysis:Task"]]}
{"doc_id": "210702798", "sentence": "The following are some of the most popular : PASCAL Visual Object Classes ( VOC ) [ 1 4 1 ] is one of most popular datasets in computer vision , with annotated images available for 5 tasks - classification , segmentation , detection , action recognition , and person layout .", "ner": [["PASCAL Visual Object Classes", "Dataset"], ["VOC", "Dataset"], ["computer vision", "Task"], ["classification", "Task"], ["segmentation", "Task"], ["detection", "Task"], ["action recognition", "Task"], ["person layout", "Task"]], "rel": [["VOC", "Synonym-Of", "PASCAL Visual Object Classes"], ["PASCAL Visual Object Classes", "Benchmark-For", "computer vision"], ["classification", "SubTask-Of", "computer vision"], ["segmentation", "SubTask-Of", "computer vision"], ["detection", "SubTask-Of", "computer vision"], ["action recognition", "SubTask-Of", "computer vision"], ["PASCAL Visual Object Classes", "Benchmark-For", "classification"], ["PASCAL Visual Object Classes", "Benchmark-For", "segmentation"], ["PASCAL Visual Object Classes", "Benchmark-For", "detection"], ["PASCAL Visual Object Classes", "Benchmark-For", "action recognition"], ["PASCAL Visual Object Classes", "Benchmark-For", "person layout"]], "rel_plus": [["VOC:Dataset", "Synonym-Of", "PASCAL Visual Object Classes:Dataset"], ["PASCAL Visual Object Classes:Dataset", "Benchmark-For", "computer vision:Task"], ["classification:Task", "SubTask-Of", "computer vision:Task"], ["segmentation:Task", "SubTask-Of", "computer vision:Task"], ["detection:Task", "SubTask-Of", "computer vision:Task"], ["action recognition:Task", "SubTask-Of", "computer vision:Task"], ["PASCAL Visual Object Classes:Dataset", "Benchmark-For", "classification:Task"], ["PASCAL Visual Object Classes:Dataset", "Benchmark-For", "segmentation:Task"], ["PASCAL Visual Object Classes:Dataset", "Benchmark-For", "detection:Task"], ["PASCAL Visual Object Classes:Dataset", "Benchmark-For", "action recognition:Task"], ["PASCAL Visual Object Classes:Dataset", "Benchmark-For", "person layout:Task"]]}
{"doc_id": "210702798", "sentence": "From [ 1 4 1 ] . [ 1 4 2 ] is an extension of the PASCAL VOC 2 0 1 0 detection challenge , and it contains pixel - wise labels for all training images .", "ner": [["PASCAL VOC 2 0 1 0 detection", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "210702798", "sentence": "It contains more than 4 0 0 classes ( including the original 2 0 classes plus backgrounds from PASCAL VOC segmentation ) , divided into three categories ( objects , stuff , and hybrids ) .", "ner": [["PASCAL VOC segmentation", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "210702798", "sentence": "Figure 4 3 shows the segmentation map of three sample images of this dataset . [ 1 4 3 ] is another large - scale object detection , segmentation , and captioning dataset .", "ner": [["segmentation map", "Task"], ["object detection", "Task"], ["segmentation", "Task"], ["captioning", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "210702798", "sentence": "A sample image and its segmentation map in COCO , and its comparison with previous datasets .", "ner": [["segmentation map", "Task"], ["COCO", "Dataset"]], "rel": [["COCO", "Benchmark-For", "segmentation map"]], "rel_plus": [["COCO:Dataset", "Benchmark-For", "segmentation map:Task"]]}
{"doc_id": "210702798", "sentence": "From [ 1 4 3 ] .   Cityscapes [ 1 4 4 ] is a large - scale database with a focus on semantic understanding of urban street scenes .", "ner": [["Cityscapes", "Dataset"], ["semantic understanding of urban street scenes", "Task"]], "rel": [["Cityscapes", "Benchmark-For", "semantic understanding of urban street scenes"]], "rel_plus": [["Cityscapes:Dataset", "Benchmark-For", "semantic understanding of urban street scenes:Task"]]}
{"doc_id": "210702798", "sentence": "ADE 2 0 K / MIT Scene Parsing ( SceneParse 1 5 0 ) offers a standard training and evaluation platform for scene parsing algorithms .", "ner": [["ADE 2 0 K", "Dataset"], ["MIT Scene Parsing", "Dataset"], ["SceneParse 1 5 0", "Dataset"], ["scene parsing", "Method"]], "rel": [["scene parsing", "Evaluated-With", "ADE 2 0 K"], ["scene parsing", "Evaluated-With", "MIT Scene Parsing"], ["scene parsing", "Evaluated-With", "SceneParse 1 5 0"]], "rel_plus": [["scene parsing:Method", "Evaluated-With", "ADE 2 0 K:Dataset"], ["scene parsing:Method", "Evaluated-With", "MIT Scene Parsing:Dataset"], ["scene parsing:Method", "Evaluated-With", "SceneParse 1 5 0:Dataset"]]}
{"doc_id": "210702798", "sentence": "Stanford background [ 1 4 6 ] contains outdoor images of scenes from existing datasets , such as LabelMe , MSRC , and PASCAL VOC .", "ner": [["Stanford background", "Dataset"], ["LabelMe", "Dataset"], ["MSRC", "Dataset"], ["PASCAL VOC", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "210702798", "sentence": "Berkeley Segmentation Dataset ( BSD ) [ 1 4 7 ] contains 1 2 , 0 0 0 hand - labeled segmentations of 1, 0 0 0 Corel dataset images from 3 0 human subjects .", "ner": [["Berkeley Segmentation Dataset", "Dataset"], ["BSD", "Dataset"]], "rel": [["BSD", "Synonym-Of", "Berkeley Segmentation Dataset"]], "rel_plus": [["BSD:Dataset", "Synonym-Of", "Berkeley Segmentation Dataset:Dataset"]]}
{"doc_id": "210702798", "sentence": "It aims to provide an empirical basis for research on image segmentation and boundary detection .", "ner": [["image segmentation", "Task"], ["boundary detection", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "210702798", "sentence": "Youtube - Objects [ 1 4 8 ] contains videos collected from YouTube , which include objects from ten PASCAL VOC classes ( aeroplane , bird , boat , car , cat , cow , dog , horse , motorbike , and train ) .", "ner": [["Youtube - Objects", "Dataset"], ["PASCAL VOC", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "210702798", "sentence": "KITTI [ 1 5 0 ] is one of the most popular datasets for mobile robotics and autonomous driving .", "ner": [["KITTI", "Dataset"], ["mobile robotics", "Task"], ["autonomous driving", "Task"]], "rel": [["KITTI", "Benchmark-For", "mobile robotics"], ["KITTI", "Benchmark-For", "autonomous driving"]], "rel_plus": [["KITTI:Dataset", "Benchmark-For", "mobile robotics:Task"], ["KITTI:Dataset", "Benchmark-For", "autonomous driving:Task"]]}
{"doc_id": "210702798", "sentence": "Other Datasets are available for image segmentation purposes too , such as Semantic Boundaries Dataset ( SBD ) [ 1 5 2 ] , PASCAL Part [ 1 5 3 ] , SYNTHIA [ 1 5 4 ] , and Adobes Portrait Segmentation [ 1 5 5 ] .", "ner": [["image segmentation", "Task"], ["Semantic Boundaries Dataset", "Dataset"], ["SBD", "Dataset"], ["PASCAL Part", "Dataset"], ["SYNTHIA", "Dataset"], ["Adobes Portrait Segmentation", "Dataset"]], "rel": [["Semantic Boundaries Dataset", "Benchmark-For", "image segmentation"], ["PASCAL Part", "Benchmark-For", "image segmentation"], ["SYNTHIA", "Benchmark-For", "image segmentation"], ["Adobes Portrait Segmentation", "Benchmark-For", "image segmentation"], ["SBD", "Synonym-Of", "Semantic Boundaries Dataset"]], "rel_plus": [["Semantic Boundaries Dataset:Dataset", "Benchmark-For", "image segmentation:Task"], ["PASCAL Part:Dataset", "Benchmark-For", "image segmentation:Task"], ["SYNTHIA:Dataset", "Benchmark-For", "image segmentation:Task"], ["Adobes Portrait Segmentation:Dataset", "Benchmark-For", "image segmentation:Task"], ["SBD:Dataset", "Synonym-Of", "Semantic Boundaries Dataset:Dataset"]]}
{"doc_id": "210702798", "sentence": "SUN RGB - D [ 1 5 8 ] provides an RGB - D benchmark for the goal of advancing the state - of - the - art in all major scene understanding tasks .", "ner": [["SUN RGB - D", "Dataset"], ["scene understanding", "Task"]], "rel": [["SUN RGB - D", "Benchmark-For", "scene understanding"]], "rel_plus": [["SUN RGB - D:Dataset", "Benchmark-For", "scene understanding:Task"]]}
{"doc_id": "210702798", "sentence": "The objects are organized into 5 1 categories , arranged using WordNet hypernym - hyponym relationships ( similar to ImageNet ) .", "ner": [["WordNet", "Dataset"], ["ImageNet", "Dataset"]], "rel": [["WordNet", "Compare-With", "ImageNet"]], "rel_plus": [["WordNet:Dataset", "Compare-With", "ImageNet:Dataset"]]}
{"doc_id": "210702798", "sentence": "ScanNet [ 1 6 0 ] is an RGB - D video dataset containing 2. 5 million views in more than 1, 5 0 0 scans , annotated with 3D camera poses , surface reconstructions , and instancelevel semantic segmentations .", "ner": [["ScanNet", "Dataset"], ["3D camera poses", "Task"], ["surface reconstructions", "Task"], ["instancelevel semantic segmentations", "Task"]], "rel": [["ScanNet", "Benchmark-For", "3D camera poses"], ["ScanNet", "Benchmark-For", "surface reconstructions"], ["ScanNet", "Benchmark-For", "instancelevel semantic segmentations"]], "rel_plus": [["ScanNet:Dataset", "Benchmark-For", "3D camera poses:Task"], ["ScanNet:Dataset", "Benchmark-For", "surface reconstructions:Task"], ["ScanNet:Dataset", "Benchmark-For", "instancelevel semantic segmentations:Task"]]}
{"doc_id": "210702798", "sentence": "Using this data helped achieve state - of - the - art performance on several 3D scene understanding tasks , including 3D object classification , semantic voxel labeling , and CAD model retrieval . 3D image datasets are popular in robotic , medical image analysis , 3D scene analysis , and construction applications .", "ner": [["3D scene understanding", "Task"], ["3D object classification", "Task"], ["semantic voxel labeling", "Task"], ["CAD model retrieval", "Task"], ["robotic", "Task"], ["medical image analysis", "Task"], ["3D scene analysis", "Task"]], "rel": [["3D object classification", "SubTask-Of", "3D scene understanding"], ["semantic voxel labeling", "SubTask-Of", "3D scene understanding"], ["CAD model retrieval", "SubTask-Of", "3D scene understanding"]], "rel_plus": [["3D object classification:Task", "SubTask-Of", "3D scene understanding:Task"], ["semantic voxel labeling:Task", "SubTask-Of", "3D scene understanding:Task"], ["CAD model retrieval:Task", "SubTask-Of", "3D scene understanding:Task"]]}
{"doc_id": "210702798", "sentence": "ShapeNet Core : ShapeNetCore is a subset of the full ShapeNet dataset [ 1 6 2 ] with single clean 3D models and manually verified category and alignment annotations [ 1 6 3 ] .", "ner": [["ShapeNet Core", "Dataset"], ["ShapeNetCore", "Dataset"], ["ShapeNet", "Dataset"]], "rel": [["ShapeNetCore", "SubClass-Of", "ShapeNet"]], "rel_plus": [["ShapeNetCore:Dataset", "SubClass-Of", "ShapeNet:Dataset"]]}
{"doc_id": "210702798", "sentence": "In this section , we first provide a summary of some of the popular metrics used in evaluating the performance of segmentation models , and then we provide the quantitative performance of the promising DL - based segmentation models on popular datasets .", "ner": [["segmentation", "Task"], ["DL", "Method"], ["segmentation", "Task"]], "rel": [["DL", "Used-For", "segmentation"]], "rel_plus": [["DL:Method", "Used-For", "segmentation:Task"]]}
{"doc_id": "210702798", "sentence": "It is defined as the area of intersection between the predicted segmentation map and the ground truth , divided by the area of union between the predicted segmentation map and the ground truth : where A and B denote the ground truth and the predicted segmentation maps , respectively .", "ner": [["segmentation map", "Task"], ["segmentation map", "Task"], ["segmentation maps", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "210702798", "sentence": "Furthermore , only a small percentage of publications provide additional information , such as execution time and memory footprint , in a reproducible way , which is important to industrial applications of segmentation models ( such as drones , selfdriving cars , robotics , etc . ) that may run on embedded consumer devices with limited computational power and storage , making fast , light - weight models crucial .", "ner": [["segmentation models", "Method"], ["drones", "Task"], ["selfdriving cars", "Task"], ["robotics", "Task"]], "rel": [["segmentation models", "Used-For", "drones"], ["segmentation models", "Used-For", "selfdriving cars"], ["segmentation models", "Used-For", "robotics"]], "rel_plus": [["segmentation models:Method", "Used-For", "drones:Task"], ["segmentation models:Method", "Used-For", "selfdriving cars:Task"], ["segmentation models:Method", "Used-For", "robotics:Task"]]}
{"doc_id": "210702798", "sentence": "ResNet - 5 0 3 7 . 9 DSSPN [ 1 3 2 ] ResNet - 1 0 1 3 7 . 3 EMA - Net [ 9 5 ] ResNet - 5 0 3 7 . 5 SGR [ 1 3 3 ] ResNet - 1 0 1 3 9 . 1 OCR [ 1 1 9 ] ResNet - 1 0 1 3 9 . 5 DANet [ 9 3 ] ResNet - 1 0 1 3 9 . 7 EMA - Net [ 9 5 ] ResNet - 5 0 3 9 . 9 AC - Net [ 1 3 1 ] ResNet - 1 0 1 4 0 . 1 OCR [ 1 1 9 ] HRNetV 2 - W 4 8 4 0 . 5 ResNet - 1 0 1 4 3 . 6 8 DM - Net [ 5 9 ] ResNet - 1 0 1 4 5 . 5 OCR [ 1 1 9 ] HRNetV 2 - W 4 8 4 5 . 6 AC - Net [ 1 3 1 ] ResNet - 1 0 1 4 5 . 9 The following tables summarize the performances of several of the prominent DL - based segmentation models on different datasets .", "ner": [["ResNet - 5 0", "Method"], ["DSSPN", "Method"], ["ResNet - 1 0 1", "Method"], ["EMA - Net", "Method"], ["ResNet - 5 0", "Method"], ["SGR", "Method"], ["ResNet - 1 0 1", "Method"], ["OCR", "Method"], ["ResNet - 1 0 1", "Method"], ["DANet", "Method"], ["ResNet - 1 0 1", "Method"], ["EMA - Net", "Method"], ["ResNet - 5 0", "Method"], ["AC - Net", "Method"], ["ResNet - 1 0 1", "Method"], ["OCR", "Method"], ["HRNetV 2 - W 4 8", "Method"], ["ResNet - 1 0 1", "Method"], ["DM - Net", "Method"], ["ResNet - 1 0 1", "Method"], ["OCR", "Method"], ["HRNetV 2 - W 4 8", "Method"], ["AC - Net", "Method"], ["ResNet - 1 0 1", "Method"], ["DL", "Method"], ["segmentation", "Task"]], "rel": [["DL", "Used-For", "segmentation"]], "rel_plus": [["DL:Method", "Used-For", "segmentation:Task"]]}
{"doc_id": "210702798", "sentence": "Clearly , there has been much improvement in the accuracy of the models since the introduction of the FCN , the first DL - based image segmentation model . 1 Table 2 focuses on the Cityscape test dataset .", "ner": [["FCN", "Method"], ["DL - based image segmentation model", "Method"], ["Cityscape", "Dataset"]], "rel": [["FCN", "SubClass-Of", "DL - based image segmentation model"]], "rel_plus": [["FCN:Method", "SubClass-Of", "DL - based image segmentation model:Method"]]}
{"doc_id": "210702798", "sentence": "This dataset is more challenging than PASCAL VOC , and Cityescapes , as the highest mIoU is approximately 4 0 % .", "ner": [["PASCAL VOC", "Dataset"], ["Cityescapes", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "210702798", "sentence": "This dataset is also more challenging than the PASCAL VOC and Cityescapes datasets .", "ner": [["PASCAL VOC", "Dataset"], ["Cityescapes", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "210702798", "sentence": "Finally , Table 5 summarizes the performance of several prominent models for RGB - D segmentation on the NYUD - v 2 and SUN - RGBD datasets .", "ner": [["RGB - D segmentation", "Task"], ["NYUD - v 2", "Dataset"], ["SUN - RGBD", "Dataset"]], "rel": [["NYUD - v 2", "Benchmark-For", "RGB - D segmentation"], ["SUN - RGBD", "Benchmark-For", "RGB - D segmentation"]], "rel_plus": [["NYUD - v 2:Dataset", "Benchmark-For", "RGB - D segmentation:Task"], ["SUN - RGBD:Dataset", "Benchmark-For", "RGB - D segmentation:Task"]]}
{"doc_id": "210702798", "sentence": "Note that some works report two versions of their models : one which is only trained on PASCAL VOC and another that is pre - trained on a different dataset ( such as MS - COCO , ImageNet , or JFT - 3 0 0 M ) and then fine - tuned on VOC .", "ner": [["PASCAL VOC", "Dataset"], ["MS - COCO", "Dataset"], ["ImageNet", "Dataset"], ["JFT - 3 0 0 M", "Dataset"], ["VOC", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "210702798", "sentence": "Performance of segmentation models on the NYUD - v 2 , and SUN - RGBD datasets , in terms of mIoU , and mean Accuracy ( mAcc ) .    There is not doubt that image segmentation has benefited greatly from deep learning , but several challenges lie ahead .", "ner": [["segmentation", "Task"], ["NYUD - v 2", "Dataset"], ["SUN - RGBD", "Dataset"], ["image segmentation", "Task"]], "rel": [["NYUD - v 2", "Benchmark-For", "segmentation"], ["SUN - RGBD", "Benchmark-For", "segmentation"]], "rel_plus": [["NYUD - v 2:Dataset", "Benchmark-For", "segmentation:Task"], ["SUN - RGBD:Dataset", "Benchmark-For", "segmentation:Task"]]}
{"doc_id": "210702798", "sentence": "Several large - scale image datasets have been created for semantic segmentation and instance segmentation .", "ner": [["semantic segmentation", "Task"], ["instance segmentation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "210702798", "sentence": "Weakly - supervised ( a.k.a . few shot learning ) and unsupervised learning are becoming very active research areas .", "ner": [["Weakly - supervised", "Task"], ["few shot learning", "Task"], ["unsupervised learning", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "210702798", "sentence": "These techniques promise to be specially valuable for image segmentation , as collecting labeled samples for segmentation problem is problematic in many application domains , particularly so in medical image analysis .", "ner": [["image segmentation", "Task"], ["segmentation", "Task"], ["medical image analysis", "Task"]], "rel": [["image segmentation", "SubTask-Of", "segmentation"], ["image segmentation", "Used-For", "medical image analysis"]], "rel_plus": [["image segmentation:Task", "SubTask-Of", "segmentation:Task"], ["image segmentation:Task", "Used-For", "medical image analysis:Task"]]}
{"doc_id": "210702798", "sentence": "The transfer learning approach is to train a generic image segmentation model on a large set of labeled samples ( perhaps from a public benchmark ) , and then fine - tune that model on a few samples from some specific target application .", "ner": [["transfer learning", "Method"], ["image segmentation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "210702798", "sentence": "There are many details in images that that can be captured to train a segmentation models with far fewer training samples , with the help of self - supervised learning .", "ner": [["segmentation", "Task"], ["self - supervised learning", "Method"]], "rel": [["self - supervised learning", "Used-For", "segmentation"]], "rel_plus": [["self - supervised learning:Method", "Used-For", "segmentation:Task"]]}
{"doc_id": "210702798", "sentence": "This is useful for computer vision systems that are , for example , deployed in autonomous vehicles .", "ner": [["computer vision", "Task"], ["autonomous vehicles", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "210702798", "sentence": "Models based on dilated convolution help to increase the speed of segmentation models to some extent , but there is still plenty of room for improvement .", "ner": [["dilated convolution", "Method"], ["segmentation", "Task"]], "rel": [["dilated convolution", "Used-For", "segmentation"]], "rel_plus": [["dilated convolution:Method", "Used-For", "segmentation:Task"]]}
{"doc_id": "210702798", "sentence": "This can be done either by using simpler models , or by using model compression techniques , or even training a complex model and then using knowledge distillation techniques to compress it into a smaller , memory efficient network that mimics the complex model .", "ner": [["model compression", "Method"], ["knowledge distillation", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210702798", "sentence": "However , there has been an increasing interest in pointcloud segmentation , which has a wide range of applications , in 3D modeling , self - driving cars , robotics , building modeling , etc .", "ner": [["pointcloud segmentation", "Task"], ["3D modeling", "Task"], ["self - driving cars", "Task"], ["robotics", "Task"], ["building modeling", "Task"]], "rel": [["pointcloud segmentation", "Used-For", "3D modeling"], ["pointcloud segmentation", "Used-For", "self - driving cars"], ["pointcloud segmentation", "Used-For", "robotics"], ["pointcloud segmentation", "Used-For", "building modeling"]], "rel_plus": [["pointcloud segmentation:Task", "Used-For", "3D modeling:Task"], ["pointcloud segmentation:Task", "Used-For", "self - driving cars:Task"], ["pointcloud segmentation:Task", "Used-For", "robotics:Task"], ["pointcloud segmentation:Task", "Used-For", "building modeling:Task"]]}
{"doc_id": "210702798", "sentence": "We have surveyed more than 1 0 0 recent image segmentation algorithms based on deep learning models , which have achieved impressive performance in various image segmentation tasks and benchmarks , grouped into ten categories such as : CNN and FCN , RNN , R - CNN , dilated CNN , attentionbased models , generative and adversarial models , among others .", "ner": [["image segmentation", "Task"], ["deep learning", "Method"], ["image segmentation", "Task"], ["CNN", "Method"], ["FCN", "Method"], ["RNN", "Method"], ["R - CNN", "Method"], ["dilated CNN", "Method"]], "rel": [["deep learning", "Used-For", "image segmentation"], ["CNN", "Used-For", "image segmentation"], ["FCN", "Used-For", "image segmentation"], ["RNN", "Used-For", "image segmentation"], ["R - CNN", "Used-For", "image segmentation"], ["dilated CNN", "Used-For", "image segmentation"]], "rel_plus": [["deep learning:Method", "Used-For", "image segmentation:Task"], ["CNN:Method", "Used-For", "image segmentation:Task"], ["FCN:Method", "Used-For", "image segmentation:Task"], ["RNN:Method", "Used-For", "image segmentation:Task"], ["R - CNN:Method", "Used-For", "image segmentation:Task"], ["dilated CNN:Method", "Used-For", "image segmentation:Task"]]}
{"doc_id": "210702798", "sentence": "We summarized quantitative performance analyses of these models on some popular benchmarks , such as the PASCAL VOC , MS COCO , Cityscapes , and ADE 2 0 k datasets .", "ner": [["PASCAL VOC", "Dataset"], ["MS COCO", "Dataset"], ["Cityscapes", "Dataset"], ["ADE 2 0 k", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "160009536", "sentence": "In our experiments , we show how our method improves not only optical flow estimation , but also gesture recognition , offering a speed - accuracy trade - off more realistic for practical robot applications .", "ner": [["optical flow estimation", "Method"], ["gesture recognition", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "160009536", "sentence": "The speech recognition modules installed in house service robots ( e.g. , DeepSpeech [ 4 ] or Google Speech API ) have achieved near human level performance .", "ner": [["speech recognition modules", "Method"], ["DeepSpeech", "Method"], ["Google Speech API", "Method"]], "rel": [["DeepSpeech", "SubClass-Of", "speech recognition modules"], ["Google Speech API", "SubClass-Of", "speech recognition modules"]], "rel_plus": [["DeepSpeech:Method", "SubClass-Of", "speech recognition modules:Method"], ["Google Speech API:Method", "SubClass-Of", "speech recognition modules:Method"]]}
{"doc_id": "160009536", "sentence": "In order to achieve a more reliable human - robot interaction ( HRI ) , an alternative way to communicate with the robot is necessary .", "ner": [["human - robot interaction", "Task"], ["HRI", "Task"]], "rel": [["HRI", "Synonym-Of", "human - robot interaction"]], "rel_plus": [["HRI:Task", "Synonym-Of", "human - robot interaction:Task"]]}
{"doc_id": "160009536", "sentence": "As with humans , gestures provide an alternative modality for communication , and thus , gesture recognition plays an important role in HRI . 1 is with the Graduate School of Information Science and Technology , The University of Tokyo , Japan .", "ner": [["gesture recognition", "Task"], ["HRI", "Task"]], "rel": [["gesture recognition", "Used-For", "HRI"]], "rel_plus": [["gesture recognition:Task", "Used-For", "HRI:Task"]]}
{"doc_id": "160009536", "sentence": "Improved optical flow estimation Gesture recognition Pick up object Open door \u2026 In computer vision , gestures can be recognized using an action recognition method [ 5 ] [ 6 ] .", "ner": [["optical flow estimation", "Method"], ["Gesture recognition", "Task"], ["computer vision", "Task"], ["action recognition", "Task"]], "rel": [["action recognition", "SubTask-Of", "computer vision"]], "rel_plus": [["action recognition:Task", "SubTask-Of", "computer vision:Task"]]}
{"doc_id": "160009536", "sentence": "In this work , we present a gesture recognition pipeline ( Fig. 1 ) designed for human - robot interaction that achieves a good trade - off balance between speed and accuracy .", "ner": [["gesture recognition pipeline", "Method"], ["human - robot interaction", "Task"]], "rel": [["gesture recognition pipeline", "Used-For", "human - robot interaction"]], "rel_plus": [["gesture recognition pipeline:Method", "Used-For", "human - robot interaction:Task"]]}
{"doc_id": "160009536", "sentence": "Our contributions are : \u2022 Three novel optical flow estimation methods that incorporate attention to allow for better gesture recognition . \u2022 An optical - flow based gesture recognition pipeline with improved speed - accuracy trade - off , designed for human - robot interaction . \u2022 A self - generated dataset for human - robot interaction , MIBURI 1 , designed to command the robot to complete household tasks .   Nowadays , most human - robot interaction methods are speech recognition - based [ 7 ] [ 8 ] .", "ner": [["optical flow estimation methods", "Method"], ["gesture recognition", "Task"], ["optical - flow based gesture recognition pipeline", "Method"], ["human - robot interaction", "Task"], ["human - robot interaction", "Task"], ["MIBURI", "Dataset"], ["human - robot interaction", "Task"], ["speech recognition - based", "Method"]], "rel": [["optical flow estimation methods", "Used-For", "gesture recognition"], ["optical - flow based gesture recognition pipeline", "Used-For", "human - robot interaction"], ["MIBURI", "Benchmark-For", "human - robot interaction"], ["speech recognition - based", "Used-For", "human - robot interaction"]], "rel_plus": [["optical flow estimation methods:Method", "Used-For", "gesture recognition:Task"], ["optical - flow based gesture recognition pipeline:Method", "Used-For", "human - robot interaction:Task"], ["MIBURI:Dataset", "Benchmark-For", "human - robot interaction:Task"], ["speech recognition - based:Method", "Used-For", "human - robot interaction:Task"]]}
{"doc_id": "160009536", "sentence": "Most existing works on human - robot interaction ( HRI ) via gestures focus on hand gesture recognition [ 1 1 ] .", "ner": [["human - robot interaction", "Task"], ["HRI", "Task"], ["hand gesture recognition", "Task"]], "rel": [["HRI", "Synonym-Of", "human - robot interaction"], ["hand gesture recognition", "Used-For", "human - robot interaction"]], "rel_plus": [["HRI:Task", "Synonym-Of", "human - robot interaction:Task"], ["hand gesture recognition:Task", "Used-For", "human - robot interaction:Task"]]}
{"doc_id": "160009536", "sentence": "Recent approaches such as Temporal Segment Networks ( TSN ) [ 6 ] , and [ 1 4 ] further extended the two - stream approach .", "ner": [["Temporal Segment Networks", "Method"], ["TSN", "Method"]], "rel": [["TSN", "Synonym-Of", "Temporal Segment Networks"]], "rel_plus": [["TSN:Method", "Synonym-Of", "Temporal Segment Networks:Method"]]}
{"doc_id": "160009536", "sentence": "In optical flow - based gesture recognition , such as the previously described [ 1 4 ] and [ 6 ] , in order to estimate optical flow from the input images , researchers deployed traditional optical flow estimation methods , namely the TVL 1 [ 1 5 ] , and the work of Brox et al. [ 1 6 ] .", "ner": [["optical flow - based gesture recognition", "Method"], ["estimate optical flow", "Task"], ["optical flow estimation methods", "Method"], ["TVL 1", "Method"]], "rel": [["optical flow - based gesture recognition", "Used-For", "estimate optical flow"], ["TVL 1", "SubClass-Of", "optical flow estimation methods"]], "rel_plus": [["optical flow - based gesture recognition:Method", "Used-For", "estimate optical flow:Task"], ["TVL 1:Method", "SubClass-Of", "optical flow estimation methods:Method"]]}
{"doc_id": "160009536", "sentence": "The first attempt of a deep optical flow estimation network , FlowNet [ 1 7 ] , proposed two networks , namely FlowNetS and FlowNetC , and greatly improved the runtime of previous work .", "ner": [["deep optical flow estimation network", "Method"], ["FlowNet", "Method"], ["FlowNetS", "Method"], ["FlowNetC", "Method"]], "rel": [["FlowNet", "SubClass-Of", "deep optical flow estimation network"], ["FlowNetS", "Part-Of", "deep optical flow estimation network"], ["FlowNetC", "Part-Of", "deep optical flow estimation network"]], "rel_plus": [["FlowNet:Method", "SubClass-Of", "deep optical flow estimation network:Method"], ["FlowNetS:Method", "Part-Of", "deep optical flow estimation network:Method"], ["FlowNetC:Method", "Part-Of", "deep optical flow estimation network:Method"]]}
{"doc_id": "160009536", "sentence": "Zhu et al. [ 1 8 ] developed an optical flow estimation method called DenseNet by extending a deep network for image classification , and demonstrated that it outperforms other unsupervised methods .", "ner": [["optical flow estimation method", "Method"], ["DenseNet", "Method"], ["image classification", "Task"]], "rel": [["DenseNet", "SubClass-Of", "optical flow estimation method"], ["DenseNet", "Used-For", "image classification"]], "rel_plus": [["DenseNet:Method", "SubClass-Of", "optical flow estimation method:Method"], ["DenseNet:Method", "Used-For", "image classification:Task"]]}
{"doc_id": "160009536", "sentence": "FlowNet 2. 0 [ 1 9 ] is a follow - up paper of FlowNet [ 1 7 ] .", "ner": [["FlowNet 2. 0", "Method"], ["FlowNet", "Method"]], "rel": [["FlowNet 2. 0", "SubClass-Of", "FlowNet"]], "rel_plus": [["FlowNet 2. 0:Method", "SubClass-Of", "FlowNet:Method"]]}
{"doc_id": "160009536", "sentence": "They stacked two optical flow estimation networks ( FlowNetS and FlowNetC ) and achieved state - of - the - art results comparable to traditional optical flow methods while running on 8 to 1 4 0 fps depending on the desired accuracy ( the higher the accuracy , the slower the runtime ) .", "ner": [["optical flow estimation networks", "Method"], ["FlowNetS", "Method"], ["FlowNetC", "Method"], ["optical flow methods", "Method"]], "rel": [["FlowNetS", "Part-Of", "optical flow estimation networks"], ["FlowNetC", "Part-Of", "optical flow estimation networks"], ["optical flow estimation networks", "Compare-With", "optical flow methods"]], "rel_plus": [["FlowNetS:Method", "Part-Of", "optical flow estimation networks:Method"], ["FlowNetC:Method", "Part-Of", "optical flow estimation networks:Method"], ["optical flow estimation networks:Method", "Compare-With", "optical flow methods:Method"]]}
{"doc_id": "160009536", "sentence": "Figure 1 depicts an overview of the basic pipeline of our gesture recognition method for human - robot interaction ( HRI ) .", "ner": [["gesture recognition method", "Method"], ["human - robot interaction", "Task"], ["HRI", "Task"]], "rel": [["HRI", "Synonym-Of", "human - robot interaction"], ["gesture recognition method", "Used-For", "human - robot interaction"]], "rel_plus": [["HRI:Task", "Synonym-Of", "human - robot interaction:Task"], ["gesture recognition method:Method", "Used-For", "human - robot interaction:Task"]]}
{"doc_id": "160009536", "sentence": "While most gesture/action recognition methods employ optical flow ( Sec. II - C ) , they mainly use traditional optical flow estimation .", "ner": [["gesture/action recognition methods", "Method"], ["optical flow", "Method"], ["optical flow estimation", "Method"]], "rel": [["optical flow", "Part-Of", "gesture/action recognition methods"], ["optical flow estimation", "Part-Of", "gesture/action recognition methods"]], "rel_plus": [["optical flow:Method", "Part-Of", "gesture/action recognition methods:Method"], ["optical flow estimation:Method", "Part-Of", "gesture/action recognition methods:Method"]]}
{"doc_id": "160009536", "sentence": "We argue that using deep learning for estimating optical flow results in a better trade - off between gesture recognition runtime and accuracy , and thus , is adequate for HRI .", "ner": [["deep learning", "Method"], ["estimating optical flow", "Task"], ["gesture recognition", "Task"], ["HRI", "Task"]], "rel": [["deep learning", "Used-For", "estimating optical flow"], ["deep learning", "Used-For", "gesture recognition"], ["gesture recognition", "Used-For", "HRI"]], "rel_plus": [["deep learning:Method", "Used-For", "estimating optical flow:Task"], ["deep learning:Method", "Used-For", "gesture recognition:Task"], ["gesture recognition:Task", "Used-For", "HRI:Task"]]}
{"doc_id": "160009536", "sentence": "We also argue that adding an attention mechanism to deep learning optical flow estimation results in a higher accuracy , not only for optical flow estimation ( reduces blurriness in contours ) , but also for gesture recognition ( refines the user 's silhouette ) .", "ner": [["attention mechanism", "Method"], ["deep learning optical flow estimation", "Method"], ["optical flow estimation", "Task"], ["gesture recognition", "Task"]], "rel": [["attention mechanism", "Part-Of", "deep learning optical flow estimation"], ["deep learning optical flow estimation", "Used-For", "optical flow estimation"], ["deep learning optical flow estimation", "Used-For", "gesture recognition"]], "rel_plus": [["attention mechanism:Method", "Part-Of", "deep learning optical flow estimation:Method"], ["deep learning optical flow estimation:Method", "Used-For", "optical flow estimation:Task"], ["deep learning optical flow estimation:Method", "Used-For", "gesture recognition:Task"]]}
{"doc_id": "160009536", "sentence": "Following the same criteria , we feed our optical flow to the TSN action recognition network [ 6 ] ( Sec. II - B ) , as it features state - of - the - art accuracy with short recognition time .", "ner": [["optical flow", "Method"], ["TSN action recognition network", "Method"], ["recognition", "Task"]], "rel": [["optical flow", "Part-Of", "TSN action recognition network"], ["TSN action recognition network", "Used-For", "recognition"]], "rel_plus": [["optical flow:Method", "Part-Of", "TSN action recognition network:Method"], ["TSN action recognition network:Method", "Used-For", "recognition:Task"]]}
{"doc_id": "160009536", "sentence": "As shown in [ 1 8 ] , in optical flow estimation , feature extractors trained on image classification tasks are recommendable for unsupervised learning .", "ner": [["optical flow estimation", "Method"], ["feature extractors", "Method"], ["image classification", "Task"], ["unsupervised learning", "Method"]], "rel": [["feature extractors", "Part-Of", "optical flow estimation"], ["feature extractors", "Used-For", "image classification"]], "rel_plus": [["feature extractors:Method", "Part-Of", "optical flow estimation:Method"], ["feature extractors:Method", "Used-For", "image classification:Task"]]}
{"doc_id": "160009536", "sentence": "Our novel optical flow estimation uses stronger feature extraction blocks ( i.e. blocks capable of extracting more discriminative features , tested in image classification tasks ) onto an existing baseline estimation network .", "ner": [["optical flow estimation", "Method"], ["feature extraction blocks", "Method"], ["image classification", "Task"]], "rel": [["feature extraction blocks", "Part-Of", "optical flow estimation"]], "rel_plus": [["feature extraction blocks:Method", "Part-Of", "optical flow estimation:Method"]]}
{"doc_id": "160009536", "sentence": "For this purpose , we resorted to four feature extractors widely used for classifying the Im - ageNet dataset [ 2 1 ] : ResNet [ 2 2 ] , Inception [ 2 3 ] , ResNext ( 4x 3 2 d block ) and ResNext ( 4x 6 4 d block ) [ 2 4 ] .", "ner": [["feature extractors", "Method"], ["Im - ageNet", "Dataset"], ["ResNet", "Method"], ["Inception", "Method"], ["ResNext ( 4x 3 2 d block )", "Method"], ["ResNext ( 4x 6 4 d block )", "Method"]], "rel": [["ResNet", "Evaluated-With", "Im - ageNet"], ["Inception", "Evaluated-With", "Im - ageNet"], ["ResNext ( 4x 3 2 d block )", "Evaluated-With", "Im - ageNet"], ["ResNext ( 4x 6 4 d block )", "Evaluated-With", "Im - ageNet"]], "rel_plus": [["ResNet:Method", "Evaluated-With", "Im - ageNet:Dataset"], ["Inception:Method", "Evaluated-With", "Im - ageNet:Dataset"], ["ResNext ( 4x 3 2 d block ):Method", "Evaluated-With", "Im - ageNet:Dataset"], ["ResNext ( 4x 6 4 d block ):Method", "Evaluated-With", "Im - ageNet:Dataset"]]}
{"doc_id": "160009536", "sentence": "We replaced the simple convolutional layers present in FlowNetS [ 1 7 ] with an adaptation of ResNet , Inception and ResNext .", "ner": [["convolutional layers", "Method"], ["FlowNetS", "Method"], ["ResNet", "Method"], ["Inception", "Method"], ["ResNext", "Method"]], "rel": [["convolutional layers", "Part-Of", "FlowNetS"], ["ResNext", "Part-Of", "FlowNetS"], ["Inception", "Part-Of", "FlowNetS"], ["ResNet", "Part-Of", "FlowNetS"]], "rel_plus": [["convolutional layers:Method", "Part-Of", "FlowNetS:Method"], ["ResNext:Method", "Part-Of", "FlowNetS:Method"], ["Inception:Method", "Part-Of", "FlowNetS:Method"], ["ResNet:Method", "Part-Of", "FlowNetS:Method"]]}
{"doc_id": "160009536", "sentence": "We named these optical flow estimation networks as FlowNe - tRes , FlowNetInc , FlowNeXt 3 2 and FlowNeXt 6 4 respectively ( Fig. 2 ) .", "ner": [["optical flow estimation networks", "Method"], ["FlowNe - tRes", "Method"], ["FlowNetInc", "Method"], ["FlowNeXt 3 2", "Method"], ["FlowNeXt 6 4", "Method"]], "rel": [["FlowNe - tRes", "SubClass-Of", "optical flow estimation networks"], ["FlowNetInc", "SubClass-Of", "optical flow estimation networks"], ["FlowNeXt 3 2", "SubClass-Of", "optical flow estimation networks"], ["FlowNeXt 6 4", "SubClass-Of", "optical flow estimation networks"]], "rel_plus": [["FlowNe - tRes:Method", "SubClass-Of", "optical flow estimation networks:Method"], ["FlowNetInc:Method", "SubClass-Of", "optical flow estimation networks:Method"], ["FlowNeXt 3 2:Method", "SubClass-Of", "optical flow estimation networks:Method"], ["FlowNeXt 6 4:Method", "SubClass-Of", "optical flow estimation networks:Method"]]}
{"doc_id": "160009536", "sentence": "Once the feature extraction is improved , we target the optical - flow estimation .", "ner": [["feature extraction", "Task"], ["optical - flow estimation", "Task"]], "rel": [["feature extraction", "Used-For", "optical - flow estimation"]], "rel_plus": [["feature extraction:Task", "Used-For", "optical - flow estimation:Task"]]}
{"doc_id": "160009536", "sentence": "We named these optical flow estimation networks as AttFlowNetRes , AttFlowNetInc , AttFlowNext 3 2 , and At - tFlowNext 6 4 respectively .", "ner": [["optical flow estimation networks", "Method"], ["AttFlowNetRes", "Method"], ["AttFlowNetInc", "Method"], ["AttFlowNext 3 2", "Method"], ["At - tFlowNext 6 4", "Method"]], "rel": [["AttFlowNetRes", "SubClass-Of", "optical flow estimation networks"], ["AttFlowNetInc", "SubClass-Of", "optical flow estimation networks"], ["AttFlowNext 3 2", "SubClass-Of", "optical flow estimation networks"], ["At - tFlowNext 6 4", "SubClass-Of", "optical flow estimation networks"]], "rel_plus": [["AttFlowNetRes:Method", "SubClass-Of", "optical flow estimation networks:Method"], ["AttFlowNetInc:Method", "SubClass-Of", "optical flow estimation networks:Method"], ["AttFlowNext 3 2:Method", "SubClass-Of", "optical flow estimation networks:Method"], ["At - tFlowNext 6 4:Method", "SubClass-Of", "optical flow estimation networks:Method"]]}
{"doc_id": "160009536", "sentence": "It is also noted that starting from here , we added additional upscaling back to the original resolution , as the final estimation resolution of FlowNetS [ 1 7 ] is only to the 1/ 4 of the original resolution .   One of the major challenges with optical flow estimation is predicting a correct general direction of the motion .", "ner": [["FlowNetS", "Method"], ["optical flow estimation", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "160009536", "sentence": "We named these optical flow estimation networks MidFlowNetRes , MidFlowNetInc , MidFlowNext 3 2 , and MidFlowNext 6 4 respectively .", "ner": [["optical flow estimation networks", "Method"], ["MidFlowNetRes", "Method"], ["MidFlowNetInc", "Method"], ["MidFlowNext 3 2", "Method"], ["MidFlowNext 6 4", "Method"]], "rel": [["MidFlowNetRes", "SubClass-Of", "optical flow estimation networks"], ["MidFlowNetInc", "SubClass-Of", "optical flow estimation networks"], ["MidFlowNext 3 2", "SubClass-Of", "optical flow estimation networks"], ["MidFlowNext 6 4", "SubClass-Of", "optical flow estimation networks"]], "rel_plus": [["MidFlowNetRes:Method", "SubClass-Of", "optical flow estimation networks:Method"], ["MidFlowNetInc:Method", "SubClass-Of", "optical flow estimation networks:Method"], ["MidFlowNext 3 2:Method", "SubClass-Of", "optical flow estimation networks:Method"], ["MidFlowNext 6 4:Method", "SubClass-Of", "optical flow estimation networks:Method"]]}
{"doc_id": "160009536", "sentence": "We named these optical flow estimation networks as AttMidFlowNetRes , AttMidFlowNetInc , AttMidFlowNext 3 2 , and AttMidFlowNext 6 4 respectively .", "ner": [["optical flow estimation networks", "Method"], ["AttMidFlowNetRes", "Method"], ["AttMidFlowNetInc", "Method"], ["AttMidFlowNext 3 2", "Method"], ["AttMidFlowNext 6 4", "Method"]], "rel": [["AttMidFlowNetRes", "SubClass-Of", "optical flow estimation networks"], ["AttMidFlowNetInc", "SubClass-Of", "optical flow estimation networks"], ["AttMidFlowNext 3 2", "SubClass-Of", "optical flow estimation networks"], ["AttMidFlowNext 6 4", "SubClass-Of", "optical flow estimation networks"]], "rel_plus": [["AttMidFlowNetRes:Method", "SubClass-Of", "optical flow estimation networks:Method"], ["AttMidFlowNetInc:Method", "SubClass-Of", "optical flow estimation networks:Method"], ["AttMidFlowNext 3 2:Method", "SubClass-Of", "optical flow estimation networks:Method"], ["AttMidFlowNext 6 4:Method", "SubClass-Of", "optical flow estimation networks:Method"]]}
{"doc_id": "160009536", "sentence": "To the best of our knowledge , previous gesture recognition works do not discuss whether having a more accurate optical flow estimator results on more accurate gesture recognition .", "ner": [["gesture recognition", "Task"], ["optical flow estimator", "Method"], ["gesture recognition", "Task"]], "rel": [["optical flow estimator", "Used-For", "gesture recognition"]], "rel_plus": [["optical flow estimator:Method", "Used-For", "gesture recognition:Task"]]}
{"doc_id": "160009536", "sentence": "In each experiment , our proposed method for optical flow estimation and gesture recognition is trained end - to - end .", "ner": [["optical flow estimation", "Task"], ["gesture recognition", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "160009536", "sentence": "For this , in order to evaluate the recognition of gestures to command a home service robot , we generated our own dataset , MIBURI .", "ner": [["recognition", "Task"], ["MIBURI", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "160009536", "sentence": "We evaluated our networks with the FlyingChairs dataset [ 1 7 ] , and the MPI Sintel dataset [ 2 9 ] .", "ner": [["FlyingChairs", "Dataset"], ["MPI Sintel", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "160009536", "sentence": "FlyingChairs [ 1 7 ] is a synthetic dataset designed specifically for training CNNs to estimate optical flow .", "ner": [["FlyingChairs", "Dataset"], ["CNNs", "Method"], ["estimate optical flow", "Task"]], "rel": [["CNNs", "Trained-With", "FlyingChairs"], ["CNNs", "Used-For", "estimate optical flow"], ["FlyingChairs", "Benchmark-For", "estimate optical flow"]], "rel_plus": [["CNNs:Method", "Trained-With", "FlyingChairs:Dataset"], ["CNNs:Method", "Used-For", "estimate optical flow:Task"], ["FlyingChairs:Dataset", "Benchmark-For", "estimate optical flow:Task"]]}
{"doc_id": "160009536", "sentence": "For training , we used exclusively FlyingChairs train set , since MPI Sintel is too small for large - scale deep learning training .", "ner": [["FlyingChairs", "Dataset"], ["MPI Sintel", "Dataset"], ["deep learning", "Method"]], "rel": [["deep learning", "Trained-With", "FlyingChairs"]], "rel_plus": [["deep learning:Method", "Trained-With", "FlyingChairs:Dataset"]]}
{"doc_id": "160009536", "sentence": "Then , FlyingChairs is tested in its test set , and both Sintel results are on their respective train sets . 1 ) Discussion : Stronger feature extractors generalize better : Our base networks with stronger feature extractors perform similarly to FlowNetS and FlowNetC on our training dataset , Fly - ingChairs , but outperform both of these networks on both of the Sintel passes .", "ner": [["FlyingChairs", "Dataset"], ["Sintel", "Dataset"], ["feature extractors", "Method"], ["FlowNetS", "Method"], ["FlowNetC", "Method"], ["Fly - ingChairs", "Dataset"], ["Sintel", "Dataset"]], "rel": [["FlowNetS", "Evaluated-With", "Fly - ingChairs"], ["FlowNetC", "Evaluated-With", "Fly - ingChairs"], ["FlowNetS", "Evaluated-With", "Sintel"], ["FlowNetC", "Evaluated-With", "Sintel"]], "rel_plus": [["FlowNetS:Method", "Evaluated-With", "Fly - ingChairs:Dataset"], ["FlowNetC:Method", "Evaluated-With", "Fly - ingChairs:Dataset"], ["FlowNetS:Method", "Evaluated-With", "Sintel:Dataset"], ["FlowNetC:Method", "Evaluated-With", "Sintel:Dataset"]]}
{"doc_id": "160009536", "sentence": "This is because , while FlowNetS and FlowNetC achieved a good EPE on FlyingChairs , they do not generalize well to other datasets .", "ner": [["FlowNetS", "Method"], ["FlowNetC", "Method"], ["FlyingChairs", "Dataset"]], "rel": [["FlowNetC", "Evaluated-With", "FlyingChairs"], ["FlowNetS", "Evaluated-With", "FlyingChairs"]], "rel_plus": [["FlowNetC:Method", "Evaluated-With", "FlyingChairs:Dataset"], ["FlowNetS:Method", "Evaluated-With", "FlyingChairs:Dataset"]]}
{"doc_id": "160009536", "sentence": "Stronger feature extractors are more robust to noise : Although some related works , e.g. MSCSL , achieved better Sintel clean results , we observed that our base networks achieved better Sintel final results .", "ner": [["feature extractors", "Method"], ["MSCSL", "Method"], ["Sintel", "Dataset"], ["Sintel", "Dataset"]], "rel": [["MSCSL", "Evaluated-With", "Sintel"]], "rel_plus": [["MSCSL:Method", "Evaluated-With", "Sintel:Dataset"]]}
{"doc_id": "160009536", "sentence": "This proves that by adapting stronger feature extractors onto learning optical flow , the networks can understand motion better and ignore the additional atmospheric effects and motion blurs included in the final pass better , thus obtaining closer results between the Sintel clean and Sintel final benchmarks .", "ner": [["feature extractors", "Method"], ["optical flow", "Method"], ["Sintel", "Dataset"], ["Sintel", "Dataset"]], "rel": [["feature extractors", "Part-Of", "optical flow"]], "rel_plus": [["feature extractors:Method", "Part-Of", "optical flow:Method"]]}
{"doc_id": "160009536", "sentence": "Our method outperforms others in FlyingChairs : By adding the attention mechanism , midway estimations and attention - midway combined , we significantly outperformed other methods on the training set , FlyingChairs , with an improvement of between 2 0 % to 4 5 % .", "ner": [["FlyingChairs", "Dataset"], ["attention mechanism", "Method"], ["FlyingChairs", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "160009536", "sentence": "We think it is because of the dataset difference between Sintel and Fly - ingChairs , also discussed in [ 1 7 ] .", "ner": [["Sintel", "Dataset"], ["Fly - ingChairs", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "160009536", "sentence": "Sintel has larger motions than FlyingChairs .", "ner": [["Sintel", "Dataset"], ["FlyingChairs", "Dataset"]], "rel": [["Sintel", "Compare-With", "FlyingChairs"]], "rel_plus": [["Sintel:Dataset", "Compare-With", "FlyingChairs:Dataset"]]}
{"doc_id": "160009536", "sentence": "Hence , when we reached very accurate FlyingChairs estimations , which feature more smaller motions , we will suffer on Sintel .", "ner": [["FlyingChairs", "Dataset"], ["Sintel", "Dataset"]], "rel": [["FlyingChairs", "Compare-With", "Sintel"]], "rel_plus": [["FlyingChairs:Dataset", "Compare-With", "Sintel:Dataset"]]}
{"doc_id": "160009536", "sentence": "Table IV shows the gesture recognition accuracy when using our optical flow estimation methods .", "ner": [["gesture recognition", "Task"], ["optical flow estimation methods", "Method"]], "rel": [["optical flow estimation methods", "Used-For", "gesture recognition"]], "rel_plus": [["optical flow estimation methods:Method", "Used-For", "gesture recognition:Task"]]}
{"doc_id": "160009536", "sentence": "For that , we used our MIBURI dataset and the Isolated Gesture Recognition Challenge ( IGR ) dataset .", "ner": [["MIBURI", "Dataset"], ["Isolated Gesture Recognition Challenge", "Dataset"], ["IGR", "Dataset"]], "rel": [["IGR", "Synonym-Of", "Isolated Gesture Recognition Challenge"]], "rel_plus": [["IGR:Dataset", "Synonym-Of", "Isolated Gesture Recognition Challenge:Dataset"]]}
{"doc_id": "160009536", "sentence": "The IGR dataset ( in ICPR 2 0 1 6 ) was derived from one of the popular gesture recognition datasets , the ChaLearn Gesture Dataset 2 0 1 1 [ 3 0 ] .", "ner": [["IGR dataset", "Dataset"], ["gesture recognition", "Task"], ["ChaLearn Gesture Dataset 2 0 1 1", "Dataset"]], "rel": [["IGR dataset", "Benchmark-For", "gesture recognition"], ["ChaLearn Gesture Dataset 2 0 1 1", "Benchmark-For", "gesture recognition"]], "rel_plus": [["IGR dataset:Dataset", "Benchmark-For", "gesture recognition:Task"], ["ChaLearn Gesture Dataset 2 0 1 1:Dataset", "Benchmark-For", "gesture recognition:Task"]]}
{"doc_id": "160009536", "sentence": "We chose the optical flow branch of the Temporal Segment Network ( TSN ) [ 6 ] as our gesture recognition method , since its performance is state - of - the - art and the speed - accuracy trade - off is well balanced .", "ner": [["optical flow branch", "Method"], ["Temporal Segment Network", "Method"], ["TSN", "Method"], ["gesture recognition", "Task"]], "rel": [["TSN", "Synonym-Of", "Temporal Segment Network"], ["optical flow branch", "Part-Of", "Temporal Segment Network"], ["optical flow branch", "Used-For", "gesture recognition"]], "rel_plus": [["TSN:Method", "Synonym-Of", "Temporal Segment Network:Method"], ["optical flow branch:Method", "Part-Of", "Temporal Segment Network:Method"], ["optical flow branch:Method", "Used-For", "gesture recognition:Task"]]}
{"doc_id": "160009536", "sentence": "We compared it to three baselines : First , improved dense trajectories ( iDT ) [ 3 1 ] with Fisher vector ( FV ) encoding [ 3 2 ] and support vector machines ( SVM ) classifier , which combined were the state of the art before deep learning [ 3 1 ] .", "ner": [["dense trajectories", "Method"], ["iDT", "Method"], ["Fisher vector", "Method"], ["FV", "Method"], ["support vector machines", "Method"], ["SVM", "Method"], ["deep learning", "Method"]], "rel": [["iDT", "Synonym-Of", "dense trajectories"], ["Fisher vector", "Part-Of", "dense trajectories"], ["FV", "Synonym-Of", "Fisher vector"], ["SVM", "Synonym-Of", "support vector machines"]], "rel_plus": [["iDT:Method", "Synonym-Of", "dense trajectories:Method"], ["Fisher vector:Method", "Part-Of", "dense trajectories:Method"], ["FV:Method", "Synonym-Of", "Fisher vector:Method"], ["SVM:Method", "Synonym-Of", "support vector machines:Method"]]}
{"doc_id": "160009536", "sentence": "Second , TSN ( default ) uses traditional ( i.e. , non - deep learning - based ) optical flow estimation [ 1 5 ] .", "ner": [["TSN", "Method"], ["deep learning", "Method"], ["optical flow estimation", "Method"]], "rel": [["TSN", "Used-For", "optical flow estimation"]], "rel_plus": [["TSN:Method", "Used-For", "optical flow estimation:Method"]]}
{"doc_id": "160009536", "sentence": "Third , TSN/FlowNetS is TSN 's optical flow branch replaced with optical flow estimation from FlowNetS [ 1 7 ] .", "ner": [["TSN/FlowNetS", "Method"], ["TSN", "Method"], ["optical flow branch", "Method"], ["optical flow estimation", "Method"], ["FlowNetS", "Method"]], "rel": [["optical flow branch", "Part-Of", "TSN"], ["TSN/FlowNetS", "SubClass-Of", "optical flow branch"], ["optical flow estimation", "Part-Of", "FlowNetS"]], "rel_plus": [["optical flow branch:Method", "Part-Of", "TSN:Method"], ["TSN/FlowNetS:Method", "SubClass-Of", "optical flow branch:Method"], ["optical flow estimation:Method", "Part-Of", "FlowNetS:Method"]]}
{"doc_id": "160009536", "sentence": "Attention improves gesture recognition : Adding attention to our deep learning - based networks translated to a better gesture recognition performance in both datasets .", "ner": [["Attention", "Method"], ["gesture recognition", "Task"], ["attention", "Method"], ["deep learning - based networks", "Method"], ["gesture recognition", "Task"]], "rel": [["Attention", "Used-For", "gesture recognition"], ["attention", "Part-Of", "deep learning - based networks"], ["deep learning - based networks", "Used-For", "gesture recognition"]], "rel_plus": [["Attention:Method", "Used-For", "gesture recognition:Task"], ["attention:Method", "Part-Of", "deep learning - based networks:Method"], ["deep learning - based networks:Method", "Used-For", "gesture recognition:Task"]]}
{"doc_id": "160009536", "sentence": "This effect can be observed for TSN/FlowNeXt{ 3 2 / 6 4 } vs TSN/AttFlowNeXt{ 3 2 / 6 4 } and TSN/MidFlowNeXt{ 3 2 / 6 4 } vs TSN/AttMidFlowNext{ 3 2 / 6 4 }. As we hypothesized , since attention provides a more refined human silhouette , human motion is better represented , and thus , the performance of optical flow - based gesture recognition methods is improved .", "ner": [["TSN/FlowNeXt{", "Method"], ["TSN/AttFlowNeXt{", "Method"], ["TSN/MidFlowNeXt{", "Method"], ["TSN/AttMidFlowNext{", "Method"], ["attention", "Method"], ["optical flow - based gesture recognition methods", "Method"]], "rel": [["TSN/FlowNeXt{", "Compare-With", "TSN/AttFlowNeXt{"], ["TSN/MidFlowNeXt{", "Compare-With", "TSN/AttMidFlowNext{"], ["attention", "Part-Of", "optical flow - based gesture recognition methods"]], "rel_plus": [["TSN/FlowNeXt{:Method", "Compare-With", "TSN/AttFlowNeXt{:Method"], ["TSN/MidFlowNeXt{:Method", "Compare-With", "TSN/AttMidFlowNext{:Method"], ["attention:Method", "Part-Of", "optical flow - based gesture recognition methods:Method"]]}
{"doc_id": "160009536", "sentence": "Better EPE does not necessarily imply better gesture recognition : In our experiments with MIBURI , improvement in optical flow and gesture recognition is correlated .", "ner": [["gesture recognition", "Task"], ["MIBURI", "Dataset"], ["optical flow", "Method"], ["gesture recognition", "Task"]], "rel": [["MIBURI", "Benchmark-For", "gesture recognition"]], "rel_plus": [["MIBURI:Dataset", "Benchmark-For", "gesture recognition:Task"]]}
{"doc_id": "160009536", "sentence": "For ChaLearn IGR on the other hand , in spite of being outperformed in optical flow estimation by our networks , TSN/FlowNetS achieves better recognition accuracy than our base .", "ner": [["ChaLearn IGR", "Dataset"], ["optical flow estimation", "Method"], ["TSN/FlowNetS", "Method"], ["recognition", "Task"]], "rel": [["optical flow estimation", "Evaluated-With", "ChaLearn IGR"], ["TSN/FlowNetS", "Evaluated-With", "ChaLearn IGR"], ["TSN/FlowNetS", "Used-For", "recognition"], ["optical flow estimation", "Used-For", "recognition"], ["ChaLearn IGR", "Benchmark-For", "recognition"]], "rel_plus": [["optical flow estimation:Method", "Evaluated-With", "ChaLearn IGR:Dataset"], ["TSN/FlowNetS:Method", "Evaluated-With", "ChaLearn IGR:Dataset"], ["TSN/FlowNetS:Method", "Used-For", "recognition:Task"], ["optical flow estimation:Method", "Used-For", "recognition:Task"], ["ChaLearn IGR:Dataset", "Benchmark-For", "recognition:Task"]]}
{"doc_id": "160009536", "sentence": "Our method improves the speed - accuracy trade - off : Compared to traditional optical flow estimation methods , deep learning - based methods allow for a faster gesture recognition , which has a lot of impact in HRI .", "ner": [["optical flow estimation", "Method"], ["deep learning - based methods", "Method"], ["gesture recognition", "Task"], ["HRI", "Task"]], "rel": [["deep learning - based methods", "Compare-With", "optical flow estimation"], ["deep learning - based methods", "Used-For", "gesture recognition"], ["deep learning - based methods", "Used-For", "HRI"]], "rel_plus": [["deep learning - based methods:Method", "Compare-With", "optical flow estimation:Method"], ["deep learning - based methods:Method", "Used-For", "gesture recognition:Task"], ["deep learning - based methods:Method", "Used-For", "HRI:Task"]]}
{"doc_id": "160009536", "sentence": "With our MIBURI dataset , comparing TSN/FlowNeXt{ 3 2 / 6 4 } vs TSN/AttFlowNeXt{ 3 2 / 6 4 } , adding attention improved recognition accuracy by 7% to 1 0 % while runtime decreases , achieving the best speed - accuracy trade - off .", "ner": [["MIBURI", "Dataset"], ["TSN/FlowNeXt{", "Method"], ["TSN/AttFlowNeXt{", "Method"], ["attention", "Method"], ["recognition", "Task"]], "rel": [["TSN/FlowNeXt{", "Evaluated-With", "MIBURI"], ["TSN/AttFlowNeXt{", "Evaluated-With", "MIBURI"], ["TSN/FlowNeXt{", "Compare-With", "TSN/AttFlowNeXt{"], ["TSN/FlowNeXt{", "Used-For", "recognition"], ["TSN/AttFlowNeXt{", "Used-For", "recognition"], ["attention", "Used-For", "recognition"]], "rel_plus": [["TSN/FlowNeXt{:Method", "Evaluated-With", "MIBURI:Dataset"], ["TSN/AttFlowNeXt{:Method", "Evaluated-With", "MIBURI:Dataset"], ["TSN/FlowNeXt{:Method", "Compare-With", "TSN/AttFlowNeXt{:Method"], ["TSN/FlowNeXt{:Method", "Used-For", "recognition:Task"], ["TSN/AttFlowNeXt{:Method", "Used-For", "recognition:Task"], ["attention:Method", "Used-For", "recognition:Task"]]}
{"doc_id": "160009536", "sentence": "In this paper , we presented a novel optical flow estimation method and evaluated it in a gesture recognition pipeline for human - robot interaction .", "ner": [["optical flow estimation", "Method"], ["gesture recognition", "Task"], ["human - robot interaction", "Task"]], "rel": [["optical flow estimation", "Used-For", "gesture recognition"], ["optical flow estimation", "Used-For", "human - robot interaction"]], "rel_plus": [["optical flow estimation:Method", "Used-For", "gesture recognition:Task"], ["optical flow estimation:Method", "Used-For", "human - robot interaction:Task"]]}
{"doc_id": "202888938", "sentence": "To address this deficiency , we develop a new HSI classification method based on the recently proposed Graph Convolutional Network ( GCN ) , as it can flexibly encode the relations among arbitrarily structured non - Euclidean data .", "ner": [["HSI classification", "Task"], ["Graph Convolutional Network", "Method"], ["GCN", "Method"]], "rel": [["Graph Convolutional Network", "Used-For", "HSI classification"], ["GCN", "Synonym-Of", "Graph Convolutional Network"]], "rel_plus": [["Graph Convolutional Network:Method", "Used-For", "HSI classification:Task"], ["GCN:Method", "Synonym-Of", "Graph Convolutional Network:Method"]]}
{"doc_id": "202888938", "sentence": "Different from traditional GCN , there are two novel strategies adopted by our method to further exploit the contextual relations for accurate HSI classification .", "ner": [["GCN", "Method"], ["HSI classification", "Task"]], "rel": [["GCN", "Used-For", "HSI classification"]], "rel_plus": [["GCN:Method", "Used-For", "HSI classification:Task"]]}
{"doc_id": "202888938", "sentence": "H YPERSPECTRAL image ( HSI ) has recently received considerable attention in a variety of applications such as military target detection , mineral identification , and disaster prevention [ 1 ] .", "ner": [["military target detection", "Task"], ["mineral identification", "Task"], ["disaster prevention", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "202888938", "sentence": "The earlystaged algorithms are mainly based on the simple combination of spectral signatures and conventional pattern recognition methods , such as nearest neighbor classifier and Support Vector Machines ( SVM ) [ 3 ] , [ 4 ] .", "ner": [["pattern recognition methods", "Method"], ["nearest neighbor classifier", "Method"], ["Support Vector Machines", "Method"], ["SVM", "Method"]], "rel": [["nearest neighbor classifier", "SubClass-Of", "pattern recognition methods"], ["Support Vector Machines", "SubClass-Of", "pattern recognition methods"], ["SVM", "Synonym-Of", "Support Vector Machines"]], "rel_plus": [["nearest neighbor classifier:Method", "SubClass-Of", "pattern recognition methods:Method"], ["Support Vector Machines:Method", "SubClass-Of", "pattern recognition methods:Method"], ["SVM:Method", "Synonym-Of", "Support Vector Machines:Method"]]}
{"doc_id": "202888938", "sentence": "After that , Markov Random Field ( MRF ) [ 8 ] , which is an undirected graphical model [ 9 ] , became a popular approach to include spatial context for HSI classification .", "ner": [["Markov Random Field", "Method"], ["MRF", "Method"], ["undirected graphical model", "Method"], ["HSI classification", "Task"]], "rel": [["MRF", "Synonym-Of", "Markov Random Field"], ["Markov Random Field", "SubClass-Of", "undirected graphical model"], ["Markov Random Field", "Used-For", "HSI classification"]], "rel_plus": [["MRF:Method", "Synonym-Of", "Markov Random Field:Method"], ["Markov Random Field:Method", "SubClass-Of", "undirected graphical model:Method"], ["Markov Random Field:Method", "Used-For", "HSI classification:Task"]]}
{"doc_id": "202888938", "sentence": "For instance , in [ 1 0 ] , a relative homogeneity index for each pixel is introduced in MRF - based classification to determine an appropriate weighting coefficient for the contextual contribution .", "ner": [["MRF", "Method"], ["classification", "Task"]], "rel": [["MRF", "Used-For", "classification"]], "rel_plus": [["MRF:Method", "Used-For", "classification:Task"]]}
{"doc_id": "202888938", "sentence": "Apart from this , a novel framework combining Support Vector Machine ( SVM ) and MRF is proposed for contextual HSI classification [ 1 1 ] .", "ner": [["Support Vector Machine", "Method"], ["SVM", "Method"], ["MRF", "Method"], ["contextual HSI classification", "Task"]], "rel": [["SVM", "Synonym-Of", "Support Vector Machine"], ["Support Vector Machine", "Used-For", "contextual HSI classification"], ["MRF", "Used-For", "contextual HSI classification"]], "rel_plus": [["SVM:Method", "Synonym-Of", "Support Vector Machine:Method"], ["Support Vector Machine:Method", "Used-For", "contextual HSI classification:Task"], ["MRF:Method", "Used-For", "contextual HSI classification:Task"]]}
{"doc_id": "202888938", "sentence": "Additionally , multiple kernel learning [ 1 4 ] based on spatial context is proposed to improve the classification performance of SVM classifier on hyperspectral data .", "ner": [["classification", "Task"], ["SVM", "Method"]], "rel": [["SVM", "Used-For", "classification"]], "rel_plus": [["SVM:Method", "Used-For", "classification:Task"]]}
{"doc_id": "202888938", "sentence": "In order to effectively and precisely exploit the contextual relations in HSI , in this paper , we propose a novel ' Context - Aware Dynamic Graph Convolutional Network ' ( CAD - GCN ) which includes the following three key techniques : ( 1 ) The incorporation of Graph Convolutional Network ( GCN ) for sufficiently exploiting contextual relations among pixels ; ( 2 ) The employment of graph projection and re - projection framework for exploring long range contextual relations ; and ( 3 ) The utilization of dynamic graph refinement for accurately characterizing contextual relations and timely finding precise region representations .", "ner": [["Context - Aware Dynamic Graph Convolutional Network", "Method"], ["CAD - GCN", "Method"], ["Graph Convolutional Network", "Method"], ["GCN", "Method"], ["dynamic graph refinement", "Method"], ["characterizing contextual relations", "Task"], ["finding precise region representations", "Task"]], "rel": [["CAD - GCN", "Synonym-Of", "Context - Aware Dynamic Graph Convolutional Network"], ["GCN", "Synonym-Of", "Graph Convolutional Network"], ["dynamic graph refinement", "Used-For", "characterizing contextual relations"], ["dynamic graph refinement", "Used-For", "finding precise region representations"]], "rel_plus": [["CAD - GCN:Method", "Synonym-Of", "Context - Aware Dynamic Graph Convolutional Network:Method"], ["GCN:Method", "Synonym-Of", "Graph Convolutional Network:Method"], ["dynamic graph refinement:Method", "Used-For", "characterizing contextual relations:Task"], ["dynamic graph refinement:Method", "Used-For", "finding precise region representations:Task"]]}
{"doc_id": "202888938", "sentence": "Specifically , in our CAD - GCN , the recently proposed GCN [ 1 7 ] is utilized .", "ner": [["CAD - GCN", "Method"], ["GCN", "Method"]], "rel": [["GCN", "Part-Of", "CAD - GCN"]], "rel_plus": [["GCN:Method", "Part-Of", "CAD - GCN:Method"]]}
{"doc_id": "202888938", "sentence": "GCN is the extension of Convolutional Neural Network ( CNN ) for the non - grid data , and is able to aggregate features and propagate information across graph nodes .", "ner": [["GCN", "Method"], ["Convolutional Neural Network", "Method"], ["CNN", "Method"]], "rel": [["CNN", "Synonym-Of", "Convolutional Neural Network"], ["GCN", "SubClass-Of", "Convolutional Neural Network"]], "rel_plus": [["CNN:Method", "Synonym-Of", "Convolutional Neural Network:Method"], ["GCN:Method", "SubClass-Of", "Convolutional Neural Network:Method"]]}
{"doc_id": "202888938", "sentence": "Consequently , the convolution operation of GCN is adaptively dominated by the neighborhood structure and can be applied to the non - Euclidean irregular data based on the graph which encodes contextual relations among graph nodes .", "ner": [["convolution operation", "Method"], ["GCN", "Method"]], "rel": [["convolution operation", "Part-Of", "GCN"]], "rel_plus": [["convolution operation:Method", "Part-Of", "GCN:Method"]]}
{"doc_id": "202888938", "sentence": "Nevertheless , the contextual relations revealed by a predefined fixed graph [ 1 7 ] for implementing GCN is still inadequate for HSI classification .", "ner": [["GCN", "Method"], ["HSI classification", "Task"]], "rel": [["GCN", "Used-For", "HSI classification"]], "rel_plus": [["GCN:Method", "Used-For", "HSI classification:Task"]]}
{"doc_id": "202888938", "sentence": "In this section , we review some representative works on HSI classification and GCN , as they are related to this work .", "ner": [["HSI classification", "Task"], ["GCN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202888938", "sentence": "Many classical HSI classification approaches are only based on spectral information [ 3 ] , [ 2 2 ] and ignore the crucial spatial information contained in HSI , which may decrease the classification performance [ 9 ] .", "ner": [["HSI classification", "Task"], ["classification", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "202888938", "sentence": "Since then , AP and its extensions , including extended AP [ 3 4 ] and extended multi - attribute profiles [ 3 5 ] , have attracted increasing attention in HSI classification . 3 ) Superpixel establishment .", "ner": [["HSI classification", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "202888938", "sentence": "Recently , some works focus on developing segmentationbased methods for HSI classification with superpixel technique [ 3 6 ] , [ 3 7 ] , in order to jointly combine the spectral - spatial correlations and discrimination to improve classification performance .", "ner": [["HSI classification", "Task"], ["superpixel technique", "Method"], ["classification", "Task"]], "rel": [["superpixel technique", "Used-For", "HSI classification"]], "rel_plus": [["superpixel technique:Method", "Used-For", "HSI classification:Task"]]}
{"doc_id": "202888938", "sentence": "For instance , in [ 3 8 ] , superpixel technique is utilized to generate homogeneous region before constructing a graph on superpixels , which produces satisfactory classification results .", "ner": [["superpixel technique", "Method"], ["generate homogeneous region", "Task"], ["classification", "Task"]], "rel": [["superpixel technique", "Used-For", "generate homogeneous region"]], "rel_plus": [["superpixel technique:Method", "Used-For", "generate homogeneous region:Task"]]}
{"doc_id": "202888938", "sentence": "In additional to spatial methods , graph convolution can also be defined by spectral methods via convolution theorem .", "ner": [["graph convolution", "Method"], ["convolution", "Method"]], "rel": [["graph convolution", "SubClass-Of", "convolution"]], "rel_plus": [["graph convolution:Method", "SubClass-Of", "convolution:Method"]]}
{"doc_id": "202888938", "sentence": "As the pioneering work of spectral methods , spectral CNN [ 4 1 ] converts signals defined in vertex domain into spectral domain by leveraging graph Fourier transform , where the convolution kernel is taken as a set of learnable coefficients associated with Fourier bases ( i.e. , the eigenvectors of Laplacian matrix ) .", "ner": [["CNN", "Method"], ["graph Fourier transform", "Method"], ["convolution kernel", "Method"]], "rel": [["graph Fourier transform", "Part-Of", "CNN"], ["convolution kernel", "Part-Of", "graph Fourier transform"]], "rel_plus": [["graph Fourier transform:Method", "Part-Of", "CNN:Method"], ["convolution kernel:Method", "Part-Of", "graph Fourier transform:Method"]]}
{"doc_id": "202888938", "sentence": "Subsequently , ChebyNet [ 4 2 ] considers the convolution kernel as a polynomial function of the diagonal matrix containing the eigenvalues of Laplacian matrix .", "ner": [["ChebyNet", "Method"], ["convolution kernel", "Method"]], "rel": [["convolution kernel", "Part-Of", "ChebyNet"]], "rel_plus": [["convolution kernel:Method", "Part-Of", "ChebyNet:Method"]]}
{"doc_id": "202888938", "sentence": "Afterwards , GCN [ 1 7 ] was proposed by Kipf and Welling via using a localized first - order approximation to ChebyNet , which brings about more efficient filtering operations than spectral CNN .", "ner": [["GCN", "Method"], ["first - order approximation", "Method"], ["ChebyNet", "Method"], ["spectral CNN", "Method"]], "rel": [["first - order approximation", "Part-Of", "ChebyNet"], ["ChebyNet", "Compare-With", "spectral CNN"]], "rel_plus": [["first - order approximation:Method", "Part-Of", "ChebyNet:Method"], ["ChebyNet:Method", "Compare-With", "spectral CNN:Method"]]}
{"doc_id": "202888938", "sentence": "Recently , GCN gains remarkable success in processing graph - structured data and has been widely adopted in many areas , such as social network mining [ 4 3 ] , recommendation system [ 4 4 ] , natural language processing [ 4 5 ] , and scene understanding [ 4 6 ] .", "ner": [["GCN", "Method"], ["social network mining", "Task"], ["recommendation system", "Task"], ["natural language processing", "Task"], ["scene understanding", "Task"]], "rel": [["GCN", "Used-For", "social network mining"], ["GCN", "Used-For", "recommendation system"], ["GCN", "Used-For", "natural language processing"], ["GCN", "Used-For", "scene understanding"]], "rel_plus": [["GCN:Method", "Used-For", "social network mining:Task"], ["GCN:Method", "Used-For", "recommendation system:Task"], ["GCN:Method", "Used-For", "natural language processing:Task"], ["GCN:Method", "Used-For", "scene understanding:Task"]]}
{"doc_id": "202888938", "sentence": "Due to the effectiveness of GCN in handling the non - Euclidean data , we plan to employ GCN to capture the contextual relations among pixels in HSI .", "ner": [["GCN", "Method"], ["GCN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202888938", "sentence": "To the best of our knowledge , only one prior work has employed GCN for HSI classification , i.e. , [ 4 7 ] .", "ner": [["GCN", "Method"], ["HSI classification", "Task"]], "rel": [["GCN", "Used-For", "HSI classification"]], "rel_plus": [["GCN:Method", "Used-For", "HSI classification:Task"]]}
{"doc_id": "202888938", "sentence": "To address these problems , we proposed a novel contextaware dynamic GCN which dynamically refines the graph along with graph convolution process and captures long range contextual relations among the image pixels via using a graph projection technique .", "ner": [["contextaware dynamic GCN", "Method"], ["graph convolution", "Method"]], "rel": [["graph convolution", "Part-Of", "contextaware dynamic GCN"]], "rel_plus": [["graph convolution:Method", "Part-Of", "contextaware dynamic GCN:Method"]]}
{"doc_id": "202888938", "sentence": "The critical operations in the proposed CAD - GCN will be detailed by presenting the GCN backbone ( Section III - A ) , explaining the graph projection with pixelto - region assignment ( Section III - B ) , describing the dynamic graph refinement ( Section III - C ) , and elaborating the graph re - projection with region - to - pixel assignment ( Section III - D ) .", "ner": [["CAD - GCN", "Method"], ["GCN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202888938", "sentence": "Inspired by CNN , GCN [ 1 7 ] is a multi - layer neural network which directly operates on a graph and aims to extract highlevel features through aggregating feature information from the neighborhoods of graph nodes .", "ner": [["CNN", "Method"], ["GCN", "Method"], ["multi - layer neural network", "Method"]], "rel": [["GCN", "SubClass-Of", "multi - layer neural network"]], "rel_plus": [["GCN:Method", "SubClass-Of", "multi - layer neural network:Method"]]}
{"doc_id": "202888938", "sentence": "In our CAD - GCN model , the first - order neighborhood is considered , i.e. , K = 1 , and thus Eq. ( 4 ) turns to a linear function on the graph Laplacian spectrum with respect to L. Afterwards , a neural network based on graph convolutions can be built by stacking multiple convolutional layers in the form of Eq. ( 4 ) , where each layer is followed by an elementwise non - linear operation ( i.e. , softplus ( \u00b7 ) [ 4 8 ] ) .", "ner": [["CAD - GCN", "Method"], ["non - linear operation", "Method"], ["softplus", "Method"]], "rel": [["softplus", "Synonym-Of", "non - linear operation"]], "rel_plus": [["softplus:Method", "Synonym-Of", "non - linear operation:Method"]]}
{"doc_id": "202888938", "sentence": "To solve this deficiency , Kipf and Welling [ 1 7 ] performed the re - normalization trick As a result , the convolution operation of GCN model can then be expressed as where H ( l ) denotes the output of the l th layer , \u03c3 ( \u00b7 ) represents an activation function , such as the softplus function [ 4 8 ] used in our proposed CAD - GCN , and W ( l ) is the trainable weight matrix involved in the l th layer .", "ner": [["convolution operation", "Method"], ["GCN", "Method"], ["softplus", "Method"], ["CAD - GCN", "Method"]], "rel": [["convolution operation", "Part-Of", "GCN"], ["softplus", "Part-Of", "CAD - GCN"]], "rel_plus": [["convolution operation:Method", "Part-Of", "GCN:Method"], ["softplus:Method", "Part-Of", "CAD - GCN:Method"]]}
{"doc_id": "202888938", "sentence": "Specifically , the Simple Linear Iterative Clustering ( SLIC ) algorithm [ 5 0 ] , which has been widely used for image segmentation , is employed to obtain the initial regions .", "ner": [["Simple Linear Iterative Clustering", "Method"], ["SLIC", "Method"], ["image segmentation", "Task"]], "rel": [["SLIC", "Synonym-Of", "Simple Linear Iterative Clustering"], ["Simple Linear Iterative Clustering", "Used-For", "image segmentation"]], "rel_plus": [["SLIC:Method", "Synonym-Of", "Simple Linear Iterative Clustering:Method"], ["Simple Linear Iterative Clustering:Method", "Used-For", "image segmentation:Task"]]}
{"doc_id": "202888938", "sentence": "Then Eq. ( 1 1 ) can be rewritten as where h ( l ) i is the representation of x i generated by the l th layer with h During graph construction , connections among the regions from different classes may be incorporated , which will lead to the aggregation of inter - class feature information and further degrade the discriminability of graph convolution results .", "ner": [["graph construction", "Task"], ["graph convolution", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202888938", "sentence": "With the edge filter , the graph convolutional layer can then be reformulated as with H ( 0 ) = X. After conducting the dynamic graph convolution on the region level , we need to re - project the new region features ( i.e. , the learned graph representation ) H ( L ) back into 2D image with grids of pixels , and this process is called graph re - projection , Specifically , the region - to - pixel assignment is accomplished by linearly interpolating pixel features based on the soft assignment matrix P , namely PH ( L ) , where L denotes the number of graph convolutional layers .", "ner": [["graph convolutional layer", "Method"], ["dynamic graph convolution", "Method"], ["graph convolutional layers", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202888938", "sentence": "With the region - to - pixel assignment , the output of our proposed CAD - GCN can be obtained as Algorithm 1 The Proposed CAD - GCN for HSI Classification Input : Input image ; number of iterations T ; learning rate \u03b7 ; number of graph convolutional layers L ; 1 : Initialize the anchor point matrix V with SLIC algorithm ; 2 : // Train the CAD - GCN model 3 : for t = 1 to T do 4 : Learn the region features X through Eq. ( 8) and Eq. ( 9 ) ; 5 : Dynamically refine the graph A ( l ) using Eq. ( 1 2 ) and Eq. ( 1 3 ) along with the graph convolution operation of Eq. ( 1 4 ) ; 6 : Interpolate the region features back into the original 2D grids by Eq. ( 1 5 ) ; 7 : Calculate the error term according to Eq. ( 1 6 ) , and update the weight matrices W ( l ) ( 1 \u2264 l \u2264 L ) using fullbatch gradient descent ; 8 : end for 9 : Conduct label prediction via Eq. ( 1 4 ) and Eq. ( 1 5 ) ; Output : Predicted label for each pixel .", "ner": [["region - to - pixel assignment", "Task"], ["CAD - GCN", "Method"], ["CAD - GCN", "Method"], ["HSI Classification", "Task"], ["SLIC", "Method"], ["CAD - GCN", "Method"], ["graph convolution", "Method"]], "rel": [["CAD - GCN", "Used-For", "HSI Classification"]], "rel_plus": [["CAD - GCN:Method", "Used-For", "HSI Classification:Task"]]}
{"doc_id": "202888938", "sentence": "Algorithm 1 shows the summarization of our proposed CAD - GCN classification method .", "ner": [["CAD - GCN", "Method"], ["classification", "Task"]], "rel": [["CAD - GCN", "Used-For", "classification"]], "rel_plus": [["CAD - GCN:Method", "Used-For", "classification:Task"]]}
{"doc_id": "202888938", "sentence": "To test the effectiveness of the proposed CAD - GCN model , in this section , we conduct exhaustive experiments on three real - world benchmark datasets , namely Indian Pines , University of Pavia , and Salinas .", "ner": [["CAD - GCN", "Method"], ["Indian Pines", "Dataset"], ["University of Pavia", "Dataset"], ["Salinas", "Dataset"]], "rel": [["CAD - GCN", "Evaluated-With", "Indian Pines"], ["CAD - GCN", "Evaluated-With", "University of Pavia"], ["CAD - GCN", "Evaluated-With", "Salinas"]], "rel_plus": [["CAD - GCN:Method", "Evaluated-With", "Indian Pines:Dataset"], ["CAD - GCN:Method", "Evaluated-With", "University of Pavia:Dataset"], ["CAD - GCN:Method", "Evaluated-With", "Salinas:Dataset"]]}
{"doc_id": "202888938", "sentence": "The performance of our proposed CAD - GCN is evaluated on three real - world benchmark datasets , i.e. , the Indian Pines 1 , Grass - trees 3 0 7 0 0 7 Grass - pasture - mowed 1 5 1 3 8 Hay -windrowed   3 0   4 4 8   9   Oats   1 5   5   1 0   Soybean - notill   3 0   9 4 2   1 1   Soybean - mintill   3 0   2 4 2 5   1 2   Soybean - clean   3 0   5 6 3   1 3   Wheat   3 0   1 7 5   1 4   Woods   3 0   1 2 3 5   1 5 Buildings - grass - trees - drives 3 0 3 5 6 1 6 Stone - steel - towers 3 0 6 3 the University of Pavia 2 , and the Salinas 3 , which will be introduced below . 1 ) Indian Pines : The Indian Pines dataset was gathered by Airborne Visible/Infrared Imaging Spectrometer sensor in 1 9 9 2 , which records north - western India .", "ner": [["CAD - GCN", "Method"], ["Indian Pines", "Dataset"], ["Grass - trees", "Dataset"], ["Grass - pasture - mowed", "Dataset"], ["Indian Pines", "Dataset"], ["Indian Pines dataset", "Dataset"]], "rel": [["CAD - GCN", "Evaluated-With", "Indian Pines"], ["CAD - GCN", "Evaluated-With", "Grass - trees"], ["CAD - GCN", "Evaluated-With", "Grass - pasture - mowed"]], "rel_plus": [["CAD - GCN:Method", "Evaluated-With", "Indian Pines:Dataset"], ["CAD - GCN:Method", "Evaluated-With", "Grass - trees:Dataset"], ["CAD - GCN:Method", "Evaluated-With", "Grass - pasture - mowed:Dataset"]]}
{"doc_id": "202888938", "sentence": "The amounts of labeled and unlabeled pixels of various classes are listed in Table I . 2 ) University of Pavia : The University of Pavia dataset captures the Pavia University of Italy with the ROSIS sensor .", "ner": [["University of Pavia", "Dataset"], ["University of Pavia", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "202888938", "sentence": "Table II shows the amounts of labeled and unlabeled pixels of each class . 3 ) Salinas : The Salinas dataset is another classic HSI which is collected by the AVIRIS sensor over Salinas Valley , California .", "ner": [["Salinas", "Dataset"], ["Salinas", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "202888938", "sentence": "In our experiments , the proposed CAD - GCN algorithm is implemented by TensorFlow with Adam optimizer .", "ner": [["CAD - GCN", "Method"], ["Adam optimizer", "Method"]], "rel": [["Adam optimizer", "Part-Of", "CAD - GCN"]], "rel_plus": [["Adam optimizer:Method", "Part-Of", "CAD - GCN:Method"]]}
{"doc_id": "202888938", "sentence": "For all the adopted three datasets mentioned in Section IV - A , usually 3 0 To evaluate the classification ability of our proposed CAD - GCN , other recent state - of - the - art HSI classification methods are also utilized for comparison .", "ner": [["classification", "Task"], ["CAD - GCN", "Method"], ["HSI classification", "Task"]], "rel": [["CAD - GCN", "Used-For", "classification"], ["CAD - GCN", "Used-For", "HSI classification"]], "rel_plus": [["CAD - GCN:Method", "Used-For", "classification:Task"], ["CAD - GCN:Method", "Used-For", "HSI classification:Task"]]}
{"doc_id": "202888938", "sentence": "Specifically , we employ two GCN - based methods , i.e. , Graph Convolutional Network ( GCN ) [ 1 7 ] and Spectral - Spatial Graph Convolutional Network ( S 2 GCN ) [ 4 7 ] , together with two CNN - based methods , i.e. , Recurrent 2D - CNN ( R - 2 D - CNN ) [ 5 1 ] and CNN - Pixel - Pair Features ( CNN - PPF ) [ 5 2 ] .", "ner": [["GCN - based methods", "Method"], ["Graph Convolutional Network", "Method"], ["GCN", "Method"], ["Spectral - Spatial Graph Convolutional Network", "Method"], ["S 2 GCN", "Method"], ["CNN - based methods", "Method"], ["Recurrent 2D - CNN", "Method"], ["R - 2 D - CNN", "Method"], ["CNN - Pixel - Pair Features", "Method"], ["CNN - PPF", "Method"]], "rel": [["Graph Convolutional Network", "SubClass-Of", "GCN - based methods"], ["Spectral - Spatial Graph Convolutional Network", "SubClass-Of", "GCN - based methods"], ["GCN", "Synonym-Of", "Graph Convolutional Network"], ["S 2 GCN", "Synonym-Of", "Spectral - Spatial Graph Convolutional Network"], ["Recurrent 2D - CNN", "SubClass-Of", "CNN - based methods"], ["CNN - Pixel - Pair Features", "SubClass-Of", "CNN - based methods"], ["R - 2 D - CNN", "Synonym-Of", "Recurrent 2D - CNN"], ["CNN - PPF", "Synonym-Of", "CNN - Pixel - Pair Features"]], "rel_plus": [["Graph Convolutional Network:Method", "SubClass-Of", "GCN - based methods:Method"], ["Spectral - Spatial Graph Convolutional Network:Method", "SubClass-Of", "GCN - based methods:Method"], ["GCN:Method", "Synonym-Of", "Graph Convolutional Network:Method"], ["S 2 GCN:Method", "Synonym-Of", "Spectral - Spatial Graph Convolutional Network:Method"], ["Recurrent 2D - CNN:Method", "SubClass-Of", "CNN - based methods:Method"], ["CNN - Pixel - Pair Features:Method", "SubClass-Of", "CNN - based methods:Method"], ["R - 2 D - CNN:Method", "Synonym-Of", "Recurrent 2D - CNN:Method"], ["CNN - PPF:Method", "Synonym-Of", "CNN - Pixel - Pair Features:Method"]]}
{"doc_id": "202888938", "sentence": "Meanwhile , we also compare the proposed CAD - GCN with two traditional HSI classification methods , namely Joint collaborative representation and SVM with Decision Fusion ( JSDF ) [ 5 4 ] and Multiple Feature Learning ( MFL ) [ 5 3 ] , respectively .", "ner": [["CAD - GCN", "Method"], ["HSI classification", "Task"], ["Joint collaborative representation and SVM with Decision Fusion", "Method"], ["JSDF", "Method"], ["Multiple Feature Learning", "Method"], ["MFL", "Method"]], "rel": [["CAD - GCN", "Used-For", "HSI classification"], ["Joint collaborative representation and SVM with Decision Fusion", "Used-For", "HSI classification"], ["Multiple Feature Learning", "Used-For", "HSI classification"], ["JSDF", "Synonym-Of", "Joint collaborative representation and SVM with Decision Fusion"], ["CAD - GCN", "Compare-With", "Joint collaborative representation and SVM with Decision Fusion"], ["MFL", "Synonym-Of", "Multiple Feature Learning"], ["CAD - GCN", "Compare-With", "Multiple Feature Learning"]], "rel_plus": [["CAD - GCN:Method", "Used-For", "HSI classification:Task"], ["Joint collaborative representation and SVM with Decision Fusion:Method", "Used-For", "HSI classification:Task"], ["Multiple Feature Learning:Method", "Used-For", "HSI classification:Task"], ["JSDF:Method", "Synonym-Of", "Joint collaborative representation and SVM with Decision Fusion:Method"], ["CAD - GCN:Method", "Compare-With", "Joint collaborative representation and SVM with Decision Fusion:Method"], ["MFL:Method", "Synonym-Of", "Multiple Feature Learning:Method"], ["CAD - GCN:Method", "Compare-With", "Multiple Feature Learning:Method"]]}
{"doc_id": "202888938", "sentence": "To show the effectiveness of our proposed CAD - GCN , here we quantitatively and qualitatively evaluate the classification performance by comparing CAD - GCN with the aforementioned baseline methods . 1 ) Results on the Indian Pines Dataset : The quantitative results acquired by different methods on the Indian Pines dataset are presented in Table IV , and the highest record regarding each class ( i.e. , each row ) has been highlighted in bold .", "ner": [["CAD - GCN", "Method"], ["classification", "Task"], ["CAD - GCN", "Method"], ["Indian Pines", "Dataset"], ["Indian Pines", "Dataset"]], "rel": [["CAD - GCN", "Used-For", "classification"]], "rel_plus": [["CAD - GCN:Method", "Used-For", "classification:Task"]]}
{"doc_id": "202888938", "sentence": "Meanwhile , the proposed CAD - GCN acquires stable and very high classification accuracies on most of the land - cover classes .", "ner": [["CAD - GCN", "Method"], ["classification", "Task"]], "rel": [["CAD - GCN", "Used-For", "classification"]], "rel_plus": [["CAD - GCN:Method", "Used-For", "classification:Task"]]}
{"doc_id": "202888938", "sentence": "All these statistics demonstrate the effectiveness of our CAD - GCN in HSI classification .", "ner": [["CAD - GCN", "Method"], ["HSI classification", "Task"]], "rel": [["CAD - GCN", "Used-For", "HSI classification"]], "rel_plus": [["CAD - GCN:Method", "Used-For", "HSI classification:Task"]]}
{"doc_id": "202888938", "sentence": "The classification maps generated by different methods on the Indian Pines dataset are exhibited in Fig. 6 .", "ner": [["classification", "Task"], ["Indian Pines", "Dataset"]], "rel": [["Indian Pines", "Benchmark-For", "classification"]], "rel_plus": [["Indian Pines:Dataset", "Benchmark-For", "classification:Task"]]}
{"doc_id": "202888938", "sentence": "A visual inspection reveals that the proposed CAD - GCN method produces much more compact classification map and shows fewer misclassifications than other methods .", "ner": [["CAD - GCN", "Method"], ["classification", "Task"]], "rel": [["CAD - GCN", "Used-For", "classification"]], "rel_plus": [["CAD - GCN:Method", "Used-For", "classification:Task"]]}
{"doc_id": "202888938", "sentence": "More concretely , in the classification maps of GCN , S 2 GCN , R - 2 D - CNN , CNN - PPF , and MFL , the errors are almost uniformly distributed ( the salt - and - pepper effect in the homogeneous regions ) , while in the classification maps of JSDF and our proposed CAD - GCN , the errors only appear in some highly heterogeneous areas , where the spatial separability between classes is quite low .", "ner": [["classification", "Task"], ["GCN", "Method"], ["S 2 GCN", "Method"], ["R - 2 D - CNN", "Method"], ["CNN - PPF", "Method"], ["MFL", "Method"], ["classification", "Task"], ["JSDF", "Method"], ["CAD - GCN", "Method"]], "rel": [["GCN", "Used-For", "classification"], ["S 2 GCN", "Used-For", "classification"], ["R - 2 D - CNN", "Used-For", "classification"], ["CNN - PPF", "Used-For", "classification"], ["MFL", "Used-For", "classification"], ["JSDF", "Used-For", "classification"], ["CAD - GCN", "Used-For", "classification"]], "rel_plus": [["GCN:Method", "Used-For", "classification:Task"], ["S 2 GCN:Method", "Used-For", "classification:Task"], ["R - 2 D - CNN:Method", "Used-For", "classification:Task"], ["CNN - PPF:Method", "Used-For", "classification:Task"], ["MFL:Method", "Used-For", "classification:Task"], ["JSDF:Method", "Used-For", "classification:Task"], ["CAD - GCN:Method", "Used-For", "classification:Task"]]}
{"doc_id": "202888938", "sentence": "Moreover , by comparing CAD - GCN with JSDF , we can also find that JSDF produces more errors around class boundaries than our CAD - GCN method , which reveals the good discriminability of the proposed CAD - GCN in boundary regions . 2 ) Results on the University of Pavia Dataset : In Table V , different methods are compared on the aforementioned three datasets , where per - class accuracy , OA , AA , and Kappa coefficient are reported , and the highest value in each row is highlighted in bold .", "ner": [["CAD - GCN", "Method"], ["JSDF", "Method"], ["JSDF", "Method"], ["CAD - GCN", "Method"], ["CAD - GCN", "Method"], ["University of Pavia", "Dataset"]], "rel": [["CAD - GCN", "Compare-With", "JSDF"], ["JSDF", "Compare-With", "CAD - GCN"]], "rel_plus": [["CAD - GCN:Method", "Compare-With", "JSDF:Method"], ["JSDF:Method", "Compare-With", "CAD - GCN:Method"]]}
{"doc_id": "202888938", "sentence": "Compared with the two CNN - based methods ( i.e. , R - 2 D - CNN and CNN - PPF ) , the proposed CAD - GCN increases the OA by 9. 9 4 % and 3. 6 0 % , respectively , which suggests that the refined contextual relations captured by our CAD - GCN is superior to the spatial context characterized by the fixed convolutional kernels of CNN .", "ner": [["CNN - based methods", "Method"], ["R - 2 D - CNN", "Method"], ["CNN - PPF", "Method"], ["CAD - GCN", "Method"], ["CAD - GCN", "Method"], ["convolutional kernels", "Method"], ["CNN", "Method"]], "rel": [["R - 2 D - CNN", "SubClass-Of", "CNN - based methods"], ["CNN - PPF", "SubClass-Of", "CNN - based methods"], ["CNN - based methods", "Compare-With", "CAD - GCN"], ["R - 2 D - CNN", "Compare-With", "CAD - GCN"], ["CNN - PPF", "Compare-With", "CAD - GCN"], ["convolutional kernels", "Part-Of", "CNN"]], "rel_plus": [["R - 2 D - CNN:Method", "SubClass-Of", "CNN - based methods:Method"], ["CNN - PPF:Method", "SubClass-Of", "CNN - based methods:Method"], ["CNN - based methods:Method", "Compare-With", "CAD - GCN:Method"], ["R - 2 D - CNN:Method", "Compare-With", "CAD - GCN:Method"], ["CNN - PPF:Method", "Compare-With", "CAD - GCN:Method"], ["convolutional kernels:Method", "Part-Of", "CNN:Method"]]}
{"doc_id": "202888938", "sentence": "Fig. 7 visualizes the classification results generated by the seven different methods on the University of Pavia dataset .", "ner": [["classification", "Task"], ["University of Pavia", "Dataset"]], "rel": [["University of Pavia", "Benchmark-For", "classification"]], "rel_plus": [["University of Pavia:Dataset", "Benchmark-For", "classification:Task"]]}
{"doc_id": "202888938", "sentence": "As depicted in Fig. 7(h ) , the classification map of our proposed CAD - GCN are noticeably closer to the ground truth map ( see Fig. 7(a ) ) than other methods , which is consistent with previous results in Table V .", "ner": [["classification", "Task"], ["CAD - GCN", "Method"]], "rel": [["CAD - GCN", "Used-For", "classification"]], "rel_plus": [["CAD - GCN:Method", "Used-For", "classification:Task"]]}
{"doc_id": "202888938", "sentence": "Different from GCN and S 2 GCN , our CAD - GCN employs graph projection and dynamic graph refinement operations to effectively exploit the improved contextual relations of HSI .", "ner": [["GCN", "Method"], ["S 2 GCN", "Method"], ["CAD - GCN", "Method"], ["graph projection", "Method"], ["dynamic graph refinement operations", "Method"]], "rel": [["dynamic graph refinement operations", "Part-Of", "CAD - GCN"], ["graph projection", "Part-Of", "CAD - GCN"], ["S 2 GCN", "Compare-With", "CAD - GCN"], ["GCN", "Compare-With", "CAD - GCN"]], "rel_plus": [["dynamic graph refinement operations:Method", "Part-Of", "CAD - GCN:Method"], ["graph projection:Method", "Part-Of", "CAD - GCN:Method"], ["S 2 GCN:Method", "Compare-With", "CAD - GCN:Method"], ["GCN:Method", "Compare-With", "CAD - GCN:Method"]]}
{"doc_id": "202888938", "sentence": "As a result , GCN and S 2 GCN which use the fixed coarse graph convolution produce more errors than our proposed CAD - GCN . 3 ) Results on the Salinas Dataset : Table VI presents the experimental results of different methods on the Salinas dataset .", "ner": [["GCN", "Method"], ["S 2 GCN", "Method"], ["CAD - GCN", "Method"], ["Salinas", "Dataset"], ["Salinas", "Dataset"]], "rel": [["S 2 GCN", "Compare-With", "CAD - GCN"], ["GCN", "Compare-With", "CAD - GCN"]], "rel_plus": [["S 2 GCN:Method", "Compare-With", "CAD - GCN:Method"], ["GCN:Method", "Compare-With", "CAD - GCN:Method"]]}
{"doc_id": "202888938", "sentence": "The proposed CAD - GCN is obviously superior to the CNNbased methods ( i.e. , R - 2 D - CNN and CNN - PPF ) and all the other competitors .", "ner": [["CAD - GCN", "Method"], ["CNNbased methods", "Method"], ["R - 2 D - CNN", "Method"], ["CNN - PPF", "Method"]], "rel": [["CAD - GCN", "Compare-With", "CNNbased methods"], ["CNN - PPF", "SubClass-Of", "CNNbased methods"], ["R - 2 D - CNN", "SubClass-Of", "CNNbased methods"]], "rel_plus": [["CAD - GCN:Method", "Compare-With", "CNNbased methods:Method"], ["CNN - PPF:Method", "SubClass-Of", "CNNbased methods:Method"], ["R - 2 D - CNN:Method", "SubClass-Of", "CNNbased methods:Method"]]}
{"doc_id": "202888938", "sentence": "For instance , in Table VI , CAD - GCN yields over 1 0 % higher OA than R - 2 D - CNN and approximately 8% higher OA than CNN - PPF .", "ner": [["CAD - GCN", "Method"], ["R - 2 D - CNN", "Method"], ["CNN - PPF", "Method"]], "rel": [["CAD - GCN", "Compare-With", "R - 2 D - CNN"], ["CAD - GCN", "Compare-With", "CNN - PPF"]], "rel_plus": [["CAD - GCN:Method", "Compare-With", "R - 2 D - CNN:Method"], ["CAD - GCN:Method", "Compare-With", "CNN - PPF:Method"]]}
{"doc_id": "202888938", "sentence": "Especially in some classes such as ' Grapes untrained ' ( ID = 8) and ' Vineyard untrained ' ( ID = 1 5 ) , the class - specific accuracies of our proposed CAD - GCN are even approximately 2 0 % higher than those of the CNN - based methods . results obtained by different methods .", "ner": [["CAD - GCN", "Method"], ["CNN - based methods", "Method"]], "rel": [["CAD - GCN", "Compare-With", "CNN - based methods"]], "rel_plus": [["CAD - GCN:Method", "Compare-With", "CNN - based methods:Method"]]}
{"doc_id": "202888938", "sentence": "It is observable that some areas in the classification map of our proposed CAD - GCN are less noisy than those of other methods , e.g. , the regions of ' Grapes untrained ' and ' Vineyard untrained ' , which is in consistence with the results listed in Table VI .", "ner": [["classification", "Task"], ["CAD - GCN", "Method"]], "rel": [["CAD - GCN", "Used-For", "classification"]], "rel_plus": [["CAD - GCN:Method", "Used-For", "classification:Task"]]}
{"doc_id": "202888938", "sentence": "To be specific , we vary the number of labeled examples per class form 5 to 3 0 with an interval of 5 , and report the OA acquired by all the methods on the Indian Pines , the University of Pavia , and the Salinas datasets ( see Fig. 9 ) .", "ner": [["Indian Pines", "Dataset"], ["University of Pavia", "Dataset"], ["Salinas", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "202888938", "sentence": "From the results , we can find that the proposed CAD - GCN consistently outperforms the GCN , S 2 GCN and all the other competitors on the three   There are several important hyperparameters that should be manually tuned in the designed CAD - GCN architecture .", "ner": [["CAD - GCN", "Method"], ["GCN", "Method"], ["S 2 GCN", "Method"], ["CAD - GCN", "Method"]], "rel": [["CAD - GCN", "Compare-With", "GCN"], ["CAD - GCN", "Compare-With", "S 2 GCN"]], "rel_plus": [["CAD - GCN:Method", "Compare-With", "GCN:Method"], ["CAD - GCN:Method", "Compare-With", "S 2 GCN:Method"]]}
{"doc_id": "202888938", "sentence": "Here , we will evaluate in detail the sensitivity of the classification performance to different hyperparameter settings of the proposed CAD - GCN .", "ner": [["classification", "Task"], ["CAD - GCN", "Method"]], "rel": [["CAD - GCN", "Used-For", "classification"]], "rel_plus": [["CAD - GCN:Method", "Used-For", "classification:Task"]]}
{"doc_id": "202888938", "sentence": "The results on Indian Pines , University of Pavia , and Salinas datasets are shown in Fig. 1 0 .", "ner": [["Indian Pines", "Dataset"], ["University of Pavia", "Dataset"], ["Salinas", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "202888938", "sentence": "In order to obtain promising classification performance , we set the hyperparameters to T = 1 5 0 0 , \u03b7 = 0.0 0 1 , u = 6 0 , \u03b2 = 0.0 1 for the Indian Pines dataset , T = 5 0 0 , \u03b7 = 0.0 0 1 , u = 2 1 0 , \u03b2 = 0.0 5 for the University of Pavia dataset , and T = 2 0 0 0 , \u03b7 = 0.0 0 0 1 , u = 1 1 0 , \u03b2 = 0.0 2 for the Salinas dataset , respectively .", "ner": [["classification", "Task"], ["Indian Pines", "Dataset"], ["University of Pavia", "Dataset"], ["Salinas", "Dataset"]], "rel": [["Indian Pines", "Benchmark-For", "classification"], ["University of Pavia", "Benchmark-For", "classification"], ["Salinas", "Benchmark-For", "classification"]], "rel_plus": [["Indian Pines:Dataset", "Benchmark-For", "classification:Task"], ["University of Pavia:Dataset", "Benchmark-For", "classification:Task"], ["Salinas:Dataset", "Benchmark-For", "classification:Task"]]}
{"doc_id": "202888938", "sentence": "To shed light on the contributions of these three components , every time we report the classification results of CAD - GCN without one of the three components on the three adopted datasets ( namely , the Indian Pines , the University of Pavia , and the Salinas ) .", "ner": [["classification", "Task"], ["CAD - GCN", "Method"], ["Indian Pines", "Dataset"], ["University of Pavia", "Dataset"], ["Salinas", "Dataset"]], "rel": [["CAD - GCN", "Used-For", "classification"], ["CAD - GCN", "Evaluated-With", "Indian Pines"], ["CAD - GCN", "Evaluated-With", "University of Pavia"], ["CAD - GCN", "Evaluated-With", "Salinas"]], "rel_plus": [["CAD - GCN:Method", "Used-For", "classification:Task"], ["CAD - GCN:Method", "Evaluated-With", "Indian Pines:Dataset"], ["CAD - GCN:Method", "Evaluated-With", "University of Pavia:Dataset"], ["CAD - GCN:Method", "Evaluated-With", "Salinas:Dataset"]]}
{"doc_id": "202888938", "sentence": "For simplicity , we adopt ' CAD - GCN - v 1 ' , ' CAD - GCN - v 2 ' , and ' CAD - GCN - v 3 ' to represent the reduced model by removing dynamic refinement of node similarities , the edge filter , and the graph projection framework , respectively .", "ner": [["CAD - GCN - v 1", "Method"], ["CAD - GCN - v 2", "Method"], ["CAD - GCN - v 3", "Method"], ["dynamic refinement of node similarities", "Method"], ["edge filter", "Method"], ["graph projection framework", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202888938", "sentence": "In order to reveal the advantage of our proposed CAD - GCN to the baselines in terms of efficiency , in Table X , we report the running time of different deep models , including GCN , S 2 GCN , R - 2 D - CNN , CNN - PPF , and the proposed CAD - GCN on three datasets ( i.e. , the Indian Pines , the University of Pavia , and the Salinas ) , where the number of labeled pixels IP   5 8   7 1   2 1 5 6   1 4 9 5   4 4   paviaU   1 7 8 3   1 8 0 3   2 2 7 2   1 5 4 5   9 0   Salinas   3 4 9 7   3 5 2 8   2 3 6 1   1 7 6 9   5 5 6 per class is kept identical to the experiments presented in Section IV - C. The codes for all methods are written in Python , and the running time is reported on a server with a 3. 6 0 - GHz Intel Xeon CPU with 2 6 4 GB of RAM and a Tesla P 4 0 GPU .", "ner": [["CAD - GCN", "Method"], ["GCN", "Method"], ["S 2 GCN", "Method"], ["R - 2 D - CNN", "Method"], ["CNN - PPF", "Method"], ["CAD - GCN", "Method"], ["Indian Pines", "Dataset"], ["University of Pavia", "Dataset"], ["Salinas", "Dataset"], ["paviaU", "Dataset"], ["Salinas", "Dataset"]], "rel": [["CAD - GCN", "Evaluated-With", "Indian Pines"], ["CNN - PPF", "Evaluated-With", "Indian Pines"], ["R - 2 D - CNN", "Evaluated-With", "Indian Pines"], ["S 2 GCN", "Evaluated-With", "Indian Pines"], ["GCN", "Evaluated-With", "Indian Pines"], ["CAD - GCN", "Evaluated-With", "University of Pavia"], ["CNN - PPF", "Evaluated-With", "University of Pavia"], ["R - 2 D - CNN", "Evaluated-With", "University of Pavia"], ["S 2 GCN", "Evaluated-With", "University of Pavia"], ["GCN", "Evaluated-With", "University of Pavia"], ["CAD - GCN", "Evaluated-With", "Salinas"], ["CNN - PPF", "Evaluated-With", "Salinas"], ["R - 2 D - CNN", "Evaluated-With", "Salinas"], ["S 2 GCN", "Evaluated-With", "Salinas"], ["GCN", "Evaluated-With", "Salinas"]], "rel_plus": [["CAD - GCN:Method", "Evaluated-With", "Indian Pines:Dataset"], ["CNN - PPF:Method", "Evaluated-With", "Indian Pines:Dataset"], ["R - 2 D - CNN:Method", "Evaluated-With", "Indian Pines:Dataset"], ["S 2 GCN:Method", "Evaluated-With", "Indian Pines:Dataset"], ["GCN:Method", "Evaluated-With", "Indian Pines:Dataset"], ["CAD - GCN:Method", "Evaluated-With", "University of Pavia:Dataset"], ["CNN - PPF:Method", "Evaluated-With", "University of Pavia:Dataset"], ["R - 2 D - CNN:Method", "Evaluated-With", "University of Pavia:Dataset"], ["S 2 GCN:Method", "Evaluated-With", "University of Pavia:Dataset"], ["GCN:Method", "Evaluated-With", "University of Pavia:Dataset"], ["CAD - GCN:Method", "Evaluated-With", "Salinas:Dataset"], ["CNN - PPF:Method", "Evaluated-With", "Salinas:Dataset"], ["R - 2 D - CNN:Method", "Evaluated-With", "Salinas:Dataset"], ["S 2 GCN:Method", "Evaluated-With", "Salinas:Dataset"], ["GCN:Method", "Evaluated-With", "Salinas:Dataset"]]}
{"doc_id": "202888938", "sentence": "Compared with the running time of GCN , S 2 GCN , our proposed CAD - GCN shows remarkably higher efficiency in large - scale datasets ( i.e. , the University of Pavia and the Salinas dataset ) , which is owing much to the employment of graph projection operation .", "ner": [["GCN", "Method"], ["S 2 GCN", "Method"], ["CAD - GCN", "Method"], ["University of Pavia", "Dataset"], ["Salinas", "Dataset"]], "rel": [["GCN", "Compare-With", "CAD - GCN"], ["S 2 GCN", "Compare-With", "CAD - GCN"], ["CAD - GCN", "Evaluated-With", "University of Pavia"], ["GCN", "Evaluated-With", "University of Pavia"], ["S 2 GCN", "Evaluated-With", "University of Pavia"], ["CAD - GCN", "Evaluated-With", "Salinas"], ["GCN", "Evaluated-With", "Salinas"], ["S 2 GCN", "Evaluated-With", "Salinas"]], "rel_plus": [["GCN:Method", "Compare-With", "CAD - GCN:Method"], ["S 2 GCN:Method", "Compare-With", "CAD - GCN:Method"], ["CAD - GCN:Method", "Evaluated-With", "University of Pavia:Dataset"], ["GCN:Method", "Evaluated-With", "University of Pavia:Dataset"], ["S 2 GCN:Method", "Evaluated-With", "University of Pavia:Dataset"], ["CAD - GCN:Method", "Evaluated-With", "Salinas:Dataset"], ["GCN:Method", "Evaluated-With", "Salinas:Dataset"], ["S 2 GCN:Method", "Evaluated-With", "Salinas:Dataset"]]}
{"doc_id": "202888938", "sentence": "In this paper , we have developed a novel Context - Aware Dynamic Graph Convolutional Network ( CAD - GCN ) for HSI classification .", "ner": [["Context - Aware Dynamic Graph Convolutional Network", "Method"], ["CAD - GCN", "Method"], ["HSI classification", "Task"]], "rel": [["CAD - GCN", "Synonym-Of", "Context - Aware Dynamic Graph Convolutional Network"], ["Context - Aware Dynamic Graph Convolutional Network", "Used-For", "HSI classification"]], "rel_plus": [["CAD - GCN:Method", "Synonym-Of", "Context - Aware Dynamic Graph Convolutional Network:Method"], ["Context - Aware Dynamic Graph Convolutional Network:Method", "Used-For", "HSI classification:Task"]]}
{"doc_id": "202888938", "sentence": "Therefore , the contextual relations among pixels can be gradually refined along with graph convolution , which significantly improves the performance of CAD - GCN on representation and classification of HSI .", "ner": [["graph convolution", "Method"], ["CAD - GCN", "Method"], ["classification of HSI", "Task"]], "rel": [["graph convolution", "Part-Of", "CAD - GCN"], ["CAD - GCN", "Used-For", "classification of HSI"]], "rel_plus": [["graph convolution:Method", "Part-Of", "CAD - GCN:Method"], ["CAD - GCN:Method", "Used-For", "classification of HSI:Task"]]}
{"doc_id": "202888938", "sentence": "The experimental results on three realworld HSI datasets indicate that the proposed CAD - GCN is able to yield better performance when compared with the stateof - the - art HSI classification methods .", "ner": [["CAD - GCN", "Method"], ["HSI classification", "Task"]], "rel": [["CAD - GCN", "Used-For", "HSI classification"]], "rel_plus": [["CAD - GCN:Method", "Used-For", "HSI classification:Task"]]}
{"doc_id": "244256", "sentence": "Specifically relevant to ADAS application is the dramatic increase in accuracy of image object classification [ 1 6 , 2 2 , 2 1 , 1 2 ] and localization [ 2 0 , 2 6 , 1 0 , 9 , 1 9 ] in the last few years .", "ner": [["image object classification", "Task"], ["localization", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "244256", "sentence": "R - CNN and its faster variants have become the state of the art in different object detection tasks .", "ner": [["R - CNN", "Method"], ["object detection", "Task"]], "rel": [["R - CNN", "Used-For", "object detection"]], "rel_plus": [["R - CNN:Method", "Used-For", "object detection:Task"]]}
{"doc_id": "244256", "sentence": "In this work , we leverage this method to establish a number of observations related to car detection on the challenging KITTI [ 8 ] dataset .", "ner": [["car detection", "Task"], ["KITTI", "Dataset"]], "rel": [["KITTI", "Benchmark-For", "car detection"]], "rel_plus": [["KITTI:Dataset", "Benchmark-For", "car detection:Task"]]}
{"doc_id": "244256", "sentence": "Surprisingly , even shallow models like AlexNet provide high accuracy on the detection task .", "ner": [["AlexNet", "Method"], ["detection", "Task"]], "rel": [["AlexNet", "Used-For", "detection"]], "rel_plus": [["AlexNet:Method", "Used-For", "detection:Task"]]}
{"doc_id": "244256", "sentence": "We do additional exploration of R - CNN based configurations in Section 7 We summarize our findings in the context of the related work in Section 8 , and we conclude in Section 9 .   Deformable parts models ( DPM ) were the state of the art for image object detection [ 7 ] before the emergence of deep convolutional neural nets .", "ner": [["R - CNN", "Method"], ["Deformable parts models", "Method"], ["DPM", "Method"], ["image object detection", "Task"], ["convolutional neural nets", "Method"]], "rel": [["DPM", "Synonym-Of", "Deformable parts models"], ["Deformable parts models", "Used-For", "image object detection"], ["convolutional neural nets", "Used-For", "image object detection"]], "rel_plus": [["DPM:Method", "Synonym-Of", "Deformable parts models:Method"], ["Deformable parts models:Method", "Used-For", "image object detection:Task"], ["convolutional neural nets:Method", "Used-For", "image object detection:Task"]]}
{"doc_id": "244256", "sentence": "The R - CNN method uses selective search for object region proposal [ 1 0 ] .", "ner": [["R - CNN", "Method"], ["selective search", "Method"]], "rel": [["selective search", "Part-Of", "R - CNN"]], "rel_plus": [["selective search:Method", "Part-Of", "R - CNN:Method"]]}
{"doc_id": "244256", "sentence": "The proposed regions in an image are warped to a fixed size and fed into a classification network called R - CNN .", "ner": [["classification", "Task"], ["R - CNN", "Method"]], "rel": [["R - CNN", "Used-For", "classification"]], "rel_plus": [["R - CNN:Method", "Used-For", "classification:Task"]]}
{"doc_id": "244256", "sentence": "Fast R - CNN was introduced to reuse the shared convolution features for the region proposals [ 9 ] .", "ner": [["Fast R - CNN", "Method"], ["convolution", "Method"], ["region proposals", "Task"]], "rel": [["convolution", "Part-Of", "Fast R - CNN"], ["Fast R - CNN", "Used-For", "region proposals"]], "rel_plus": [["convolution:Method", "Part-Of", "Fast R - CNN:Method"], ["Fast R - CNN:Method", "Used-For", "region proposals:Task"]]}
{"doc_id": "244256", "sentence": "In Fast R - CNN [ 9 ] , the inference speed is still dominated by the region proposal in the selective search method .", "ner": [["Fast R - CNN", "Method"], ["region proposal", "Task"], ["selective search", "Method"]], "rel": [["selective search", "Part-Of", "Fast R - CNN"], ["Fast R - CNN", "Used-For", "region proposal"]], "rel_plus": [["selective search:Method", "Part-Of", "Fast R - CNN:Method"], ["Fast R - CNN:Method", "Used-For", "region proposal:Task"]]}
{"doc_id": "244256", "sentence": "Inspired by the SPPNet [ 1 1 ] method , Faster R - CNN uses a region proposal network ( RPN ) to regress proposal boxes to ground truth boxes .", "ner": [["SPPNet", "Method"], ["Faster R - CNN", "Method"], ["region proposal network", "Method"], ["RPN", "Method"]], "rel": [["region proposal network", "Part-Of", "Faster R - CNN"], ["RPN", "Synonym-Of", "region proposal network"]], "rel_plus": [["region proposal network:Method", "Part-Of", "Faster R - CNN:Method"], ["RPN:Method", "Synonym-Of", "region proposal network:Method"]]}
{"doc_id": "244256", "sentence": "The regions proposed by the RPN network is fed into the R - CNN network for classification .", "ner": [["RPN", "Method"], ["R - CNN", "Method"], ["classification", "Task"]], "rel": [["RPN", "Part-Of", "R - CNN"], ["R - CNN", "Used-For", "classification"]], "rel_plus": [["RPN:Method", "Part-Of", "R - CNN:Method"], ["R - CNN:Method", "Used-For", "classification:Task"]]}
{"doc_id": "244256", "sentence": "For example , the OverFeat [ 2 0 ] method predicts a single box for localization whereas the Multibox [ 5 , 2 3 ] method predicts multiple boxes in a class - agnostic way .", "ner": [["OverFeat", "Method"], ["Multibox", "Method"]], "rel": [["OverFeat", "Compare-With", "Multibox"]], "rel_plus": [["OverFeat:Method", "Compare-With", "Multibox:Method"]]}
{"doc_id": "244256", "sentence": "The SPP method [ 1 1 ] uses shared convolutional feature maps for fast object detection .", "ner": [["SPP", "Method"], ["object detection", "Task"]], "rel": [["SPP", "Used-For", "object detection"]], "rel_plus": [["SPP:Method", "Used-For", "object detection:Task"]]}
{"doc_id": "244256", "sentence": "Deep neural networks are the backbone of most high - accuracy approaches to identifying objects such as cars in KITTI and similar datasets .", "ner": [["Deep neural networks", "Method"], ["KITTI", "Dataset"]], "rel": [["Deep neural networks", "Evaluated-With", "KITTI"]], "rel_plus": [["Deep neural networks:Method", "Evaluated-With", "KITTI:Dataset"]]}
{"doc_id": "244256", "sentence": "A high - accuracy method for identifying objects in KITTI dataset is scale dependent pooling ( SDP ) combined with cascaded region classifiers ( CRC ) [ 2 5 ] .", "ner": [["KITTI", "Dataset"], ["scale dependent pooling", "Method"], ["SDP", "Method"], ["cascaded region classifiers", "Method"], ["CRC", "Method"]], "rel": [["scale dependent pooling", "Evaluated-With", "KITTI"], ["SDP", "Synonym-Of", "scale dependent pooling"], ["cascaded region classifiers", "Part-Of", "scale dependent pooling"], ["CRC", "Synonym-Of", "cascaded region classifiers"]], "rel_plus": [["scale dependent pooling:Method", "Evaluated-With", "KITTI:Dataset"], ["SDP:Method", "Synonym-Of", "scale dependent pooling:Method"], ["cascaded region classifiers:Method", "Part-Of", "scale dependent pooling:Method"], ["CRC:Method", "Synonym-Of", "cascaded region classifiers:Method"]]}
{"doc_id": "244256", "sentence": "The crux of SDP+CRC lies in selecting a high - resolution CNN layer ( e.g. conv 3 3 in VGG 1 6 [ 2 1 ] ) or a heavily downsampled CNN layer ( e.g. conv 5 3 ) , depending on the resolution of each region proposal .", "ner": [["SDP+CRC", "Method"], ["high - resolution CNN layer", "Method"], ["conv 3 3", "Method"], ["VGG 1 6", "Method"], ["downsampled CNN layer", "Method"], ["conv 5 3", "Method"], ["region proposal", "Task"]], "rel": [["conv 3 3", "Synonym-Of", "high - resolution CNN layer"], ["conv 3 3", "Part-Of", "VGG 1 6"], ["conv 5 3", "Synonym-Of", "downsampled CNN layer"], ["SDP+CRC", "Used-For", "region proposal"]], "rel_plus": [["conv 3 3:Method", "Synonym-Of", "high - resolution CNN layer:Method"], ["conv 3 3:Method", "Part-Of", "VGG 1 6:Method"], ["conv 5 3:Method", "Synonym-Of", "downsampled CNN layer:Method"], ["SDP+CRC:Method", "Used-For", "region proposal:Task"]]}
{"doc_id": "244256", "sentence": "By combining features from multiple convolution layers , they were able to achieve very high accuracy on KITTI 's object detection task .", "ner": [["convolution", "Method"], ["KITTI", "Dataset"], ["object detection", "Task"]], "rel": [["KITTI", "Benchmark-For", "object detection"]], "rel_plus": [["KITTI:Dataset", "Benchmark-For", "object detection:Task"]]}
{"doc_id": "244256", "sentence": "Another approach is Monocular 3D ( Mono 3 D ) which actually uses 2D images , but it aims to identify the pose of objects , with the goal of detecting objects as 3D bounding boxes .", "ner": [["Monocular 3D", "Method"], ["Mono 3 D", "Method"]], "rel": [["Mono 3 D", "Synonym-Of", "Monocular 3D"]], "rel_plus": [["Mono 3 D:Method", "Synonym-Of", "Monocular 3D:Method"]]}
{"doc_id": "244256", "sentence": "Like SDP+CRC , Mono 3 D is built around a version of R - CNN .", "ner": [["SDP+CRC", "Method"], ["Mono 3 D", "Method"], ["R - CNN", "Method"]], "rel": [["Mono 3 D", "Compare-With", "SDP+CRC"], ["Mono 3 D", "SubClass-Of", "R - CNN"]], "rel_plus": [["Mono 3 D:Method", "Compare-With", "SDP+CRC:Method"], ["Mono 3 D:Method", "SubClass-Of", "R - CNN:Method"]]}
{"doc_id": "244256", "sentence": "There are also a number of anonymous and/or sparsely - explained submissions to the KITTI website 's leaderboard that are reportedly built on top of R - CNN .", "ner": [["KITTI", "Dataset"], ["R - CNN", "Method"]], "rel": [["R - CNN", "Evaluated-With", "KITTI"]], "rel_plus": [["R - CNN:Method", "Evaluated-With", "KITTI:Dataset"]]}
{"doc_id": "244256", "sentence": "To build supervised 2D datasets such as ImageNet [ 4 ] and PASCAL [ 6 ] , a widely - used approach is to have mechanical turk workers annotate user - generated images and videos from websites such as Flickr or YouTube .", "ner": [["ImageNet", "Dataset"], ["PASCAL", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "244256", "sentence": "For example , in VGG 1 6 , for a standard input image size 2 2 4 x 2 2 4 , the features calculated by the first convolution layer is 2 2 4 x 2 2 4 which reduce to 1 4 x 1 4 at the output of conv 5 3 .", "ner": [["VGG 1 6", "Method"], ["convolution", "Method"], ["conv 5 3", "Method"]], "rel": [["convolution", "Part-Of", "VGG 1 6"], ["conv 5 3", "Part-Of", "VGG 1 6"]], "rel_plus": [["convolution:Method", "Part-Of", "VGG 1 6:Method"], ["conv 5 3:Method", "Part-Of", "VGG 1 6:Method"]]}
{"doc_id": "244256", "sentence": "Similarly for AlexNet , the convolution feature dimension reduces from 5 5 x 5 5 in conv 1 to 6x 6 in pool 5 .", "ner": [["AlexNet", "Method"], ["convolution", "Method"]], "rel": [["convolution", "Part-Of", "AlexNet"]], "rel_plus": [["convolution:Method", "Part-Of", "AlexNet:Method"]]}
{"doc_id": "244256", "sentence": "In some CNN architectures such as AlexNet [ 1 6 ] and VGG 1 6 [ 2 1 ] , the first fully - connected layer expects a specific height and width for its input data ( e.g. 6x 6 for AlexNet ) .", "ner": [["CNN", "Method"], ["AlexNet", "Method"], ["VGG 1 6", "Method"], ["fully - connected layer", "Method"], ["AlexNet", "Method"]], "rel": [["AlexNet", "SubClass-Of", "CNN"], ["VGG 1 6", "SubClass-Of", "CNN"], ["fully - connected layer", "Part-Of", "AlexNet"], ["fully - connected layer", "Part-Of", "VGG 1 6"], ["fully - connected layer", "Part-Of", "AlexNet"]], "rel_plus": [["AlexNet:Method", "SubClass-Of", "CNN:Method"], ["VGG 1 6:Method", "SubClass-Of", "CNN:Method"], ["fully - connected layer:Method", "Part-Of", "AlexNet:Method"], ["fully - connected layer:Method", "Part-Of", "VGG 1 6:Method"], ["fully - connected layer:Method", "Part-Of", "AlexNet:Method"]]}
{"doc_id": "244256", "sentence": "At the CNN architecture level , an easy way around this is to design a CNN architecture that has global average pooling prior to the first FC layer -this approach was popularized in the Network - in - Network ( NiN ) architecture [ 1 7 ] , and it is now used in other architectures such as SqueezeNet [ 1 5 ] and ResNet architectures [ 1 2 ] .", "ner": [["CNN", "Method"], ["CNN", "Method"], ["global average pooling", "Method"], ["FC layer", "Method"], ["Network - in - Network", "Method"], ["NiN", "Method"], ["SqueezeNet", "Method"], ["ResNet", "Method"]], "rel": [["global average pooling", "Part-Of", "CNN"], ["FC layer", "Part-Of", "CNN"], ["NiN", "Synonym-Of", "Network - in - Network"]], "rel_plus": [["global average pooling:Method", "Part-Of", "CNN:Method"], ["FC layer:Method", "Part-Of", "CNN:Method"], ["NiN:Method", "Synonym-Of", "Network - in - Network:Method"]]}
{"doc_id": "244256", "sentence": "However , when using AlexNet or VGG 1 9 , the R - CNN authors developed a technique called ROI Pooling that allows any size input image to be used in concert with AlexNet/VGG FC layers .", "ner": [["AlexNet", "Method"], ["VGG 1 9", "Method"], ["R - CNN", "Method"], ["ROI Pooling", "Method"], ["AlexNet/VGG FC layers", "Method"]], "rel": [["AlexNet", "Part-Of", "R - CNN"], ["VGG 1 9", "Part-Of", "R - CNN"], ["ROI Pooling", "Part-Of", "R - CNN"], ["AlexNet/VGG FC layers", "Part-Of", "R - CNN"]], "rel_plus": [["AlexNet:Method", "Part-Of", "R - CNN:Method"], ["VGG 1 9:Method", "Part-Of", "R - CNN:Method"], ["ROI Pooling:Method", "Part-Of", "R - CNN:Method"], ["AlexNet/VGG FC layers:Method", "Part-Of", "R - CNN:Method"]]}
{"doc_id": "244256", "sentence": "We briefly review how the region proposal network ( RPN ) in Faster R - CNN generate proposals [ 1 9 ] that will be useful later .", "ner": [["region proposal network", "Method"], ["RPN", "Method"], ["Faster R - CNN", "Method"]], "rel": [["RPN", "Synonym-Of", "region proposal network"], ["region proposal network", "Part-Of", "Faster R - CNN"]], "rel_plus": [["RPN:Method", "Synonym-Of", "region proposal network:Method"], ["region proposal network:Method", "Part-Of", "Faster R - CNN:Method"]]}
{"doc_id": "244256", "sentence": "RPN starts with convolution layers , which computes a high dimensional , low resolution feature map for the input image .", "ner": [["RPN", "Method"], ["convolution", "Method"]], "rel": [["convolution", "Part-Of", "RPN"]], "rel_plus": [["convolution:Method", "Part-Of", "RPN:Method"]]}
{"doc_id": "244256", "sentence": "As we will see later in this paper , reducing this distance , or relatively , increasing the \" anchor density \" will significantly increase the localization accuracy , thus improve the detection accuracy .   We train faster R - CNN networks built on the VGG 1 6 [ 2 1 ] and AlexNet [ 1 6 ] , pretrained on the ImageNet - 1 k [ 4 ] classification dataset .", "ner": [["detection", "Task"], ["faster R - CNN", "Method"], ["VGG 1 6", "Method"], ["AlexNet", "Method"], ["ImageNet - 1 k", "Dataset"], ["classification", "Task"]], "rel": [["VGG 1 6", "Part-Of", "faster R - CNN"], ["AlexNet", "Part-Of", "faster R - CNN"], ["AlexNet", "Trained-With", "ImageNet - 1 k"], ["VGG 1 6", "Trained-With", "ImageNet - 1 k"], ["ImageNet - 1 k", "Benchmark-For", "classification"]], "rel_plus": [["VGG 1 6:Method", "Part-Of", "faster R - CNN:Method"], ["AlexNet:Method", "Part-Of", "faster R - CNN:Method"], ["AlexNet:Method", "Trained-With", "ImageNet - 1 k:Dataset"], ["VGG 1 6:Method", "Trained-With", "ImageNet - 1 k:Dataset"], ["ImageNet - 1 k:Dataset", "Benchmark-For", "classification:Task"]]}
{"doc_id": "244256", "sentence": "VGG 1 6 has sixteen convolution layers [ 2 1 ] and AlexNet has only five convolutional layers [ 1 6 ] .", "ner": [["VGG 1 6", "Method"], ["convolution", "Method"], ["AlexNet", "Method"], ["convolutional layers", "Method"]], "rel": [["convolution", "Part-Of", "VGG 1 6"], ["convolutional layers", "Part-Of", "AlexNet"]], "rel_plus": [["convolution:Method", "Part-Of", "VGG 1 6:Method"], ["convolutional layers:Method", "Part-Of", "AlexNet:Method"]]}
{"doc_id": "244256", "sentence": "Rather than using the convolution features of the last pooled layer ( as is done in the original faster R - CNN paper ) , we use features from convolutional layers that are the bottom layers of the previous pooling layer .", "ner": [["convolution", "Method"], ["faster R - CNN", "Method"], ["convolutional layers", "Method"], ["pooling layer", "Method"]], "rel": [["convolution", "Part-Of", "faster R - CNN"]], "rel_plus": [["convolution:Method", "Part-Of", "faster R - CNN:Method"]]}
{"doc_id": "244256", "sentence": "For VGG 1 6 and AlexNet , fully connected layers are used as the R - CNN branch .", "ner": [["VGG 1 6", "Method"], ["AlexNet", "Method"], ["fully connected layers", "Method"], ["R - CNN", "Method"]], "rel": [["fully connected layers", "Part-Of", "R - CNN"]], "rel_plus": [["fully connected layers:Method", "Part-Of", "R - CNN:Method"]]}
{"doc_id": "244256", "sentence": "As the standard procedure introduced in faster R - CNN , we randomly sample 1 2 8 positive and 1 2 8 negative roi proposals per batch to train the R - CNN layer .", "ner": [["faster R - CNN", "Method"], ["R - CNN layer", "Method"]], "rel": [["R - CNN layer", "Part-Of", "faster R - CNN"]], "rel_plus": [["R - CNN layer:Method", "Part-Of", "faster R - CNN:Method"]]}
{"doc_id": "244256", "sentence": "A total of 7 0 K iterations are run during R - CNN training starting from imagenet pre - trained weights for the convolution layers .", "ner": [["R - CNN", "Method"], ["imagenet", "Dataset"], ["convolution", "Method"]], "rel": [["convolution", "Part-Of", "R - CNN"], ["R - CNN", "Trained-With", "imagenet"]], "rel_plus": [["convolution:Method", "Part-Of", "R - CNN:Method"], ["R - CNN:Method", "Trained-With", "imagenet:Dataset"]]}
{"doc_id": "244256", "sentence": "We use the KITTI object detection dataset .", "ner": [["KITTI object detection", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "244256", "sentence": "In KITTI 's evaluation criteria , proposal boxes having overlap with the ground truth or IoU greater than 7 0 % are counted as true detection for cars .", "ner": [["KITTI", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "244256", "sentence": "The Faster R - CNN algorithm has been shown to deliver high accuracy on the PASCAL [ 6 ] dataset .", "ner": [["Faster R - CNN", "Method"], ["PASCAL", "Dataset"]], "rel": [["Faster R - CNN", "Evaluated-With", "PASCAL"]], "rel_plus": [["Faster R - CNN:Method", "Evaluated-With", "PASCAL:Dataset"]]}
{"doc_id": "244256", "sentence": "Adapting that pipeline from PASCAL to KITTI poses a few natural challenges .", "ner": [["PASCAL", "Dataset"], ["KITTI", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "244256", "sentence": "First , the image sizes in the KITTI dataset is 1 2 4 2 x 3 7 5 pixels whereas the image sizes in PASCAL dataset is 5 0 0 pixels in the longest dimension ( many PASCAL images are 5 0 0 x 3 3 3 or 3 3 3 x 5 0 0 ) .", "ner": [["KITTI", "Dataset"], ["PASCAL", "Dataset"], ["PASCAL", "Dataset"]], "rel": [["KITTI", "Compare-With", "PASCAL"]], "rel_plus": [["KITTI:Dataset", "Compare-With", "PASCAL:Dataset"]]}
{"doc_id": "244256", "sentence": "We performed extensive design space search of Faster R - CNN configurations on the KITTI dataset .", "ner": [["Faster R - CNN", "Method"], ["KITTI", "Dataset"]], "rel": [["Faster R - CNN", "Evaluated-With", "KITTI"]], "rel_plus": [["Faster R - CNN:Method", "Evaluated-With", "KITTI:Dataset"]]}
{"doc_id": "244256", "sentence": "Our starting point is the VGG 1 6 network that has achieved high accuracy in both image classification [ 2 1 ] and localization [ 1 9 ] .", "ner": [["VGG 1 6", "Method"], ["image classification", "Task"], ["localization", "Task"]], "rel": [["VGG 1 6", "Used-For", "image classification"], ["VGG 1 6", "Used-For", "localization"]], "rel_plus": [["VGG 1 6:Method", "Used-For", "image classification:Task"], ["VGG 1 6:Method", "Used-For", "localization:Task"]]}
{"doc_id": "244256", "sentence": "KITTI images have a native resolution of 1 2 9 5 x 3 7 5 , so the default Faster R - CNN behavior is to resize KITTI images to 1 0 0 0 x 3 0 2 .", "ner": [["KITTI", "Dataset"], ["Faster R - CNN", "Method"], ["KITTI", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "244256", "sentence": "For example , the conv 5 3 activations -which serves as input to both the region proposal network ( RPN ) and the classification network -double in height and width .", "ner": [["region proposal network", "Method"], ["RPN", "Method"], ["classification", "Task"]], "rel": [["RPN", "Synonym-Of", "region proposal network"], ["region proposal network", "Used-For", "classification"]], "rel_plus": [["RPN:Method", "Synonym-Of", "region proposal network:Method"], ["region proposal network:Method", "Used-For", "classification:Task"]]}
{"doc_id": "244256", "sentence": "In the next section , we consider shallower networks with fewer layers of activation planes , which enables us to move to even higher input resolutions .   So far , we have upsampled the input image until we ran out of on - chip GPU memory when training R - CNN models with a VGG 1 6 - based feature representation .", "ner": [["R - CNN", "Method"], ["VGG 1 6", "Method"]], "rel": [["VGG 1 6", "Part-Of", "R - CNN"]], "rel_plus": [["VGG 1 6:Method", "Part-Of", "R - CNN:Method"]]}
{"doc_id": "244256", "sentence": "In VGG 1 6 's scheme for naming layers , conv 4 3 is the 1 0 th layer , and conv 5 3 is the 1 3 th layer in the CNN .", "ner": [["VGG 1 6", "Method"], ["conv 4 3", "Method"], ["conv 5 3", "Method"], ["CNN", "Method"]], "rel": [["conv 5 3", "Part-Of", "CNN"], ["conv 4 3", "Part-Of", "CNN"]], "rel_plus": [["conv 5 3:Method", "Part-Of", "CNN:Method"], ["conv 4 3:Method", "Part-Of", "CNN:Method"]]}
{"doc_id": "244256", "sentence": "We expected that the accuracy of R - CNN with conv 4 3 would be slightly lower than R - CNN with conv 5 3 , but as we show in Table 2 that the accuracy is higher with conv 4 3 by 5. 5 , 9. 4 , and 1 2 . 4 percentage - points for easy , medium , and hard detections .", "ner": [["R - CNN with conv 4 3", "Method"], ["R - CNN with conv 5 3", "Method"]], "rel": [["R - CNN with conv 4 3", "Compare-With", "R - CNN with conv 5 3"]], "rel_plus": [["R - CNN with conv 4 3:Method", "Compare-With", "R - CNN with conv 5 3:Method"]]}
{"doc_id": "244256", "sentence": "We initially considered using the earlier layers of VGG 1 6 as input to the Region Proposal Network .", "ner": [["VGG 1 6", "Method"], ["Region Proposal Network", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "244256", "sentence": "Besides depth , one of the differences between VGG 1 6 and AlexNet is that AlexNet downsamples more aggressively in the early layers -for example AlexNet has stride= 4 in the conv 1 layer ( 4x downsampling ) , while the conv 1 layer of VGG 1 6 has stride= 1 ( no downsampling ) .", "ner": [["VGG 1 6", "Method"], ["AlexNet", "Method"], ["AlexNet", "Method"], ["AlexNet", "Method"], ["conv 1 layer", "Method"], ["conv 1 layer", "Method"], ["VGG 1 6", "Method"]], "rel": [["VGG 1 6", "Compare-With", "AlexNet"], ["conv 1 layer", "Part-Of", "AlexNet"], ["conv 1 layer", "Part-Of", "VGG 1 6"]], "rel_plus": [["VGG 1 6:Method", "Compare-With", "AlexNet:Method"], ["conv 1 layer:Method", "Part-Of", "AlexNet:Method"], ["conv 1 layer:Method", "Part-Of", "VGG 1 6:Method"]]}
{"doc_id": "244256", "sentence": "So , to evaluate this question of how using shallower ( < 1 0 conv layers ) network impacts accuracy , we use AlexNet instead of VGG 1 6 .", "ner": [["AlexNet", "Method"], ["VGG 1 6", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "244256", "sentence": "We use conv 5 ( 5th layer ) activations as input to the R - CNN region - proposal and classification branches , and we report the results in Table 2 .", "ner": [["conv 5", "Method"], ["R - CNN region - proposal", "Method"], ["classification branches", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "244256", "sentence": "With resolution of 2 0 0 0 x 6 0 4 for both AlexNet - conv 5 and VGG 1 6 - conv 4 3 , the VGG 1 6 - based configuration delivers significantly higher accuracy on easy , medium , and hard detections in Table 2 .", "ner": [["AlexNet - conv 5", "Method"], ["VGG 1 6 - conv 4 3", "Method"], ["VGG 1 6", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "244256", "sentence": "We have additional memory available when running AlexNet with 2 0 0 0 x 6 0 4 input images , so we now try upsampling the AlexNet input images to 5 0 0 0 x 1 5 1 0 .", "ner": [["AlexNet", "Method"], ["AlexNet", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "244256", "sentence": "In this configuration , on the easy detections , AlexNet with 5 0 0 0 x 1 5 1 0 input is within 0. 5 of a percentage - point of our best VGG 1 6 - based result so far .", "ner": [["AlexNet", "Method"], ["VGG 1 6", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "244256", "sentence": "On medium and hard categories , VGG 1 6 conv 4 3 with an input resolution of 2 0 0 0 x 6 0 4 delivers higher accuracy than AlexNet with 2 0 0 0 x 6 0 4 or 5 0 0 0 x 1 5 1 0 input images .", "ner": [["VGG 1 6 conv 4 3", "Method"], ["AlexNet", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "244256", "sentence": "We also conduct a sweep of input image sizes applied to an AlexNet - based R - CNN model that uses conv 5 features .", "ner": [["AlexNet", "Method"], ["R - CNN", "Method"], ["conv 5", "Method"]], "rel": [["AlexNet", "Part-Of", "R - CNN"], ["conv 5", "Part-Of", "R - CNN"]], "rel_plus": [["AlexNet:Method", "Part-Of", "R - CNN:Method"], ["conv 5:Method", "Part-Of", "R - CNN:Method"]]}
{"doc_id": "244256", "sentence": "In the experiments with context window , in addition to the original R - CNN branch , an extra R - CNN branch is added that trains on the features extracted from context window .", "ner": [["R - CNN", "Method"], ["R - CNN branch", "Method"]], "rel": [["R - CNN branch", "Part-Of", "R - CNN"]], "rel_plus": [["R - CNN branch:Method", "Part-Of", "R - CNN:Method"]]}
{"doc_id": "244256", "sentence": "The original R - CNN features and the context R - CNN features are concatenated before classification .", "ner": [["R - CNN", "Method"], ["R - CNN", "Method"], ["classification", "Task"]], "rel": [["R - CNN", "Used-For", "classification"]], "rel_plus": [["R - CNN:Method", "Used-For", "classification:Task"]]}
{"doc_id": "244256", "sentence": "When applying the context window to an AlexNet - based R - CNN configuration , we find that the accuracy of all the categories improve as shown in Table 4 .", "ner": [["AlexNet", "Method"], ["R - CNN", "Method"]], "rel": [["AlexNet", "Part-Of", "R - CNN"]], "rel_plus": [["AlexNet:Method", "Part-Of", "R - CNN:Method"]]}
{"doc_id": "244256", "sentence": "We tested the AlexNet - based Faster R - CNN 's detection accuracy with different anchor box selection schemes , and the result is shown in Table 3 .", "ner": [["AlexNet", "Method"], ["Faster R - CNN", "Method"], ["detection", "Task"]], "rel": [["AlexNet", "Part-Of", "Faster R - CNN"], ["Faster R - CNN", "Used-For", "detection"]], "rel_plus": [["AlexNet:Method", "Part-Of", "Faster R - CNN:Method"], ["Faster R - CNN:Method", "Used-For", "detection:Task"]]}
{"doc_id": "244256", "sentence": "The inference time using AlexNet conv 5 layer with input image size of 5 0 0 0 x 1 5 1 0 and VGG 1 6 conv 4 3 layer with input image size of 2 0 0 0 x 6 0 4 is 0. 3 4 s and 0. 6 s respectively .", "ner": [["AlexNet conv 5 layer", "Method"], ["VGG 1 6 conv 4 3 layer", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "244256", "sentence": "Inference times for other published high accuracy methods on the KITTI dataset are 3s for 3DOP [ 3 ] and 0. 4 s for SDP+CRC [ 2 5 ] .", "ner": [["KITTI", "Dataset"], ["3DOP", "Method"], ["SDP+CRC", "Method"]], "rel": [["SDP+CRC", "Evaluated-With", "KITTI"], ["3DOP", "Evaluated-With", "KITTI"]], "rel_plus": [["SDP+CRC:Method", "Evaluated-With", "KITTI:Dataset"], ["3DOP:Method", "Evaluated-With", "KITTI:Dataset"]]}
{"doc_id": "244256", "sentence": "We have shown that input image resolution has a large impact on the accuracy of car detection using the faster R - CNN network .", "ner": [["car detection", "Task"], ["faster R - CNN", "Method"]], "rel": [["faster R - CNN", "Used-For", "car detection"]], "rel_plus": [["faster R - CNN:Method", "Used-For", "car detection:Task"]]}
