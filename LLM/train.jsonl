{"doc_id": "51923817", "sentence": "We propose CornerNet , a new approach to object detection where we detect an object bounding box as a pair of keypoints , the top - left corner and the bottom - right corner , using a single convolution neural network .", "ner": [["CornerNet", "Method"], ["object detection", "Task"], ["convolution neural network", "Method"]], "rel": [["convolution neural network", "Part-Of", "CornerNet"], ["CornerNet", "Used-For", "object detection"]], "rel_plus": [["convolution neural network:Method", "Part-Of", "CornerNet:Method"], ["CornerNet:Method", "Used-For", "object detection:Task"]]}
{"doc_id": "51923817", "sentence": "Experiments show that CornerNet achieves a 4 2 . 2 % AP on MS COCO , outperforming all existing one - stage detectors .", "ner": [["CornerNet", "Method"], ["MS COCO", "Dataset"]], "rel": [["CornerNet", "Evaluated-With", "MS COCO"]], "rel_plus": [["CornerNet:Method", "Evaluated-With", "MS COCO:Dataset"]]}
{"doc_id": "51923817", "sentence": "Object detectors based on convolutional neural networks ( ConvNets ) ( Krizhevsky et al. , 2 0 1 2 ; Simonyan and Zisserman , 2 0 1 4 ; He et al. , 2 0 1 6 ) have achieved state - of - the - art results on various challenging benchmarks ( Lin et al. , 2 0 1 4 ; Deng et al. , 2 0 0 9 ; Everingham et al. , 2 0 1 5 ) .", "ner": [["convolutional neural networks", "Method"], ["ConvNets", "Method"]], "rel": [["ConvNets", "Synonym-Of", "convolutional neural networks"]], "rel_plus": [["ConvNets:Method", "Synonym-Of", "convolutional neural networks:Method"]]}
{"doc_id": "51923817", "sentence": "In this paper we introduce CornerNet , a new onestage approach to object detection that does away with anchor boxes .", "ner": [["CornerNet", "Method"], ["object detection", "Task"]], "rel": [["CornerNet", "Used-For", "object detection"]], "rel_plus": [["CornerNet:Method", "Used-For", "object detection:Task"]]}
{"doc_id": "51923817", "sentence": "We use a single convolutional network to predict a heatmap for the top - left corners of all instances of the same object category , a heatmap for all bottomright corners , and an embedding vector for each detected corner .", "ner": [["convolutional network", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "51923817", "sentence": "A convolutional network outputs a heatmap for all top - left corners , a heatmap for all bottom - right corners , and an embedding vector for each detected corner .", "ner": [["convolutional network", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "51923817", "sentence": "Another novel component of CornerNet is corner pooling , a new type of pooling layer that helps a convolutional network better localize corners of bounding boxes .", "ner": [["CornerNet", "Method"], ["corner pooling", "Method"]], "rel": [["corner pooling", "Part-Of", "CornerNet"]], "rel_plus": [["corner pooling:Method", "Part-Of", "CornerNet:Method"]]}
{"doc_id": "51923817", "sentence": "We demonstrate the effectiveness of CornerNet on MS COCO ( Lin et al. , 2 0 1 4 ) .", "ner": [["CornerNet", "Method"], ["MS COCO", "Dataset"]], "rel": [["CornerNet", "Used-For", "MS COCO"]], "rel_plus": [["CornerNet:Method", "Used-For", "MS COCO:Dataset"]]}
{"doc_id": "51923817", "sentence": "In addition , through ablation studies we show that corner pooling is critical to the superior performance of CornerNet .", "ner": [["corner pooling", "Method"], ["CornerNet", "Method"]], "rel": [["corner pooling", "Part-Of", "CornerNet"]], "rel_plus": [["corner pooling:Method", "Part-Of", "CornerNet:Method"]]}
{"doc_id": "51923817", "sentence": "Code is available at https://github.com/ princeton - vl/CornerNet .   Two - stage approach was first introduced and popularized by R - CNN ( Girshick et al. , 2 0 1 4 ) .", "ner": [["vl/CornerNet", "Method"], ["R - CNN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "51923817", "sentence": "Later , SPP ( He et al. , 2 0 1 4 ) and Fast - RCNN ( Girshick , 2 0 1 5 ) improve R - CNN by designing a special pooling layer that pools each region from feature maps instead .", "ner": [["SPP", "Method"], ["Fast - RCNN", "Method"], ["R - CNN", "Method"]], "rel": [["SPP", "Compare-With", "R - CNN"], ["Fast - RCNN", "Compare-With", "R - CNN"]], "rel_plus": [["SPP:Method", "Compare-With", "R - CNN:Method"], ["Fast - RCNN:Method", "Compare-With", "R - CNN:Method"]]}
{"doc_id": "51923817", "sentence": "Faster - RCNN ( Ren et al. , 2 0 1 5 ) does away low level proposal algorithms by introducing a region proposal network ( RPN ) , which generates proposals from a set of pre - determined candidate boxes , usually known as anchor boxes .", "ner": [["Faster - RCNN", "Method"], ["region proposal network", "Method"], ["RPN", "Method"]], "rel": [["region proposal network", "Part-Of", "Faster - RCNN"], ["RPN", "Synonym-Of", "region proposal network"]], "rel_plus": [["region proposal network:Method", "Part-Of", "Faster - RCNN:Method"], ["RPN:Method", "Synonym-Of", "region proposal network:Method"]]}
{"doc_id": "51923817", "sentence": "R - FCN ( Dai et al. , 2 0 1 6 ) further improves the efficiency of Faster - RCNN by replacing the fully connected sub - detection network with a fully convolutional subdetection network .", "ner": [["R - FCN", "Method"], ["Faster - RCNN", "Method"], ["fully connected sub - detection network", "Method"], ["fully convolutional subdetection network", "Method"]], "rel": [["fully convolutional subdetection network", "Part-Of", "R - FCN"], ["R - FCN", "Compare-With", "Faster - RCNN"]], "rel_plus": [["fully convolutional subdetection network:Method", "Part-Of", "R - FCN:Method"], ["R - FCN:Method", "Compare-With", "Faster - RCNN:Method"]]}
{"doc_id": "51923817", "sentence": "On the other hand , YOLO and SSD ( Liu et al. , 2 0 1 6 ) have popularized the one - stage approach , which removes the RoI pooling step and detects objects in a single network .", "ner": [["YOLO", "Method"], ["SSD", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "51923817", "sentence": "YOLO predicts bounding box coordinates directly from an image , and is later improved in YOLO 9 0 0 0 by switching to anchor boxes .", "ner": [["YOLO", "Method"], ["YOLO 9 0 0 0", "Method"]], "rel": [["YOLO", "Compare-With", "YOLO 9 0 0 0"]], "rel_plus": [["YOLO:Method", "Compare-With", "YOLO 9 0 0 0:Method"]]}
{"doc_id": "51923817", "sentence": "DSSD and RON ( Kong et al. , 2 0 1 7 ) adopt networks similar to the hourglass network ( Newell et al. , 2 0 1 6 ) , enabling them to combine low - level and high - level features via skip connections to predict bounding boxes more accurately .", "ner": [["DSSD", "Method"], ["RON", "Method"], ["hourglass network", "Method"], ["skip connections", "Method"]], "rel": [["skip connections", "Part-Of", "DSSD"], ["hourglass network", "Part-Of", "DSSD"], ["skip connections", "Part-Of", "RON"], ["hourglass network", "Part-Of", "RON"]], "rel_plus": [["skip connections:Method", "Part-Of", "DSSD:Method"], ["hourglass network:Method", "Part-Of", "DSSD:Method"], ["skip connections:Method", "Part-Of", "RON:Method"], ["hourglass network:Method", "Part-Of", "RON:Method"]]}
{"doc_id": "51923817", "sentence": "Second , DeNet selects features at manually determined locations relative to a region for classification , while our approach does not require any feature selection step .", "ner": [["DeNet", "Method"], ["classification", "Task"], ["feature selection", "Task"]], "rel": [["DeNet", "Used-For", "classification"], ["feature selection", "Used-For", "classification"]], "rel_plus": [["DeNet:Method", "Used-For", "classification:Task"], ["feature selection:Task", "Used-For", "classification:Task"]]}
{"doc_id": "51923817", "sentence": "Point Linking Network ( PLN ) is an one - stage detector without anchor boxes .", "ner": [["Point Linking Network", "Method"], ["PLN", "Method"]], "rel": [["PLN", "Synonym-Of", "Point Linking Network"]], "rel_plus": [["PLN:Method", "Synonym-Of", "Point Linking Network:Method"]]}
{"doc_id": "51923817", "sentence": "CornerNet is very different from PLN .", "ner": [["CornerNet", "Method"], ["PLN", "Method"]], "rel": [["CornerNet", "Compare-With", "PLN"]], "rel_plus": [["CornerNet:Method", "Compare-With", "PLN:Method"]]}
{"doc_id": "51923817", "sentence": "Second , CornerNet uses corner pooling to better localize the corners .", "ner": [["CornerNet", "Method"], ["corner pooling", "Method"]], "rel": [["corner pooling", "Part-Of", "CornerNet"]], "rel_plus": [["corner pooling:Method", "Part-Of", "CornerNet:Method"]]}
{"doc_id": "51923817", "sentence": "We also significantly modify the hourglass architecture and add our novel variant of focal loss to help better train the network .   In CornerNet , we detect an object as a pair of keypointsthe top - left corner and bottom - right corner of the bounding box .", "ner": [["focal loss", "Method"], ["CornerNet", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "51923817", "sentence": "We use the hourglass network ( Newell et al. , 2 0 1 6 ) as the backbone network of CornerNet .", "ner": [["hourglass network", "Method"], ["CornerNet", "Method"]], "rel": [["hourglass network", "Part-Of", "CornerNet"]], "rel_plus": [["hourglass network:Method", "Part-Of", "CornerNet:Method"]]}
{"doc_id": "51923817", "sentence": "Each module has its own corner pooling module to pool features from the hourglass network before predicting the heatmaps , embeddings and offsets .", "ner": [["corner pooling", "Method"], ["hourglass network", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "51923817", "sentence": "In this modified residual block , we replace the first 3 \u00d7 3 convolution module with a corner pooling module , which first processes the features from the backbone network by two 3 \u00d7 3 convolution modules 1 with 1 2 8 channels and then applies a corner pooling layer .", "ner": [["residual block", "Method"], ["3 \u00d7 3 convolution", "Method"], ["corner pooling", "Method"], ["3 \u00d7 3 convolution", "Method"], ["corner pooling", "Method"]], "rel": [["corner pooling", "Part-Of", "residual block"]], "rel_plus": [["corner pooling:Method", "Part-Of", "residual block:Method"]]}
{"doc_id": "51923817", "sentence": "Following the design of a residual block , we then feed the pooled features into a 3 \u00d7 3 Conv - BN layer with 2 5 6 channels and add back the projection shortcut .", "ner": [["residual block", "Method"], ["3 \u00d7 3 Conv - BN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "51923817", "sentence": "The modified residual block is followed by a 3 \u00d7 3 convolution module with 2 5 6 channels , and 3 Conv - ReLU - Conv layers to produce the heatmaps , embeddings and offsets .", "ner": [["residual block", "Method"], ["3 \u00d7 3 convolution", "Method"], ["Conv - ReLU - Conv", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "51923817", "sentence": "CornerNet uses the hourglass network ( Newell et al. , 2 0 1 6 ) as its backbone network .", "ner": [["CornerNet", "Method"], ["hourglass network", "Method"]], "rel": [["hourglass network", "Part-Of", "CornerNet"]], "rel_plus": [["hourglass network:Method", "Part-Of", "CornerNet:Method"]]}
{"doc_id": "51923817", "sentence": "The hourglass network was first introduced for the human pose estimation task .", "ner": [["hourglass network", "Method"], ["human pose estimation", "Task"]], "rel": [["hourglass network", "Used-For", "human pose estimation"]], "rel_plus": [["hourglass network:Method", "Used-For", "human pose estimation:Task"]]}
{"doc_id": "51923817", "sentence": "An hourglass module first downsamples the input features by a series of convolution and max pooling layers .", "ner": [["hourglass module", "Method"], ["convolution", "Method"], ["max pooling", "Method"]], "rel": [["convolution", "Part-Of", "hourglass module"], ["max pooling", "Part-Of", "hourglass module"]], "rel_plus": [["convolution:Method", "Part-Of", "hourglass module:Method"], ["max pooling:Method", "Part-Of", "hourglass module:Method"]]}
{"doc_id": "51923817", "sentence": "These properties make the hourglass network an ideal choice for object detection as well .", "ner": [["hourglass network", "Method"], ["object detection", "Task"]], "rel": [["hourglass network", "Used-For", "object detection"]], "rel_plus": [["hourglass network:Method", "Used-For", "object detection:Task"]]}
{"doc_id": "51923817", "sentence": "Our hourglass network consists of two hourglasses , and we make some modifications to the architecture of the hourglass module .", "ner": [["hourglass network", "Method"], ["hourglasses", "Method"], ["hourglass module", "Method"]], "rel": [["hourglasses", "Part-Of", "hourglass network"]], "rel_plus": [["hourglasses:Method", "Part-Of", "hourglass network:Method"]]}
{"doc_id": "51923817", "sentence": "We scan from right to left for the horizontal max - pooling and from bottom to top for the vertical max - pooling .", "ner": [["horizontal max - pooling", "Method"], ["vertical max - pooling", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "51923817", "sentence": "Fig. 7 The prediction module starts with a modified residual block , in which we replace the first convolution module with our corner pooling module .", "ner": [["residual block", "Method"], ["convolution", "Method"], ["corner pooling", "Method"]], "rel": [["corner pooling", "Part-Of", "residual block"]], "rel_plus": [["corner pooling:Method", "Part-Of", "residual block:Method"]]}
{"doc_id": "51923817", "sentence": "We apply a 1 \u00d7 1 Conv - BN module to both the input and output of the first hourglass module .", "ner": [["1 \u00d7 1 Conv - BN", "Method"], ["hourglass module", "Method"]], "rel": [["1 \u00d7 1 Conv - BN", "Part-Of", "hourglass module"]], "rel_plus": [["1 \u00d7 1 Conv - BN:Method", "Part-Of", "hourglass module:Method"]]}
{"doc_id": "51923817", "sentence": "We then merge them by element - wise addition followed by a ReLU and a residual block with 2 5 6 channels , which is then used as the input to the second hourglass module .", "ner": [["ReLU", "Method"], ["residual block", "Method"], ["hourglass module", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "51923817", "sentence": "To reduce overfitting , we adopt standard data augmentation techniques including random horizontal flipping , random scaling , random cropping and random color jittering , which includes adjusting the brightness , saturation and contrast of an image .", "ner": [["data augmentation", "Method"], ["horizontal flipping", "Method"], ["random scaling", "Method"], ["random cropping", "Method"]], "rel": [["horizontal flipping", "SubClass-Of", "data augmentation"], ["random scaling", "SubClass-Of", "data augmentation"], ["random cropping", "SubClass-Of", "data augmentation"]], "rel_plus": [["horizontal flipping:Method", "SubClass-Of", "data augmentation:Method"], ["random scaling:Method", "SubClass-Of", "data augmentation:Method"], ["random cropping:Method", "SubClass-Of", "data augmentation:Method"]]}
{"doc_id": "51923817", "sentence": "We first apply non - maximal suppression ( NMS ) by using a 3 \u00d7 3 max pooling layer on the corner heatmaps .", "ner": [["non - maximal suppression", "Method"], ["NMS", "Method"], ["3 \u00d7 3 max pooling", "Method"]], "rel": [["NMS", "Synonym-Of", "non - maximal suppression"], ["3 \u00d7 3 max pooling", "Part-Of", "non - maximal suppression"]], "rel_plus": [["NMS:Method", "Synonym-Of", "non - maximal suppression:Method"], ["3 \u00d7 3 max pooling:Method", "Part-Of", "non - maximal suppression:Method"]]}
{"doc_id": "51923817", "sentence": "We evaluate CornerNet on the very challenging MS COCO dataset ( Lin et al. , 2 0 1 4 ) .", "ner": [["CornerNet", "Method"], ["MS COCO", "Dataset"]], "rel": [["CornerNet", "Evaluated-With", "MS COCO"]], "rel_plus": [["CornerNet:Method", "Evaluated-With", "MS COCO:Dataset"]]}
{"doc_id": "51923817", "sentence": "MS COCO uses average precisions ( APs ) at different IoUs and APs for different object sizes as the main evaluation metrics .    Corner pooling is a key component of CornerNet .", "ner": [["MS COCO", "Dataset"], ["Corner pooling", "Method"], ["CornerNet", "Method"]], "rel": [["Corner pooling", "Part-Of", "CornerNet"]], "rel_plus": [["Corner pooling:Method", "Part-Of", "CornerNet:Method"]]}
{"doc_id": "51923817", "sentence": "Corner pooling gives similar improvement to corners at different quadrants , show that corner pooling is effective and stable over both small and large areas .", "ner": [["Corner pooling", "Method"], ["corner pooling", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "51923817", "sentence": "CornerNet uses the hourglass network ( Newell et al. , 2 0 1 6 ) as its backbone network .", "ner": [["CornerNet", "Method"], ["hourglass network", "Method"]], "rel": [["hourglass network", "Part-Of", "CornerNet"]], "rel_plus": [["hourglass network:Method", "Part-Of", "CornerNet:Method"]]}
{"doc_id": "51923817", "sentence": "Since the hourglass network is not commonly used in other state - of - the - art detectors , we perform an experiment to study the contribution of the hourglass network in CornerNet .", "ner": [["hourglass network", "Method"], ["hourglass network", "Method"], ["CornerNet", "Method"]], "rel": [["hourglass network", "Part-Of", "CornerNet"]], "rel_plus": [["hourglass network:Method", "Part-Of", "CornerNet:Method"]]}
{"doc_id": "51923817", "sentence": "We train a CornerNet in which we replace the hourglass network with FPN ( w/ ResNet - 1 0 1 ) , which is more commonly used in state - of - the - art object detectors .", "ner": [["CornerNet", "Method"], ["hourglass network", "Method"], ["FPN", "Method"], ["ResNet - 1 0 1", "Method"]], "rel": [["FPN", "Part-Of", "CornerNet"], ["ResNet - 1 0 1", "Part-Of", "CornerNet"]], "rel_plus": [["FPN:Method", "Part-Of", "CornerNet:Method"], ["ResNet - 1 0 1:Method", "Part-Of", "CornerNet:Method"]]}
{"doc_id": "51923817", "sentence": "Tab . 4 shows that CornerNet with hourglass network outperforms CornerNet with FPN by 8. 2 % AP , and the anchor box based detector with hourglass network by 5. 5 % AP .", "ner": [["CornerNet with hourglass network", "Method"], ["CornerNet with FPN", "Method"], ["anchor box based detector with hourglass network", "Method"]], "rel": [["CornerNet with hourglass network", "Compare-With", "CornerNet with FPN"], ["CornerNet with hourglass network", "Compare-With", "anchor box based detector with hourglass network"]], "rel_plus": [["CornerNet with hourglass network:Method", "Compare-With", "CornerNet with FPN:Method"], ["CornerNet with hourglass network:Method", "Compare-With", "anchor box based detector with hourglass network:Method"]]}
{"doc_id": "51923817", "sentence": "The results suggest that the choice of the backbone network is important and the hourglass network is crucial to the performance of CornerNet .   A good detector should predict high quality bounding boxes that cover objects tightly .", "ner": [["hourglass network", "Method"], ["CornerNet", "Method"]], "rel": [["hourglass network", "Part-Of", "CornerNet"]], "rel_plus": [["hourglass network:Method", "Part-Of", "CornerNet:Method"]]}
{"doc_id": "51923817", "sentence": "To understand the quality of the bounding boxes predicted by CornerNet , we evaluate the performance of CornerNet at multiple IoU thresholds , and compare the results with other state - of - the - art detectors , including RetinaNet , Cascade R - CNN ( Cai and Vasconcelos , 2 0 1 7 ) and IoU - Net ( Jiang et al. , 2 0 1 8) .", "ner": [["CornerNet", "Method"], ["CornerNet", "Method"], ["RetinaNet", "Method"], ["Cascade R - CNN", "Method"], ["IoU - Net", "Method"]], "rel": [["CornerNet", "Compare-With", "RetinaNet"], ["CornerNet", "Compare-With", "Cascade R - CNN"], ["CornerNet", "Compare-With", "IoU - Net"]], "rel_plus": [["CornerNet:Method", "Compare-With", "RetinaNet:Method"], ["CornerNet:Method", "Compare-With", "Cascade R - CNN:Method"], ["CornerNet:Method", "Compare-With", "IoU - Net:Method"]]}
{"doc_id": "51923817", "sentence": "Tab . 5 shows that CornerNet achieves a much higher AP at 0. 9 IoU than other detectors , outperforming Cascade R - CNN + IoU - Net by 3. 9 % , Cascade R - CNN by 7. 6 % and RetinaNet 2 by 7. 3 % .", "ner": [["CornerNet", "Method"], ["Cascade R - CNN + IoU - Net", "Method"], ["Cascade R - CNN", "Method"], ["RetinaNet", "Method"]], "rel": [["CornerNet", "Compare-With", "Cascade R - CNN + IoU - Net"], ["CornerNet", "Compare-With", "Cascade R - CNN"], ["CornerNet", "Compare-With", "RetinaNet"]], "rel_plus": [["CornerNet:Method", "Compare-With", "Cascade R - CNN + IoU - Net:Method"], ["CornerNet:Method", "Compare-With", "Cascade R - CNN:Method"], ["CornerNet:Method", "Compare-With", "RetinaNet:Method"]]}
{"doc_id": "51923817", "sentence": "CornerNet simultaneously outputs heatmaps , offsets , and embeddings , all of which affect detection performance .", "ner": [["CornerNet", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "51923817", "sentence": "We compare CornerNet with other state - of - the - art detectors on MS COCO test - dev ( Tab . 7 ) .", "ner": [["CornerNet", "Method"], ["MS COCO test - dev", "Dataset"]], "rel": [["CornerNet", "Evaluated-With", "MS COCO test - dev"]], "rel_plus": [["CornerNet:Method", "Evaluated-With", "MS COCO test - dev:Dataset"]]}
{"doc_id": "51923817", "sentence": "We have presented CornerNet , a new approach to object detection that detects bounding boxes as pairs of corners .", "ner": [["CornerNet", "Method"], ["object detection", "Task"]], "rel": [["CornerNet", "Used-For", "object detection"]], "rel_plus": [["CornerNet:Method", "Used-For", "object detection:Task"]]}
{"doc_id": "51923817", "sentence": "We evaluate CornerNet on MS COCO and demonstrate competitive results .", "ner": [["CornerNet", "Method"], ["MS COCO", "Dataset"]], "rel": [["CornerNet", "Evaluated-With", "MS COCO"]], "rel_plus": [["CornerNet:Method", "Evaluated-With", "MS COCO:Dataset"]]}
{"doc_id": "104291983", "sentence": "High - resolution representation learning plays an essential role in many vision problems , e.g. , pose estimation and semantic segmentation .", "ner": [["High - resolution representation learning", "Method"], ["pose estimation", "Task"], ["semantic segmentation", "Task"]], "rel": [["High - resolution representation learning", "Used-For", "pose estimation"], ["High - resolution representation learning", "Used-For", "semantic segmentation"]], "rel_plus": [["High - resolution representation learning:Method", "Used-For", "pose estimation:Task"], ["High - resolution representation learning:Method", "Used-For", "semantic segmentation:Task"]]}
{"doc_id": "104291983", "sentence": "The high - resolution network (HRNet)~\\cite{SunXLW 1 9 } , recently developed for human pose estimation , maintains high - resolution representations through the whole process by connecting high - to - low resolution convolutions in \\emph{parallel } and produces strong high - resolution representations by repeatedly conducting fusions across parallel convolutions .    In this paper , we conduct a further study on high - resolution representations by introducing a simple yet effective modification and apply it to a wide range of vision tasks .", "ner": [["high - resolution network", "Method"], ["(HRNet)~\\cite{SunXLW", "Method"], ["human pose estimation", "Task"], ["convolutions", "Method"], ["parallel convolutions", "Method"]], "rel": [["(HRNet)~\\cite{SunXLW", "Synonym-Of", "high - resolution network"], ["high - resolution network", "Used-For", "human pose estimation"]], "rel_plus": [["(HRNet)~\\cite{SunXLW:Method", "Synonym-Of", "high - resolution network:Method"], ["high - resolution network:Method", "Used-For", "human pose estimation:Task"]]}
{"doc_id": "104291983", "sentence": "We show top results in semantic segmentation on Cityscapes , LIP , and PASCAL Context , and facial landmark detection on AFLW , COFW , $ 3 0 0 $W , and WFLW .", "ner": [["semantic segmentation", "Task"], ["Cityscapes", "Dataset"], ["LIP", "Dataset"], ["PASCAL Context", "Dataset"], ["facial landmark detection", "Task"], ["AFLW", "Dataset"], ["COFW", "Dataset"], ["$ 3 0 0 $W", "Dataset"], ["WFLW", "Dataset"]], "rel": [["Cityscapes", "Benchmark-For", "semantic segmentation"], ["LIP", "Benchmark-For", "semantic segmentation"], ["PASCAL Context", "Benchmark-For", "semantic segmentation"], ["AFLW", "Benchmark-For", "facial landmark detection"], ["COFW", "Benchmark-For", "facial landmark detection"], ["$ 3 0 0 $W", "Benchmark-For", "facial landmark detection"], ["WFLW", "Benchmark-For", "facial landmark detection"]], "rel_plus": [["Cityscapes:Dataset", "Benchmark-For", "semantic segmentation:Task"], ["LIP:Dataset", "Benchmark-For", "semantic segmentation:Task"], ["PASCAL Context:Dataset", "Benchmark-For", "semantic segmentation:Task"], ["AFLW:Dataset", "Benchmark-For", "facial landmark detection:Task"], ["COFW:Dataset", "Benchmark-For", "facial landmark detection:Task"], ["$ 3 0 0 $W:Dataset", "Benchmark-For", "facial landmark detection:Task"], ["WFLW:Dataset", "Benchmark-For", "facial landmark detection:Task"]]}
{"doc_id": "104291983", "sentence": "In addition , we build a multi - level representation from the high - resolution representation and apply it to the Faster R - CNN object detection framework and the extended frameworks .", "ner": [["Faster R - CNN", "Method"], ["object detection", "Task"]], "rel": [["Faster R - CNN", "Used-For", "object detection"]], "rel_plus": [["Faster R - CNN:Method", "Used-For", "object detection:Task"]]}
{"doc_id": "104291983", "sentence": "The proposed approach achieves superior results to existing single - model networks on COCO object detection .", "ner": [["COCO", "Dataset"], ["object detection", "Task"]], "rel": [["COCO", "Benchmark-For", "object detection"]], "rel_plus": [["COCO:Dataset", "Benchmark-For", "object detection:Task"]]}
{"doc_id": "104291983", "sentence": "There are two main kinds of representations : low - resolution representations that are mainly for image classification , and high - resolution representations that are essential for many other vision problems , e.g. , semantic segmentation , object detection , human pose estimation , etc .", "ner": [["image classification", "Task"], ["semantic segmentation", "Task"], ["object detection", "Task"], ["human pose estimation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "104291983", "sentence": "One is to recover high - resolution representations from low - resolution representations outputted by a network ( e.g. , ResNet ) and optionally intermediate medium - resolution representations , e.g. , Hourglass [ 7 2 ] , SegNet [ 2 ] , DeconvNet [ 7 4 ] , U - Net [ 8 3 ] , and encoderdecoder [ 7 7 ] .", "ner": [["ResNet", "Method"], ["Hourglass", "Method"], ["SegNet", "Method"], ["DeconvNet", "Method"], ["U - Net", "Method"], ["encoderdecoder", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "104291983", "sentence": "In addition , dilated convolutions are used to replace some strided convolutions and associated regular convolutions in classification networks to compute medium - resolution representations [ 1 3 , 1 2 6 ] .", "ner": [["dilated convolutions", "Method"], ["strided convolutions", "Method"], ["convolutions", "Method"], ["classification", "Task"]], "rel": [["dilated convolutions", "Used-For", "classification"]], "rel_plus": [["dilated convolutions:Method", "Used-For", "classification:Task"]]}
{"doc_id": "104291983", "sentence": "We go along the research line of maintaining highresolution representations and further study the highresolution network ( HRNet ) , which is initially developed for human pose estimation [ 9 1 ] , for a broad range of vision tasks .", "ner": [["highresolution network", "Method"], ["HRNet", "Method"], ["human pose estimation", "Task"]], "rel": [["HRNet", "Synonym-Of", "highresolution network"], ["highresolution network", "Used-For", "human pose estimation"]], "rel_plus": [["HRNet:Method", "Synonym-Of", "highresolution network:Method"], ["highresolution network:Method", "Used-For", "human pose estimation:Task"]]}
{"doc_id": "104291983", "sentence": "An HRNet maintains high - resolution representations by connecting high - to - low resolution convolutions in parallel and repeatedly conducting multi - scale fusions across parallel convolutions .", "ner": [["HRNet", "Method"], ["high - to - low resolution convolutions", "Method"], ["parallel convolutions", "Method"]], "rel": [["high - to - low resolution convolutions", "Part-Of", "HRNet"], ["parallel convolutions", "Part-Of", "HRNet"]], "rel_plus": [["high - to - low resolution convolutions:Method", "Part-Of", "HRNet:Method"], ["parallel convolutions:Method", "Part-Of", "HRNet:Method"]]}
{"doc_id": "104291983", "sentence": "We apply our proposed network to semantic segmentation/facial landmark detection through estimating segmentation maps/facial landmark heatmaps from the output high - The 2nd ( 3rd , 4th ) stage repeats two - resolution ( three - resolution , four - resolution ) blocks .", "ner": [["semantic segmentation/facial landmark detection", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "104291983", "sentence": "In semantic segmentation , the proposed approach achieves state - of - the - art results on PAS - CAL Context , Cityscapes , and LIP with similar model sizes and lower computation complexity .", "ner": [["semantic segmentation", "Task"], ["PAS - CAL Context", "Dataset"], ["Cityscapes", "Dataset"], ["LIP", "Dataset"]], "rel": [["PAS - CAL Context", "Benchmark-For", "semantic segmentation"], ["Cityscapes", "Benchmark-For", "semantic segmentation"], ["LIP", "Benchmark-For", "semantic segmentation"]], "rel_plus": [["PAS - CAL Context:Dataset", "Benchmark-For", "semantic segmentation:Task"], ["Cityscapes:Dataset", "Benchmark-For", "semantic segmentation:Task"], ["LIP:Dataset", "Benchmark-For", "semantic segmentation:Task"]]}
{"doc_id": "104291983", "sentence": "In facial landmark detection , our approach achieves overall best results on four standard datasets : AFLW , COFW , 3 0 0 W , and WFLW .", "ner": [["facial landmark detection", "Task"], ["AFLW", "Dataset"], ["COFW", "Dataset"], ["3 0 0 W", "Dataset"], ["WFLW", "Dataset"]], "rel": [["AFLW", "Benchmark-For", "facial landmark detection"], ["COFW", "Benchmark-For", "facial landmark detection"], ["3 0 0 W", "Benchmark-For", "facial landmark detection"], ["WFLW", "Benchmark-For", "facial landmark detection"]], "rel_plus": [["AFLW:Dataset", "Benchmark-For", "facial landmark detection:Task"], ["COFW:Dataset", "Benchmark-For", "facial landmark detection:Task"], ["3 0 0 W:Dataset", "Benchmark-For", "facial landmark detection:Task"], ["WFLW:Dataset", "Benchmark-For", "facial landmark detection:Task"]]}
{"doc_id": "104291983", "sentence": "In addition , we construct a multi - level representation from the high - resolution representation , and apply it to the Faster R - CNN object detection framework and its extended frameworks , Mask R - CNN [ 3 8 ] and Cascade R - CNN [ 9 ] .", "ner": [["Faster R - CNN", "Method"], ["object detection", "Task"], ["Mask R - CNN", "Method"], ["Cascade R - CNN", "Method"]], "rel": [["Mask R - CNN", "SubClass-Of", "Faster R - CNN"], ["Cascade R - CNN", "SubClass-Of", "Faster R - CNN"], ["Faster R - CNN", "Used-For", "object detection"]], "rel_plus": [["Mask R - CNN:Method", "SubClass-Of", "Faster R - CNN:Method"], ["Cascade R - CNN:Method", "SubClass-Of", "Faster R - CNN:Method"], ["Faster R - CNN:Method", "Used-For", "object detection:Task"]]}
{"doc_id": "104291983", "sentence": "With single - scale training and testing , the proposed approach achieves better COCO object detection results than existing single - model methods .", "ner": [["COCO", "Dataset"], ["object detection", "Task"]], "rel": [["COCO", "Benchmark-For", "object detection"]], "rel_plus": [["COCO:Dataset", "Benchmark-For", "object detection:Task"]]}
{"doc_id": "104291983", "sentence": "Strong high - resolution representations play an essential role in pixel and region labeling problems , e.g. , semantic segmentation , human pose estimation , facial landmark detection , and object detection .", "ner": [["semantic segmentation", "Task"], ["human pose estimation", "Task"], ["facial landmark detection", "Task"], ["object detection", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "104291983", "sentence": "We review representation learning techniques developed mainly in the semantic segmentation , facial landmark detection [ 9 2 , 5 0 , 6 9 , 1 0 4 , 1 2 3 , 9 4 , 1 1 9 ] and object detection areas 1 , from low - resolution representation learning , high - resolution representation recovering , to high - resolution representation maintaining .", "ner": [["representation learning", "Task"], ["semantic segmentation", "Task"], ["facial landmark detection", "Task"], ["object detection", "Task"], ["low - resolution representation learning", "Task"], ["high - resolution representation recovering", "Task"], ["high - resolution representation maintaining", "Task"]], "rel": [["representation learning", "Used-For", "semantic segmentation"], ["representation learning", "Used-For", "facial landmark detection"], ["representation learning", "Used-For", "object detection"], ["representation learning", "Used-For", "low - resolution representation learning"], ["representation learning", "Used-For", "high - resolution representation recovering"], ["representation learning", "Used-For", "high - resolution representation maintaining"]], "rel_plus": [["representation learning:Task", "Used-For", "semantic segmentation:Task"], ["representation learning:Task", "Used-For", "facial landmark detection:Task"], ["representation learning:Task", "Used-For", "object detection:Task"], ["representation learning:Task", "Used-For", "low - resolution representation learning:Task"], ["representation learning:Task", "Used-For", "high - resolution representation recovering:Task"], ["representation learning:Task", "Used-For", "high - resolution representation maintaining:Task"]]}
{"doc_id": "104291983", "sentence": "The fullyconvolutional network ( FCN ) approaches [ 6 7 , 8 7 ] compute low - resolution representations by removing the fullyconnected layers in a classification network , and estimate from their coarse segmentation confidence maps .", "ner": [["fullyconvolutional network", "Method"], ["FCN", "Method"], ["fullyconnected layers", "Method"]], "rel": [["FCN", "Synonym-Of", "fullyconvolutional network"]], "rel_plus": [["FCN:Method", "Synonym-Of", "fullyconvolutional network:Method"]]}
{"doc_id": "104291983", "sentence": "The estimated segmentation maps are improved by combining the fine segmentation score maps estimated from intermediate low - level medium - resolution representations [ 6 7 ] , or iterating the processes [ 5 0 ] .", "ner": [], "rel": [], "rel_plus": []}
{"doc_id": "104291983", "sentence": "Similar techniques have also been applied to edge detection , e.g. , holistic edge detection [ 1 0 6 ] .", "ner": [["edge detection", "Task"], ["holistic edge detection", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "104291983", "sentence": "The fully convolutional network is extended , by replacing a few ( typically two ) strided convolutions and the associated convolutions with dilated convolutions , to the dilation version , leading to medium - resolution representations [ 1 2 6 , 1 3 , 1 1 5 , 1 2 , 5 7 ] .", "ner": [["fully convolutional network", "Method"], ["strided convolutions", "Method"], ["convolutions", "Method"], ["dilated convolutions", "Method"]], "rel": [["dilated convolutions", "Part-Of", "fully convolutional network"], ["convolutions", "Part-Of", "fully convolutional network"]], "rel_plus": [["dilated convolutions:Method", "Part-Of", "fully convolutional network:Method"], ["convolutions:Method", "Part-Of", "fully convolutional network:Method"]]}
{"doc_id": "104291983", "sentence": "The upsample subnetwork could be a symmetric version of the downsample subnetwork , with skipping connection over some mirrored layers to transform the pooling indices , e.g. , SegNet [ 2 ] and DeconvNet [ 7 4 ] , or copying the feature maps , e.g. , U - Net [ 8 3 ] and Hourglass [ 7 2 , 1 1 1 , 7 , 2 2 , 6 ] , encoder - decoder [ 7 7 ] , FPN [ 6 2 ] , and so on .", "ner": [["SegNet", "Method"], ["DeconvNet", "Method"], ["U - Net", "Method"], ["Hourglass", "Method"], ["encoder - decoder", "Method"], ["FPN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "104291983", "sentence": "The full - resolution residual network [ 7 8 ] introduces an extra full - resolution stream that carries information at the full image resolution , to replace the skip connections , and each unit in the downsample and upsample subnetworks receives information from and sends information to the full - resolution stream .", "ner": [["full - resolution residual network", "Method"], ["full - resolution stream", "Method"], ["skip connections", "Method"], ["full - resolution stream", "Method"]], "rel": [["full - resolution stream", "Part-Of", "full - resolution residual network"]], "rel_plus": [["full - resolution stream:Method", "Part-Of", "full - resolution residual network:Method"]]}
{"doc_id": "104291983", "sentence": "Other works include : light upsample process [ 5 ] ; light downsample and heavy upsample processes [ 9 7 ] , recombinator networks [ 4 0 ] ; improving skip connections with more or complicated convolutional units [ 7 6 , 1 2 5 , 4 2 ] , as well as sending information from low - resolution skip connections to high - resolution skip connections [ 1 3 3 ] or exchanging information between them [ 3 6 ] ; studying the details the upsample process [ 1 0 0 ] ; combining multi - scale pyramid representations [ 1 6 , 1 0 5 ] ; stacking multiple DeconvNets/UNets/Hourglass [ 3 1 , 1 0 1 ] with dense connections [ 9 3 ] .", "ner": [["light upsample process", "Method"], ["light downsample", "Method"], ["heavy upsample processes", "Method"], ["recombinator networks", "Method"], ["skip connections", "Method"], ["convolutional units", "Method"], ["low - resolution skip connections", "Method"], ["high - resolution skip connections", "Method"], ["upsample process", "Method"], ["multi - scale pyramid representations", "Method"], ["DeconvNets/UNets/Hourglass", "Method"], ["dense connections", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "104291983", "sentence": "Representative works include GridNet [ 3 0 ] , convolutional neural fabrics [ 8 6 ] , interlinked CNNs [ 1 3 2 ] , and the recently - developed high - resolution networks ( HRNet ) [ 9 1 ] that is our interest .", "ner": [["GridNet", "Method"], ["convolutional neural fabrics", "Method"], ["interlinked CNNs", "Method"], ["high - resolution networks", "Method"], ["HRNet", "Method"]], "rel": [["HRNet", "Synonym-Of", "high - resolution networks"]], "rel_plus": [["HRNet:Method", "Synonym-Of", "high - resolution networks:Method"]]}
{"doc_id": "104291983", "sentence": "The two early works , convolutional neural fabrics [ 8 6 ] and interlinked CNNs [ 1 3 2 ] , lack careful design on when to start low - resolution parallel streams and how and when to exchange information across parallel streams , and do not use batch normalization and residual connections , thus not showing satisfactory performance .", "ner": [["convolutional neural fabrics", "Method"], ["interlinked CNNs", "Method"], ["batch normalization", "Method"], ["residual connections", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "104291983", "sentence": "GridNet [ 3 0 ] is like a combination of multiple U - Nets and includes two symmetric information exchange stages : the first stage only passes information from high - resolution to low - resolution , and the second stage only passes information from low - resolution to high - resolution .", "ner": [["GridNet", "Method"], ["U - Nets", "Method"]], "rel": [["U - Nets", "Part-Of", "GridNet"]], "rel_plus": [["U - Nets:Method", "Part-Of", "GridNet:Method"]]}
{"doc_id": "104291983", "sentence": "The high - resolution network [ 9 1 ] , which we named HRNetV 1 for convenience , maintains high - resolution representations by connecting high - to - low resolution convolutions in parallel , where there are repeated multi - scale fusions across parallel convolutions .", "ner": [["high - resolution network", "Method"], ["HRNetV 1", "Method"], ["high - to - low resolution convolutions", "Method"]], "rel": [["HRNetV 1", "Synonym-Of", "high - resolution network"], ["high - to - low resolution convolutions", "Part-Of", "high - resolution network"]], "rel_plus": [["HRNetV 1:Method", "Synonym-Of", "high - resolution network:Method"], ["high - to - low resolution convolutions:Method", "Part-Of", "high - resolution network:Method"]]}
{"doc_id": "104291983", "sentence": "The multi - resolution group convolution is a simple extension of the group convolution , which divides the input channels into several subsets of channels and performs a regular convolution over each subset over different spatial resolutions separately .", "ner": [["multi - resolution group convolution", "Method"], ["group convolution", "Method"], ["convolution", "Method"]], "rel": [["multi - resolution group convolution", "SubClass-Of", "group convolution"]], "rel_plus": [["multi - resolution group convolution:Method", "SubClass-Of", "group convolution:Method"]]}
{"doc_id": "104291983", "sentence": "A regular convolution can be divided as multiple small convolutions as explained in [ 1 2 2 ] .", "ner": [["convolution", "Method"], ["small convolutions", "Method"]], "rel": [["small convolutions", "SubClass-Of", "convolution"]], "rel_plus": [["small convolutions:Method", "SubClass-Of", "convolution:Method"]]}
{"doc_id": "104291983", "sentence": "The differences lie in two - fold . ( i ) In a multi - resolution convolution each subset of channels is over a different resolution . ( ii ) The connection between input channels and output channels needs to handle The resolution decrease is implemented in [ 9 1 ] by using several 2 - strided 3 \u00d7 3 convolutions .", "ner": [["multi - resolution convolution", "Method"], ["2 - strided 3 \u00d7 3 convolutions", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "104291983", "sentence": "In the original approach HRNetV 1 , only the representation ( feature maps ) from the high - resolution convolutions in [ 9 1 ] are outputted , which is illustrated in Figure 3 ( a ) .", "ner": [["HRNetV 1", "Method"], ["high - resolution convolutions", "Method"]], "rel": [["high - resolution convolutions", "Part-Of", "HRNetV 1"]], "rel_plus": [["high - resolution convolutions:Method", "Part-Of", "HRNetV 1:Method"]]}
{"doc_id": "104291983", "sentence": "In application to object detection , we construct a multi - level representation by downsampling the highresolution representation with average pooling to multiple levels , which is depicted in Figure 3 ( c ) .", "ner": [["object detection", "Task"], ["average pooling", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "104291983", "sentence": "We name the two modifications as HRNetV 2 and HRNetV 2 p , respectively , and empirically compare them in Section 4. 4 .", "ner": [["HRNetV 2", "Method"], ["HRNetV 2 p", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "104291983", "sentence": "In applications to semantic segmentation and facial landmark detection , we mix the output representations ( Figure 3 ( b ) ) , from all the four resolutions through a 1 \u00d7 1 convolution , and produce a 1 5 C - dimensional representation .", "ner": [["semantic segmentation", "Task"], ["facial landmark detection", "Task"], ["1 \u00d7 1 convolution", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "104291983", "sentence": "Then , we pass the mixed representation at each position to a linear classifier/regressor with the softmax/MSE loss to predict the segmentation maps/facial landmark heatmaps .", "ner": [["linear classifier/regressor", "Method"], ["softmax/MSE loss", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "104291983", "sentence": "For semantic segmentation , the segmentation maps are upsampled ( 4 times ) to the input size by bilinear upsampling for both training and testing .", "ner": [["semantic segmentation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "104291983", "sentence": "In application to object detection , we reduce the dimension of the high - resolution representation to 2 5 6 , similar to FPN [ 6 2 ] , through a 1 \u00d7 1 convolution before forming the feature pyramid in Figure 3 ( c ) .", "ner": [["object detection", "Task"], ["FPN", "Method"], ["1 \u00d7 1 convolution", "Method"], ["feature pyramid", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "104291983", "sentence": "We report the results over two scene parsing datasets , PASCAL Context [ 7 1 ] and Cityscapes [ 1 9 ] , and a human parsing dataset , LIP [ 3 4 ] .", "ner": [["scene parsing", "Task"], ["PASCAL Context", "Dataset"], ["Cityscapes", "Dataset"], ["human parsing", "Task"], ["LIP", "Dataset"]], "rel": [["PASCAL Context", "Benchmark-For", "scene parsing"], ["Cityscapes", "Benchmark-For", "scene parsing"], ["LIP", "Benchmark-For", "human parsing"]], "rel_plus": [["PASCAL Context:Dataset", "Benchmark-For", "scene parsing:Task"], ["Cityscapes:Dataset", "Benchmark-For", "scene parsing:Task"], ["LIP:Dataset", "Benchmark-For", "human parsing:Task"]]}
{"doc_id": "104291983", "sentence": "The data are augmented by random cropping ( from 1 0 2 4 \u00d7 2 0 4 8 to 5 1 2 \u00d7 1 0 2 4 ) , random scaling in the range of [ 0 . 5 , 2 ] , and random horizontal flipping .", "ner": [["random cropping", "Method"], ["random scaling", "Method"], ["random horizontal flipping", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "104291983", "sentence": "We use the SGD optimizer with the base learning rate of 0.0 1 , the momentum of 0. 9 and the weight decay of 0.0 0 0 5 .", "ner": [["SGD", "Method"], ["momentum", "Method"], ["weight decay", "Method"]], "rel": [["momentum", "Part-Of", "SGD"], ["weight decay", "Part-Of", "SGD"]], "rel_plus": [["momentum:Method", "Part-Of", "SGD:Method"], ["weight decay:Method", "Part-Of", "SGD:Method"]]}
{"doc_id": "104291983", "sentence": "Table 1 provides the comparison with several representative methods on the Cityscapes validation set in on ImageNet 3 and has similar model size as most DilatedResNet - 1 0 1 based methods .", "ner": [["Cityscapes validation set", "Dataset"], ["ImageNet", "Dataset"], ["DilatedResNet - 1 0 1", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "104291983", "sentence": "The data augmentation and learning rate policy are the same as Cityscapes .", "ner": [["data augmentation", "Method"], ["Cityscapes", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "104291983", "sentence": "In both cases , HRNetV 2 - W 4 8 performs su - 3 The description about ImageNet pretraining is given in the Appendix . [ 6 6 ] , the images are resized to 4 7 3 \u00d7 4 7 3 and the performance is evaluated on the average of the segmentation maps of the original and flipped images .", "ner": [["HRNetV 2 - W 4 8", "Method"], ["ImageNet", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "104291983", "sentence": "The data augmentation and learning rate policy are the same as Cityscapes .", "ner": [["data augmentation", "Method"], ["Cityscapes", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "104291983", "sentence": "We apply our multi - level representations ( HRNetV 2 p ) 4 , shown in Figure 3 ( c ) , in the Faster R - CNN [ 8 2 ] and Mask R - CNN [ 3 8 ] frameworks .", "ner": [["HRNetV 2 p", "Method"], ["Faster R - CNN", "Method"], ["Mask R - CNN", "Method"]], "rel": [["HRNetV 2 p", "Used-For", "Faster R - CNN"], ["HRNetV 2 p", "Used-For", "Mask R - CNN"]], "rel_plus": [["HRNetV 2 p:Method", "Used-For", "Faster R - CNN:Method"], ["HRNetV 2 p:Method", "Used-For", "Mask R - CNN:Method"]]}
{"doc_id": "104291983", "sentence": "We perform the evaluation on the MS - COCO 2 0 1 7 detection dataset , which contains \u223c 1 1 8 k images for training , 5k for validation ( val ) and \u223c 2 0 k testing without provided annotations ( test - dev ) .", "ner": [["MS - COCO 2 0 1 7 detection dataset", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "104291983", "sentence": "We train the models for both our HRNetV 2 p and the ResNet on the public mmdetection platform [ 1 1 ] with the provided training setup , except that we use the learning rate schedule suggested in [ 3 7 ] for 2 \u00d7 .", "ner": [["HRNetV 2 p", "Method"], ["ResNet", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "104291983", "sentence": "In the Faster R - CNN framework , our networks perform better than ResNets with similar parameter and computation complexity : HRNetV 2 p - W 3 2 vs. ResNet - 1 0 1 - FPN , HRNetV 2 p - W 4 0 vs. ResNet - 1 5 2 - FPN , HRNetV 2 p - W 4 8 vs. X - 1 0 1 - 6 4 \u00d7 4 d - FPN .", "ner": [["Faster R - CNN", "Method"], ["ResNets", "Method"], ["HRNetV 2 p - W 3 2", "Method"], ["ResNet - 1 0 1 - FPN", "Method"], ["HRNetV 2 p - W 4 0", "Method"], ["ResNet - 1 5 2 - FPN", "Method"], ["HRNetV 2 p - W 4 8", "Method"], ["X - 1 0 1 - 6 4 \u00d7 4 d - FPN", "Method"]], "rel": [["Faster R - CNN", "Compare-With", "ResNets"], ["HRNetV 2 p - W 3 2", "Compare-With", "ResNet - 1 0 1 - FPN"], ["HRNetV 2 p - W 4 0", "Compare-With", "ResNet - 1 5 2 - FPN"], ["HRNetV 2 p - W 4 8", "Compare-With", "X - 1 0 1 - 6 4 \u00d7 4 d - FPN"]], "rel_plus": [["Faster R - CNN:Method", "Compare-With", "ResNets:Method"], ["HRNetV 2 p - W 3 2:Method", "Compare-With", "ResNet - 1 0 1 - FPN:Method"], ["HRNetV 2 p - W 4 0:Method", "Compare-With", "ResNet - 1 5 2 - FPN:Method"], ["HRNetV 2 p - W 4 8:Method", "Compare-With", "X - 1 0 1 - 6 4 \u00d7 4 d - FPN:Method"]]}
{"doc_id": "104291983", "sentence": "In the Cascade R - CNN framework , our HRNetV 2 p - W 3 2 performs better .", "ner": [["Cascade R - CNN", "Method"], ["HRNetV 2 p - W 3 2", "Method"]], "rel": [["HRNetV 2 p - W 3 2", "Part-Of", "Cascade R - CNN"]], "rel_plus": [["HRNetV 2 p - W 3 2:Method", "Part-Of", "Cascade R - CNN:Method"]]}
{"doc_id": "104291983", "sentence": "Facial landmark detection a.k.a . face alignment is a problem of detecting the keypoints from a face image .", "ner": [["Facial landmark detection", "Task"], ["face alignment", "Task"]], "rel": [["face alignment", "Synonym-Of", "Facial landmark detection"]], "rel_plus": [["face alignment:Task", "Synonym-Of", "Facial landmark detection:Task"]]}
{"doc_id": "104291983", "sentence": "We perform the evaluation over four standard datasets : WFLW [ 1 0 1 ] , AFLW [ 4 9 ] , COFW [ 8 ] , and 3 0 0 W [ 8 5 ] .", "ner": [["WFLW", "Dataset"], ["AFLW", "Dataset"], ["COFW", "Dataset"], ["3 0 0 W", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "104291983", "sentence": "We use the inter - ocular distance as normalization for WFLW , COFW , and 3 0 0 W , and the face bounding box as normalization for AFLW .", "ner": [["WFLW", "Dataset"], ["COFW", "Dataset"], ["3 0 0 W", "Dataset"], ["AFLW", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "104291983", "sentence": "We adopt HRNetV 2 - W 1 8 for face landmark detection whose parameter and computation cost are similar to or smaller than models with widely - used backbones : ResNet - 5 0 and Hourglass [ 7 2 ] .", "ner": [["HRNetV 2 - W 1 8", "Method"], ["face landmark detection", "Task"], ["ResNet - 5 0", "Method"], ["Hourglass", "Method"]], "rel": [["HRNetV 2 - W 1 8", "Used-For", "face landmark detection"], ["HRNetV 2 - W 1 8", "Compare-With", "ResNet - 5 0"], ["HRNetV 2 - W 1 8", "Compare-With", "Hourglass"]], "rel_plus": [["HRNetV 2 - W 1 8:Method", "Used-For", "face landmark detection:Task"], ["HRNetV 2 - W 1 8:Method", "Compare-With", "ResNet - 5 0:Method"], ["HRNetV 2 - W 1 8:Method", "Compare-With", "Hourglass:Method"]]}
{"doc_id": "104291983", "sentence": "HRNetV 2 - W 1 8 : # param - eters = 9. 3 M , GFLOPs WFLW .", "ner": [["HRNetV 2 - W 1 8", "Method"], ["WFLW", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "104291983", "sentence": "The WFLW dataset [ 1 0 1 ] is a recently - built dataset based on the WIDER Face [ 1 1 2 ] .", "ner": [["WFLW", "Dataset"], ["WIDER Face", "Dataset"]], "rel": [["WFLW", "SubClass-Of", "WIDER Face"]], "rel_plus": [["WFLW:Dataset", "SubClass-Of", "WIDER Face:Dataset"]]}
{"doc_id": "104291983", "sentence": "Following [ 1 3 4 , 1 0 1 ] , we train our models on 2 0 , 0 0 0 training images , and report the results on the AFLW - Full set ( 4 , 3 8 6 testing images ) and the AFLW - Frontal set ( 1 3 1 4 testing images selected from 4 3 8 6 testing images ) .", "ner": [["AFLW - Full set", "Dataset"], ["AFLW - Frontal set", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "104291983", "sentence": "We obtain the results of Faster R - CNN and Cascade R - CNN by using our implementations publicly available from the mmdetection platform [ 1 1 ] except that * is from the original paper [ 9 ] .", "ner": [["Faster R - CNN", "Method"], ["Cascade R - CNN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "104291983", "sentence": "Facial landmark detection results ( NME ) on WFLW test and 6 subsets : pose , expression ( expr . ) , illumination ( illu . ) , make - up ( mu . ) , occlusion ( occu . ) and blur .", "ner": [["Facial landmark detection", "Task"], ["WFLW", "Dataset"], ["pose", "Dataset"], ["expression", "Dataset"], ["expr .", "Dataset"], ["illumination", "Dataset"], ["illu .", "Dataset"], ["make - up", "Dataset"], ["mu .", "Dataset"], ["occlusion", "Dataset"], ["occu .", "Dataset"], ["blur", "Dataset"]], "rel": [["WFLW", "Benchmark-For", "Facial landmark detection"], ["pose", "SubClass-Of", "WFLW"], ["expression", "SubClass-Of", "WFLW"], ["illumination", "SubClass-Of", "WFLW"], ["make - up", "SubClass-Of", "WFLW"], ["occlusion", "SubClass-Of", "WFLW"], ["blur", "SubClass-Of", "WFLW"], ["expr .", "Synonym-Of", "expression"], ["illu .", "Synonym-Of", "illumination"], ["mu .", "Synonym-Of", "make - up"], ["occu .", "Synonym-Of", "occlusion"]], "rel_plus": [["WFLW:Dataset", "Benchmark-For", "Facial landmark detection:Task"], ["pose:Dataset", "SubClass-Of", "WFLW:Dataset"], ["expression:Dataset", "SubClass-Of", "WFLW:Dataset"], ["illumination:Dataset", "SubClass-Of", "WFLW:Dataset"], ["make - up:Dataset", "SubClass-Of", "WFLW:Dataset"], ["occlusion:Dataset", "SubClass-Of", "WFLW:Dataset"], ["blur:Dataset", "SubClass-Of", "WFLW:Dataset"], ["expr .:Dataset", "Synonym-Of", "expression:Dataset"], ["illu .:Dataset", "Synonym-Of", "illumination:Dataset"], ["mu .:Dataset", "Synonym-Of", "make - up:Dataset"], ["occu .:Dataset", "Synonym-Of", "occlusion:Dataset"]]}
{"doc_id": "104291983", "sentence": "In particular , it achieves the better performance than LAB with extra boundary information and PDB with stronger data augmentation . 3 0 0 W .", "ner": [["LAB", "Method"], ["PDB", "Method"], ["data augmentation", "Method"], ["3 0 0 W", "Dataset"]], "rel": [["data augmentation", "Used-For", "PDB"]], "rel_plus": [["data augmentation:Method", "Used-For", "PDB:Method"]]}
{"doc_id": "104291983", "sentence": "The dataset [ 8 5 ] is a combination of HELEN [ 5 3 ] , the 3 , 1 4 8 training images , which contains the training subsets of HELEN and LFPW and the full set of AFW .", "ner": [["HELEN", "Dataset"], ["HELEN", "Dataset"], ["LFPW", "Dataset"], ["AFW", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "104291983", "sentence": "The full set contains 6 8 9 images and is further divided into a common subset ( 5 5 4 images ) from HELEN and LFPW , and a challenging subset ( 1 3 5 images ) from IBUG .", "ner": [["HELEN", "Dataset"], ["LFPW", "Dataset"], ["IBUG", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "104291983", "sentence": "Our HRNetV 2 gets the overall best performance among methods without extra information and stronger data augmentation , and is even better than LAB with extra boundary information and DCFE [ 9 7 ] that explores extra 3D information .", "ner": [["HRNetV 2", "Method"], ["data augmentation", "Method"], ["LAB", "Method"], ["DCFE", "Method"]], "rel": [["HRNetV 2", "Compare-With", "LAB"], ["HRNetV 2", "Compare-With", "DCFE"]], "rel_plus": [["HRNetV 2:Method", "Compare-With", "LAB:Method"], ["HRNetV 2:Method", "Compare-With", "DCFE:Method"]]}
{"doc_id": "104291983", "sentence": "We compare the modified networks , HRNetV 2 and HRNetV 2 p , to the original network [ 9 1 ] ( shortened as HRNetV 1 ) on semantic segmentation and COCO object detection .", "ner": [["HRNetV 2", "Method"], ["HRNetV 2 p", "Method"], ["HRNetV 1", "Method"], ["semantic segmentation", "Task"], ["COCO", "Dataset"], ["object detection", "Task"]], "rel": [["HRNetV 2", "Compare-With", "HRNetV 1"], ["HRNetV 2 p", "Compare-With", "HRNetV 1"], ["HRNetV 2", "Used-For", "semantic segmentation"], ["HRNetV 2", "Used-For", "object detection"]], "rel_plus": [["HRNetV 2:Method", "Compare-With", "HRNetV 1:Method"], ["HRNetV 2 p:Method", "Compare-With", "HRNetV 1:Method"], ["HRNetV 2:Method", "Used-For", "semantic segmentation:Task"], ["HRNetV 2:Method", "Used-For", "object detection:Task"]]}
{"doc_id": "104291983", "sentence": "The segmentation and object detection results , given in Figure 4 ( a ) and Figure 4 ( b ) , imply that HRNetV 2 outperforms HRNetV 1 significantly , except that the gain is minor in the large model case in segmentation for Cityscapes .", "ner": [["segmentation", "Task"], ["object detection", "Task"], ["HRNetV 2", "Method"], ["HRNetV 1", "Method"], ["segmentation", "Task"], ["Cityscapes", "Dataset"]], "rel": [["HRNetV 1", "Used-For", "segmentation"], ["HRNetV 2", "Used-For", "object detection"], ["HRNetV 2", "Compare-With", "HRNetV 1"], ["Cityscapes", "Benchmark-For", "segmentation"]], "rel_plus": [["HRNetV 1:Method", "Used-For", "segmentation:Task"], ["HRNetV 2:Method", "Used-For", "object detection:Task"], ["HRNetV 2:Method", "Compare-With", "HRNetV 1:Method"], ["Cityscapes:Dataset", "Benchmark-For", "segmentation:Task"]]}
{"doc_id": "104291983", "sentence": "We also test a variant ( denoted by HRNetV 1 h ) , which is built by appending a 1 \u00d7 1 convolution to increase the dimension of the output high - resolution representation .", "ner": [["HRNetV 1 h", "Method"], ["1 \u00d7 1 convolution", "Method"]], "rel": [["1 \u00d7 1 convolution", "Part-Of", "HRNetV 1 h"]], "rel_plus": [["1 \u00d7 1 convolution:Method", "Part-Of", "HRNetV 1 h:Method"]]}
{"doc_id": "104291983", "sentence": "The results in Figure 4 ( a ) and Figure 4 ( b ) show that the variant achieves slight improvement to HRNetV 1 , implying that aggregating the representations from low - resolution parallel convolutions in our HRNetV 2 is essential for increasing the capability .", "ner": [["HRNetV 1", "Method"], ["low - resolution parallel convolutions", "Method"], ["HRNetV 2", "Method"]], "rel": [["low - resolution parallel convolutions", "Part-Of", "HRNetV 2"]], "rel_plus": [["low - resolution parallel convolutions:Method", "Part-Of", "HRNetV 2:Method"]]}
{"doc_id": "104291983", "sentence": "Experimental results demonstrate the effectiveness of strong highresolution representations and multi - level representations learned by the modified networks on semantic segmentation , facial landmark detection as well as object detection .", "ner": [["semantic segmentation", "Task"], ["facial landmark detection", "Task"], ["object detection", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "104291983", "sentence": "The project page is https://jingdongwang 2 0 1 7 . github.io/Projects/HRNet/. We pretrain our network , which is augmented by a classification head shown in Figure 5 , on ImageNet [ 8 4 ] .", "ner": [["ImageNet", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "104291983", "sentence": "Last , we transform the 1 0 2 4 channels to 2 0 4 8 channels through a 1 \u00d7 1 convolution , followed by a global average pooling operation .", "ner": [["1 \u00d7 1 convolution", "Method"], ["global average pooling operation", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "104291983", "sentence": "We use SGD with a weight decay of 0.0 0 0 1 and a Nesterov momentum of 0. 9 .", "ner": [["SGD", "Method"], ["weight decay", "Method"], ["Nesterov momentum", "Method"]], "rel": [["weight decay", "Part-Of", "SGD"], ["Nesterov momentum", "Part-Of", "SGD"]], "rel_plus": [["weight decay:Method", "Part-Of", "SGD:Method"], ["Nesterov momentum:Method", "Part-Of", "SGD:Method"]]}
{"doc_id": "104291983", "sentence": "Table 1 4 shows our ImageNet classification results .", "ner": [["ImageNet", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "104291983", "sentence": "We follow the PyTorch implementation of ResNets and replace the 7 \u00d7 7 convolution in the input stem with two 2 - strided 3 \u00d7 3 convolutions decreasing the resolution to 1/ 4 as in our networks .", "ner": [["ResNets", "Method"], ["7 \u00d7 7 convolution", "Method"], ["2 - strided 3 \u00d7 3 convolutions", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "104291983", "sentence": "In addition , we look at the results of two alternative schemes : ( i ) the feature maps on each resolution go through a global pooling separately and then are concatenated together to output a 1 5 C - dimensional representation vector , named HRNet - Wx - Ci ; ( ii ) the feature maps on each resolution are fed into several 2 - strided residual units ( bottleneck , each dimension is increased to the double ) to increase the dimension to 5 1 2 , and concatenate and average - pool them together to reach a 2 0 4 8 - dimensional representation vector , named HRNet - Wx - Cii , which is used in [ 9 1 ] .", "ner": [["global pooling", "Method"], ["HRNet - Wx - Ci", "Method"], ["2 - strided residual units", "Method"], ["HRNet - Wx - Cii", "Method"]], "rel": [["global pooling", "Part-Of", "HRNet - Wx - Ci"], ["2 - strided residual units", "Part-Of", "HRNet - Wx - Cii"]], "rel_plus": [["global pooling:Method", "Part-Of", "HRNet - Wx - Ci:Method"], ["2 - strided residual units:Method", "Part-Of", "HRNet - Wx - Cii:Method"]]}
{"doc_id": "104291983", "sentence": "Ablation study on ImageNet classification by comparing our approach ( abbreviated as HRNet - Wx - C ) with two alternatives : HRNet - Wx - Ci and HRNet - Wx - Cii ( residual branch formed by two 3 \u00d7 3 convolutions ) . # Params .", "ner": [["ImageNet", "Dataset"], ["HRNet - Wx - C", "Method"], ["HRNet - Wx - Ci", "Method"], ["HRNet - Wx - Cii", "Method"], ["3 \u00d7 3 convolutions", "Method"]], "rel": [["HRNet - Wx - C", "Evaluated-With", "ImageNet"], ["HRNet - Wx - Ci", "Evaluated-With", "ImageNet"], ["HRNet - Wx - Cii", "Evaluated-With", "ImageNet"], ["HRNet - Wx - Ci", "SubClass-Of", "HRNet - Wx - C"], ["HRNet - Wx - Cii", "SubClass-Of", "HRNet - Wx - C"], ["3 \u00d7 3 convolutions", "Part-Of", "HRNet - Wx - Cii"]], "rel_plus": [["HRNet - Wx - C:Method", "Evaluated-With", "ImageNet:Dataset"], ["HRNet - Wx - Ci:Method", "Evaluated-With", "ImageNet:Dataset"], ["HRNet - Wx - Cii:Method", "Evaluated-With", "ImageNet:Dataset"], ["HRNet - Wx - Ci:Method", "SubClass-Of", "HRNet - Wx - C:Method"], ["HRNet - Wx - Cii:Method", "SubClass-Of", "HRNet - Wx - C:Method"], ["3 \u00d7 3 convolutions:Method", "Part-Of", "HRNet - Wx - Cii:Method"]]}
{"doc_id": "52009210", "sentence": "To address this problem , we present SimpleDBpediaQA , a new benchmark dataset for simple question answering over knowledge graphs that was created by mapping SimpleQuestions entities and predicates from Freebase to DBpedia .", "ner": [["SimpleDBpediaQA", "Dataset"], ["question answering over knowledge graphs", "Task"], ["mapping SimpleQuestions entities", "Task"], ["predicates from Freebase to DBpedia", "Task"]], "rel": [["SimpleDBpediaQA", "Benchmark-For", "question answering over knowledge graphs"], ["mapping SimpleQuestions entities", "SubTask-Of", "question answering over knowledge graphs"], ["predicates from Freebase to DBpedia", "SubTask-Of", "question answering over knowledge graphs"]], "rel_plus": [["SimpleDBpediaQA:Dataset", "Benchmark-For", "question answering over knowledge graphs:Task"], ["mapping SimpleQuestions entities:Task", "SubTask-Of", "question answering over knowledge graphs:Task"], ["predicates from Freebase to DBpedia:Task", "SubTask-Of", "question answering over knowledge graphs:Task"]]}
{"doc_id": "52009210", "sentence": "This creates a number of insurmountable challenges : First , because the knowledge graph is stale , it is no longer possible to build a \" real - world \" operational QA system using models trained on SIMPLEQUESTIONS .", "ner": [["QA system", "Method"], ["SIMPLEQUESTIONS", "Dataset"]], "rel": [["QA system", "Trained-With", "SIMPLEQUESTIONS"]], "rel_plus": [["QA system:Method", "Trained-With", "SIMPLEQUESTIONS:Dataset"]]}
{"doc_id": "52009210", "sentence": "While it may be the case that one can apply transfer learning so that models trained on SIMPLEQUESTIONS can be re - targeted to another \" live \" knowledge graph , we are not aware of research along these lines .", "ner": [["transfer learning", "Method"], ["SIMPLEQUESTIONS", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "52009210", "sentence": "To address these issues , we present SIMPLEDBPEDIAQA , a new dataset that we have created by mapping entities and predicates that comprise the answers to SIMPLEQUESTIONS from Freebase to DBpedia .", "ner": [["SIMPLEDBPEDIAQA", "Dataset"], ["mapping entities", "Task"], ["predicates that comprise the answers", "Task"], ["SIMPLEQUESTIONS", "Dataset"], ["Freebase", "Dataset"], ["DBpedia", "Dataset"]], "rel": [["SIMPLEDBPEDIAQA", "Benchmark-For", "mapping entities"], ["SIMPLEDBPEDIAQA", "Benchmark-For", "predicates that comprise the answers"]], "rel_plus": [["SIMPLEDBPEDIAQA:Dataset", "Benchmark-For", "mapping entities:Task"], ["SIMPLEDBPEDIAQA:Dataset", "Benchmark-For", "predicates that comprise the answers:Task"]]}
{"doc_id": "52009210", "sentence": "Unlike Freebase , DBpedia is actively maintained by a dedicated community .", "ner": [["Freebase", "Dataset"], ["DBpedia", "Dataset"]], "rel": [["DBpedia", "Compare-With", "Freebase"]], "rel_plus": [["DBpedia:Dataset", "Compare-With", "Freebase:Dataset"]]}
{"doc_id": "52009210", "sentence": "Summary statistics of SIMPLEDBPEDIAQA and SIMPLEQUESTIONS are shown in Table 1 .", "ner": [["SIMPLEDBPEDIAQA", "Dataset"], ["SIMPLEQUESTIONS", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "52009210", "sentence": "An additional contribution of this paper is that having two parallel datasets allows us to examine the effects of different conceptual organizations and knowledge graph structures : For example , we notice that many single - fact triples in Freebase require two - hop traversals in the DBpedia knowledge graph , which makes them no longer \" simple \" questions .", "ner": [["Freebase", "Dataset"], ["DBpedia", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "52009210", "sentence": "For instance , FREE 9 1 7 ( Cai and Yates , 2 0 1 3 ) contains 9 1 7 questions involving 6 3 5 distinct Freebase predicates .", "ner": [["FREE 9 1 7", "Dataset"], ["Freebase", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "52009210", "sentence": "Like Freebase , DBpedia ( Bizer et al. , 2 0 0 9 ) has also been used as the target knowledge graph for multiple question answering datasets .", "ner": [["Freebase", "Dataset"], ["DBpedia", "Dataset"], ["question answering", "Task"]], "rel": [["DBpedia", "Compare-With", "Freebase"], ["DBpedia", "Benchmark-For", "question answering"]], "rel_plus": [["DBpedia:Dataset", "Compare-With", "Freebase:Dataset"], ["DBpedia:Dataset", "Benchmark-For", "question answering:Task"]]}
{"doc_id": "52009210", "sentence": "For example , QALD 1 ( Question Answering over Linked Data ) is a series of evaluation campaigns focused on question answering over linked data .", "ner": [["QALD", "Dataset"], ["Question Answering over Linked Data", "Dataset"], ["question answering", "Task"]], "rel": [["Question Answering over Linked Data", "Synonym-Of", "QALD"], ["QALD", "Benchmark-For", "question answering"]], "rel_plus": [["Question Answering over Linked Data:Dataset", "Synonym-Of", "QALD:Dataset"], ["QALD:Dataset", "Benchmark-For", "question answering:Task"]]}
{"doc_id": "52009210", "sentence": "LC - QUAD ( Trivedi et al. , 2 0 1 7 ) is another recent dataset that comprises 5, 0 0 0 questions with answers in the form of SPARQL queries over DBpedia .", "ner": [["LC - QUAD", "Dataset"], ["DBpedia", "Dataset"]], "rel": [["LC - QUAD", "SubClass-Of", "DBpedia"]], "rel_plus": [["LC - QUAD:Dataset", "SubClass-Of", "DBpedia:Dataset"]]}
{"doc_id": "52009210", "sentence": "Diefenbach et al. ( 2 0 1 7 ) mapped the dataset from Freebase to Wikidata . 2 However , our migrated SIMPLE - DBPEDIAQA dataset has roughly twice the number of mapped questions .", "ner": [["Freebase", "Dataset"], ["Wikidata", "Dataset"], ["SIMPLE - DBPEDIAQA", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "52009210", "sentence": "DBpedia is generally considered to be more mature than Wikidata due to its longer history , and thus we believe targeting DBpedia will ultimately yield higher - impact applications .", "ner": [["DBpedia", "Dataset"], ["Wikidata", "Dataset"], ["DBpedia", "Dataset"]], "rel": [["DBpedia", "Compare-With", "Wikidata"]], "rel_plus": [["DBpedia:Dataset", "Compare-With", "Wikidata:Dataset"]]}
{"doc_id": "52009210", "sentence": "In this context , t = ( s , p , o ) denotes a Resource Description Framework ( RDF ) triple , comprised of a subject s \u2208 S , a predicate p \u2208 P , and an object o \u2208 O. Given this formalism , Freebase ( Bollacker et al. , 2 0 0 8) represents a specific knowledge graph T b , where , a set of Freebase triples ) .", "ner": [["Freebase", "Dataset"], ["Freebase", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "52009210", "sentence": "Similarly , DBpedia ( Bizer et al. , 2 0 0 9 ) represents another knowledge graph T d , where The SIMPLEQUESTIONS dataset is a collection of natural language questions and answers based on Freebase .", "ner": [["DBpedia", "Dataset"], ["SIMPLEQUESTIONS", "Dataset"], ["Freebase", "Dataset"]], "rel": [["SIMPLEQUESTIONS", "SubClass-Of", "Freebase"]], "rel_plus": [["SIMPLEQUESTIONS:Dataset", "SubClass-Of", "Freebase:Dataset"]]}
{"doc_id": "52009210", "sentence": "Given Freebase T b , DBpedia T d , and SIMPLEQUESTIONS Q b , our problem can be formally defined as follows : for each Although this characterizes the basic structure of the problem , there are a number of nuances that deviate from this formalism , which we describe in the following sections .", "ner": [["Freebase", "Dataset"], ["DBpedia", "Dataset"], ["SIMPLEQUESTIONS", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "52009210", "sentence": "At a high level , we begin by first mapping the topic and answer entities from Freebase to DBpedia ; these then serve as anchors from which we can project the Freebase predicates to DBpedia .", "ner": [["Freebase", "Dataset"], ["DBpedia", "Dataset"], ["Freebase", "Dataset"], ["DBpedia", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "52009210", "sentence": "For this effort , we use the latest version of DBpedia released in 2 0 1 7 . 3 The first step is to map Freebase entities from SIMPLEQUESTIONS to entities in DBpedia .", "ner": [["DBpedia", "Dataset"], ["Freebase", "Dataset"], ["SIMPLEQUESTIONS", "Dataset"], ["DBpedia", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "52009210", "sentence": "Freebase MIDs and DBpedia URIs are linked through the predicate http://www.w 3 .org/ 2 0 0 2 / 0 7 /owl#sameAs ; these official mappings are released as part of DBpedia . 4 For each Freebase entity MID ( topic entity or answer entity ) , we issue a SPARQL query to retrieve the corresponding DBpedia URI .", "ner": [["Freebase MIDs", "Dataset"], ["DBpedia URIs", "Dataset"], ["DBpedia", "Dataset"], ["Freebase entity MID", "Dataset"], ["DBpedia URI", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "52009210", "sentence": "For example , Justin Trudeau , the current Prime Minister of Canada , is mapped via the triple : Here and throughout the paper we use dbr as the DBpedia prefix for http://dbpedia.org/resource/. For approximately 5 6 % of questions in SIMPLEQUESTIONS , we can map both the topic entity and the answer entity from Freebase to DBpedia .", "ner": [["DBpedia", "Dataset"], ["SIMPLEQUESTIONS", "Dataset"], ["Freebase", "Dataset"], ["DBpedia", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "52009210", "sentence": "Let us consider the case where we are able to map both the topic entity and the answer entity from Freebase to DBpedia .", "ner": [["Freebase", "Dataset"], ["DBpedia", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "52009210", "sentence": "In the simplest case , there is a single predicate connecting the topic entity to the answer entity , which yields a straightforward mapping of the triple from Freebase to DBpedia .", "ner": [["Freebase", "Dataset"], ["DBpedia", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "52009210", "sentence": "Consider the question \" Which city is McCormick Field in ? \" The Freebase topic entity fb : m/ 0 5 xgn is mapped to DBpedia as dbr : McCormick Field and the answer entity is mapped from fb : m/ 0 ydpd to dbr : Asheville , North Carolina .", "ner": [["Freebase", "Dataset"], ["DBpedia", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "52009210", "sentence": "The DBpedia predicate dbo : location connects those two entities , which provides a valid and correct mapping for the Freebase predicate fb : location/location/containedby .", "ner": [["DBpedia", "Dataset"], ["Freebase", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "52009210", "sentence": "Here and throughout the paper we use dbo as the DBpedia prefix for http://dbpedia.org/ontology/. Due to differences in the conceptual organization of the two knowledge graphs , the directionality of equivalent predicates in Freebase and DBpedia may differ .", "ner": [["DBpedia", "Dataset"], ["Freebase", "Dataset"], ["DBpedia", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "52009210", "sentence": "For example , the DBpedia predicate dbo : birthPlace takes a person as the subject and a location as the object , whereas the equivalent predicate in Freebase fb : location/location/people born here inverts the subject and object .", "ner": [["DBpedia", "Dataset"], ["Freebase", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "52009210", "sentence": "Therefore , for a question such as \" Who was born in Aguascalientes ? \" , the subject in the Freebase triple becomes the object in the DBpedia triple .", "ner": [["Freebase", "Dataset"], ["DBpedia", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "52009210", "sentence": "During the migration from Freebase to DBpedia , if the directionality of the mapped predicate is the same , we refer to the result as a forward predicate ; if the directionality is reversed , we refer to the result as a backward predicate .", "ner": [["Freebase", "Dataset"], ["DBpedia", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "52009210", "sentence": "The DBpedia sameAs links , however , might map a Freebase MID to an ambiguous URI , thus yielding a two - hop traversal from the topic entity to the answer entity .", "ner": [["DBpedia", "Dataset"], ["Freebase MID", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "52009210", "sentence": "Once again , this can occur with both forward and backward predicates . \u2022 Complex Predicates : Due to differences in the conceptual organization of Freebase and DBpedia , there is no direct equivalent in DBpedia for some Freebase predicates .", "ner": [["Freebase", "Dataset"], ["DBpedia", "Dataset"], ["DBpedia", "Dataset"], ["Freebase", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "52009210", "sentence": "An example is shown in Figure 3a : the question \" What army was involved in Siege of Clonmel ? \" can be answered using the Freebase predicate fb : base/culturalevent/event/entity involved , but in DBpedia the same fact requires a chain of two predicates , dbo : commander and dbo : militaryBranch .", "ner": [["Freebase", "Dataset"], ["DBpedia", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "52009210", "sentence": "Note that this can also occur with backward predicates , as shown in Figure 3b . \u2022 Missing Predicates : Some questions in DBpedia are answered using two - hop predicates even though there exists a one - hop predicate in the knowledge graph that represents a better mapping ; this situation arises due to the incompleteness of DBpedia .", "ner": [["DBpedia", "Dataset"], ["DBpedia", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "52009210", "sentence": "Nevertheless , it seems appropriate to separately categorize this particular type of predicate mismatch between Freebase and DBpedia .", "ner": [["Freebase", "Dataset"], ["DBpedia", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "52009210", "sentence": "We believe that DBpedia can be enhanced by inserting these missing links , but augmenting DBpedia is beyond the scope of this work .", "ner": [["DBpedia", "Dataset"], ["DBpedia", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "52009210", "sentence": "However , the Freebase predicate fb : book/author/works written can only be mapped to the DBpedia predicate dbo : author ( in the backward direction ) if the DBpedia subject has the type dbo : WrittenWork .", "ner": [["Freebase", "Dataset"], ["DBpedia", "Dataset"], ["DBpedia", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "52009210", "sentence": "To tackle these challenges with minimal manual effort , we construct manual rules that map highfrequency Freebase predicates in the initial mappings to all potentially correct ( at the semantic level ) DBpedia predicates .", "ner": [["Freebase", "Dataset"], ["DBpedia", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "52009210", "sentence": "Each rule includes a Freebase predicate and a list of corresponding DBpedia predicates , an associated directionality ( forward or backward ) , and an optional type constraint .", "ner": [["Freebase", "Dataset"], ["DBpedia", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "52009210", "sentence": "Heuristically , this means that the question does have an answer in DBpedia , just not the same as the one provided in Freebase .", "ner": [["DBpedia", "Dataset"], ["Freebase", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "52009210", "sentence": "Based on our rules , the Freebase predicate fb : music/artist/track is mapped to the DBpedia predicate dbo : artist with a constraint of dbo : MusicalWork in the backward direction .", "ner": [["Freebase", "Dataset"], ["DBpedia", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "52009210", "sentence": "The final output of entity mapping , predicate mapping , and candidate refinement is our SIMPLEDBPEDIAQA dataset , which successfully migrates SIMPLEQUESTIONS from Freebase over to DBpedia .   To lay the foundation for future work on our new dataset , we provide simple yet strong baselines using recent work by Mohammed et al. ( 2 0 1 8) , who applied techniques with and without neural networks to SIMPLEQUESTIONS .", "ner": [["entity mapping", "Task"], ["predicate mapping", "Task"], ["candidate refinement", "Task"], ["SIMPLEDBPEDIAQA", "Dataset"], ["SIMPLEQUESTIONS", "Dataset"], ["Freebase", "Dataset"], ["DBpedia", "Dataset"], ["neural networks", "Method"], ["SIMPLEQUESTIONS", "Dataset"]], "rel": [["SIMPLEDBPEDIAQA", "Benchmark-For", "entity mapping"], ["SIMPLEDBPEDIAQA", "Benchmark-For", "predicate mapping"], ["SIMPLEDBPEDIAQA", "Benchmark-For", "candidate refinement"], ["neural networks", "Used-For", "SIMPLEQUESTIONS"]], "rel_plus": [["SIMPLEDBPEDIAQA:Dataset", "Benchmark-For", "entity mapping:Task"], ["SIMPLEDBPEDIAQA:Dataset", "Benchmark-For", "predicate mapping:Task"], ["SIMPLEDBPEDIAQA:Dataset", "Benchmark-For", "candidate refinement:Task"], ["neural networks:Method", "Used-For", "SIMPLEQUESTIONS:Dataset"]]}
{"doc_id": "52009210", "sentence": "For this task , we examined bidirectional LSTMs and Conditional Random Fields ( CRFs ) . \u2022 Entity Linking : Detected entities ( text strings ) need to be linked to entities in the knowledge graph ( e.g. , URI from DBpedia in our case ) .", "ner": [["bidirectional LSTMs", "Method"], ["Conditional Random Fields", "Method"], ["CRFs", "Method"], ["Entity Linking", "Task"], ["DBpedia", "Dataset"]], "rel": [["CRFs", "Synonym-Of", "Conditional Random Fields"]], "rel_plus": [["CRFs:Method", "Synonym-Of", "Conditional Random Fields:Method"]]}
{"doc_id": "52009210", "sentence": "We examined three models : bidirectional GRU , convolutional neural network ( CNN ) , and logistic regression ( LR ) .", "ner": [["bidirectional GRU", "Method"], ["convolutional neural network", "Method"], ["CNN", "Method"], ["logistic regression", "Method"], ["LR", "Method"]], "rel": [["CNN", "Synonym-Of", "convolutional neural network"], ["LR", "Synonym-Of", "logistic regression"]], "rel_plus": [["CNN:Method", "Synonym-Of", "convolutional neural network:Method"], ["LR:Method", "Synonym-Of", "logistic regression:Method"]]}
{"doc_id": "52009210", "sentence": "The first two are standard neural network models ; for logistic regression we used as input the average of the word embeddings of each word .", "ner": [["neural network models", "Method"], ["logistic regression", "Method"], ["word embeddings", "Method"]], "rel": [["logistic regression", "SubClass-Of", "neural network models"], ["word embeddings", "Part-Of", "logistic regression"]], "rel_plus": [["logistic regression:Method", "SubClass-Of", "neural network models:Method"], ["word embeddings:Method", "Part-Of", "logistic regression:Method"]]}
{"doc_id": "52009210", "sentence": "BiGRU was selected over BiLSTM based on the experiments of Mohammed et al. ( 2 0 1 8) , where it was found to be slightly more accurate . \u2022 Evidence Integration : With m candidate entities and r candidate predicates from the previous components , the evidence integration model selects the best ( entity , predicate ) pair based on the product of each component score as well as a number of heuristics .", "ner": [["BiGRU", "Method"], ["BiLSTM", "Method"], ["Evidence Integration", "Task"], ["evidence integration model", "Method"]], "rel": [["evidence integration model", "Used-For", "Evidence Integration"]], "rel_plus": [["evidence integration model:Method", "Used-For", "Evidence Integration:Task"]]}
{"doc_id": "52009210", "sentence": "In SIMPLEQUESTIONS , the topic entity is not explicitly tagged in the natural language question at the token level ; as a result , SIMPLEDBPEDIAQA does not have token - level annotations either .", "ner": [["SIMPLEQUESTIONS", "Dataset"], ["SIMPLEDBPEDIAQA", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "52009210", "sentence": "For entity detection , on the validation set , the BiLSTM achieves 9 0 . 3 F 1 , compared to the CRF at 8 8 . 1 .", "ner": [["BiLSTM", "Method"], ["CRF", "Method"]], "rel": [["BiLSTM", "Compare-With", "CRF"]], "rel_plus": [["BiLSTM:Method", "Compare-With", "CRF:Method"]]}
{"doc_id": "52009210", "sentence": "The top of Table 5a shows the entity linking results for the BiLSTM and the CRF .", "ner": [["entity linking", "Task"], ["BiLSTM", "Method"], ["CRF", "Method"]], "rel": [["BiLSTM", "Used-For", "entity linking"], ["CRF", "Used-For", "entity linking"]], "rel_plus": [["BiLSTM:Method", "Used-For", "entity linking:Task"], ["CRF:Method", "Used-For", "entity linking:Task"]]}
{"doc_id": "52009210", "sentence": "These results are consistent with the findings of Mohammed et al. ( 2 0 1 8) : the BiLSTM achieves a higher F 1 score than the CRF , which translates into higher recall in entity linking ( both R@ 1 and R@ 5 ) .", "ner": [["BiLSTM", "Method"], ["CRF", "Method"], ["entity linking", "Task"]], "rel": [["BiLSTM", "Compare-With", "CRF"], ["CRF", "Used-For", "entity linking"], ["BiLSTM", "Used-For", "entity linking"]], "rel_plus": [["BiLSTM:Method", "Compare-With", "CRF:Method"], ["CRF:Method", "Used-For", "entity linking:Task"], ["BiLSTM:Method", "Used-For", "entity linking:Task"]]}
{"doc_id": "52009210", "sentence": "Predicate prediction results are shown on the bottom of Table 5a : the CNN slightly outperforms the BiGRU on R@ 1 , but in terms of R@ 5 the accuracy of both are quite similar .", "ner": [["CNN", "Method"], ["BiGRU", "Method"]], "rel": [["CNN", "Compare-With", "BiGRU"]], "rel_plus": [["CNN:Method", "Compare-With", "BiGRU:Method"]]}
{"doc_id": "52009210", "sentence": "The best model combination uses the BiLSTM for entity detection and the CNN for predicate prediction , achieving 7 8 . 5 % accuracy .", "ner": [["BiLSTM", "Method"], ["entity detection", "Task"], ["CNN", "Method"], ["predicate prediction", "Task"]], "rel": [["BiLSTM", "Used-For", "entity detection"], ["CNN", "Used-For", "predicate prediction"]], "rel_plus": [["BiLSTM:Method", "Used-For", "entity detection:Task"], ["CNN:Method", "Used-For", "predicate prediction:Task"]]}
{"doc_id": "52009210", "sentence": "By swapping the BiLSTM with the CRF for entity detection , we observe a 2. 4 % absolute decrease in endto - end accuracy .", "ner": [["BiLSTM", "Method"], ["CRF", "Method"], ["entity detection", "Task"]], "rel": [["CRF", "Part-Of", "BiLSTM"], ["CRF", "Used-For", "entity detection"], ["BiLSTM", "Used-For", "entity detection"]], "rel_plus": [["CRF:Method", "Part-Of", "BiLSTM:Method"], ["CRF:Method", "Used-For", "entity detection:Task"], ["BiLSTM:Method", "Used-For", "entity detection:Task"]]}
{"doc_id": "52009210", "sentence": "Note that using the CRF for entity detection and logistic regression ( LR ) for predicate prediction , which is a baseline that does not use neural networks ( with the exception of word embeddings ) , is also reasonably accurate .", "ner": [["CRF", "Method"], ["entity detection", "Task"], ["logistic regression", "Method"], ["LR", "Method"], ["predicate prediction", "Task"], ["neural networks", "Method"], ["word embeddings", "Method"]], "rel": [["CRF", "Used-For", "entity detection"], ["LR", "Synonym-Of", "logistic regression"], ["logistic regression", "Used-For", "predicate prediction"]], "rel_plus": [["CRF:Method", "Used-For", "entity detection:Task"], ["LR:Method", "Synonym-Of", "logistic regression:Method"], ["logistic regression:Method", "Used-For", "predicate prediction:Task"]]}
{"doc_id": "52009210", "sentence": "Following Lukovnikov et al. ( 2 0 1 7 ) , we sampled 2 0 0 examples of errors on the test set from the Bi - LSTM + CNN model to analyze their causes .", "ner": [["Bi - LSTM + CNN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "52009210", "sentence": "This paper presents SIMPLEDBPEDIAQA , a new benchmark dataset for simple question answering over knowledge graphs created by migrating the SIMPLEQUESTIONS dataset from Freebase to DBpedia .", "ner": [["SIMPLEDBPEDIAQA", "Dataset"], ["question answering over knowledge graphs", "Task"], ["SIMPLEQUESTIONS", "Dataset"], ["Freebase", "Dataset"], ["DBpedia", "Dataset"]], "rel": [["SIMPLEDBPEDIAQA", "Used-For", "question answering over knowledge graphs"]], "rel_plus": [["SIMPLEDBPEDIAQA:Dataset", "Used-For", "question answering over knowledge graphs:Task"]]}
{"doc_id": "52009210", "sentence": "We hope that our efforts better connect existing research communities , in particular , NLP researchers with the open linked data community , and spur additional work in question answering over knowledge graphs .", "ner": [["NLP", "Task"], ["question answering over knowledge graphs", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "204402755", "sentence": "Results are demonstrated across a variety of settings and architectures , including image classification , generative models , and language models . \\textsc{Demon } is easy to implement , requires no additional tuning , and incurs almost no extra computational overhead compared to the vanilla counterparts .", "ner": [["image classification", "Task"], ["generative models", "Method"], ["language models", "Method"], ["\\textsc{Demon", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "204402755", "sentence": "Deep Neural Networks ( DNNs ) have drastically advanced the state - of - the - art performance in many computer science applications , including computer vision ( Krizhevsky et al. , 2 0 1 2 ) , ( He et al. , 2 0 1 6 ; Ren et al. , 2 0 1 5 ) , natural language processing ( Mikolov et al. , 2 0 1 3 ; Bahdanau et al. , 2 0 1 4 ; Gehring et al. , 2 0 1 7 ) and speech recognition ( Sak et al. , 2 0 1 4 ; Sercu et al. , 2 0 1 6 ) .", "ner": [["Deep Neural Networks", "Method"], ["DNNs", "Method"], ["computer vision", "Task"], ["natural language processing", "Task"], ["speech recognition", "Task"]], "rel": [["DNNs", "Synonym-Of", "Deep Neural Networks"], ["Deep Neural Networks", "Used-For", "computer vision"], ["Deep Neural Networks", "Used-For", "natural language processing"], ["Deep Neural Networks", "Used-For", "speech recognition"]], "rel_plus": [["DNNs:Method", "Synonym-Of", "Deep Neural Networks:Method"], ["Deep Neural Networks:Method", "Used-For", "computer vision:Task"], ["Deep Neural Networks:Method", "Used-For", "natural language processing:Task"], ["Deep Neural Networks:Method", "Used-For", "speech recognition:Task"]]}
{"doc_id": "204402755", "sentence": "Yet , in the face of such significant developments , the age - old ( accelerated ) stochastic gradient descent ( SGD ) algorithm remains one of the most , if not the most , popular method for training DNNs Goodfellow et al. , 2 0 1 6 ; Wilson et al. , 2 0 1 7 ) .", "ner": [["stochastic gradient descent", "Method"], ["SGD", "Method"], ["DNNs", "Method"]], "rel": [["SGD", "Synonym-Of", "stochastic gradient descent"], ["stochastic gradient descent", "Part-Of", "DNNs"]], "rel_plus": [["SGD:Method", "Synonym-Of", "stochastic gradient descent:Method"], ["stochastic gradient descent:Method", "Part-Of", "DNNs:Method"]]}
{"doc_id": "204402755", "sentence": "At the same time , much of the state - of - the - art performance on highly contested benchmarks - such as the image classification dataset ImageNet - have been produced with accelerated SGD ( Krizhevsky et al. , 2 0 1 2 ; He et al. , 2 0 1 6 ; Xie et al. , 2 0 1 7 ; Zagoruyko & Komodakis , 2 0 1 6 ; Huang et al. , 2 0 1 7 ; Ren et al. , 2 0 1 5 ; Howard et al. , 2 0 1 7 ) .", "ner": [["image classification", "Task"], ["ImageNet", "Dataset"], ["SGD", "Method"]], "rel": [["ImageNet", "Benchmark-For", "image classification"], ["SGD", "Used-For", "image classification"]], "rel_plus": [["ImageNet:Dataset", "Benchmark-For", "image classification:Task"], ["SGD:Method", "Used-For", "image classification:Task"]]}
{"doc_id": "204402755", "sentence": "Minimizing the objective function L ( \u00b7 ) , the simplest and most common momentum method , classical momentum ( CM ) ( Polyak , 1 9 6 4 ) , is given by the following recursion for variable vector \u03b8 t \u2208 R p : The coefficient \u03b2 - traditionally , selected constant in [ 0 , 1] - controls how quickly the momentum decays , g t represents a stochastic gradient , usually E[g t ] = \u2207L(\u03b8 t ) , and \u03b7 > 0 is the step size .", "ner": [["momentum", "Method"], ["classical momentum", "Method"], ["CM", "Method"]], "rel": [["CM", "Synonym-Of", "classical momentum"]], "rel_plus": [["CM:Method", "Synonym-Of", "classical momentum:Method"]]}
{"doc_id": "204402755", "sentence": "Under an asynchronous distributed setting , ( Mitliagkas et al. , 2 0 1 6 ) observe that running SGD asynchronously is similar to adding a momentum - like term to SGD ; they also provide experimental evidence that naively setting \u03b2 = 0. 9 would result in a momentum \" overdose \" , leading to suboptimal performance .", "ner": [["SGD", "Method"], ["momentum - like term", "Method"], ["SGD", "Method"]], "rel": [["momentum - like term", "Part-Of", "SGD"]], "rel_plus": [["momentum - like term:Method", "Part-Of", "SGD:Method"]]}
{"doc_id": "204402755", "sentence": "Finally , moving from classical DNN settings towards generative adversarial networks ( GANs ) , the proposed momentum values tend to decrease from \u03b2 = 0. 9 ( Mirza & Osindero , 2 0 1 4 ; Radford et al. , 2 0 1 5 ; Arjovsky et al. , 2 0 1 7 ) , taking even negative values ( Gidel et al. , 2 0 1 8) .", "ner": [["DNN", "Method"], ["generative adversarial networks", "Method"], ["GANs", "Method"]], "rel": [["GANs", "Synonym-Of", "generative adversarial networks"]], "rel_plus": [["GANs:Method", "Synonym-Of", "generative adversarial networks:Method"]]}
{"doc_id": "204402755", "sentence": "In this paper , we introduce a novel momentum decay rule which significantly surpasses the performance of both Adam and CM ( as they are used currently ) , in addition to other state - of - the - art adaptive learning rate and adaptive momentum methods , across a variety of datasets and networks .", "ner": [["momentum decay rule", "Method"], ["Adam", "Method"], ["CM", "Method"]], "rel": [["momentum decay rule", "Compare-With", "Adam"], ["momentum decay rule", "Compare-With", "CM"]], "rel_plus": [["momentum decay rule:Method", "Compare-With", "Adam:Method"], ["momentum decay rule:Method", "Compare-With", "CM:Method"]]}
{"doc_id": "204402755", "sentence": "In particular , our findings can be summarized as follows : i ) We propose a new momentum decay rule , motivated by decaying the total contribution of a gradient to all future updates , with limited overhead and additional computation . ii ) Using the momentum decay rule with Adam , we observe large performance gains - relative to vanilla Adam - where the network continues to learn for far longer after Adam begins to plateau , and suggest that the momentum decay rule should be used as default for this method . iii ) We observe comparative performance for CM between momentum decay and learning rate decay ; a surprising finding given the unparalleled effectiveness of learning rate decay schedule .   Plain stochastic gradient descent motions .", "ner": [["momentum decay rule", "Method"], ["momentum decay rule", "Method"], ["Adam", "Method"], ["Adam", "Method"], ["Adam", "Method"], ["momentum decay rule", "Method"], ["CM", "Method"], ["stochastic gradient descent", "Method"]], "rel": [["momentum decay rule", "Part-Of", "Adam"]], "rel_plus": [["momentum decay rule:Method", "Part-Of", "Adam:Method"]]}
{"doc_id": "204402755", "sentence": "Then , plain stochastic gradient descent ( SGD ) uses the recursion : \u03b8 t+ 1 = \u03b8 t \u2212 \u03b7 \u00b7 g t , \u2200t .", "ner": [["stochastic gradient descent", "Method"], ["SGD", "Method"]], "rel": [["SGD", "Synonym-Of", "stochastic gradient descent"]], "rel_plus": [["SGD:Method", "Synonym-Of", "stochastic gradient descent:Method"]]}
{"doc_id": "204402755", "sentence": "Beginning with AdaGrad ( Duchi et al. , 2 0 1 1 ) , the SGD recursion , per coordinate i of \u03b8 , becomes : where G t \u2208 R p \u00d7 p is usually a diagonal preconditioning matrix as a summation of squares of past gradients , and \u03b5 > 0 a small constant .", "ner": [["AdaGrad", "Method"], ["SGD recursion", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "204402755", "sentence": "Then , RMSprop updates as - where a momentum term can also be optionally added : Finally , Adam ( Kingma & Ba , 2 0 1 4 ) , in addition , keeps an exponentially decaying average of past gradients : E g t+ 1 = \u03b2 1 \u00b7 E g t + ( 1 \u2212 \u03b2 1 ) \u00b7 g t , leading to the recursion : 1 where usually \u03b2 1 = 0. 9 and \u03b2 2 = 0. 9 9 9 .", "ner": [["RMSprop", "Method"], ["Adam", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "204402755", "sentence": "Observe that Adam is equivalent to RMSprop when \u03b2 1 = 0 , and when no bias correction is applied results in the same recursion .", "ner": [["Adam", "Method"], ["RMSprop", "Method"]], "rel": [["Adam", "Compare-With", "RMSprop"]], "rel_plus": [["Adam:Method", "Compare-With", "RMSprop:Method"]]}
{"doc_id": "204402755", "sentence": "Algorithm 1 DEMON in momentum SGD 1 : Parameters : # of iterations T , step size \u03b7 , momentum initial value \u03b2 init . 2 : v t = \u03b8 t = 0 , otherwise randomly initialized . 3 : for t = 0 , . . . , T do 4 : 8 : end for Motivation and interpretation .", "ner": [["DEMON", "Method"], ["momentum SGD", "Method"]], "rel": [["DEMON", "Part-Of", "momentum SGD"]], "rel_plus": [["DEMON:Method", "Part-Of", "momentum SGD:Method"]]}
{"doc_id": "204402755", "sentence": "E.g. , for \u03b2 t = \u03b2 = 0. 9 we obtain SGD with momentum , and for \u03b2 = 0 we obtain plain SGD in Algorithm 1 ; in Algorithm 2 , for \u03b2 1 = 0. 9 with a slightly adjustment of learning rate we obtain Adam , while for \u03b2 1 = 0 we obtain a non - accumulative AdaGrad algorithm .", "ner": [["SGD with momentum", "Method"], ["SGD", "Method"], ["Adam", "Method"], ["AdaGrad", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "204402755", "sentence": "We choose to apply DEMON to a slightly adjusted Adam - instead of vanilla Adam - to isolate the effect of the momentum parameter , since the momentum parameter adjusts the magnitude of the current gradient as well in vanilla Adam .", "ner": [["DEMON", "Method"], ["adjusted Adam", "Method"], ["Adam", "Method"], ["momentum", "Method"], ["momentum", "Method"], ["Adam", "Method"]], "rel": [["DEMON", "Part-Of", "adjusted Adam"]], "rel_plus": [["DEMON:Method", "Part-Of", "adjusted Adam:Method"]]}
{"doc_id": "204402755", "sentence": "The most widely used are learning rate adaptive methods , starting with AdaGrad ( Duchi et al. , 2 0 1 1 ) , AdaDelta ( Zeiler , 2 0 1 2 ) , RMSprop , and Adam ( Kingma & Ba , 2 0 1 4 ) .", "ner": [["AdaGrad", "Method"], ["AdaDelta", "Method"], ["RMSprop", "Method"], ["Adam", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "204402755", "sentence": "Interest in closing the generalization difference between adaptive methods and CM led to AdamW ( Loshchilov & Hutter , 2 0 1 7 ) , by fixing the weight decay of Adam , and Padam ( Chen & Gu , 2 0 1 8) , by lowering the exponent of the second moment .", "ner": [["CM", "Method"], ["AdamW", "Method"], ["weight decay", "Method"], ["Adam", "Method"], ["Padam", "Method"]], "rel": [["weight decay", "Part-Of", "Adam"]], "rel_plus": [["weight decay:Method", "Part-Of", "Adam:Method"]]}
{"doc_id": "204402755", "sentence": "Asynchronous methods are commonly used in deep learning , and ( Mitliagkas et al. , 2 0 1 6 ) show that running SGD asynchronously is similar to adding a momentum - like term to SGD without assumptions of convexity of the objective function .", "ner": [["deep learning", "Method"], ["SGD", "Method"], ["momentum - like term", "Method"], ["SGD", "Method"]], "rel": [["momentum - like term", "Part-Of", "SGD"]], "rel_plus": [["momentum - like term:Method", "Part-Of", "SGD:Method"]]}
{"doc_id": "204402755", "sentence": "We are aware of the theoretical work of ( Yuan , 2 0 1 6 ) which prove under certain conditions that momentum SGD is equivalent to SGD with a rescaled learning rate , however our experiments in the deep learning setting show slightly different behavior and understanding why is an exciting direction of research .", "ner": [["momentum SGD", "Method"], ["SGD", "Method"], ["deep learning", "Method"]], "rel": [["momentum SGD", "Compare-With", "SGD"]], "rel_plus": [["momentum SGD:Method", "Compare-With", "SGD:Method"]]}
{"doc_id": "204402755", "sentence": "Smaller values of \u03b2 have gradually been employed for Generative Adversarial Networks ( GAN ) , and recent developments in game dynamics ( Gidel et al. , 2 0 1 8) show a negative momentum is helpful for GANs .", "ner": [["Generative Adversarial Networks", "Method"], ["GAN", "Method"], ["GANs", "Method"]], "rel": [["GAN", "Synonym-Of", "Generative Adversarial Networks"]], "rel_plus": [["GAN:Method", "Synonym-Of", "Generative Adversarial Networks:Method"]]}
{"doc_id": "204402755", "sentence": "We select vanilla Adam as the baseline algorithm and include more recent state - of - the - art adaptive learning rate methods Quasi - Hyperbolic Adam ( QHAdam ) ( Ma & Yarats , 2 0 1 8) and AMSGrad ( Reddi et al. , 2 0 1 9 ) in our comparison .", "ner": [["Adam", "Method"], ["Quasi - Hyperbolic Adam", "Method"], ["QHAdam", "Method"], ["AMSGrad", "Method"]], "rel": [["QHAdam", "Synonym-Of", "Quasi - Hyperbolic Adam"], ["Adam", "Compare-With", "Quasi - Hyperbolic Adam"], ["Adam", "Compare-With", "AMSGrad"]], "rel_plus": [["QHAdam:Method", "Synonym-Of", "Quasi - Hyperbolic Adam:Method"], ["Adam:Method", "Compare-With", "Quasi - Hyperbolic Adam:Method"], ["Adam:Method", "Compare-With", "AMSGrad:Method"]]}
{"doc_id": "204402755", "sentence": "Residual Neural Network ( RN 1 8 - CIFAR 1 0 - DEMONAdam ) .", "ner": [["Residual Neural Network", "Method"], ["RN 1 8 - CIFAR 1 0 - DEMONAdam", "Method"]], "rel": [["RN 1 8 - CIFAR 1 0 - DEMONAdam", "Synonym-Of", "Residual Neural Network"]], "rel_plus": [["RN 1 8 - CIFAR 1 0 - DEMONAdam:Method", "Synonym-Of", "Residual Neural Network:Method"]]}
{"doc_id": "204402755", "sentence": "We train a ResNet 1 8 ( He et al. , 2 0 1 6 ) model on the CIFAR - 1 0 dataset .", "ner": [["ResNet 1 8", "Method"], ["CIFAR - 1 0", "Dataset"]], "rel": [["ResNet 1 8", "Trained-With", "CIFAR - 1 0"]], "rel_plus": [["ResNet 1 8:Method", "Trained-With", "CIFAR - 1 0:Dataset"]]}
{"doc_id": "204402755", "sentence": "With DEMON Adam , we achieve the generalization error reported in the literature ( He et al. , 2 0 1 6 ) for this model , attained using CM and a curated learning rate decay schedule , whilst all other methods are non - competitive .", "ner": [["DEMON Adam", "Method"], ["CM", "Method"], ["curated learning rate decay schedule", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "204402755", "sentence": "Non - Residual Neural Network ( VGG 1 6 - CIFAR 1 0 0 - DEMONAdam ) .", "ner": [["Non - Residual Neural Network", "Method"], ["VGG 1 6 - CIFAR 1 0 0 - DEMONAdam", "Method"]], "rel": [["VGG 1 6 - CIFAR 1 0 0 - DEMONAdam", "Synonym-Of", "Non - Residual Neural Network"]], "rel_plus": [["VGG 1 6 - CIFAR 1 0 0 - DEMONAdam:Method", "Synonym-Of", "Non - Residual Neural Network:Method"]]}
{"doc_id": "204402755", "sentence": "For the CIFAR - 1 0 0 dataset , we train an adjusted VGG - 1 6 model ( Simonyan & Zisserman , 2 0 1 4 ) .", "ner": [["CIFAR - 1 0 0", "Dataset"], ["VGG - 1 6", "Method"]], "rel": [["VGG - 1 6", "Trained-With", "CIFAR - 1 0 0"]], "rel_plus": [["VGG - 1 6:Method", "Trained-With", "CIFAR - 1 0 0:Dataset"]]}
{"doc_id": "204402755", "sentence": "Wide Residual Neural Network ( WRN - STL 1 0 - DEMONAdam ) .", "ner": [["Wide Residual Neural Network", "Method"], ["WRN - STL 1 0 - DEMONAdam", "Method"]], "rel": [["WRN - STL 1 0 - DEMONAdam", "Synonym-Of", "Wide Residual Neural Network"]], "rel_plus": [["WRN - STL 1 0 - DEMONAdam:Method", "Synonym-Of", "Wide Residual Neural Network:Method"]]}
{"doc_id": "204402755", "sentence": "The STL - 1 0 dataset presents a different challenge with a significantly smaller number of images than the CIFAR datasets , but in higher resolution .", "ner": [["STL - 1 0", "Dataset"], ["CIFAR", "Dataset"]], "rel": [["STL - 1 0", "Compare-With", "CIFAR"]], "rel_plus": [["STL - 1 0:Dataset", "Compare-With", "CIFAR:Dataset"]]}
{"doc_id": "204402755", "sentence": "LSTM ( PTB - LSTM - DEMONAdam ) .", "ner": [["LSTM", "Method"], ["PTB - LSTM - DEMONAdam", "Method"]], "rel": [["PTB - LSTM - DEMONAdam", "Synonym-Of", "LSTM"]], "rel_plus": [["PTB - LSTM - DEMONAdam:Method", "Synonym-Of", "LSTM:Method"]]}
{"doc_id": "204402755", "sentence": "Variational AutoEncoder ( VAE - MNIST - DEMONAdam ) .", "ner": [["Variational AutoEncoder", "Method"], ["VAE - MNIST - DEMONAdam", "Method"]], "rel": [["VAE - MNIST - DEMONAdam", "Synonym-Of", "Variational AutoEncoder"]], "rel_plus": [["VAE - MNIST - DEMONAdam:Method", "Synonym-Of", "Variational AutoEncoder:Method"]]}
{"doc_id": "204402755", "sentence": "VAEs ( Kingma & Welling , 2 0 1 5 ) pair a generator network with a second Neural Network , a recognition model that performs approximate inference , and can be trained with backpropagation .", "ner": [["VAEs", "Method"], ["generator network", "Method"], ["Neural Network", "Method"], ["backpropagation", "Method"]], "rel": [["generator network", "Part-Of", "VAEs"], ["Neural Network", "Part-Of", "VAEs"], ["VAEs", "Trained-With", "backpropagation"]], "rel_plus": [["generator network:Method", "Part-Of", "VAEs:Method"], ["Neural Network:Method", "Part-Of", "VAEs:Method"], ["VAEs:Method", "Trained-With", "backpropagation:Method"]]}
{"doc_id": "204402755", "sentence": "We train VAEs on the MNIST dataset .", "ner": [["VAEs", "Method"], ["MNIST", "Dataset"]], "rel": [["VAEs", "Trained-With", "MNIST"]], "rel_plus": [["VAEs:Method", "Trained-With", "MNIST:Dataset"]]}
{"doc_id": "204402755", "sentence": "Noise Conditional Score Network ( NCSN - CIFAR 1 0 - DEMONAdam ) .", "ner": [["Noise Conditional Score Network", "Method"], ["NCSN - CIFAR 1 0 - DEMONAdam", "Method"]], "rel": [["NCSN - CIFAR 1 0 - DEMONAdam", "Synonym-Of", "Noise Conditional Score Network"]], "rel_plus": [["NCSN - CIFAR 1 0 - DEMONAdam:Method", "Synonym-Of", "Noise Conditional Score Network:Method"]]}
{"doc_id": "204402755", "sentence": "NCSN ( Song & Ermon , 2 0 1 9 ) is a recent generative network achieving state - of - the - art inception score on CIFAR 1 0 .", "ner": [["NCSN", "Method"], ["generative network", "Method"], ["CIFAR 1 0", "Dataset"]], "rel": [["NCSN", "SubClass-Of", "generative network"], ["NCSN", "Evaluated-With", "CIFAR 1 0"]], "rel_plus": [["NCSN:Method", "SubClass-Of", "generative network:Method"], ["NCSN:Method", "Evaluated-With", "CIFAR 1 0:Dataset"]]}
{"doc_id": "204402755", "sentence": "We train a NCSN on the CIFAR 1 0 dataset and , using the official implementation , were unable to reproduce the reported score in the literature .", "ner": [["NCSN", "Method"], ["CIFAR 1 0", "Dataset"]], "rel": [["NCSN", "Trained-With", "CIFAR 1 0"]], "rel_plus": [["NCSN:Method", "Trained-With", "CIFAR 1 0:Dataset"]]}
{"doc_id": "204402755", "sentence": "NSCN trained with Adam achieves a superior inception score in Table 4 , however the produced images in Figure 1 exhibit a noticeably unnatural green compared to those produced by DEMON Adam .   We apply DEMON CM ( Algorithm 1 ) to a variety of models and tasks .", "ner": [["NSCN", "Method"], ["Adam", "Method"], ["DEMON Adam", "Method"], ["DEMON CM", "Method"]], "rel": [["NSCN", "Trained-With", "Adam"], ["Adam", "Compare-With", "DEMON Adam"], ["NSCN", "Trained-With", "DEMON Adam"]], "rel_plus": [["NSCN:Method", "Trained-With", "Adam:Method"], ["Adam:Method", "Compare-With", "DEMON Adam:Method"], ["NSCN:Method", "Trained-With", "DEMON Adam:Method"]]}
{"doc_id": "204402755", "sentence": "Since CM with learning rate decay is most often used to achieve the state - of - the - art results with the architectures and tasks in question , we include CM with learning rate decay as the target to beat .", "ner": [["CM with learning rate decay", "Method"], ["CM with learning rate decay", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "204402755", "sentence": "Recent adaptive momentum methods included in this section are Aggregated Momentum ( AggMo ) ( Lucas et al. , 2 0 1 8) , and Quasi - Hyperbolic Momentum ( QHM ) ( Ma & Yarats , 2 0 1 8) .", "ner": [["adaptive momentum", "Method"], ["Aggregated Momentum", "Method"], ["AggMo", "Method"], ["Quasi - Hyperbolic Momentum", "Method"], ["QHM", "Method"]], "rel": [["AggMo", "Synonym-Of", "Aggregated Momentum"], ["QHM", "Synonym-Of", "Quasi - Hyperbolic Momentum"]], "rel_plus": [["AggMo:Method", "Synonym-Of", "Aggregated Momentum:Method"], ["QHM:Method", "Synonym-Of", "Quasi - Hyperbolic Momentum:Method"]]}
{"doc_id": "204402755", "sentence": "Residual Neural Network ( RN 1 8 - CIFAR 1 0 - DEMONCM ) .", "ner": [["Residual Neural Network", "Method"], ["RN 1 8 - CIFAR 1 0 - DEMONCM", "Method"]], "rel": [["RN 1 8 - CIFAR 1 0 - DEMONCM", "Synonym-Of", "Residual Neural Network"]], "rel_plus": [["RN 1 8 - CIFAR 1 0 - DEMONCM:Method", "Synonym-Of", "Residual Neural Network:Method"]]}
{"doc_id": "204402755", "sentence": "We train a ResNet 1 8 model on the CIFAR - 1 0 dataset .", "ner": [["ResNet 1 8", "Method"], ["CIFAR - 1 0", "Dataset"]], "rel": [["ResNet 1 8", "Trained-With", "CIFAR - 1 0"]], "rel_plus": [["ResNet 1 8:Method", "Trained-With", "CIFAR - 1 0:Dataset"]]}
{"doc_id": "204402755", "sentence": "With DEMON CM , we achieve better generalization error than CM with learning rate decay , the optimizer for producing state - of - the - art results with ResNet architecture .", "ner": [["DEMON CM", "Method"], ["CM with learning rate decay", "Method"], ["ResNet", "Method"]], "rel": [["DEMON CM", "Compare-With", "CM with learning rate decay"], ["CM with learning rate decay", "Part-Of", "ResNet"]], "rel_plus": [["DEMON CM:Method", "Compare-With", "CM with learning rate decay:Method"], ["CM with learning rate decay:Method", "Part-Of", "ResNet:Method"]]}
{"doc_id": "204402755", "sentence": "Running 5 seeds , DEMON CM outperforms all other adaptive momentum methods by a large 3% - 8 % validation error margin with a small and large number of epochs and is competitive or better than CM with learning rate decay .", "ner": [["DEMON CM", "Method"], ["adaptive momentum", "Method"], ["CM with learning rate decay", "Method"]], "rel": [["DEMON CM", "Compare-With", "adaptive momentum"], ["DEMON CM", "Compare-With", "CM with learning rate decay"]], "rel_plus": [["DEMON CM:Method", "Compare-With", "adaptive momentum:Method"], ["DEMON CM:Method", "Compare-With", "CM with learning rate decay:Method"]]}
{"doc_id": "204402755", "sentence": "Non - Residual Neural Network ( VGG 1 6 - CIFAR 1 0 0 - DEMONCM ) .", "ner": [["Non - Residual Neural Network", "Method"], ["VGG 1 6 - CIFAR 1 0 0 - DEMONCM", "Method"]], "rel": [["VGG 1 6 - CIFAR 1 0 0 - DEMONCM", "Synonym-Of", "Non - Residual Neural Network"]], "rel_plus": [["VGG 1 6 - CIFAR 1 0 0 - DEMONCM:Method", "Synonym-Of", "Non - Residual Neural Network:Method"]]}
{"doc_id": "204402755", "sentence": "For the CIFAR - 1 0 0 dataset , we train an adjusted VGG - 1 6 model .", "ner": [["CIFAR - 1 0 0", "Dataset"], ["VGG - 1 6", "Method"]], "rel": [["VGG - 1 6", "Trained-With", "CIFAR - 1 0 0"]], "rel_plus": [["VGG - 1 6:Method", "Trained-With", "CIFAR - 1 0 0:Dataset"]]}
{"doc_id": "204402755", "sentence": "Wide Residual Neural Network ( WRN - STL 1 0 - DEMONCM ) .", "ner": [["Wide Residual Neural Network", "Method"], ["WRN - STL 1 0 - DEMONCM", "Method"]], "rel": [["WRN - STL 1 0 - DEMONCM", "Synonym-Of", "Wide Residual Neural Network"]], "rel_plus": [["WRN - STL 1 0 - DEMONCM:Method", "Synonym-Of", "Wide Residual Neural Network:Method"]]}
{"doc_id": "204402755", "sentence": "We train a Wide Residual 1 6 - 8 model for the STL - 1 0 dataset .", "ner": [["Wide Residual 1 6 - 8", "Method"], ["STL - 1 0", "Dataset"]], "rel": [["Wide Residual 1 6 - 8", "Trained-With", "STL - 1 0"]], "rel_plus": [["Wide Residual 1 6 - 8:Method", "Trained-With", "STL - 1 0:Dataset"]]}
{"doc_id": "204402755", "sentence": "DEMON CM continues to improve and eventually catches up to CM learning rate decay .", "ner": [["DEMON CM", "Method"], ["CM learning rate decay", "Method"]], "rel": [["DEMON CM", "Compare-With", "CM learning rate decay"]], "rel_plus": [["DEMON CM:Method", "Compare-With", "CM learning rate decay:Method"]]}
{"doc_id": "204402755", "sentence": "LSTM ( PTB - LSTM - DEMONCM ) .", "ner": [["LSTM", "Method"], ["PTB - LSTM - DEMONCM", "Method"]], "rel": [["PTB - LSTM - DEMONCM", "Synonym-Of", "LSTM"]], "rel_plus": [["PTB - LSTM - DEMONCM:Method", "Synonym-Of", "LSTM:Method"]]}
{"doc_id": "204402755", "sentence": "We train an RNN with LSTM architecture for the PTB language modeling task .", "ner": [["RNN", "Method"], ["LSTM", "Method"], ["PTB language modeling", "Task"]], "rel": [["LSTM", "Part-Of", "RNN"], ["RNN", "Used-For", "PTB language modeling"]], "rel_plus": [["LSTM:Method", "Part-Of", "RNN:Method"], ["RNN:Method", "Used-For", "PTB language modeling:Task"]]}
{"doc_id": "204402755", "sentence": "Running 5 seeds , DEMON CM slightly outperforms other adaptive momentum methods in generalization perplexity , and is competitive with CM with learning rate decay .", "ner": [["DEMON CM", "Method"], ["adaptive momentum", "Method"], ["CM with learning rate decay", "Method"]], "rel": [["DEMON CM", "Compare-With", "adaptive momentum"], ["DEMON CM", "Compare-With", "CM with learning rate decay"]], "rel_plus": [["DEMON CM:Method", "Compare-With", "adaptive momentum:Method"], ["DEMON CM:Method", "Compare-With", "CM with learning rate decay:Method"]]}
{"doc_id": "204402755", "sentence": "Variational AutoEncoder ( VAE - MNIST - DEMONCM ) .", "ner": [["Variational AutoEncoder", "Method"], ["VAE - MNIST - DEMONCM", "Method"]], "rel": [["VAE - MNIST - DEMONCM", "Synonym-Of", "Variational AutoEncoder"]], "rel_plus": [["VAE - MNIST - DEMONCM:Method", "Synonym-Of", "Variational AutoEncoder:Method"]]}
{"doc_id": "204402755", "sentence": "We train the generative model VAE on the MNIST dataset .", "ner": [["VAE", "Method"], ["MNIST", "Dataset"]], "rel": [["VAE", "Trained-With", "MNIST"]], "rel_plus": [["VAE:Method", "Trained-With", "MNIST:Dataset"]]}
{"doc_id": "204402755", "sentence": "Refer to Figure 3 ( Bottom row , right - most plot ) and Table 7 for more details .   We evaluated the momentum decay rule with Adam and CM on Residual CNNs , Non Residual CNNS , RNNs and generative models .", "ner": [["momentum decay rule", "Method"], ["Adam", "Method"], ["CM", "Method"], ["Residual CNNs", "Method"], ["Non Residual CNNS", "Method"], ["RNNs", "Method"], ["generative models", "Method"]], "rel": [["momentum decay rule", "Part-Of", "Adam"], ["momentum decay rule", "Part-Of", "CM"], ["Adam", "Part-Of", "Residual CNNs"], ["CM", "Part-Of", "Non Residual CNNS"], ["Adam", "Part-Of", "Non Residual CNNS"], ["CM", "Part-Of", "RNNs"], ["Adam", "Part-Of", "RNNs"], ["CM", "Part-Of", "generative models"], ["Adam", "Part-Of", "generative models"]], "rel_plus": [["momentum decay rule:Method", "Part-Of", "Adam:Method"], ["momentum decay rule:Method", "Part-Of", "CM:Method"], ["Adam:Method", "Part-Of", "Residual CNNs:Method"], ["CM:Method", "Part-Of", "Non Residual CNNS:Method"], ["Adam:Method", "Part-Of", "Non Residual CNNS:Method"], ["CM:Method", "Part-Of", "RNNs:Method"], ["Adam:Method", "Part-Of", "RNNs:Method"], ["CM:Method", "Part-Of", "generative models:Method"], ["Adam:Method", "Part-Of", "generative models:Method"]]}
{"doc_id": "204402755", "sentence": "For CNNs , we used the image classification datasets CI - FAR 1 0 , CIFAR 1 0 0 and STL 1 0 datasets .", "ner": [["CNNs", "Method"], ["image classification", "Task"], ["CI - FAR 1 0", "Dataset"], ["CIFAR 1 0 0", "Dataset"], ["STL 1 0", "Dataset"]], "rel": [["CI - FAR 1 0", "Used-For", "CNNs"], ["CIFAR 1 0 0", "Used-For", "CNNs"], ["STL 1 0", "Used-For", "CNNs"], ["CI - FAR 1 0", "Benchmark-For", "image classification"], ["CIFAR 1 0 0", "Benchmark-For", "image classification"], ["STL 1 0", "Benchmark-For", "image classification"]], "rel_plus": [["CI - FAR 1 0:Dataset", "Used-For", "CNNs:Method"], ["CIFAR 1 0 0:Dataset", "Used-For", "CNNs:Method"], ["STL 1 0:Dataset", "Used-For", "CNNs:Method"], ["CI - FAR 1 0:Dataset", "Benchmark-For", "image classification:Task"], ["CIFAR 1 0 0:Dataset", "Benchmark-For", "image classification:Task"], ["STL 1 0:Dataset", "Benchmark-For", "image classification:Task"]]}
{"doc_id": "204402755", "sentence": "For RNNs , we used the language modeling dataset PTB .", "ner": [["RNNs", "Method"], ["language modeling", "Task"], ["PTB", "Dataset"]], "rel": [["PTB", "Used-For", "RNNs"], ["PTB", "Benchmark-For", "language modeling"]], "rel_plus": [["PTB:Dataset", "Used-For", "RNNs:Method"], ["PTB:Dataset", "Benchmark-For", "language modeling:Task"]]}
{"doc_id": "204402755", "sentence": "For generative modeling , we used the MNIST and CIFAR 1 0 datasets .", "ner": [["generative modeling", "Method"], ["MNIST", "Dataset"], ["CIFAR 1 0", "Dataset"]], "rel": [["MNIST", "Used-For", "generative modeling"], ["CIFAR 1 0", "Used-For", "generative modeling"]], "rel_plus": [["MNIST:Dataset", "Used-For", "generative modeling:Method"], ["CIFAR 1 0:Dataset", "Used-For", "generative modeling:Method"]]}
{"doc_id": "204402755", "sentence": "For each network dataset pair other than NSCN , we evaluated Adam , QHAdam , AMSGrad , DEMON Adam , AggMo , QHM , DEMON CM , and CM with learning rate decay .", "ner": [["NSCN", "Dataset"], ["Adam", "Method"], ["QHAdam", "Method"], ["AMSGrad", "Method"], ["DEMON Adam", "Method"], ["AggMo", "Method"], ["QHM", "Method"], ["DEMON CM", "Method"], ["CM with learning rate decay", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "204402755", "sentence": "We describe the six test problems in this paper . \u2022 CIFAR 1 0 -ResNet 1 8 CIFAR 1 0 contains 6 0 , 0 0 0 3 2 x 3 2 x 3 images with a 5 0 , 0 0 0 training set , 1 0 , 0 0 0 test set split .", "ner": [["CIFAR 1 0 -ResNet 1 8 CIFAR 1 0", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "204402755", "sentence": "ResNet 1 8 ( He et al. , 2 0 1 6 ) is an 1 8 layers deep CNN with skip connections for image classification .", "ner": [["ResNet 1 8", "Method"], ["CNN", "Method"], ["skip connections", "Method"], ["image classification", "Task"]], "rel": [["skip connections", "Part-Of", "ResNet 1 8"], ["ResNet 1 8", "SubClass-Of", "CNN"], ["ResNet 1 8", "Used-For", "image classification"]], "rel_plus": [["skip connections:Method", "Part-Of", "ResNet 1 8:Method"], ["ResNet 1 8:Method", "SubClass-Of", "CNN:Method"], ["ResNet 1 8:Method", "Used-For", "image classification:Task"]]}
{"doc_id": "204402755", "sentence": "Trained with a batch size of 1 2 8 . \u2022 CIFAR 1 0 0 -VGG 1 6 CIFAR 1 0 0 is a fine - grained version of CIFAR - 1 0 and contains 6 0 , 0 0 0 3 2 x 3 2 x 3 images with a 5 0 , 0 0 0 training set , 1 0 , 0 0 0 test set split . \u2022 PTB -LSTM PTB is an English text corpus containing 9 2 9 , 0 0 0 training words , 7 3 , 0 0 0 validation words , and 8 2 , 0 0 0 test words .", "ner": [["CIFAR 1 0 0 -VGG 1 6", "Method"], ["CIFAR 1 0 0", "Dataset"], ["CIFAR - 1 0", "Dataset"], ["PTB -LSTM", "Method"], ["PTB", "Dataset"]], "rel": [["CIFAR 1 0 0 -VGG 1 6", "Trained-With", "CIFAR 1 0 0"], ["CIFAR 1 0 0", "Compare-With", "CIFAR - 1 0"], ["PTB -LSTM", "Trained-With", "PTB"]], "rel_plus": [["CIFAR 1 0 0 -VGG 1 6:Method", "Trained-With", "CIFAR 1 0 0:Dataset"], ["CIFAR 1 0 0:Dataset", "Compare-With", "CIFAR - 1 0:Dataset"], ["PTB -LSTM:Method", "Trained-With", "PTB:Dataset"]]}
{"doc_id": "204402755", "sentence": "The model is stacked LSTMs ( Hochreiter & Schmidhuber , 1 9 9 7 ) with 2 layers , 6 5 0 units per layer , and dropout of 0. 5 .", "ner": [["LSTMs", "Method"], ["dropout", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "204402755", "sentence": "Trained with a batch size of 2 0 . \u2022 MNIST -VAE MNIST contains 6 0 , 0 0 0 3 2 x 3 2 x 1 grayscale images with a 5 0 , 0 0 0 training set , 1 0 , 0 0 0 test set split .", "ner": [["MNIST -VAE", "Method"], ["MNIST", "Dataset"]], "rel": [["MNIST -VAE", "Trained-With", "MNIST"]], "rel_plus": [["MNIST -VAE:Method", "Trained-With", "MNIST:Dataset"]]}
{"doc_id": "204402755", "sentence": "Trained with a batch size of 1 0 0 . \u2022 CIFAR 1 0 -NCSN CIFAR 1 0 contains 6 0 , 0 0 0 3 2 x 3 2 x 3 images with a 5 0 , 0 0 0 training set , 1 0 , 0 0 0 test set split .", "ner": [["CIFAR 1 0 -NCSN", "Method"], ["CIFAR 1 0", "Dataset"]], "rel": [["CIFAR 1 0 -NCSN", "Trained-With", "CIFAR 1 0"]], "rel_plus": [["CIFAR 1 0 -NCSN:Method", "Trained-With", "CIFAR 1 0:Dataset"]]}
{"doc_id": "204402755", "sentence": "The Adam algorithm is parameterized by learning rate \u03b7 > 0 , discount factors \u03b2 1 < 1 and \u03b2 2 < 1 , a small constant , and uses the update rule : AMSGrad ( Reddi et al. , 2 0 1 9 ) resolves an issue in the proof of Adam related to the exponential moving average E g\u2022g t , where Adam does not converge for a simple optimization problem .", "ner": [["Adam", "Method"], ["AMSGrad", "Method"], ["Adam", "Method"], ["Adam", "Method"]], "rel": [["AMSGrad", "Used-For", "Adam"]], "rel_plus": [["AMSGrad:Method", "Used-For", "Adam:Method"]]}
{"doc_id": "204402755", "sentence": "The AMSGrad algorithm is parameterized by learning rate \u03b7 > 0 , discount factors \u03b2 1 < 1 and \u03b2 2 < 1 , a small constant , and uses the update rule : where E g t+ 1 and E g\u2022g t+ 1 are defined identically to Adam .", "ner": [["AMSGrad", "Method"], ["Adam", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "204402755", "sentence": "QHAdam ( Quasi - Hyperbolic Adam ) ( Ma & Yarats , 2 0 1 8) extends QHM ( Quasi - Hyperbolic Momentum ) , introduced further below , to replace both momentum estimators in Adam with quasihyperbolic terms .", "ner": [["QHAdam", "Method"], ["Quasi - Hyperbolic Adam", "Method"], ["QHM", "Method"], ["Quasi - Hyperbolic Momentum", "Method"], ["Adam", "Method"]], "rel": [["QHAdam", "Synonym-Of", "Quasi - Hyperbolic Adam"], ["QHAdam", "SubClass-Of", "QHM"], ["QHM", "Synonym-Of", "Quasi - Hyperbolic Momentum"], ["QHAdam", "Part-Of", "Adam"]], "rel_plus": [["QHAdam:Method", "Synonym-Of", "Quasi - Hyperbolic Adam:Method"], ["QHAdam:Method", "SubClass-Of", "QHM:Method"], ["QHM:Method", "Synonym-Of", "Quasi - Hyperbolic Momentum:Method"], ["QHAdam:Method", "Part-Of", "Adam:Method"]]}
{"doc_id": "204402755", "sentence": "This quasi - hyperbolic formulation is capable of recovering Adam and NAdam ( Dozat , 2 0 1 6 ) , amongst others .", "ner": [["Adam", "Method"], ["NAdam", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "204402755", "sentence": "The QHAdam algorithm is parameterized by learning rate \u03b7 > 0 , discount factors \u03b2 1 < 1 and \u03b2 2 < 1 , \u03bd 1 , \u03bd 2 \u2208 R , a small constant , and uses the update rule : where E g t+ 1 and E g\u2022g t+ 1 are defined identically to Adam .", "ner": [["QHAdam", "Method"], ["Adam", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "204402755", "sentence": "AggMo ( Aggregated Momentum ) ( Lucas et al. , 2 0 1 8) takes a linear combination of multiple momentum buffers .", "ner": [["AggMo", "Method"], ["Aggregated Momentum", "Method"]], "rel": [["AggMo", "Synonym-Of", "Aggregated Momentum"]], "rel_plus": [["AggMo:Method", "Synonym-Of", "Aggregated Momentum:Method"]]}
{"doc_id": "204402755", "sentence": "The AggMo algorithm is parameterized by learning rate \u03b7 > 0 , discount factors \u03b2 \u2208 R K , and uses the update rule : QHM ( Quasi - Hyperbolic Momentum ) ( Ma & Yarats , 2 0 1 8 ) is a weighted average of the momentum and plain SGD .", "ner": [["AggMo", "Method"], ["QHM", "Method"], ["Quasi - Hyperbolic Momentum", "Method"], ["momentum", "Method"], ["SGD", "Method"]], "rel": [["QHM", "Used-For", "AggMo"], ["QHM", "Synonym-Of", "Quasi - Hyperbolic Momentum"], ["QHM", "SubClass-Of", "momentum"], ["QHM", "SubClass-Of", "SGD"]], "rel_plus": [["QHM:Method", "Used-For", "AggMo:Method"], ["QHM:Method", "Synonym-Of", "Quasi - Hyperbolic Momentum:Method"], ["QHM:Method", "SubClass-Of", "momentum:Method"], ["QHM:Method", "SubClass-Of", "SGD:Method"]]}
{"doc_id": "204402755", "sentence": "QHM is capable of recovering Nesterov Momentum ( Nesterov , 1 9 8 3 ) , Synthesized Nesterov Variants ( Lessard et al. , 2 0 1 6 ) , accSGD ( Jain et al. , 2 0 1 7 ) and others .", "ner": [["QHM", "Method"], ["Nesterov Momentum", "Method"], ["Synthesized Nesterov Variants", "Method"], ["accSGD", "Method"]], "rel": [["QHM", "Used-For", "Nesterov Momentum"], ["QHM", "Used-For", "Synthesized Nesterov Variants"], ["QHM", "Used-For", "accSGD"]], "rel_plus": [["QHM:Method", "Used-For", "Nesterov Momentum:Method"], ["QHM:Method", "Used-For", "Synthesized Nesterov Variants:Method"], ["QHM:Method", "Used-For", "accSGD:Method"]]}
{"doc_id": "204402755", "sentence": "The QHM algorithm is parameterized by learning rate \u03b7 > 0 , discount factor \u03b2 < 1 , immediate discount factor \u03bd \u2208 R , and uses the update rule : A. 3 OPTIMIZER HYPERPARAMETERS CM learning rate decay 5 0 0.0 0 0 0 1 \u03b2 1 = 0. 9 , patience = 5 CM learning rate decay 1 0 0 0.0 0 0 0 0 3 \u03b2 1 = 0. 9 , patience = 5 CM learning rate decay", "ner": [["QHM", "Method"], ["CM learning rate decay", "Method"], ["CM learning rate decay", "Method"], ["CM learning rate decay", "Method"]], "rel": [["CM learning rate decay", "Used-For", "QHM"], ["CM learning rate decay", "Used-For", "QHM"], ["CM learning rate decay", "Used-For", "QHM"]], "rel_plus": [["CM learning rate decay:Method", "Used-For", "QHM:Method"], ["CM learning rate decay:Method", "Used-For", "QHM:Method"], ["CM learning rate decay:Method", "Used-For", "QHM:Method"]]}
{"doc_id": "146808333", "sentence": "MobileNetV 3 is tuned to mobile phone CPUs through a combination of hardware - aware network architecture search ( NAS ) complemented by the NetAdapt algorithm and then subsequently improved through novel architecture advances .", "ner": [["MobileNetV 3", "Method"], ["network architecture search", "Method"], ["NAS", "Method"], ["NetAdapt", "Method"]], "rel": [["network architecture search", "Used-For", "MobileNetV 3"], ["NetAdapt", "Used-For", "MobileNetV 3"], ["NAS", "Synonym-Of", "network architecture search"]], "rel_plus": [["network architecture search:Method", "Used-For", "MobileNetV 3:Method"], ["NetAdapt:Method", "Used-For", "MobileNetV 3:Method"], ["NAS:Method", "Synonym-Of", "network architecture search:Method"]]}
{"doc_id": "146808333", "sentence": "Through this process we create two new MobileNet models for release : MobileNetV 3 - Large and MobileNetV 3 - Small which are targeted for high and low resource use cases .", "ner": [["MobileNet", "Method"], ["MobileNetV 3 - Large", "Method"], ["MobileNetV 3 - Small", "Method"]], "rel": [["MobileNetV 3 - Large", "SubClass-Of", "MobileNet"], ["MobileNetV 3 - Small", "SubClass-Of", "MobileNet"]], "rel_plus": [["MobileNetV 3 - Large:Method", "SubClass-Of", "MobileNet:Method"], ["MobileNetV 3 - Small:Method", "SubClass-Of", "MobileNet:Method"]]}
{"doc_id": "146808333", "sentence": "These models are then adapted and applied to the tasks of object detection and semantic segmentation .", "ner": [["object detection", "Task"], ["semantic segmentation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "146808333", "sentence": "For the task of semantic segmentation ( or any dense pixel prediction ) , we propose a new efficient segmentation decoder Lite Reduced Atrous Spatial Pyramid Pooling ( LR - ASPP ) .", "ner": [["semantic segmentation", "Task"], ["dense pixel prediction", "Task"], ["Lite Reduced Atrous Spatial Pyramid Pooling", "Method"], ["LR - ASPP", "Method"]], "rel": [["Lite Reduced Atrous Spatial Pyramid Pooling", "Used-For", "semantic segmentation"], ["Lite Reduced Atrous Spatial Pyramid Pooling", "Used-For", "dense pixel prediction"], ["LR - ASPP", "Synonym-Of", "Lite Reduced Atrous Spatial Pyramid Pooling"]], "rel_plus": [["Lite Reduced Atrous Spatial Pyramid Pooling:Method", "Used-For", "semantic segmentation:Task"], ["Lite Reduced Atrous Spatial Pyramid Pooling:Method", "Used-For", "dense pixel prediction:Task"], ["LR - ASPP:Method", "Synonym-Of", "Lite Reduced Atrous Spatial Pyramid Pooling:Method"]]}
{"doc_id": "146808333", "sentence": "We achieve new state of the art results for mobile classification , detection and segmentation .", "ner": [["mobile classification", "Task"], ["detection", "Task"], ["segmentation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "146808333", "sentence": "MobileNetV 3 - Large is 3. 2 \\% more accurate on ImageNet classification while reducing latency by 1 5 \\% compared to MobileNetV 2 .", "ner": [["MobileNetV 3 - Large", "Method"], ["ImageNet", "Dataset"], ["MobileNetV 2 .", "Method"]], "rel": [["MobileNetV 3 - Large", "Evaluated-With", "ImageNet"], ["MobileNetV 3 - Large", "Compare-With", "MobileNetV 2 ."]], "rel_plus": [["MobileNetV 3 - Large:Method", "Evaluated-With", "ImageNet:Dataset"], ["MobileNetV 3 - Large:Method", "Compare-With", "MobileNetV 2 .:Method"]]}
{"doc_id": "146808333", "sentence": "MobileNetV 3 - Small is 4. 6 \\% more accurate while reducing latency by 5\\% compared to MobileNetV 2 .", "ner": [["MobileNetV 3 - Small", "Method"], ["MobileNetV 2 .", "Method"]], "rel": [["MobileNetV 3 - Small", "Compare-With", "MobileNetV 2 ."]], "rel_plus": [["MobileNetV 3 - Small:Method", "Compare-With", "MobileNetV 2 .:Method"]]}
{"doc_id": "146808333", "sentence": "MobileNetV 3 - Large detection is 2 5 \\% faster at roughly the same accuracy as MobileNetV 2 on COCO detection .", "ner": [["MobileNetV 3 - Large detection", "Method"], ["MobileNetV 2", "Method"], ["COCO", "Dataset"]], "rel": [["MobileNetV 3 - Large detection", "Compare-With", "MobileNetV 2"], ["MobileNetV 2", "Evaluated-With", "COCO"], ["MobileNetV 3 - Large detection", "Evaluated-With", "COCO"]], "rel_plus": [["MobileNetV 3 - Large detection:Method", "Compare-With", "MobileNetV 2:Method"], ["MobileNetV 2:Method", "Evaluated-With", "COCO:Dataset"], ["MobileNetV 3 - Large detection:Method", "Evaluated-With", "COCO:Dataset"]]}
{"doc_id": "146808333", "sentence": "MobileNetV 3 - Large LR - ASPP is 3 0 \\% faster than MobileNetV 2 R - ASPP at similar accuracy for Cityscapes segmentation .", "ner": [["MobileNetV 3 - Large LR - ASPP", "Method"], ["MobileNetV 2 R - ASPP", "Method"], ["Cityscapes", "Dataset"], ["segmentation", "Task"]], "rel": [["MobileNetV 3 - Large LR - ASPP", "Compare-With", "MobileNetV 2 R - ASPP"], ["MobileNetV 2 R - ASPP", "Evaluated-With", "Cityscapes"], ["MobileNetV 3 - Large LR - ASPP", "Evaluated-With", "Cityscapes"], ["Cityscapes", "Benchmark-For", "segmentation"]], "rel_plus": [["MobileNetV 3 - Large LR - ASPP:Method", "Compare-With", "MobileNetV 2 R - ASPP:Method"], ["MobileNetV 2 R - ASPP:Method", "Evaluated-With", "Cityscapes:Dataset"], ["MobileNetV 3 - Large LR - ASPP:Method", "Evaluated-With", "Cityscapes:Dataset"], ["Cityscapes:Dataset", "Benchmark-For", "segmentation:Task"]]}
{"doc_id": "146808333", "sentence": "This paper describes the approach we took to develop MobileNetV 3 Large and Small models in order to deliver the next generation of high accuracy efficient neural network models to power on - device computer vision .", "ner": [["MobileNetV 3 Large and Small", "Method"], ["efficient neural network", "Method"], ["on - device computer vision", "Task"]], "rel": [["MobileNetV 3 Large and Small", "SubClass-Of", "efficient neural network"], ["MobileNetV 3 Large and Small", "Used-For", "on - device computer vision"]], "rel_plus": [["MobileNetV 3 Large and Small:Method", "SubClass-Of", "efficient neural network:Method"], ["MobileNetV 3 Large and Small:Method", "Used-For", "on - device computer vision:Task"]]}
{"doc_id": "146808333", "sentence": "Section 4 reviews architecture search and the complementary nature of MnasNet and NetAdapt algorithms .", "ner": [["MnasNet", "Method"], ["NetAdapt", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "146808333", "sentence": "Section 6 presents extensive experiments for classification , detection and segmentation in order do demonstrate efficacy and understand the contributions of different elements .", "ner": [["classification", "Task"], ["detection", "Task"], ["segmentation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "146808333", "sentence": "SqueezeNet [ 2 2 ] extensively uses 1x 1 convolutions with squeeze and expand modules primarily focusing on reducing the number of parameters .", "ner": [["SqueezeNet", "Method"], ["1x 1 convolutions", "Method"]], "rel": [["1x 1 convolutions", "Part-Of", "SqueezeNet"]], "rel_plus": [["1x 1 convolutions:Method", "Part-Of", "SqueezeNet:Method"]]}
{"doc_id": "146808333", "sentence": "MobileNetV 1 [ 1 9 ] employs depthwise separable convolution to substantially improve computation efficiency .", "ner": [["MobileNetV 1", "Method"], ["depthwise separable convolution", "Method"]], "rel": [["depthwise separable convolution", "Part-Of", "MobileNetV 1"]], "rel_plus": [["depthwise separable convolution:Method", "Part-Of", "MobileNetV 1:Method"]]}
{"doc_id": "146808333", "sentence": "ShuffleNet [ 4 9 ] utilizes group convolution and channel shuffle operations to further reduce the MAdds .", "ner": [["ShuffleNet", "Method"], ["group convolution", "Method"], ["channel shuffle", "Method"]], "rel": [["group convolution", "Part-Of", "ShuffleNet"], ["channel shuffle", "Part-Of", "ShuffleNet"]], "rel_plus": [["group convolution:Method", "Part-Of", "ShuffleNet:Method"], ["channel shuffle:Method", "Part-Of", "ShuffleNet:Method"]]}
{"doc_id": "146808333", "sentence": "CondenseNet [ 2 1 ] learns group convolutions at the training stage to keep useful dense connections between layers for feature re - use .", "ner": [["CondenseNet", "Method"], ["group convolutions", "Method"], ["dense connections", "Method"]], "rel": [["dense connections", "Part-Of", "CondenseNet"], ["group convolutions", "Part-Of", "CondenseNet"]], "rel_plus": [["dense connections:Method", "Part-Of", "CondenseNet:Method"], ["group convolutions:Method", "Part-Of", "CondenseNet:Method"]]}
{"doc_id": "146808333", "sentence": "To reduce the computational cost of search , differentiable architecture search framework is used in [ 2 8 , 5 , 4 5 ] with gradient - based optimization .", "ner": [["differentiable architecture search", "Method"], ["gradient - based optimization", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "146808333", "sentence": "Depthwise separable convolutions are defined by two separate layers : light weight depthwise convolution for spatial filtering and heavier 1x 1 pointwise convolutions for feature generation .", "ner": [["Depthwise separable convolutions", "Method"], ["light weight depthwise convolution", "Method"], ["spatial filtering", "Task"], ["heavier 1x 1 pointwise convolutions", "Method"], ["feature generation", "Task"]], "rel": [["light weight depthwise convolution", "Part-Of", "Depthwise separable convolutions"], ["heavier 1x 1 pointwise convolutions", "Part-Of", "Depthwise separable convolutions"], ["light weight depthwise convolution", "Used-For", "spatial filtering"], ["heavier 1x 1 pointwise convolutions", "Used-For", "feature generation"]], "rel_plus": [["light weight depthwise convolution:Method", "Part-Of", "Depthwise separable convolutions:Method"], ["heavier 1x 1 pointwise convolutions:Method", "Part-Of", "Depthwise separable convolutions:Method"], ["light weight depthwise convolution:Method", "Used-For", "spatial filtering:Task"], ["heavier 1x 1 pointwise convolutions:Method", "Used-For", "feature generation:Task"]]}
{"doc_id": "146808333", "sentence": "MnasNet [ 4 3 ] built upon the MobileNetV 2 structure by introducing lightweight attention modules based on squeeze and excitation into the bottleneck structure .", "ner": [["MnasNet", "Method"], ["MobileNetV 2", "Method"]], "rel": [["MnasNet", "SubClass-Of", "MobileNetV 2"]], "rel_plus": [["MnasNet:Method", "SubClass-Of", "MobileNetV 2:Method"]]}
{"doc_id": "146808333", "sentence": "Both squeeze and excitation as well as the swish nonlinearity use the sigmoid which can be inefficient to compute as well challenging to maintain accuracy in fixed point arithmetic so we replace this with the hard sigmoid [ 2 , 1 1 ] as discussed in section 5. 2 . [ 2 0 ] .", "ner": [["swish nonlinearity", "Method"], ["sigmoid", "Method"], ["sigmoid", "Method"]], "rel": [["sigmoid", "Part-Of", "swish nonlinearity"]], "rel_plus": [["sigmoid:Method", "Part-Of", "swish nonlinearity:Method"]]}
{"doc_id": "146808333", "sentence": "For MobileNetV 3 we use platform - aware NAS to search for the global network structures by optimizing each network block .", "ner": [["MobileNetV 3", "Method"], ["NAS", "Method"]], "rel": [["NAS", "Used-For", "MobileNetV 3"]], "rel_plus": [["NAS:Method", "Used-For", "MobileNetV 3:Method"]]}
{"doc_id": "146808333", "sentence": "Enhanced with this new weight factor w , we start a new architecture search from scratch to find the initial seed model and then apply NetAdapt and other optimizations to obtain the final MobileNetV 3 - Small model .", "ner": [["NetAdapt", "Method"], ["MobileNetV 3 - Small", "Method"]], "rel": [["NetAdapt", "Used-For", "MobileNetV 3 - Small"]], "rel_plus": [["NetAdapt:Method", "Used-For", "MobileNetV 3 - Small:Method"]]}
{"doc_id": "146808333", "sentence": "We also introduce a new nonlinearity , h - swish , a modified version of the recent swish nonlinearity , which is faster to compute and more quantization - friendly .", "ner": [["h - swish", "Method"], ["swish", "Method"]], "rel": [["h - swish", "SubClass-Of", "swish"]], "rel_plus": [["h - swish:Method", "SubClass-Of", "swish:Method"]]}
{"doc_id": "146808333", "sentence": "We were able to reduce the number of filters to 1 6 while maintaining the same accuracy as 3 2 filters using either ReLU or swish .", "ner": [["ReLU", "Method"], ["swish", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "146808333", "sentence": "In [ 3 6 , 1 3 , 1 6 ] a nonlinearity called swish was introduced that when used as a drop - in replacement for ReLU , that significantly improves the accuracy of neural networks .", "ner": [["swish", "Method"], ["ReLU", "Method"], ["neural networks", "Method"]], "rel": [["swish", "Part-Of", "neural networks"], ["ReLU", "Part-Of", "neural networks"]], "rel_plus": [["swish:Method", "Part-Of", "neural networks:Method"], ["ReLU:Method", "Part-Of", "neural networks:Method"]]}
{"doc_id": "146808333", "sentence": "We replace sigmoid function with its piece - wise linear hard analog : ReLU 6 (x+ 3 ) 6 similar to [ 1 1 , 4 4 ] .", "ner": [["sigmoid", "Method"], ["ReLU 6 (x+ 3 ) 6", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "146808333", "sentence": "Similarly , the hard version of swish becomes ReLU 6 (x + 3 ) 6 A similar version of hard - swish was also recently proposed in [ 2 ] .", "ner": [["swish", "Method"], ["ReLU 6 (x + 3 ) 6", "Method"], ["hard - swish", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "146808333", "sentence": "The comparison of the soft and hard version of sigmoid and swish nonlinearities is shown in figure 6 .", "ner": [["sigmoid", "Method"], ["swish nonlinearities", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "146808333", "sentence": "Sigmoid and swish nonlinearities and ther \" hard \" counterparts . a piece - wise function to reduce the number of memory accesses driving the latency cost down substantially . 2 .", "ner": [["Sigmoid", "Method"], ["swish nonlinearities", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "146808333", "sentence": "MobileNetV 3 is defined as two models : MobileNetV 3 - Large and MobileNetV 3 - Small .", "ner": [["MobileNetV 3", "Method"], ["MobileNetV 3 - Large", "Method"], ["MobileNetV 3 - Small", "Method"]], "rel": [["MobileNetV 3 - Large", "Part-Of", "MobileNetV 3"], ["MobileNetV 3 - Small", "Part-Of", "MobileNetV 3"]], "rel_plus": [["MobileNetV 3 - Large:Method", "Part-Of", "MobileNetV 3:Method"], ["MobileNetV 3 - Small:Method", "Part-Of", "MobileNetV 3:Method"]]}
{"doc_id": "146808333", "sentence": "We report results on classification , detection and segmentation .", "ner": [["classification", "Task"], ["detection", "Task"], ["segmentation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "146808333", "sentence": "As has become standard , we use ImageNet [ 3 8 ] for all our classification experiments and compare accuracy ver - sus various measures of resource usage such as latency and multiply adds ( MAdds ) .", "ner": [["ImageNet", "Dataset"], ["classification", "Task"]], "rel": [["ImageNet", "Benchmark-For", "classification"]], "rel_plus": [["ImageNet:Dataset", "Benchmark-For", "classification:Task"]]}
{"doc_id": "146808333", "sentence": "We use dropout of 0. 8 , and l 2 weight decay 1e - 5 and the same image preprocessing as Inception [ 4 2 ] .", "ner": [["dropout", "Method"], ["weight decay", "Method"], ["Inception", "Method"]], "rel": [["weight decay", "Part-Of", "Inception"], ["dropout", "Part-Of", "Inception"]], "rel_plus": [["weight decay:Method", "Part-Of", "Inception:Method"], ["dropout:Method", "Part-Of", "Inception:Method"]]}
{"doc_id": "146808333", "sentence": "Top - 1 accuracy is on ImageNet . lutional layers use batch - normalization layers with average decay of 0. 9 9 .", "ner": [["ImageNet", "Dataset"], ["batch - normalization layers", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "146808333", "sentence": "As can be seen on figure 1 our models outperform the current state of the art such as MnasNet [ 4 3 ] , ProxylessNas [ 5 ] and MobileNetV 2 [ 3 9 ] .", "ner": [["MnasNet", "Method"], ["ProxylessNas", "Method"], ["MobileNetV 2", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "146808333", "sentence": "Note how MobileNetV 3 - Small outperforms the MobileNetV 3 - Large with multiplier scaled to match the performance by nearly 3% .", "ner": [["MobileNetV 3 - Small", "Method"], ["MobileNetV 3 - Large", "Method"]], "rel": [["MobileNetV 3 - Small", "Compare-With", "MobileNetV 3 - Large"]], "rel_plus": [["MobileNetV 3 - Small:Method", "Compare-With", "MobileNetV 3 - Large:Method"]]}
{"doc_id": "146808333", "sentence": "However , it should be noted that resolution is often determined by the problem ( e.g. segmentation and detection problem generally require higher resolution ) , and thus ca n't always be used as a tunable parameter .", "ner": [["segmentation", "Task"], ["detection", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "146808333", "sentence": "In h - swish @N , N denotes the number of channels , in the first layer that has h - swish enabled .", "ner": [["h - swish @N", "Method"], ["h - swish", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "146808333", "sentence": "Optimized h - swish only adds an additional 1ms compared to traditional ReLU .", "ner": [["h - swish", "Method"], ["ReLU", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "146808333", "sentence": "MobileNetV 3 uses h - swish in the middle of the network and clearly dominates ReLU .", "ner": [["MobileNetV 3", "Method"], ["h - swish", "Method"], ["ReLU", "Method"]], "rel": [["h - swish", "Part-Of", "MobileNetV 3"]], "rel_plus": [["h - swish:Method", "Part-Of", "MobileNetV 3:Method"]]}
{"doc_id": "146808333", "sentence": "Impact of h - swish vs ReLU on latency for optimized and non - optimized h - swish .", "ner": [["h - swish", "Method"], ["ReLU", "Method"], ["non - optimized h - swish", "Method"]], "rel": [["h - swish", "Compare-With", "ReLU"]], "rel_plus": [["h - swish:Method", "Compare-With", "ReLU:Method"]]}
{"doc_id": "146808333", "sentence": "Note that placing h - swish at all layers with 8 0 channels or more ( V 3 ) provides the best trade - offs for both optimized hswish and non - optimized h - swish .", "ner": [["h - swish", "Method"], ["h - swish", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "146808333", "sentence": "Top - 1 accuracy is on ImageNet and latency is in ms .   We use MobileNetV 3 as a drop - in replacement for the backbone feature extractor in SSDLite [ 3 9 ] and compare with other backbone networks on COCO dataset [ 2 6 ] .", "ner": [["ImageNet", "Dataset"], ["MobileNetV 3", "Method"], ["SSDLite", "Method"], ["COCO", "Dataset"]], "rel": [["MobileNetV 3", "Part-Of", "SSDLite"], ["SSDLite", "Evaluated-With", "COCO"]], "rel_plus": [["MobileNetV 3:Method", "Part-Of", "SSDLite:Method"], ["SSDLite:Method", "Evaluated-With", "COCO:Dataset"]]}
{"doc_id": "146808333", "sentence": "Following MobileNetV 2 [ 3 9 ] , we attach the first layer of SSDLite to the last feature extractor layer that has an output stride of 1 6 , and attach the second layer of SSDLite to the last feature extractor layer that has an output stride of 3 2 .", "ner": [["MobileNetV 2", "Method"], ["SSDLite", "Method"], ["SSDLite", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "146808333", "sentence": "This is because the last few layers of MobileNetV 3 are tuned to output 1 0 0 0 classes , which may be redundant when transferred to COCO with 9 0 classes .", "ner": [["MobileNetV 3", "Method"], ["COCO", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "146808333", "sentence": "With the channel reduction , MobileNetV 3 - Large is 2 7 % faster than MobileNetV 2 with near identical mAP .", "ner": [["MobileNetV 3 - Large", "Method"], ["MobileNetV 2", "Method"]], "rel": [["MobileNetV 3 - Large", "Compare-With", "MobileNetV 2"]], "rel_plus": [["MobileNetV 3 - Large:Method", "Compare-With", "MobileNetV 2:Method"]]}
{"doc_id": "146808333", "sentence": "MobileNetV 3 - Small with channel reduction is also 2. 4 and 0. 5 mAP   In this subsection , we employ MobileNetV 2 [ 3 9 ] and the proposed MobileNetV 3 as network backbones for the task of mobile semantic segmentation .", "ner": [["MobileNetV 3 - Small", "Method"], ["MobileNetV 2", "Method"], ["MobileNetV 3", "Method"], ["semantic segmentation", "Task"]], "rel": [["MobileNetV 2", "Used-For", "semantic segmentation"], ["MobileNetV 3", "Used-For", "semantic segmentation"]], "rel_plus": [["MobileNetV 2:Method", "Used-For", "semantic segmentation:Task"], ["MobileNetV 3:Method", "Used-For", "semantic segmentation:Task"]]}
{"doc_id": "146808333", "sentence": "R - ASPP is a reduced design of the Atrous Spatial Pyramid Pooling module [ 7 , 8 , 9 ] , which adopts only two branches consisting of a 1 \u00d7 1 convolution and a global - average pooling operation [ 2 9 , 5 0 ] .", "ner": [["R - ASPP", "Method"], ["Atrous Spatial Pyramid Pooling module", "Method"], ["1 \u00d7 1 convolution", "Method"], ["global - average pooling operation", "Method"]], "rel": [["1 \u00d7 1 convolution", "Part-Of", "R - ASPP"], ["global - average pooling operation", "Part-Of", "R - ASPP"], ["R - ASPP", "SubClass-Of", "Atrous Spatial Pyramid Pooling module"]], "rel_plus": [["1 \u00d7 1 convolution:Method", "Part-Of", "R - ASPP:Method"], ["global - average pooling operation:Method", "Part-Of", "R - ASPP:Method"], ["R - ASPP:Method", "SubClass-Of", "Atrous Spatial Pyramid Pooling module:Method"]]}
{"doc_id": "146808333", "sentence": "In this work , we propose another light - weight segmentation head , referred to as Lite R - ASPP ( or LR - ASPP ) , as shown in Fig. 1 0 .", "ner": [["Lite R - ASPP", "Method"], ["LR - ASPP", "Method"]], "rel": [["LR - ASPP", "Synonym-Of", "Lite R - ASPP"]], "rel_plus": [["LR - ASPP:Method", "Synonym-Of", "Lite R - ASPP:Method"]]}
{"doc_id": "146808333", "sentence": "Lite R - ASPP , improving over R - ASPP , deploys the global - average pooling in a fashion similar to the Squeeze - and - Excitation module [ 2 0 ] , in which we employ a large pooling kernel with a large stride ( to save some computation ) and only one 1 \u00d7 1 convolution in the module .", "ner": [["Lite R - ASPP", "Method"], ["R - ASPP", "Method"], ["global - average pooling", "Method"], ["pooling kernel", "Method"], ["1 \u00d7 1 convolution", "Method"]], "rel": [["global - average pooling", "Part-Of", "Lite R - ASPP"], ["Lite R - ASPP", "SubClass-Of", "R - ASPP"], ["pooling kernel", "Part-Of", "global - average pooling"], ["1 \u00d7 1 convolution", "Part-Of", "pooling kernel"]], "rel_plus": [["global - average pooling:Method", "Part-Of", "Lite R - ASPP:Method"], ["Lite R - ASPP:Method", "SubClass-Of", "R - ASPP:Method"], ["pooling kernel:Method", "Part-Of", "global - average pooling:Method"], ["1 \u00d7 1 convolution:Method", "Part-Of", "pooling kernel:Method"]]}
{"doc_id": "146808333", "sentence": "We think it is because the backbone is designed for 1 0 0 0 classes ImageNet image classification [ 3 8 ] while there are only 1 9 classes on Cityscapes , implying there is some channel redundancy in the backbone .", "ner": [["ImageNet", "Dataset"], ["image classification", "Task"], ["Cityscapes", "Dataset"]], "rel": [["ImageNet", "Benchmark-For", "image classification"]], "rel_plus": [["ImageNet:Dataset", "Benchmark-For", "image classification:Task"]]}
{"doc_id": "146808333", "sentence": "As shown in the table , we observe that ( 1 ) reducing the channels in the last block of network backbone by a factor of 2 significantly improves the speed while maintaining similar performances ( row 1 vs. row 2 , and row 5 vs. row 6 ) , ( 2 ) the proposed segmentation head LR - ASPP is slightly faster than R - ASPP [ 3 9 ] while performance is improved ( row 2 vs. row 3 , and row 6 vs. row 7 ) , ( 3 ) reducing the filters in the segmentation head from 2 5 6 to 1 2 8 improves the speed at the cost of slightly worse performance ( row 3 vs. row 4 , and row 7 vs. row 8) ,", "ner": [["LR - ASPP", "Method"], ["R - ASPP", "Method"]], "rel": [["LR - ASPP", "Compare-With", "R - ASPP"]], "rel_plus": [["LR - ASPP:Method", "Compare-With", "R - ASPP:Method"]]}
{"doc_id": "146808333", "sentence": "( 4 ) when employing the same setting , MobileNetV 3 model variants attain similar performance while being slightly faster than MobileNetV 2 counterparts ( row 1 vs. row 5 , row 2 vs. row 6 , row 3 vs. row 7 , and row 4 vs. row 8) , ( 5 ) MobileNetV 3 - Small attains similar performance as MobileNetV 2 - 0 . 5 while being faster , and ( 6 ) MobileNetV 3 - Small is significantly better than MobileNetV 2 - 0 . 3 5 while yielding similar speed .", "ner": [["MobileNetV 3", "Method"], ["MobileNetV 2", "Method"], ["MobileNetV 3 - Small", "Method"], ["MobileNetV 2", "Method"], ["MobileNetV 3 - Small", "Method"], ["MobileNetV 2", "Method"]], "rel": [["MobileNetV 3", "Compare-With", "MobileNetV 2"], ["MobileNetV 3 - Small", "Compare-With", "MobileNetV 2"], ["MobileNetV 3 - Small", "Compare-With", "MobileNetV 2"]], "rel_plus": [["MobileNetV 3:Method", "Compare-With", "MobileNetV 2:Method"], ["MobileNetV 3 - Small:Method", "Compare-With", "MobileNetV 2:Method"], ["MobileNetV 3 - Small:Method", "Compare-With", "MobileNetV 2:Method"]]}
{"doc_id": "146808333", "sentence": "Our segmentation models with MobileNetV 3 as network backbone outperforms ESPNetv 2 [ 3 2 ] , CCC 2 [ 3 4 ] , and ESPNetv 1 [ 3 2 ] by 6. 4 % , 1 0 . 6 % , 1 2 . 3 % , respectively while being faster in terms of MAdds .", "ner": [["MobileNetV 3", "Method"], ["ESPNetv 2", "Method"], ["CCC 2", "Method"], ["ESPNetv 1", "Method"]], "rel": [["MobileNetV 3", "Compare-With", "ESPNetv 2"], ["MobileNetV 3", "Compare-With", "CCC 2"], ["MobileNetV 3", "Compare-With", "ESPNetv 1"]], "rel_plus": [["MobileNetV 3:Method", "Compare-With", "ESPNetv 2:Method"], ["MobileNetV 3:Method", "Compare-With", "CCC 2:Method"], ["MobileNetV 3:Method", "Compare-With", "ESPNetv 1:Method"]]}
{"doc_id": "146808333", "sentence": "The performance drops slightly by 0. 6 % when not employing the atrous convolution to extract dense feature maps in the last block of MobileNetV 3 , but the speed is improved to 1. 9 8 B ( for half - resolution inputs ) , which is 1. 3 6 , 1. 5 9 , and 2. 2 7 times faster than ESPNetv 2 , CCC 2 , and ESPNetv 1 , respectively .", "ner": [["MobileNetV 3", "Method"], ["ESPNetv 2", "Method"], ["CCC 2", "Method"], ["ESPNetv 1", "Method"]], "rel": [["MobileNetV 3", "Compare-With", "ESPNetv 2"], ["MobileNetV 3", "Compare-With", "CCC 2"], ["MobileNetV 3", "Compare-With", "ESPNetv 1"]], "rel_plus": [["MobileNetV 3:Method", "Compare-With", "ESPNetv 2:Method"], ["MobileNetV 3:Method", "Compare-With", "CCC 2:Method"], ["MobileNetV 3:Method", "Compare-With", "ESPNetv 1:Method"]]}
{"doc_id": "146808333", "sentence": "In this paper we introduced MobileNetV 3 Large and Small models demonstrating new state of the art in mobile classification , detection and segmentation .", "ner": [["MobileNetV 3 Large and Small", "Method"], ["mobile classification", "Task"], ["detection", "Task"], ["segmentation", "Task"]], "rel": [["MobileNetV 3 Large and Small", "Used-For", "mobile classification"], ["MobileNetV 3 Large and Small", "Used-For", "detection"], ["MobileNetV 3 Large and Small", "Used-For", "segmentation"]], "rel_plus": [["MobileNetV 3 Large and Small:Method", "Used-For", "mobile classification:Task"], ["MobileNetV 3 Large and Small:Method", "Used-For", "detection:Task"], ["MobileNetV 3 Large and Small:Method", "Used-For", "segmentation:Task"]]}
{"doc_id": "146808333", "sentence": "We have also shown how to adapt nonlinearities like swish and apply squeeze and excite in a quantization friendly and ef - ficient manner introducing them into the mobile model domain as effective tools .", "ner": [["nonlinearities", "Method"], ["swish", "Method"]], "rel": [["swish", "SubClass-Of", "nonlinearities"]], "rel_plus": [["swish:Method", "SubClass-Of", "nonlinearities:Method"]]}
{"doc_id": "146808333", "sentence": "We also introduced a new form of lightweight segmentation decoders called LR - ASPP .", "ner": [["LR - ASPP", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "209386851", "sentence": "From this vantage , we present the PointRend ( Point - based Rendering ) neural network module : a module that performs point - based segmentation predictions at adaptively selected locations based on an iterative subdivision algorithm .", "ner": [["PointRend", "Method"], ["Point - based Rendering", "Method"], ["point - based segmentation", "Task"]], "rel": [["PointRend", "Synonym-Of", "Point - based Rendering"], ["PointRend", "Used-For", "point - based segmentation"]], "rel_plus": [["PointRend:Method", "Synonym-Of", "Point - based Rendering:Method"], ["PointRend:Method", "Used-For", "point - based segmentation:Task"]]}
{"doc_id": "209386851", "sentence": "PointRend can be flexibly applied to both instance and semantic segmentation tasks by building on top of existing state - of - the - art models .", "ner": [["PointRend", "Method"], ["instance and semantic segmentation", "Task"]], "rel": [["PointRend", "Used-For", "instance and semantic segmentation"]], "rel_plus": [["PointRend:Method", "Used-For", "instance and semantic segmentation:Task"]]}
{"doc_id": "209386851", "sentence": "Quantitatively , PointRend yields significant gains on COCO and Cityscapes , for both instance and semantic segmentation .", "ner": [["PointRend", "Method"], ["COCO", "Dataset"], ["Cityscapes", "Dataset"], ["instance and semantic segmentation", "Task"]], "rel": [["PointRend", "Evaluated-With", "COCO"], ["PointRend", "Evaluated-With", "Cityscapes"], ["PointRend", "Used-For", "instance and semantic segmentation"]], "rel_plus": [["PointRend:Method", "Evaluated-With", "COCO:Dataset"], ["PointRend:Method", "Evaluated-With", "Cityscapes:Dataset"], ["PointRend:Method", "Used-For", "instance and semantic segmentation:Task"]]}
{"doc_id": "209386851", "sentence": "The modern tools of choice for these tasks are built on convolutional neural networks ( CNNs ) [ 2 7 , 2 6 ] .", "ner": [["convolutional neural networks", "Method"], ["CNNs", "Method"]], "rel": [["CNNs", "Synonym-Of", "convolutional neural networks"]], "rel_plus": [["CNNs:Method", "Synonym-Of", "convolutional neural networks:Method"]]}
{"doc_id": "209386851", "sentence": "CNNs for image segmentation typically operate on regular grids : the input image is a regular grid of pixels , their hidden representations are feature vectors on a regular grid , and their outputs are label maps on a regular grid .", "ner": [["CNNs", "Method"], ["image segmentation", "Task"]], "rel": [["CNNs", "Used-For", "image segmentation"]], "rel_plus": [["CNNs:Method", "Used-For", "image segmentation:Task"]]}
{"doc_id": "209386851", "sentence": "The label maps predicted by these networks should be mostly smooth , i.e. , neighboring pixels often take the same label , because high - 5 6 \u00d7 5 6 1 1 2 \u00d7 1 1 2 2 2 4 \u00d7 2 2 4 Mask R - CNN + PointRend 2 8 \u00d7 2 8 2 8 \u00d7 2 8 2 2 4 \u00d7 2 2 4 Figure 1 : Instance segmentation with PointRend .", "ner": [["Mask R - CNN + PointRend", "Method"], ["Instance segmentation", "Task"], ["PointRend", "Method"]], "rel": [["PointRend", "Used-For", "Instance segmentation"]], "rel_plus": [["PointRend:Method", "Used-For", "Instance segmentation:Task"]]}
{"doc_id": "209386851", "sentence": "We introduce the PointRend ( Point - based Rendering ) module that makes predictions at adaptively sampled points on the image using a new pointbased feature representation ( see Fig. 3 ) .", "ner": [["PointRend", "Method"], ["Point - based Rendering", "Method"]], "rel": [["PointRend", "Synonym-Of", "Point - based Rendering"]], "rel_plus": [["PointRend:Method", "Synonym-Of", "Point - based Rendering:Method"]]}
{"doc_id": "209386851", "sentence": "PointRend is general and can be flexibly integrated into existing semantic and instance segmentation systems .", "ner": [["PointRend", "Method"], ["semantic and instance segmentation", "Task"]], "rel": [["PointRend", "Used-For", "semantic and instance segmentation"]], "rel_plus": [["PointRend:Method", "Used-For", "semantic and instance segmentation:Task"]]}
{"doc_id": "209386851", "sentence": "When used to replace Mask R - CNN 's default mask head [ 1 9 ] ( top - left ) , PointRend yields significantly more detailed results ( top - right ) . ( bottom ) During inference , PointRend iterative computes its prediction .", "ner": [["Mask R - CNN", "Method"], ["mask head", "Method"], ["PointRend", "Method"], ["PointRend", "Method"]], "rel": [["PointRend", "Part-Of", "Mask R - CNN"], ["mask head", "Part-Of", "Mask R - CNN"]], "rel_plus": [["PointRend:Method", "Part-Of", "Mask R - CNN:Method"], ["mask head:Method", "Part-Of", "Mask R - CNN:Method"]]}
{"doc_id": "209386851", "sentence": "Image segmentation methods often predict labels on a low - resolution regular grid , e.g. , 1/ 8 - th of the input [ 3 5 ] for semantic segmentation , or 2 8 \u00d7 2 8 [ 1 9 ] for instance segmentation , as a compromise between undersampling and oversampling .", "ner": [["Image segmentation", "Task"], ["semantic segmentation", "Task"], ["instance segmentation", "Task"]], "rel": [["Image segmentation", "Used-For", "semantic segmentation"], ["Image segmentation", "Used-For", "instance segmentation"]], "rel_plus": [["Image segmentation:Task", "Used-For", "semantic segmentation:Task"], ["Image segmentation:Task", "Used-For", "instance segmentation:Task"]]}
{"doc_id": "209386851", "sentence": "In - Figure 2 : Example result pairs from Mask R - CNN [ 1 9 ] with its standard mask head ( left image ) vs. with PointRend ( right image ) , using ResNet - 5 0 [ 2 0 ] with FPN [ 2 8 ] .", "ner": [["Mask R - CNN", "Method"], ["mask head", "Method"], ["PointRend", "Method"], ["ResNet - 5 0", "Method"], ["FPN", "Method"]], "rel": [["PointRend", "Part-Of", "Mask R - CNN"], ["mask head", "Part-Of", "Mask R - CNN"], ["ResNet - 5 0", "Part-Of", "FPN"], ["PointRend", "Part-Of", "FPN"]], "rel_plus": [["PointRend:Method", "Part-Of", "Mask R - CNN:Method"], ["mask head:Method", "Part-Of", "Mask R - CNN:Method"], ["ResNet - 5 0:Method", "Part-Of", "FPN:Method"], ["PointRend:Method", "Part-Of", "FPN:Method"]]}
{"doc_id": "209386851", "sentence": "PointRend can be incorporated into popular meta - architectures for both instance segmentation ( e.g. , Mask R - CNN [ 1 9 ] ) and semantic segmentation ( e.g. , FCN [ 3 5 ] ) .", "ner": [["PointRend", "Method"], ["instance segmentation", "Task"], ["Mask R - CNN", "Method"], ["semantic segmentation", "Task"], ["FCN", "Method"]], "rel": [["Mask R - CNN", "Used-For", "instance segmentation"], ["PointRend", "Used-For", "instance segmentation"], ["PointRend", "Used-For", "semantic segmentation"], ["PointRend", "Part-Of", "FCN"]], "rel_plus": [["Mask R - CNN:Method", "Used-For", "instance segmentation:Task"], ["PointRend:Method", "Used-For", "instance segmentation:Task"], ["PointRend:Method", "Used-For", "semantic segmentation:Task"], ["PointRend:Method", "Part-Of", "FCN:Method"]]}
{"doc_id": "209386851", "sentence": "Viewed abstractly , a PointRend module accepts one or more typical CNN feature maps f ( x i , y i ) that are defined over regular grids , and outputs high - resolution predictions p(x i , y i ) over a finer grid .", "ner": [["PointRend", "Method"], ["CNN", "Method"]], "rel": [["PointRend", "Part-Of", "CNN"]], "rel_plus": [["PointRend:Method", "Part-Of", "CNN:Method"]]}
{"doc_id": "209386851", "sentence": "We evaluate PointRend on instance and semantic segmentation tasks using the COCO [ 2 9 ] and Cityscapes [ 9 ] benchmarks .", "ner": [["PointRend", "Method"], ["instance and semantic segmentation", "Task"], ["COCO", "Dataset"], ["Cityscapes", "Dataset"]], "rel": [["PointRend", "Used-For", "instance and semantic segmentation"], ["COCO", "Benchmark-For", "instance and semantic segmentation"], ["Cityscapes", "Benchmark-For", "instance and semantic segmentation"]], "rel_plus": [["PointRend:Method", "Used-For", "instance and semantic segmentation:Task"], ["COCO:Dataset", "Benchmark-For", "instance and semantic segmentation:Task"], ["Cityscapes:Dataset", "Benchmark-For", "instance and semantic segmentation:Task"]]}
{"doc_id": "209386851", "sentence": "PointRend improves strong Mask R - CNN and DeepLabV 3 [ 5 ] models by a significant margin .", "ner": [["PointRend", "Method"], ["Mask R - CNN", "Method"], ["DeepLabV 3", "Method"]], "rel": [["PointRend", "Part-Of", "Mask R - CNN"], ["PointRend", "Part-Of", "DeepLabV 3"]], "rel_plus": [["PointRend:Method", "Part-Of", "Mask R - CNN:Method"], ["PointRend:Method", "Part-Of", "DeepLabV 3:Method"]]}
{"doc_id": "209386851", "sentence": "Efficient procedures like subdivision [ 4 8 ] and adaptive sampling [ 3 8 , 4 2 ] refine a coarse rasterization in areas where pixel values have larger variance .", "ner": [["subdivision", "Method"], ["adaptive sampling", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "209386851", "sentence": "Similar to a signed distance function , PointRend can compute segmentation values at any point .", "ner": [["PointRend", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "209386851", "sentence": "Recently , Marin et al. [ 3 6 ] propose an efficient semantic segmentation network based on non - uniform subsampling of the input image prior to processing with a standard semantic segmentation network .", "ner": [["semantic segmentation", "Task"], ["non - uniform subsampling", "Method"], ["semantic segmentation", "Task"]], "rel": [["non - uniform subsampling", "Used-For", "semantic segmentation"]], "rel_plus": [["non - uniform subsampling:Method", "Used-For", "semantic segmentation:Task"]]}
{"doc_id": "209386851", "sentence": "Instance segmentation methods based on the Mask R - CNN meta - architecture [ 1 9 ] occupy top ranks in recent challenges [ 3 2 , 3 ] .", "ner": [["Instance segmentation", "Task"], ["Mask R - CNN", "Method"]], "rel": [["Mask R - CNN", "Used-For", "Instance segmentation"]], "rel_plus": [["Mask R - CNN:Method", "Used-For", "Instance segmentation:Task"]]}
{"doc_id": "209386851", "sentence": "In this paper , we show that a region - based segmentation model equipped with PointRend can produce masks with fine - level details while improving the accuracy of region - based approaches .", "ner": [["region - based segmentation model", "Method"], ["PointRend", "Method"]], "rel": [["PointRend", "Part-Of", "region - based segmentation model"]], "rel_plus": [["PointRend:Method", "Part-Of", "region - based segmentation model:Method"]]}
{"doc_id": "209386851", "sentence": "Fully convolutional networks ( FCNs ) [ 3 5 ] are the foundation of modern semantic segmentation approaches .", "ner": [["Fully convolutional networks", "Method"], ["FCNs", "Method"], ["semantic segmentation", "Task"]], "rel": [["FCNs", "Synonym-Of", "Fully convolutional networks"], ["Fully convolutional networks", "Used-For", "semantic segmentation"]], "rel_plus": [["FCNs:Method", "Synonym-Of", "Fully convolutional networks:Method"], ["Fully convolutional networks:Method", "Used-For", "semantic segmentation:Task"]]}
{"doc_id": "209386851", "sentence": "Alternative approaches include encoder - decoder achitectures [ 6 , 2 4 , 4 4 , 4 5 ] that subsample the grid representation in the encoder and then upsample it in the decoder , using skip connections [ 4 4 ] to recover filtered details .", "ner": [["encoder - decoder achitectures", "Method"], ["skip connections", "Method"]], "rel": [["skip connections", "Part-Of", "encoder - decoder achitectures"]], "rel_plus": [["skip connections:Method", "Part-Of", "encoder - decoder achitectures:Method"]]}
{"doc_id": "209386851", "sentence": "Figure 3 : PointRend applied to instance segmentation .", "ner": [["PointRend", "Method"], ["instance segmentation", "Task"]], "rel": [["PointRend", "Used-For", "instance segmentation"]], "rel_plus": [["PointRend:Method", "Used-For", "instance segmentation:Task"]]}
{"doc_id": "209386851", "sentence": "A standard network for instance segmentation ( solid red arrows ) takes an input image and yields a coarse ( e.g. 7 \u00d7 7 ) mask prediction for each detected object ( red box ) using a lightweight segmentation head .", "ner": [["instance segmentation", "Task"], ["lightweight segmentation head", "Method"]], "rel": [["lightweight segmentation head", "Used-For", "instance segmentation"]], "rel_plus": [["lightweight segmentation head:Method", "Used-For", "instance segmentation:Task"]]}
{"doc_id": "209386851", "sentence": "To refine the coarse mask , PointRend selects a set of points ( red dots ) and makes prediction for each point independently with a small MLP .", "ner": [["PointRend", "Method"], ["MLP", "Method"]], "rel": [["MLP", "Part-Of", "PointRend"]], "rel_plus": [["MLP:Method", "Part-Of", "PointRend:Method"]]}
{"doc_id": "209386851", "sentence": "The MLP uses interpolated features computed at these points ( dashed red arrows ) from ( 1 ) a fine - grained feature map of the backbone CNN and ( 2 ) from the coarse prediction mask .", "ner": [["MLP", "Method"], ["CNN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "209386851", "sentence": "We analogize image segmentation ( of objects and/or scenes ) in computer vision to image rendering in computer graphics .", "ner": [["image segmentation", "Task"], ["computer vision", "Task"]], "rel": [["image segmentation", "SubTask-Of", "computer vision"]], "rel_plus": [["image segmentation:Task", "SubTask-Of", "computer vision:Task"]]}
{"doc_id": "209386851", "sentence": "Analogously , in computer vision , we can think of an image segmentation as the occupancy map of an underlying continuous entity , and the segmentation output , which is a regular grid of predicted labels , is \" rendered \" from it .", "ner": [["computer vision", "Task"], ["image segmentation", "Task"]], "rel": [["image segmentation", "SubTask-Of", "computer vision"]], "rel_plus": [["image segmentation:Task", "SubTask-Of", "computer vision:Task"]]}
{"doc_id": "209386851", "sentence": "Based on this analogy , we propose PointRend ( Pointbased Rendering ) as a methodology for image segmentation using point representations .", "ner": [["PointRend", "Method"], ["Pointbased Rendering", "Method"], ["image segmentation", "Task"]], "rel": [["PointRend", "Synonym-Of", "Pointbased Rendering"], ["PointRend", "Used-For", "image segmentation"]], "rel_plus": [["PointRend:Method", "Synonym-Of", "Pointbased Rendering:Method"], ["PointRend:Method", "Used-For", "image segmentation:Task"]]}
{"doc_id": "209386851", "sentence": "A PointRend module accepts one or more typical CNN feature maps of C channels f \u2208 R C \u00d7 H \u00d7 W , each defined over a regular grid ( that is typically 4 \u00d7 to 1 6 \u00d7 coarser than the image grid ) , and outputs predictions for the K class labels p \u2208 R K \u00d7 H \u00d7W over a regular grid of different ( and likely higher ) resolution .", "ner": [["PointRend", "Method"], ["CNN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "209386851", "sentence": "The PointRend architecture can be applied to instance segmentation ( e.g. , on Mask R - CNN [ 1 9 ] ) and semantic segmentation ( e.g. , on FCNs [ 3 5 ] ) tasks .", "ner": [["PointRend", "Method"], ["instance segmentation", "Task"], ["Mask R - CNN", "Method"], ["semantic segmentation", "Task"], ["FCNs", "Method"]], "rel": [["PointRend", "Used-For", "instance segmentation"], ["Mask R - CNN", "Used-For", "instance segmentation"], ["PointRend", "Part-Of", "Mask R - CNN"], ["PointRend", "Used-For", "semantic segmentation"], ["FCNs", "Used-For", "semantic segmentation"], ["PointRend", "Part-Of", "FCNs"]], "rel_plus": [["PointRend:Method", "Used-For", "instance segmentation:Task"], ["Mask R - CNN:Method", "Used-For", "instance segmentation:Task"], ["PointRend:Method", "Part-Of", "Mask R - CNN:Method"], ["PointRend:Method", "Used-For", "semantic segmentation:Task"], ["FCNs:Method", "Used-For", "semantic segmentation:Task"], ["PointRend:Method", "Part-Of", "FCNs:Method"]]}
{"doc_id": "209386851", "sentence": "For instance segmentation , PointRend is applied to each region .", "ner": [["instance segmentation", "Task"], ["PointRend", "Method"]], "rel": [["PointRend", "Used-For", "instance segmentation"]], "rel_plus": [["PointRend:Method", "Used-For", "instance segmentation:Task"]]}
{"doc_id": "209386851", "sentence": "For semantic segmentation , the whole image can be considered as a single region , and thus without loss of generality we will describe PointRend in the context of instance segmentation .", "ner": [["semantic segmentation", "Task"], ["PointRend", "Method"], ["instance segmentation", "Task"]], "rel": [["PointRend", "Used-For", "instance segmentation"]], "rel_plus": [["PointRend:Method", "Used-For", "instance segmentation:Task"]]}
{"doc_id": "209386851", "sentence": "This design is similar to the parallel training of RPN + Fast R - CNN in a Faster R - CNN system [ 1 3 ] , whose inference is sequential .", "ner": [["RPN + Fast R - CNN", "Method"], ["Faster R - CNN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "209386851", "sentence": "To allow PointRend to render fine segmentation details we extract a feature vector at each sampled point from CNN feature maps .", "ner": [["PointRend", "Method"], ["CNN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "209386851", "sentence": "Features can be extracted from a single feature map ( e.g. , res 2 in a ResNet ) ; they can also be extracted from multiple feature maps ( e.g. , res 2 to res 5 , or their feature pyramid [ 2 8 ] counterparts ) and concatenated , following the Hypercolumn method [ 1 7 ] .", "ner": [["ResNet", "Method"], ["feature pyramid", "Method"], ["Hypercolumn method", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "209386851", "sentence": "For instance segmentation , the coarse prediction can be , for example , the output of a lightweight 7 \u00d7 7 resolution mask head in Mask R - CNN .", "ner": [["instance segmentation", "Task"], ["Mask R - CNN", "Method"]], "rel": [["Mask R - CNN", "Used-For", "instance segmentation"]], "rel_plus": [["Mask R - CNN:Method", "Used-For", "instance segmentation:Task"]]}
{"doc_id": "209386851", "sentence": "Given the point - wise feature representation at each selected point , PointRend makes point - wise segmentation predictions using a simple multi - layer perceptron ( MLP ) .", "ner": [["PointRend", "Method"], ["point - wise segmentation predictions", "Task"], ["multi - layer perceptron", "Method"], ["MLP", "Method"]], "rel": [["multi - layer perceptron", "Part-Of", "PointRend"], ["multi - layer perceptron", "Used-For", "point - wise segmentation predictions"], ["MLP", "Synonym-Of", "multi - layer perceptron"]], "rel_plus": [["multi - layer perceptron:Method", "Part-Of", "PointRend:Method"], ["multi - layer perceptron:Method", "Used-For", "point - wise segmentation predictions:Task"], ["MLP:Method", "Synonym-Of", "multi - layer perceptron:Method"]]}
{"doc_id": "209386851", "sentence": "This MLP shares weights across all points ( and all regions ) , analogous to a graph convolution [ 2 3 ] or a PointNet [ 4 3 ] .", "ner": [["MLP", "Method"], ["graph convolution", "Method"], ["PointNet", "Method"]], "rel": [["MLP", "Compare-With", "graph convolution"], ["MLP", "Compare-With", "PointNet"]], "rel_plus": [["MLP:Method", "Compare-With", "graph convolution:Method"], ["MLP:Method", "Compare-With", "PointNet:Method"]]}
{"doc_id": "209386851", "sentence": "We use two standard instance segmentation datasets : COCO [ 2 9 ] and Cityscapes [ 9 ] .", "ner": [["instance segmentation", "Task"], ["COCO", "Dataset"], ["Cityscapes", "Dataset"]], "rel": [["COCO", "Benchmark-For", "instance segmentation"], ["Cityscapes", "Benchmark-For", "instance segmentation"]], "rel_plus": [["COCO:Dataset", "Benchmark-For", "instance segmentation:Task"], ["Cityscapes:Dataset", "Benchmark-For", "instance segmentation:Task"]]}
{"doc_id": "209386851", "sentence": "We report the standard mask AP metric [ 2 9 ] using the median of 3 runs for COCO and 5 for Cityscapes ( it has higher variance ) .", "ner": [["COCO", "Dataset"], ["Cityscapes", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "209386851", "sentence": "Therefore we supplement COCO results with AP measured using the 8 0 COCO category subset of LVIS [ 1 6 ] , denoted by AP .", "ner": [["COCO", "Dataset"], ["COCO", "Dataset"], ["LVIS", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "209386851", "sentence": "Note that for AP we use the same models trained on COCO and simply re - evaluate their predictions against the higherquality LVIS annotations using the LVIS evaluation API .", "ner": [["COCO", "Dataset"], ["LVIS", "Dataset"], ["LVIS", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "209386851", "sentence": "ResNet - 5 0 [ 2 0 ] + FPN [ 2 8 ] backbone .", "ner": [["ResNet - 5 0", "Method"], ["FPN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "209386851", "sentence": "The default mask head in Mask R - CNN is a region - wise FCN , which we denote by \" 4 \u00d7 conv \" . 2 We use this as our baseline for comparison .", "ner": [["Mask R - CNN", "Method"], ["region - wise FCN", "Method"]], "rel": [["Mask R - CNN", "SubClass-Of", "region - wise FCN"]], "rel_plus": [["Mask R - CNN:Method", "SubClass-Of", "region - wise FCN:Method"]]}
{"doc_id": "209386851", "sentence": "Next , we use a stride - two 2 \u00d7 2 convolution layer with 2 5 6 output channels followed by ReLU [ 3 9 ] , which reduces the spatial size to 7 \u00d7 7 .", "ner": [["2 \u00d7 2 convolution", "Method"], ["ReLU", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "209386851", "sentence": "Finally , similar to Mask R - CNN 's box head , an MLP with two 1 0 2 4 - wide hidden layers is applied to yield a 7 \u00d7 7 mask prediction for each of the K classes .", "ner": [["Mask R - CNN", "Method"], ["MLP", "Method"]], "rel": [["Mask R - CNN", "Compare-With", "MLP"]], "rel_plus": [["Mask R - CNN:Method", "Compare-With", "MLP:Method"]]}
{"doc_id": "209386851", "sentence": "ReLU is used on the MLP 's hidden layers and the sigmoid activation function is applied to its outputs .", "ner": [["ReLU", "Method"], ["MLP 's", "Method"], ["sigmoid activation", "Method"]], "rel": [["ReLU", "Part-Of", "MLP 's"], ["sigmoid activation", "Part-Of", "MLP 's"]], "rel_plus": [["ReLU:Method", "Part-Of", "MLP 's:Method"], ["sigmoid activation:Method", "Part-Of", "MLP 's:Method"]]}
{"doc_id": "209386851", "sentence": "PointRend also interpolates a 2 5 6 - dimensional feature vector from the P 2 level of the FPN .", "ner": [["PointRend", "Method"], ["FPN", "Method"]], "rel": [["PointRend", "Part-Of", "FPN"]], "rel_plus": [["PointRend:Method", "Part-Of", "FPN:Method"]]}
{"doc_id": "209386851", "sentence": "We use ReLU inside the MLP and apply sigmoid to its output .", "ner": [["ReLU", "Method"], ["MLP", "Method"], ["sigmoid", "Method"]], "rel": [["ReLU", "Part-Of", "MLP"], ["sigmoid", "Part-Of", "MLP"]], "rel_plus": [["ReLU:Method", "Part-Of", "MLP:Method"], ["sigmoid:Method", "Part-Of", "MLP:Method"]]}
{"doc_id": "209386851", "sentence": "We use the standard 1 \u00d7 training schedule and data augmentation from Detectron 2 [ 4 9 ] by default ( full details are in the appendix ) .", "ner": [["data augmentation", "Method"], ["Detectron 2", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "209386851", "sentence": "We found that training as a cascade does not improve the baseline Mask R - CNN , but PointRend can benefit from it by sampling points inside more accurate boxes , slightly improving overall performance ( \u223c 0 . 2 % AP , absolute ) .", "ner": [["Mask R - CNN", "Method"], ["PointRend", "Method"]], "rel": [["PointRend", "Part-Of", "Mask R - CNN"]], "rel_plus": [["PointRend:Method", "Part-Of", "Mask R - CNN:Method"]]}
{"doc_id": "209386851", "sentence": "We compare PointRend to the default 4 \u00d7 conv head in Mask R - CNN in [ 1 9 ] .", "ner": [["PointRend", "Method"], ["4 \u00d7 conv head", "Method"], ["Mask R - CNN", "Method"]], "rel": [["4 \u00d7 conv head", "Part-Of", "Mask R - CNN"], ["PointRend", "Part-Of", "Mask R - CNN"]], "rel_plus": [["4 \u00d7 conv head:Method", "Part-Of", "Mask R - CNN:Method"], ["PointRend:Method", "Part-Of", "Mask R - CNN:Method"]]}
{"doc_id": "209386851", "sentence": "AP is COCO mask AP evaluated against the higher - quality LVIS annotations [ 1 6 ] ( see text for details ) .", "ner": [["COCO", "Dataset"], ["LVIS", "Dataset"]], "rel": [["COCO", "Compare-With", "LVIS"]], "rel_plus": [["COCO:Dataset", "Compare-With", "LVIS:Dataset"]]}
{"doc_id": "209386851", "sentence": "A ResNet - 5 0 - FPN backbone is used for both COCO and Cityscapes models .", "ner": [["ResNet - 5 0 - FPN", "Method"], ["COCO", "Dataset"], ["Cityscapes", "Dataset"]], "rel": [["ResNet - 5 0 - FPN", "Used-For", "COCO"], ["ResNet - 5 0 - FPN", "Used-For", "Cityscapes"]], "rel_plus": [["ResNet - 5 0 - FPN:Method", "Used-For", "COCO:Dataset"], ["ResNet - 5 0 - FPN:Method", "Used-For", "Cityscapes:Dataset"]]}
{"doc_id": "209386851", "sentence": "Higher output resolution leads to more detailed predictions , see Fig. 2 ing the COCO categories using the LVIS annotations ( AP ) and for Cityscapes , which we attribute to the superior annotation quality in these datasets .", "ner": [["COCO", "Dataset"], ["LVIS", "Dataset"], ["Cityscapes", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "209386851", "sentence": "Subdivision inference allows PointRend to yield a high resolution 2 2 4 \u00d7 2 2 4 prediction using more than 3 0 times less compute ( FLOPs ) and memory than the default 4 \u00d7 conv head needs to output the same resolution ( based on taking a 1 1 2 \u00d7 1 1 2 RoIAlign input ) , see Table 3 : Subdivision inference parameters .", "ner": [["PointRend", "Method"], ["RoIAlign", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "209386851", "sentence": "AP is COCO mask AP evaluated against the higherquality LVIS annotations [ 1 6 ] ( see text for details ) . 1 1 2 \u00d7 1 1 2 2 2 4 \u00d7 2 2 4 2 8 \u00d7 2 8 Figure 7 : Anti - aliasing with PointRend .", "ner": [["COCO", "Dataset"], ["LVIS", "Dataset"], ["PointRend", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "209386851", "sentence": "In terms of wall - clock runtime , our unoptimized implementation outputs 2 2 4 \u00d7 2 2 4 masks at \u223c 1 3 fps , which is roughly the same frame - rate as a 4 \u00d7 conv head modified to output 5 6 \u00d7 5 6 masks ( by doubling the default RoIAlign size ) , a design that actually has lower COCO AP compared to the 2 8 \u00d7 2 8 4 \u00d7 conv head ( 3 4 . 5 % vs. 3 5 . 2 % ) .", "ner": [["RoIAlign", "Method"], ["COCO", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "209386851", "sentence": "The gap between PointRend and the default mask head in Mask R - CNN holds .", "ner": [["PointRend", "Method"], ["mask head", "Method"], ["Mask R - CNN", "Method"]], "rel": [["mask head", "Part-Of", "Mask R - CNN"], ["PointRend", "Part-Of", "Mask R - CNN"]], "rel_plus": [["mask head:Method", "Part-Of", "Mask R - CNN:Method"], ["PointRend:Method", "Part-Of", "Mask R - CNN:Method"]]}
{"doc_id": "209386851", "sentence": "AP is COCO mask AP evaluated against the higher - quality LVIS annotations [ 1 6 ] ( see text for details ) .", "ner": [["COCO", "Dataset"], ["LVIS", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "209386851", "sentence": "Training ResNet - 5 0 + FPN ( denoted R 5 0 - FPN ) with the 1 \u00d7 schedule under - fits on COCO .", "ner": [["ResNet - 5 0 + FPN", "Method"], ["R 5 0 - FPN", "Method"], ["COCO", "Dataset"]], "rel": [["R 5 0 - FPN", "Synonym-Of", "ResNet - 5 0 + FPN"], ["ResNet - 5 0 + FPN", "Trained-With", "COCO"]], "rel_plus": [["R 5 0 - FPN:Method", "Synonym-Of", "ResNet - 5 0 + FPN:Method"], ["ResNet - 5 0 + FPN:Method", "Trained-With", "COCO:Dataset"]]}
{"doc_id": "209386851", "sentence": "In Table 5 we show that the PointRend improvements over the baseline hold with both longer training schedule and larger models ( see the appendix for details ) .   DeeplabV 3 DeeplabV 3 + PointRend \u00d7 Figure 8 : Cityscapes example results for instance and semantic segmentation .", "ner": [["PointRend", "Method"], ["DeeplabV 3", "Method"], ["DeeplabV 3 + PointRend", "Method"], ["Cityscapes", "Dataset"], ["instance and semantic segmentation", "Task"]], "rel": [["Cityscapes", "Benchmark-For", "instance and semantic segmentation"]], "rel_plus": [["Cityscapes:Dataset", "Benchmark-For", "instance and semantic segmentation:Task"]]}
{"doc_id": "209386851", "sentence": "In instance segmentation larger objects benefit more from PointRend ability to yield high resolution output .", "ner": [["instance segmentation", "Task"], ["PointRend", "Method"]], "rel": [["PointRend", "Used-For", "instance segmentation"]], "rel_plus": [["PointRend:Method", "Used-For", "instance segmentation:Task"]]}
{"doc_id": "209386851", "sentence": "Whereas for semantic segmentation PointRend recovers small objects and details .", "ner": [["semantic segmentation", "Task"], ["PointRend", "Method"]], "rel": [["PointRend", "Used-For", "semantic segmentation"]], "rel_plus": [["PointRend:Method", "Used-For", "semantic segmentation:Task"]]}
{"doc_id": "209386851", "sentence": "PointRend is not limited to instance segmentation and can be extended to other pixel - level recognition tasks .", "ner": [["PointRend", "Method"], ["instance segmentation", "Task"], ["pixel - level recognition", "Task"]], "rel": [["PointRend", "Used-For", "instance segmentation"], ["PointRend", "Used-For", "pixel - level recognition"]], "rel_plus": [["PointRend:Method", "Used-For", "instance segmentation:Task"], ["PointRend:Method", "Used-For", "pixel - level recognition:Task"]]}
{"doc_id": "209386851", "sentence": "Here , we demonstrate that PointRend can benefit two semantic segmentation models : DeeplabV 3 [ 5 ] , which uses dilated convolutions to make prediction on a denser grid , and Se - manticFPN [ 2 4 ] , a simple encoder - decoder architecture .", "ner": [["PointRend", "Method"], ["semantic segmentation", "Task"], ["DeeplabV 3", "Method"], ["dilated convolutions", "Method"], ["Se - manticFPN", "Method"], ["encoder - decoder", "Method"]], "rel": [["PointRend", "Used-For", "semantic segmentation"], ["PointRend", "Part-Of", "DeeplabV 3"], ["dilated convolutions", "Part-Of", "DeeplabV 3"], ["PointRend", "Part-Of", "Se - manticFPN"], ["Se - manticFPN", "SubClass-Of", "encoder - decoder"]], "rel_plus": [["PointRend:Method", "Used-For", "semantic segmentation:Task"], ["PointRend:Method", "Part-Of", "DeeplabV 3:Method"], ["dilated convolutions:Method", "Part-Of", "DeeplabV 3:Method"], ["PointRend:Method", "Part-Of", "Se - manticFPN:Method"], ["Se - manticFPN:Method", "SubClass-Of", "encoder - decoder:Method"]]}
{"doc_id": "209386851", "sentence": "We use the Cityscapes [ 9 ] semantic segmentation set with 1 9 categories , 2 9 7 5 training images , and 5 0 0 validation images .", "ner": [["Cityscapes", "Dataset"], ["semantic segmentation", "Task"]], "rel": [["Cityscapes", "Benchmark-For", "semantic segmentation"]], "rel_plus": [["Cityscapes:Dataset", "Benchmark-For", "semantic segmentation:Task"]]}
{"doc_id": "209386851", "sentence": "We reimplemented DeeplabV 3 and SemanticFPN following their respective papers .", "ner": [["DeeplabV 3", "Method"], ["SemanticFPN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "209386851", "sentence": "Se - manticFPN uses a standard ResNet - 1 0 1 [ 2 0 ] , whereas DeeplabV 3 uses the ResNet - 1 0 3 proposed in [ 5 ] . 3 We follow the original papers ' training schedules and data augmentation ( details are in the appendix ) .", "ner": [["Se - manticFPN", "Method"], ["ResNet - 1 0 1", "Method"], ["DeeplabV 3", "Method"], ["ResNet - 1 0 3", "Method"], ["data augmentation", "Method"]], "rel": [["ResNet - 1 0 1", "Part-Of", "Se - manticFPN"], ["ResNet - 1 0 3", "Part-Of", "DeeplabV 3"]], "rel_plus": [["ResNet - 1 0 1:Method", "Part-Of", "Se - manticFPN:Method"], ["ResNet - 1 0 3:Method", "Part-Of", "DeeplabV 3:Method"]]}
{"doc_id": "209386851", "sentence": "We use the same PointRend architecture as for instance segmentation .", "ner": [["PointRend", "Method"], ["instance segmentation", "Task"]], "rel": [["PointRend", "Used-For", "instance segmentation"]], "rel_plus": [["PointRend:Method", "Used-For", "instance segmentation:Task"]]}
{"doc_id": "209386851", "sentence": "Fine - grained features are interpolated from res 2 for DeeplabV 3 and from P 2 for SemanticFPN .", "ner": [["DeeplabV 3", "Method"], ["SemanticFPN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "209386851", "sentence": "During training we sample as many points as there are on a stride 1 6 feature map of the input ( 2 3 0 4 for deeplabV 3 and 2 0 4 8 for Se - manticFPN ) .", "ner": [["deeplabV 3", "Method"], ["Se - manticFPN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "209386851", "sentence": "In Table 6 we compare DeepLabV 3 to DeeplabV 3 with PointRend .", "ner": [["DeepLabV 3", "Method"], ["DeeplabV 3 with PointRend", "Method"]], "rel": [["DeepLabV 3", "Compare-With", "DeeplabV 3 with PointRend"]], "rel_plus": [["DeepLabV 3:Method", "Compare-With", "DeeplabV 3 with PointRend:Method"]]}
{"doc_id": "209386851", "sentence": "Table 7 shows that SemanticFPN with PointRend improves over both 8 \u00d7 and 4 \u00d7 output stride variants without PointRend .", "ner": [["SemanticFPN with PointRend", "Method"], ["PointRend", "Method"]], "rel": [["SemanticFPN with PointRend", "Compare-With", "PointRend"]], "rel_plus": [["SemanticFPN with PointRend:Method", "Compare-With", "PointRend:Method"]]}
{"doc_id": "209386851", "sentence": "We use SGD with 0. 9 momentum ; a linear learning rate warmup [ 1 5 ] over 1 0 0 0 updates starting from a learning rate of 0.0 0 1 is applied ; weight decay 0.0 0 0 1 is applied ; horizontal flipping and scale train - time data augmentation ; the batch normalization ( BN ) [ 2 1 ] layers from the ImageNet pre - trained models are frozen ( i.e. , BN is not used ) ; no testtime augmentation is used .", "ner": [["SGD", "Method"], ["momentum", "Method"], ["linear learning rate warmup", "Method"], ["weight decay", "Method"], ["horizontal flipping", "Method"], ["data augmentation", "Method"], ["batch normalization", "Method"], ["BN", "Method"], ["ImageNet", "Dataset"], ["BN", "Method"]], "rel": [["momentum", "Part-Of", "SGD"], ["linear learning rate warmup", "Part-Of", "SGD"], ["BN", "Synonym-Of", "batch normalization"]], "rel_plus": [["momentum:Method", "Part-Of", "SGD:Method"], ["linear learning rate warmup:Method", "Part-Of", "SGD:Method"], ["BN:Method", "Synonym-Of", "batch normalization:Method"]]}
{"doc_id": "209386851", "sentence": "DeeplabV 3 [ 5 ] : We use SGD with 0. 9 momentum with 1 6 images per mini - batch cropped to a fixed 7 6 8 \u00d7 7 6 8 size ; the training schedule is 9 0 k updates with a poly learning rate [ 3 4 ] update strategy , starting from 0.0 1 ; a linear learning rate warmup [ 1 5 ] over 1 0 0 0 updates starting from a learning rate of 0.0 0 1 is applied ; the learning rate for ASPP and the prediction convolution are multiplied by 1 0 ; weight decay of 0.0 0 0 1 is applied ; random horizontal flipping and scaling of 0. 5 \u00d7 to 2. 0 \u00d7 with a 3 2 pixel step is used as training data augmentation ; BN is applied to 1 6 images minibatches ; no test - time augmentation is used ;", "ner": [["DeeplabV 3", "Method"], ["SGD", "Method"], ["momentum", "Method"], ["linear learning rate warmup", "Method"], ["ASPP", "Method"], ["convolution", "Method"], ["weight decay", "Method"], ["random horizontal flipping", "Method"], ["data augmentation", "Method"], ["BN", "Method"], ["test - time augmentation", "Method"]], "rel": [["SGD", "Part-Of", "DeeplabV 3"], ["momentum", "Part-Of", "SGD"], ["random horizontal flipping", "SubClass-Of", "data augmentation"]], "rel_plus": [["SGD:Method", "Part-Of", "DeeplabV 3:Method"], ["momentum:Method", "Part-Of", "SGD:Method"], ["random horizontal flipping:Method", "SubClass-Of", "data augmentation:Method"]]}
{"doc_id": "209386851", "sentence": "SemanticFPN [ 2 4 ] : We use SGD with 0. 9 momentum with 3 2 images per mini - batch cropped to a fixed 5 1 2 \u00d7 1 0 2 4 size ; the training schedule is 4 0 k / 1 5 k / 1 0 k updates at learning rates of 0.0 1 / 0.0 0 1 / 0.0 0 0 1 respectively ; a linear learning rate warmup [ 1 5 ] over 1 0 0 0 updates starting from a learning rate of 0.0 0 1 is applied ; weight decay 0.0 0 0 1 is applied ; horizontal flipping , color augmentation [ 3 3 ] , and crop bootstrapping [ 2 ] are used during training ; scale traintime data augmentation resizes an input image from 0. 5 \u00d7 to 2. 0 \u00d7 with a 3 2 pixel step ; BN layers are frozen ( i.e. , BN is not used ) ; no test - time augmentation is used .", "ner": [["SemanticFPN", "Method"], ["SGD", "Method"], ["momentum", "Method"], ["weight decay", "Method"], ["horizontal flipping", "Method"], ["color augmentation", "Method"], ["crop bootstrapping", "Method"], ["data augmentation", "Method"], ["BN", "Method"], ["BN", "Method"]], "rel": [["SGD", "Part-Of", "SemanticFPN"], ["momentum", "Part-Of", "SGD"]], "rel_plus": [["SGD:Method", "Part-Of", "SemanticFPN:Method"], ["momentum:Method", "Part-Of", "SGD:Method"]]}
{"doc_id": "24972096", "sentence": "For intelligent robotics applications , extending 3D mapping to 3D semantic mapping enables robots to , not only localize themselves with respect to the scene 's geometrical features but also simultaneously understand the higher level meaning of the scene contexts .", "ner": [["intelligent robotics", "Task"], ["3D mapping", "Task"], ["3D semantic mapping", "Task"]], "rel": [["3D mapping", "SubTask-Of", "intelligent robotics"], ["3D semantic mapping", "SubTask-Of", "intelligent robotics"], ["3D semantic mapping", "SubTask-Of", "3D mapping"]], "rel_plus": [["3D mapping:Task", "SubTask-Of", "intelligent robotics:Task"], ["3D semantic mapping:Task", "SubTask-Of", "intelligent robotics:Task"], ["3D semantic mapping:Task", "SubTask-Of", "3D mapping:Task"]]}
{"doc_id": "24972096", "sentence": "Most previous methods focus on geometric 3D reconstruction and scene understanding independently notwithstanding the fact that joint estimation can boost the accuracy of the semantic mapping .", "ner": [["geometric 3D reconstruction", "Task"], ["scene understanding", "Task"], ["semantic mapping", "Task"]], "rel": [["geometric 3D reconstruction", "Used-For", "semantic mapping"], ["scene understanding", "Used-For", "semantic mapping"]], "rel_plus": [["geometric 3D reconstruction:Task", "Used-For", "semantic mapping:Task"], ["scene understanding:Task", "Used-For", "semantic mapping:Task"]]}
{"doc_id": "24972096", "sentence": "In this paper , a dense RGB - D semantic mapping system with a Pixel - Voxel network is proposed , which can perform dense 3D mapping while simultaneously recognizing and semantically labelling each point in the 3D map .", "ner": [["dense RGB - D semantic mapping system", "Method"], ["Pixel - Voxel network", "Method"], ["3D mapping", "Task"]], "rel": [["Pixel - Voxel network", "Part-Of", "dense RGB - D semantic mapping system"], ["Pixel - Voxel network", "Used-For", "3D mapping"]], "rel_plus": [["Pixel - Voxel network:Method", "Part-Of", "dense RGB - D semantic mapping system:Method"], ["Pixel - Voxel network:Method", "Used-For", "3D mapping:Task"]]}
{"doc_id": "24972096", "sentence": "The proposed Pixel - Voxel network obtains global context information by using PixelNet to exploit the RGB image and meanwhile , preserves accurate local shape information by using VoxelNet to exploit the corresponding 3D point cloud .", "ner": [["Pixel - Voxel network", "Method"], ["PixelNet", "Method"], ["VoxelNet", "Method"]], "rel": [["PixelNet", "Part-Of", "Pixel - Voxel network"], ["VoxelNet", "Part-Of", "Pixel - Voxel network"]], "rel_plus": [["PixelNet:Method", "Part-Of", "Pixel - Voxel network:Method"], ["VoxelNet:Method", "Part-Of", "Pixel - Voxel network:Method"]]}
{"doc_id": "24972096", "sentence": "Unlike the existing architecture that fuses score maps from different models with equal weights , we proposed a Softmax weighted fusion stack that adaptively learns the varying contributions of PixelNet and VoxelNet , and fuses the score maps of the two models according to their respective confidence levels .", "ner": [["Softmax weighted fusion stack", "Method"], ["PixelNet", "Method"], ["VoxelNet", "Method"]], "rel": [["Softmax weighted fusion stack", "Part-Of", "PixelNet"], ["Softmax weighted fusion stack", "Part-Of", "VoxelNet"]], "rel_plus": [["Softmax weighted fusion stack:Method", "Part-Of", "PixelNet:Method"], ["Softmax weighted fusion stack:Method", "Part-Of", "VoxelNet:Method"]]}
{"doc_id": "24972096", "sentence": "The proposed Pixel - Voxel network achieves the state - of - the - art semantic segmentation performance on the SUN RGB - D benchmark dataset .", "ner": [["Pixel - Voxel network", "Method"], ["semantic segmentation", "Task"], ["SUN RGB - D", "Dataset"]], "rel": [["SUN RGB - D", "Evaluated-With", "Pixel - Voxel network"], ["Pixel - Voxel network", "Used-For", "semantic segmentation"], ["SUN RGB - D", "Benchmark-For", "semantic segmentation"]], "rel_plus": [["SUN RGB - D:Dataset", "Evaluated-With", "Pixel - Voxel network:Method"], ["Pixel - Voxel network:Method", "Used-For", "semantic segmentation:Task"], ["SUN RGB - D:Dataset", "Benchmark-For", "semantic segmentation:Task"]]}
{"doc_id": "24972096", "sentence": "Real - time 3D semantic mapping is often desired in a number of robotics applications , such as localization [ 1 , 2 ] , semantic navigation [ 3 , 4 ] and human - aware navigation [ 5 ] .", "ner": [["Real - time 3D semantic mapping", "Task"], ["robotics", "Task"], ["localization", "Task"], ["semantic navigation", "Task"], ["human - aware navigation", "Task"]], "rel": [["Real - time 3D semantic mapping", "Used-For", "robotics"], ["localization", "SubTask-Of", "robotics"], ["semantic navigation", "SubTask-Of", "robotics"], ["human - aware navigation", "SubTask-Of", "robotics"], ["Real - time 3D semantic mapping", "Used-For", "localization"], ["Real - time 3D semantic mapping", "Used-For", "semantic navigation"], ["Real - time 3D semantic mapping", "Used-For", "human - aware navigation"]], "rel_plus": [["Real - time 3D semantic mapping:Task", "Used-For", "robotics:Task"], ["localization:Task", "SubTask-Of", "robotics:Task"], ["semantic navigation:Task", "SubTask-Of", "robotics:Task"], ["human - aware navigation:Task", "SubTask-Of", "robotics:Task"], ["Real - time 3D semantic mapping:Task", "Used-For", "localization:Task"], ["Real - time 3D semantic mapping:Task", "Used-For", "semantic navigation:Task"], ["Real - time 3D semantic mapping:Task", "Used-For", "human - aware navigation:Task"]]}
{"doc_id": "24972096", "sentence": "For intelligent mobile robotics applications , extending 3D mapping to 3D semantic mapping enables robots not only to localize themselves with respect to the scene 's geometrical features , but also to simultaneously understand the higher - level semantic meaning of a complex scene .", "ner": [["intelligent mobile robotics", "Task"], ["3D mapping", "Task"], ["3D semantic mapping", "Task"]], "rel": [["3D mapping", "SubTask-Of", "intelligent mobile robotics"], ["3D semantic mapping", "SubTask-Of", "intelligent mobile robotics"], ["3D semantic mapping", "SubTask-Of", "3D mapping"]], "rel_plus": [["3D mapping:Task", "SubTask-Of", "intelligent mobile robotics:Task"], ["3D semantic mapping:Task", "SubTask-Of", "intelligent mobile robotics:Task"], ["3D semantic mapping:Task", "SubTask-Of", "3D mapping:Task"]]}
{"doc_id": "24972096", "sentence": "A variety of well - known methods such as RGB - D SLAM [ 8 ] , Kinect Fusion [ 9 ] and ElasticFusion [ 1 0 ] can generate a dense or semi - dense 3D map from RGB - D videos .", "ner": [["RGB - D SLAM", "Method"], ["Kinect Fusion", "Method"], ["ElasticFusion", "Method"], ["generate a dense or semi - dense 3D map", "Task"]], "rel": [["RGB - D SLAM", "Used-For", "generate a dense or semi - dense 3D map"], ["Kinect Fusion", "Used-For", "generate a dense or semi - dense 3D map"], ["ElasticFusion", "Used-For", "generate a dense or semi - dense 3D map"]], "rel_plus": [["RGB - D SLAM:Method", "Used-For", "generate a dense or semi - dense 3D map:Task"], ["Kinect Fusion:Method", "Used-For", "generate a dense or semi - dense 3D map:Task"], ["ElasticFusion:Method", "Used-For", "generate a dense or semi - dense 3D map:Task"]]}
{"doc_id": "24972096", "sentence": "On the contrary , impressive results in semantic segmentation have been achieved with the advancement of convolutional neural networks ( CNN ) .", "ner": [["semantic segmentation", "Task"], ["convolutional neural networks", "Method"], ["CNN", "Method"]], "rel": [["convolutional neural networks", "Used-For", "semantic segmentation"], ["CNN", "Synonym-Of", "convolutional neural networks"]], "rel_plus": [["convolutional neural networks:Method", "Used-For", "semantic segmentation:Task"], ["CNN:Method", "Synonym-Of", "convolutional neural networks:Method"]]}
{"doc_id": "24972096", "sentence": "Compared to the well - investigated research on geometric 3D reconstruction and scene understanding , limited literature is available for 3D semantic mapping [ 2 0 ] [ 2 1 ] [ 2 2 ] [ 2 3 ] .", "ner": [["geometric 3D reconstruction", "Task"], ["scene understanding", "Task"], ["3D semantic mapping", "Task"]], "rel": [["geometric 3D reconstruction", "Compare-With", "3D semantic mapping"], ["scene understanding", "Compare-With", "3D semantic mapping"]], "rel_plus": [["geometric 3D reconstruction:Task", "Compare-With", "3D semantic mapping:Task"], ["scene understanding:Task", "Compare-With", "3D semantic mapping:Task"]]}
{"doc_id": "24972096", "sentence": "In this paper , we propose a dense RGB - D semantic mapping system with a Pixel - Voxel neural network , which can perform dense 3D mapping , while simultaneously recognizing and semantically labelling each point in the 3D map .", "ner": [["dense RGB - D semantic mapping system", "Method"], ["Pixel - Voxel neural network", "Method"], ["dense 3D mapping", "Task"]], "rel": [["Pixel - Voxel neural network", "Part-Of", "dense RGB - D semantic mapping system"], ["dense RGB - D semantic mapping system", "Used-For", "dense 3D mapping"]], "rel_plus": [["Pixel - Voxel neural network:Method", "Part-Of", "dense RGB - D semantic mapping system:Method"], ["dense RGB - D semantic mapping system:Method", "Used-For", "dense 3D mapping:Task"]]}
{"doc_id": "24972096", "sentence": "A Pixel - Voxel network consuming the RGB image and point cloud is proposed , which can obtain global context information through PixelNet while preserving accurate local shape information through VoxelNet . 2 .", "ner": [["Pixel - Voxel network", "Method"], ["PixelNet", "Method"], ["VoxelNet", "Method"]], "rel": [["VoxelNet", "Part-Of", "Pixel - Voxel network"], ["PixelNet", "Part-Of", "Pixel - Voxel network"]], "rel_plus": [["VoxelNet:Method", "Part-Of", "Pixel - Voxel network:Method"], ["PixelNet:Method", "Part-Of", "Pixel - Voxel network:Method"]]}
{"doc_id": "24972096", "sentence": "A dense 3D semantic mapping system integrating a Pixel - Voxel network with RGB - D SLAM is developed .", "ner": [["dense 3D semantic mapping system", "Method"], ["Pixel - Voxel network", "Method"], ["RGB - D SLAM", "Method"]], "rel": [["Pixel - Voxel network", "Part-Of", "dense 3D semantic mapping system"], ["RGB - D SLAM", "Part-Of", "dense 3D semantic mapping system"]], "rel_plus": [["Pixel - Voxel network:Method", "Part-Of", "dense 3D semantic mapping system:Method"], ["RGB - D SLAM:Method", "Part-Of", "dense 3D semantic mapping system:Method"]]}
{"doc_id": "24972096", "sentence": "Finally , we conclude the paper in Section 5 .   To the best of our knowledge , the online dense 3D semantic mapping methods can be further grouped into three main sub - categories : semantic mapping based on 3D template matching [ 2 0 , 2 4 ] , 2D/ 2 . 5 D semantic segmentation [ 2 1 , 2 2 , [ 2 5 ] [ 2 6 ] [ 2 7 ] and RGB - D data association from multiple viewpoints [ 2 3 , 2 8 , 2 9 ] .", "ner": [["online dense 3D semantic mapping methods", "Method"], ["semantic mapping based on 3D template matching", "Method"], ["2D/ 2 . 5 D semantic segmentation", "Method"], ["RGB - D data association from multiple viewpoints", "Method"]], "rel": [["semantic mapping based on 3D template matching", "SubClass-Of", "online dense 3D semantic mapping methods"], ["2D/ 2 . 5 D semantic segmentation", "SubClass-Of", "online dense 3D semantic mapping methods"], ["RGB - D data association from multiple viewpoints", "SubClass-Of", "online dense 3D semantic mapping methods"]], "rel_plus": [["semantic mapping based on 3D template matching:Method", "SubClass-Of", "online dense 3D semantic mapping methods:Method"], ["2D/ 2 . 5 D semantic segmentation:Method", "SubClass-Of", "online dense 3D semantic mapping methods:Method"], ["RGB - D data association from multiple viewpoints:Method", "SubClass-Of", "online dense 3D semantic mapping methods:Method"]]}
{"doc_id": "24972096", "sentence": "Because of the state - of - the - art performance provided by the CNN - based scene understanding , SemanticFusion [ 2 2 ] integrates deconvolutional neural networks [ 3 0 ] with ElasticFusion [ 1 0 ] to obtain a real - time - capable ( 2 5 Hz ) semantic mapping system .", "ner": [["CNN - based scene understanding", "Method"], ["SemanticFusion", "Method"], ["deconvolutional neural networks", "Method"], ["ElasticFusion", "Method"], ["real - time - capable ( 2 5 Hz ) semantic mapping system", "Method"]], "rel": [["deconvolutional neural networks", "Part-Of", "SemanticFusion"], ["ElasticFusion", "Part-Of", "deconvolutional neural networks"], ["SemanticFusion", "Used-For", "real - time - capable ( 2 5 Hz ) semantic mapping system"]], "rel_plus": [["deconvolutional neural networks:Method", "Part-Of", "SemanticFusion:Method"], ["ElasticFusion:Method", "Part-Of", "deconvolutional neural networks:Method"], ["SemanticFusion:Method", "Used-For", "real - time - capable ( 2 5 Hz ) semantic mapping system:Method"]]}
{"doc_id": "24972096", "sentence": "All of these three methods require fully connected CRF [ 3 1 ] optimization as an offline post - processing stage , i.e. , the best performing semantic mapping methods are not capable of online operation .", "ner": [["fully connected CRF", "Method"], ["semantic mapping", "Task"]], "rel": [["fully connected CRF", "Used-For", "semantic mapping"]], "rel_plus": [["fully connected CRF:Method", "Used-For", "semantic mapping:Task"]]}
{"doc_id": "24972096", "sentence": "Zhao et al. [ 2 7 ] proposed the first system to perform simultaneous 3D mapping and pixel - wise material recognition .", "ner": [["3D mapping", "Task"], ["pixel - wise material recognition", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "24972096", "sentence": "It integrates CRF - RNN [ 3 2 ] with RGB - D SLAM [ 8 ] , and a post - processing optimization stage is not required .", "ner": [["CRF - RNN", "Method"], ["RGB - D SLAM", "Method"]], "rel": [["RGB - D SLAM", "Part-Of", "CRF - RNN"]], "rel_plus": [["RGB - D SLAM:Method", "Part-Of", "CRF - RNN:Method"]]}
{"doc_id": "24972096", "sentence": "Keisuke et al. [ 2 6 ] proposed a real - time dense monocular CNN - SLAM method , which can perform depth prediction and semantic segmentation simultaneously from a single image using a deep neural network .", "ner": [["dense monocular CNN - SLAM", "Method"], ["depth prediction", "Task"], ["semantic segmentation", "Task"], ["deep neural network", "Method"]], "rel": [["dense monocular CNN - SLAM", "Used-For", "depth prediction"], ["deep neural network", "Used-For", "depth prediction"], ["dense monocular CNN - SLAM", "Used-For", "semantic segmentation"], ["deep neural network", "Used-For", "semantic segmentation"]], "rel_plus": [["dense monocular CNN - SLAM:Method", "Used-For", "depth prediction:Task"], ["deep neural network:Method", "Used-For", "depth prediction:Task"], ["dense monocular CNN - SLAM:Method", "Used-For", "semantic segmentation:Task"], ["deep neural network:Method", "Used-For", "semantic segmentation:Task"]]}
{"doc_id": "24972096", "sentence": "Yu et al. [ 2 3 ] proposed a data - associated recurrent neural network ( DA - RNN ) integrated with Kinect Fusion [ 9 ] for 3D semantic mapping .", "ner": [["data - associated recurrent neural network", "Method"], ["DA - RNN", "Method"], ["Kinect Fusion", "Method"], ["3D semantic mapping", "Task"]], "rel": [["DA - RNN", "Synonym-Of", "data - associated recurrent neural network"], ["data - associated recurrent neural network", "Used-For", "3D semantic mapping"], ["Kinect Fusion", "Used-For", "3D semantic mapping"]], "rel_plus": [["DA - RNN:Method", "Synonym-Of", "data - associated recurrent neural network:Method"], ["data - associated recurrent neural network:Method", "Used-For", "3D semantic mapping:Task"], ["Kinect Fusion:Method", "Used-For", "3D semantic mapping:Task"]]}
{"doc_id": "24972096", "sentence": "DA - RNN employs a recurrent neural network to tightly combine the information contained in multiple viewpoints of an RGB - D video stream to improve the semantic segmentation performance .", "ner": [["DA - RNN", "Method"], ["recurrent neural network", "Method"], ["semantic segmentation", "Task"]], "rel": [["recurrent neural network", "Part-Of", "DA - RNN"], ["recurrent neural network", "Used-For", "semantic segmentation"], ["DA - RNN", "Used-For", "semantic segmentation"]], "rel_plus": [["recurrent neural network:Method", "Part-Of", "DA - RNN:Method"], ["recurrent neural network:Method", "Used-For", "semantic segmentation:Task"], ["DA - RNN:Method", "Used-For", "semantic segmentation:Task"]]}
{"doc_id": "24972096", "sentence": "It utilizes the visual odometry trajectory from RGB - D SLAM [ 8 ] to wrap semantic segmentation between two viewpoints .", "ner": [["visual odometry trajectory", "Task"], ["RGB - D SLAM", "Method"], ["semantic segmentation", "Task"]], "rel": [["RGB - D SLAM", "Used-For", "visual odometry trajectory"], ["visual odometry trajectory", "Used-For", "semantic segmentation"], ["RGB - D SLAM", "Used-For", "semantic segmentation"]], "rel_plus": [["RGB - D SLAM:Method", "Used-For", "visual odometry trajectory:Task"], ["visual odometry trajectory:Task", "Used-For", "semantic segmentation:Task"], ["RGB - D SLAM:Method", "Used-For", "semantic segmentation:Task"]]}
{"doc_id": "24972096", "sentence": "FuseNet [ 1 4 ] can fuse RGB and depth cues in a single encoder - decoder CNN architecture for RGB - D semantic segmentation .", "ner": [["FuseNet", "Method"], ["encoder - decoder CNN", "Method"], ["RGB - D semantic segmentation", "Task"]], "rel": [["encoder - decoder CNN", "Part-Of", "FuseNet"], ["FuseNet", "Used-For", "RGB - D semantic segmentation"]], "rel_plus": [["encoder - decoder CNN:Method", "Part-Of", "FuseNet:Method"], ["FuseNet:Method", "Used-For", "RGB - D semantic segmentation:Task"]]}
{"doc_id": "24972096", "sentence": "The long short - term memorized context fusion ( LSTM - CF ) network [ 1 5 ] fuses contextual information from multiple channels of RGB and depth images through stacking of several convolution layers and a long short - term memory layer .", "ner": [["long short - term memorized context fusion", "Method"], ["LSTM - CF", "Method"], ["convolution layers", "Method"], ["long short - term memory", "Method"]], "rel": [["LSTM - CF", "Synonym-Of", "long short - term memorized context fusion"], ["long short - term memory", "Part-Of", "long short - term memorized context fusion"], ["convolution layers", "Part-Of", "long short - term memorized context fusion"]], "rel_plus": [["LSTM - CF:Method", "Synonym-Of", "long short - term memorized context fusion:Method"], ["long short - term memory:Method", "Part-Of", "long short - term memorized context fusion:Method"], ["convolution layers:Method", "Part-Of", "long short - term memorized context fusion:Method"]]}
{"doc_id": "24972096", "sentence": "FuseNet normalizes the depth value into the interval of [ 0 , 2 5 5 ] to have the same spatial range as colour images , while the LSTM - CF network encodes depth to a horizontal , height , angle ( HHA ) image to obtain three channels as the colour image .", "ner": [["FuseNet", "Method"], ["LSTM - CF", "Method"]], "rel": [["FuseNet", "Compare-With", "LSTM - CF"]], "rel_plus": [["FuseNet:Method", "Compare-With", "LSTM - CF:Method"]]}
{"doc_id": "24972096", "sentence": "Spatio - temporal data - driven pooling ( STD 2 P ) [ 3 3 ] involves a novel superpixel - based multi - view convolutional neural network for RGB - D semantic segmentation , which uses the spatio - temporal pooling layer to aggregate information over space and time .", "ner": [["Spatio - temporal data - driven pooling", "Method"], ["STD 2 P", "Method"], ["superpixel - based multi - view convolutional neural network", "Method"], ["RGB - D semantic segmentation", "Task"], ["spatio - temporal pooling", "Method"]], "rel": [["STD 2 P", "Synonym-Of", "Spatio - temporal data - driven pooling"], ["superpixel - based multi - view convolutional neural network", "Part-Of", "Spatio - temporal data - driven pooling"], ["spatio - temporal pooling", "Part-Of", "superpixel - based multi - view convolutional neural network"], ["superpixel - based multi - view convolutional neural network", "Used-For", "RGB - D semantic segmentation"], ["Spatio - temporal data - driven pooling", "Used-For", "RGB - D semantic segmentation"]], "rel_plus": [["STD 2 P:Method", "Synonym-Of", "Spatio - temporal data - driven pooling:Method"], ["superpixel - based multi - view convolutional neural network:Method", "Part-Of", "Spatio - temporal data - driven pooling:Method"], ["spatio - temporal pooling:Method", "Part-Of", "superpixel - based multi - view convolutional neural network:Method"], ["superpixel - based multi - view convolutional neural network:Method", "Used-For", "RGB - D semantic segmentation:Task"], ["Spatio - temporal data - driven pooling:Method", "Used-For", "RGB - D semantic segmentation:Task"]]}
{"doc_id": "24972096", "sentence": "Locality - sensitive deconvolution networks ( LS - DeconvNets ) [ 1 6 ] involve a locality - sensitive DeconvNet to refine the boundary segmentation and also a gated fusion layer for combining modalities ( RGB and HHA ) ; however the number of input modalities is limited to two .", "ner": [["Locality - sensitive deconvolution networks", "Method"], ["LS - DeconvNets", "Method"], ["locality - sensitive DeconvNet", "Method"], ["boundary segmentation", "Task"]], "rel": [["LS - DeconvNets", "Synonym-Of", "Locality - sensitive deconvolution networks"], ["locality - sensitive DeconvNet", "Part-Of", "Locality - sensitive deconvolution networks"], ["locality - sensitive DeconvNet", "Used-For", "boundary segmentation"], ["Locality - sensitive deconvolution networks", "Used-For", "boundary segmentation"]], "rel_plus": [["LS - DeconvNets:Method", "Synonym-Of", "Locality - sensitive deconvolution networks:Method"], ["locality - sensitive DeconvNet:Method", "Part-Of", "Locality - sensitive deconvolution networks:Method"], ["locality - sensitive DeconvNet:Method", "Used-For", "boundary segmentation:Task"], ["Locality - sensitive deconvolution networks:Method", "Used-For", "boundary segmentation:Task"]]}
{"doc_id": "24972096", "sentence": "Lin et al. [ 1 7 ] introduced a cascaded feature network ( CFN ) with a context - aware receptive field ( CaRF ) with a better control on the relevant contextual information of the learned features for RGB - D semantic segmentation .", "ner": [["cascaded feature network", "Method"], ["CFN", "Method"], ["context - aware receptive field", "Method"], ["CaRF", "Method"], ["RGB - D semantic segmentation", "Task"]], "rel": [["CFN", "Synonym-Of", "cascaded feature network"], ["context - aware receptive field", "Part-Of", "cascaded feature network"], ["CaRF", "Synonym-Of", "context - aware receptive field"], ["context - aware receptive field", "Used-For", "RGB - D semantic segmentation"], ["cascaded feature network", "Used-For", "RGB - D semantic segmentation"]], "rel_plus": [["CFN:Method", "Synonym-Of", "cascaded feature network:Method"], ["context - aware receptive field:Method", "Part-Of", "cascaded feature network:Method"], ["CaRF:Method", "Synonym-Of", "context - aware receptive field:Method"], ["context - aware receptive field:Method", "Used-For", "RGB - D semantic segmentation:Task"], ["cascaded feature network:Method", "Used-For", "RGB - D semantic segmentation:Task"]]}
{"doc_id": "24972096", "sentence": "All of the above RGB - D fusion networks treat the depth image similarly to an RGB image using a CNN with a max - pooling layer .", "ner": [["RGB - D fusion networks", "Method"], ["CNN", "Method"], ["max - pooling", "Method"]], "rel": [["CNN", "Part-Of", "RGB - D fusion networks"], ["max - pooling", "Part-Of", "CNN"]], "rel_plus": [["CNN:Method", "Part-Of", "RGB - D fusion networks:Method"], ["max - pooling:Method", "Part-Of", "CNN:Method"]]}
{"doc_id": "24972096", "sentence": "The forerunner work PointNet [ 1 8 ] provides a unified architecture for both classification and segmentation , which consumes the raw unordered point clouds as input .", "ner": [["PointNet", "Method"], ["classification", "Task"], ["segmentation", "Task"]], "rel": [["PointNet", "Used-For", "classification"], ["PointNet", "Used-For", "segmentation"]], "rel_plus": [["PointNet:Method", "Used-For", "classification:Task"], ["PointNet:Method", "Used-For", "segmentation:Task"]]}
{"doc_id": "24972096", "sentence": "PointNet only employs a single max - pooling layer to generate the global feature , which describes the original input clouds ; thus , it does not capture the local structures induced by the 3D metric space points live in .", "ner": [["PointNet", "Method"], ["max - pooling", "Method"]], "rel": [["max - pooling", "Part-Of", "PointNet"]], "rel_plus": [["max - pooling:Method", "Part-Of", "PointNet:Method"]]}
{"doc_id": "24972096", "sentence": "The improved version PointNet++ [ 1 9 ] is a hierarchical neural network that applies PointNet recursively on a nested partitioning of the input point set , which can learn local features with increasing contextual scales .", "ner": [["PointNet++", "Method"], ["hierarchical neural network", "Method"], ["PointNet", "Method"]], "rel": [["PointNet", "Part-Of", "PointNet++"], ["PointNet++", "SubClass-Of", "hierarchical neural network"]], "rel_plus": [["PointNet:Method", "Part-Of", "PointNet++:Method"], ["PointNet++:Method", "SubClass-Of", "hierarchical neural network:Method"]]}
{"doc_id": "24972096", "sentence": "For the task of semantic segmentation , conventional CNN - based methods have struggled with the balance between global and local information .", "ner": [["semantic segmentation", "Task"], ["CNN - based methods", "Method"]], "rel": [["CNN - based methods", "Used-For", "semantic segmentation"]], "rel_plus": [["CNN - based methods:Method", "Used-For", "semantic segmentation:Task"]]}
{"doc_id": "24972096", "sentence": "Processing the depth image in a similar manner to the RGB image using CNN with max - pooling can not preserve all the local geometry information .", "ner": [["CNN", "Method"], ["max - pooling", "Method"]], "rel": [["max - pooling", "Part-Of", "CNN"]], "rel_plus": [["max - pooling:Method", "Part-Of", "CNN:Method"]]}
{"doc_id": "24972096", "sentence": "For example , in PointNet [ 1 8 ] , a single fully - connected multi - layer network followed by a single global max - pooling layer are used for semantic segmentation of a point cloud .", "ner": [["PointNet", "Method"], ["fully - connected multi - layer network", "Method"], ["global max - pooling layer", "Method"], ["semantic segmentation", "Task"]], "rel": [["global max - pooling layer", "Part-Of", "PointNet"], ["fully - connected multi - layer network", "Part-Of", "PointNet"], ["PointNet", "Used-For", "semantic segmentation"]], "rel_plus": [["global max - pooling layer:Method", "Part-Of", "PointNet:Method"], ["fully - connected multi - layer network:Method", "Part-Of", "PointNet:Method"], ["PointNet:Method", "Used-For", "semantic segmentation:Task"]]}
{"doc_id": "24972096", "sentence": "The RGB image can provide global context information as a supplement for point cloud segmentation , while the point cloud can help refine the boundary shape for RGB segmentation .", "ner": [["point cloud segmentation", "Task"], ["RGB segmentation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "24972096", "sentence": "This motivated us to utilize a Pixel - Voxel neural network for dense RGB - D semantic mapping .", "ner": [["Pixel - Voxel neural network", "Method"], ["RGB - D semantic mapping", "Task"]], "rel": [["Pixel - Voxel neural network", "Used-For", "RGB - D semantic mapping"]], "rel_plus": [["Pixel - Voxel neural network:Method", "Used-For", "RGB - D semantic mapping:Task"]]}
{"doc_id": "24972096", "sentence": "Therefore , in this paper , a softmax weighted fusion stack is proposed for adaptively learning the varying contribution of each modality .   The pipeline of the proposed dense RGB - D semantic mapping with a Pixel - Voxel neural network is illustrated in Figure 1 .", "ner": [["softmax weighted fusion stack", "Method"], ["dense RGB - D semantic mapping", "Method"], ["Pixel - Voxel neural network", "Method"]], "rel": [["Pixel - Voxel neural network", "Part-Of", "dense RGB - D semantic mapping"]], "rel_plus": [["Pixel - Voxel neural network:Method", "Part-Of", "dense RGB - D semantic mapping:Method"]]}
{"doc_id": "24972096", "sentence": "The output of the network - a semantically - labelled point cloud - is combined incrementally according to the visual odometry of RGB - D SLAM .", "ner": [["visual odometry", "Task"], ["RGB - D SLAM", "Method"]], "rel": [["RGB - D SLAM", "Used-For", "visual odometry"]], "rel_plus": [["RGB - D SLAM:Method", "Used-For", "visual odometry:Task"]]}
{"doc_id": "24972096", "sentence": "Note that in our current architecture , a voxel consists of just a single 3D point .   The sub - network PixelNet is comprised of three units : truncated CNN , a context stack similar to [ 3 4 ] and the skip architecture .", "ner": [["PixelNet", "Method"], ["truncated CNN", "Method"], ["context stack", "Method"], ["skip architecture", "Method"]], "rel": [["truncated CNN", "Part-Of", "PixelNet"], ["context stack", "Part-Of", "PixelNet"], ["skip architecture", "Part-Of", "PixelNet"]], "rel_plus": [["truncated CNN:Method", "Part-Of", "PixelNet:Method"], ["context stack:Method", "Part-Of", "PixelNet:Method"], ["skip architecture:Method", "Part-Of", "PixelNet:Method"]]}
{"doc_id": "24972096", "sentence": "For the truncated CNN , VGG - 1 6 ( http://www.robots.ox.ac.uk/~vgg/research/very_deep/ ) or ResNe ( https://github.com/ KaimingHe/deep - residual - networks ) ( truncated after pool 5 ) , pre - trained on ImageNet ( http://www . image - net.org/challenges/LSVRC/ ) , can be employed as a baseline .", "ner": [["truncated CNN", "Method"], ["VGG - 1 6", "Method"], ["ResNe", "Method"], ["ImageNet", "Dataset"]], "rel": [["VGG - 1 6", "Part-Of", "truncated CNN"], ["ResNe", "Part-Of", "truncated CNN"], ["VGG - 1 6", "Trained-With", "ImageNet"], ["ResNe", "Trained-With", "ImageNet"]], "rel_plus": [["VGG - 1 6:Method", "Part-Of", "truncated CNN:Method"], ["ResNe:Method", "Part-Of", "truncated CNN:Method"], ["VGG - 1 6:Method", "Trained-With", "ImageNet:Dataset"], ["ResNe:Method", "Trained-With", "ImageNet:Dataset"]]}
{"doc_id": "24972096", "sentence": "After the truncated CNN , the resolution of the feature maps is decreased 3 2 - times compared with the input image ; thus , it drops a significant amount of shape information , which is recovered utilizing the VoxelNet sub - network .", "ner": [["truncated CNN", "Method"], ["VoxelNet", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "24972096", "sentence": "Therefore , a context - stack , composed of a chain of 6 layers of 5 \u00d7 5 \u00d7 5 1 2 convolution stacks [ Conv + BN + ReLU ] , is concatenated on the top of a pre - trained truncated VGG - 1 6 network .", "ner": [["context - stack", "Method"], ["5 \u00d7 5 \u00d7 5 1 2 convolution stacks", "Method"], ["Conv + BN + ReLU", "Method"], ["truncated VGG - 1 6", "Method"]], "rel": [["5 \u00d7 5 \u00d7 5 1 2 convolution stacks", "Part-Of", "context - stack"], ["Conv + BN + ReLU", "Synonym-Of", "5 \u00d7 5 \u00d7 5 1 2 convolution stacks"]], "rel_plus": [["5 \u00d7 5 \u00d7 5 1 2 convolution stacks:Method", "Part-Of", "context - stack:Method"], ["Conv + BN + ReLU:Method", "Synonym-Of", "5 \u00d7 5 \u00d7 5 1 2 convolution stacks:Method"]]}
{"doc_id": "24972096", "sentence": "The skip architecture consists of 3 skip stacks [ Conv + BN + ReLU + Conv ( score ) ] following pool 2 , pool 3 and pool 4 separately .", "ner": [["skip stacks", "Method"], ["Conv + BN + ReLU + Conv ( score )", "Method"]], "rel": [["Conv + BN + ReLU + Conv ( score )", "Synonym-Of", "skip stacks"]], "rel_plus": [["Conv + BN + ReLU + Conv ( score ):Method", "Synonym-Of", "skip stacks:Method"]]}
{"doc_id": "24972096", "sentence": "Inspired by PointNet [ 1 8 ] , we also use max pooling as an invariant function .", "ner": [["PointNet", "Method"], ["max pooling", "Method"]], "rel": [["max pooling", "Part-Of", "PointNet"]], "rel_plus": [["max pooling:Method", "Part-Of", "PointNet:Method"]]}
{"doc_id": "24972096", "sentence": "Here , f ml p is the multi - layer perception network , i.e. , FC + BN + ReLU , and k is the number of multi - layer perception networks before max pooling .", "ner": [["multi - layer perception network", "Method"], ["FC + BN + ReLU", "Method"], ["multi - layer perception networks", "Method"], ["max pooling", "Method"]], "rel": [["FC + BN + ReLU", "Synonym-Of", "multi - layer perception network"]], "rel_plus": [["FC + BN + ReLU:Method", "Synonym-Of", "multi - layer perception network:Method"]]}
{"doc_id": "24972096", "sentence": "In detail , the feature of the point cloud in ( X , Y , Z ) can be transformed to the position ( u , v ) in the image plane , so the score map of VoxelNet can be fused with the score map of PixelNet .", "ner": [["VoxelNet", "Method"], ["PixelNet", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "24972096", "sentence": "For our problem , the three score maps from PixelNet and VoxelNet are fused together according to their respective confidence levels .", "ner": [["PixelNet", "Method"], ["VoxelNet", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "24972096", "sentence": "Thus , focusing more on the rare classes to boost their recognition accuracy can improve the average recognition performance significantly , while overall recognition performance might decrease a little .", "ner": [["recognition", "Task"], ["recognition", "Task"], ["recognition", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "24972096", "sentence": "RGB - D SLAM [ 8 ] is adopted for dense 3D mapping .", "ner": [["RGB - D SLAM", "Method"], ["dense 3D mapping", "Task"]], "rel": [["RGB - D SLAM", "Used-For", "dense 3D mapping"]], "rel_plus": [["RGB - D SLAM:Method", "Used-For", "dense 3D mapping:Task"]]}
{"doc_id": "24972096", "sentence": "The voxels from different viewpoints can be transformed to the same coordinate through the visual odometry of RGB - D SLAM .", "ner": [["visual odometry", "Task"], ["RGB - D SLAM", "Method"]], "rel": [["RGB - D SLAM", "Used-For", "visual odometry"]], "rel_plus": [["RGB - D SLAM:Method", "Used-For", "visual odometry:Task"]]}
{"doc_id": "24972096", "sentence": "We evaluate the proposed Pixel - Voxel network using two popular indoor scene datasets , i.e. , the SUN RGB - D ( http://rgbd.cs.princeton.edu/ ) and NYU V 2 ( https://cs.nyu.edu/~silberman/ datasets/nyu_depth_v 2 .html ) datasets .", "ner": [["Pixel - Voxel network", "Method"], ["SUN RGB - D", "Dataset"], ["NYU V 2", "Dataset"]], "rel": [["Pixel - Voxel network", "Evaluated-With", "SUN RGB - D"], ["Pixel - Voxel network", "Evaluated-With", "NYU V 2"]], "rel_plus": [["Pixel - Voxel network:Method", "Evaluated-With", "SUN RGB - D:Dataset"], ["Pixel - Voxel network:Method", "Evaluated-With", "NYU V 2:Dataset"]]}
{"doc_id": "24972096", "sentence": "The former is used to evaluate the semantic segmentation on a single frame , while the latter provides raw RGB - D sequences , which can be used for the semantic segmentation evaluation on multiple frames .", "ner": [["semantic segmentation", "Task"], ["semantic segmentation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "24972096", "sentence": "Therefore , using the odometry of RGB - D SLAM , the semantic segmentation based on multiple frames can be evaluated for the dense semantic mapping .", "ner": [["RGB - D SLAM", "Method"], ["semantic segmentation", "Task"], ["dense semantic mapping", "Task"]], "rel": [["RGB - D SLAM", "Used-For", "dense semantic mapping"]], "rel_plus": [["RGB - D SLAM:Method", "Used-For", "dense semantic mapping:Task"]]}
{"doc_id": "24972096", "sentence": "We generated the point cloud using the RGB - D image pairs and the corresponding intrinsic parameters of the camera through back - projection , e.g. , Equation ( 5 ) for the SUN RGB - D and NYU V 2 datasets .", "ner": [["SUN RGB - D", "Dataset"], ["NYU V 2", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "24972096", "sentence": "The whole training process can be divided into 3 stages : PixelNet training , VoxelNet training and Pixel - Voxel network training .", "ner": [["PixelNet", "Method"], ["VoxelNet", "Method"], ["Pixel - Voxel network", "Method"]], "rel": [["PixelNet", "Part-Of", "Pixel - Voxel network"], ["VoxelNet", "Part-Of", "Pixel - Voxel network"]], "rel_plus": [["PixelNet:Method", "Part-Of", "Pixel - Voxel network:Method"], ["VoxelNet:Method", "Part-Of", "Pixel - Voxel network:Method"]]}
{"doc_id": "24972096", "sentence": "Firstly , PixelNet and VoxelNet are each trained separately .", "ner": [["PixelNet", "Method"], ["VoxelNet", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "24972096", "sentence": "The step learning policy is adopted for PixelNet training , and the polynomial learning policy is adopted for PixelNet and Pixel - Voxel Network training .", "ner": [["step learning policy", "Method"], ["PixelNet", "Method"], ["polynomial learning policy", "Method"], ["PixelNet", "Method"], ["Pixel - Voxel Network", "Method"]], "rel": [["step learning policy", "Used-For", "PixelNet"], ["polynomial learning policy", "Used-For", "PixelNet"], ["polynomial learning policy", "Used-For", "Pixel - Voxel Network"]], "rel_plus": [["step learning policy:Method", "Used-For", "PixelNet:Method"], ["polynomial learning policy:Method", "Used-For", "PixelNet:Method"], ["polynomial learning policy:Method", "Used-For", "Pixel - Voxel Network:Method"]]}
{"doc_id": "24972096", "sentence": "Because there are 3 softmax weighed fusion stacks , 3 rounds of fine - tuning are required during the Pixel - Voxel network training .", "ner": [["softmax weighed fusion stacks", "Method"], ["Pixel - Voxel network", "Method"]], "rel": [["softmax weighed fusion stacks", "Part-Of", "Pixel - Voxel network"]], "rel_plus": [["softmax weighed fusion stacks:Method", "Part-Of", "Pixel - Voxel network:Method"]]}
{"doc_id": "24972096", "sentence": "The three metrics are defined as : Pixel accuracy : where n cl is the number of classes , n ij is the number of pixels of class i classified as class j and t i = \u2211 j n ij is the total number of pixels belonging to class i. In the experiment on the SUN RGB - D dataset , the performance of the Pixel - Voxel network and all the baselines are evaluated on a single frame .", "ner": [["SUN RGB - D", "Dataset"], ["Pixel - Voxel network", "Method"]], "rel": [["Pixel - Voxel network", "Evaluated-With", "SUN RGB - D"]], "rel_plus": [["Pixel - Voxel network:Method", "Evaluated-With", "SUN RGB - D:Dataset"]]}
{"doc_id": "24972096", "sentence": "From Figures 5 and 6 , it is clear that after combining VoxelNet with PixelNet , the edge prediction can be improved significantly .", "ner": [["VoxelNet", "Method"], ["PixelNet", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "24972096", "sentence": "Figure 5 . ( a , c ) are the coarse predictions from PixelNet , and ( b , d ) are the predictions after combining VoxelNet with PixelNet .", "ner": [["PixelNet", "Method"], ["VoxelNet", "Method"], ["PixelNet", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "24972096", "sentence": "The comparison of overall performance on the SUN RGB - D and NYU V 2 datasets are shown in Tables 1 and 2 .", "ner": [["SUN RGB - D", "Dataset"], ["NYU V 2", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "24972096", "sentence": "The class - wise accuracy on the SUN RGB - D and NYU V 2 datasets are shown in Tables 3   and 4 .", "ner": [["SUN RGB - D", "Dataset"], ["NYU V 2", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "24972096", "sentence": "For the SUN RGB - D dataset , we achieved 7 9 . 0 4 % for overall pixel accuracy , 5 7 . 6 5 % for mean accuracy and 4 4 . 2 4 % for mean IoU. After combining VoxelNet edge refinement , the pixel accuracy increased slightly from 7 7 . 2 5 % - 7 7 . 8 2 % for VGG - 1 6 and from 7 8 . 3 0 % - 7 8 . 7 6 % for ResNet 1 0 1 , while the mean accuracy shows a significant increase from 4 9 . 3 3 % - 5 3 . 8 6 % for VGG - 1 6 and from 5 4 . 2 2 % - 5 6 . 8 1 % for ResNet 1 0 1 .", "ner": [["SUN RGB - D", "Dataset"], ["VoxelNet", "Method"], ["VGG - 1 6", "Method"], ["ResNet 1 0 1", "Method"], ["VGG - 1 6", "Method"], ["ResNet 1 0 1", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "24972096", "sentence": "After combining VoxelNet edge refinement , the overall accuracy increases slightly from 8 0 . 7 4 % - 8 1 . 5 0 % for VGG - 1 6 and from 8 1 . 6 3 % - 8 2 . 2 2 % for ResNet 1 0 1 , while the mean accuracy shows a significant increase from 7 0 . 2 3 % - 7 2 . 2 5 % for VGG - 1 6 and from 7 2 . 1 8 % - 7 3 . 6 4 % for ResNet 1 0 1 .", "ner": [["VoxelNet", "Method"], ["VGG - 1 6", "Method"], ["ResNet 1 0 1", "Method"], ["VGG - 1 6", "Method"], ["ResNet 1 0 1", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "24972096", "sentence": "Mean IoU FCN [ 1 1 ] 6 8 . 1 8 % 3 8 . 4 1 % 2 7 . 3 9 % DeconvNet [ 3 0 ] 6 6 . 1 3 % 3 3 . 2 8 % 2 2 . 5 7 % SegNet [ 1 2 ] 7 2 . 6 3 % 4 4 . 7 6 % 3 1 . 8 4 % DeepLab [ 1 3 ] 7 1 . 9 0 % 4 2 . 2 1 % 3 2 . 0 8 % Context - CRF [ 3 6 ] 7 8 . 4 % 5 3 . 4 % 4 2 . 3 % LSTM - CF", "ner": [["FCN", "Method"], ["DeconvNet", "Method"], ["SegNet", "Method"], ["DeepLab", "Method"], ["Context - CRF", "Method"], ["LSTM - CF", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "24972096", "sentence": "[ 1 5 ] ( RGB - D ) - 4 8 . 1 % -FuseNet [ 1 4 ] ( RGB - D ) 7 6 . 2 7 % 4 8 . 3 0 % 3 7 . 2 9 % LS - DeconvNets ( RGB - D ) [ 1 6 ] - 5 8 . 0 0 % -RefineNet - Res 1 0 1 [ 3 7 ] 8 0 . 4 % 5 7 . 8 % 4 5 . 7 % RefineNet - Res 1 5 2 [ 3 7 ] 8 0 . 6 % 5 8 . 5 % 4 5 . 9 % CFN ( VGG - 1 6 , RGB - D ) [ 1 7 ] - - 4 2 . 5 % CFN ( RefineNet - 1 5 2 , RGB - D ) [ Table 2 .", "ner": [["-FuseNet", "Method"], ["LS - DeconvNets ( RGB - D )", "Method"], ["-RefineNet - Res 1 0 1", "Method"], ["RefineNet - Res 1 5 2", "Method"], ["CFN ( VGG - 1 6 , RGB - D )", "Method"], ["CFN ( RefineNet - 1 5 2 , RGB - D )", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "24972096", "sentence": "The class - wise IoU of the Pixel - Voxel network ( PVNet ) is also provided .", "ner": [["Pixel - Voxel network", "Method"], ["PVNet", "Method"]], "rel": [["PVNet", "Synonym-Of", "Pixel - Voxel network"]], "rel_plus": [["PVNet:Method", "Synonym-Of", "Pixel - Voxel network:Method"]]}
{"doc_id": "24972096", "sentence": "The class - wise IoU of the Pixel - Voxel network ( PVNet ) are also provided .", "ner": [["Pixel - Voxel network", "Method"], ["PVNet", "Method"]], "rel": [["PVNet", "Synonym-Of", "Pixel - Voxel network"]], "rel_plus": [["PVNet:Method", "Synonym-Of", "Pixel - Voxel network:Method"]]}
{"doc_id": "24972096", "sentence": "Modelling the global context information and simultaneously preserving the local shape information are the two key problems in CNN - based semantic segmentation .", "ner": [["CNN", "Method"], ["semantic segmentation", "Task"]], "rel": [["CNN", "Used-For", "semantic segmentation"]], "rel_plus": [["CNN:Method", "Used-For", "semantic segmentation:Task"]]}
{"doc_id": "24972096", "sentence": "The improvement can be attributed to three parts : the hierarchical convolutional stack in PixelNet , the boundary refinement by VoxelNet and the softmax weighted fusion stack .", "ner": [["hierarchical convolutional stack", "Method"], ["PixelNet", "Method"], ["boundary refinement", "Task"], ["VoxelNet", "Method"], ["softmax weighted fusion stack", "Method"]], "rel": [["hierarchical convolutional stack", "Part-Of", "PixelNet"], ["VoxelNet", "Used-For", "boundary refinement"]], "rel_plus": [["hierarchical convolutional stack:Method", "Part-Of", "PixelNet:Method"], ["VoxelNet:Method", "Used-For", "boundary refinement:Task"]]}
{"doc_id": "24972096", "sentence": "IAI Kinect 2 package 2 ( https : //github.com/code - iai/iaikinect 2 / ) is employed to interface with ROS and calibrate with the Kinect 2 cameras .", "ner": [["Kinect 2", "Dataset"], ["Kinect 2", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "24972096", "sentence": "Furthermore , this network was trained using the SUN RGB - D and NYU V 2 datasets , but was tested using the real - world data .", "ner": [["SUN RGB - D", "Dataset"], ["NYU V 2", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "24972096", "sentence": "Using the quad high definition ( QHD ) data from Kinect 2 , the runtime performances of our system are 5. 6 8 Hz ( VGG 1 6 ) and 3. 2 3 Hz ( ResNet 1 0 1 ) when the RGB is resized to 5 1 2 \u00d7 5 1 2 and the point cloud is down - sampled to three scales , 1 6 , 3 8 4 \u00d7 1 , 4 0 9 6 \u00d7 1 and 1 0 2 4 \u00d7 1 .", "ner": [["Kinect 2", "Dataset"], ["VGG 1 6", "Method"], ["ResNet 1 0 1", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "24972096", "sentence": "It is worth noting that the running time can be boosted to 1 3 . 3 3 Hz ( VGG 1 6 ) and 9. 0 1 Hz ( ResNet 1 0 1 ) using half - sized data with a corresponding decline in segmentation performance .", "ner": [["VGG 1 6", "Method"], ["ResNet 1 0 1", "Method"], ["segmentation", "Task"]], "rel": [["VGG 1 6", "Used-For", "segmentation"], ["ResNet 1 0 1", "Used-For", "segmentation"]], "rel_plus": [["VGG 1 6:Method", "Used-For", "segmentation:Task"], ["ResNet 1 0 1:Method", "Used-For", "segmentation:Task"]]}
{"doc_id": "24972096", "sentence": "The average inference runtime of Pixel - Voxel Net ( PVNet ) using different sizes of data .", "ner": [["Pixel - Voxel Net", "Method"], ["PVNet", "Method"]], "rel": [["PVNet", "Synonym-Of", "Pixel - Voxel Net"]], "rel_plus": [["PVNet:Method", "Synonym-Of", "Pixel - Voxel Net:Method"]]}
{"doc_id": "24972096", "sentence": "Inference Runtime PVNet ( VGG - 1 6 ) 0. 1 7 6 s 0.0 7 5 s PVNet ( ResNet 1 0 1 ) 0. 3 1 0 s 0. 1 1 1 s   This paper introduced an end - to - end discriminative Pixel - Voxel network for dense 3D semantic mapping .", "ner": [["Inference Runtime PVNet", "Method"], ["VGG - 1 6", "Method"], ["PVNet", "Method"], ["ResNet 1 0 1", "Method"], ["Pixel - Voxel network", "Method"], ["3D semantic mapping", "Task"]], "rel": [["VGG - 1 6", "Part-Of", "Inference Runtime PVNet"], ["ResNet 1 0 1", "Part-Of", "PVNet"], ["Pixel - Voxel network", "Used-For", "3D semantic mapping"]], "rel_plus": [["VGG - 1 6:Method", "Part-Of", "Inference Runtime PVNet:Method"], ["ResNet 1 0 1:Method", "Part-Of", "PVNet:Method"], ["Pixel - Voxel network:Method", "Used-For", "3D semantic mapping:Task"]]}
{"doc_id": "24972096", "sentence": "The hierarchical convolutional stack structure in PixelNet can model the high - level contextual information through an incrementally - enlarged receptive field , while the VoxelNet learns geometrical shapes via a non - linear feature transform in order to identify 3D objects with fine object boundaries .", "ner": [["hierarchical convolutional stack", "Method"], ["PixelNet", "Method"], ["VoxelNet", "Method"]], "rel": [["hierarchical convolutional stack", "Part-Of", "PixelNet"]], "rel_plus": [["hierarchical convolutional stack:Method", "Part-Of", "PixelNet:Method"]]}
{"doc_id": "24972096", "sentence": "We achieved competitive performance on the SUN RGB - D benchmark ( pixel acc . : 7 9 . 0 4 % , mean acc . : 5 7 . 6 5 % and mean IoU : 4 4 . 2 4 % ) and NYU V 2 benchmark ( pixel acc . : 8 2 . 5 3 % , mean acc . : 7 4 . 4 3 % and mean IoU : 5 9 . 3 0 % ) .", "ner": [["SUN RGB - D", "Dataset"], ["NYU V 2", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "24972096", "sentence": "Our method is faster than most state - of - the - art methods ( up to around 1 3 Hz using an i 7 eight - core PC with Titan X GPU ) and can be integrated into a SLAM system for near - real - time application in robotics .", "ner": [["SLAM", "Method"], ["robotics", "Task"]], "rel": [["SLAM", "Used-For", "robotics"]], "rel_plus": [["SLAM:Method", "Used-For", "robotics:Task"]]}
{"doc_id": "24972096", "sentence": "For future work , we will investigate the possibility of applying the proposed VoxelNet for semantic segmentation [ 4 0 ] with 3D LiDAR data , where only 3D geometric data are available .", "ner": [["VoxelNet", "Method"], ["semantic segmentation", "Task"]], "rel": [["VoxelNet", "Used-For", "semantic segmentation"]], "rel_plus": [["VoxelNet:Method", "Used-For", "semantic segmentation:Task"]]}
{"doc_id": "24972096", "sentence": "Moreover , and we will investigate adopting the proposed semantic mapping method to domestic robot navigation and manipulation tasks .", "ner": [["semantic mapping", "Task"], ["domestic robot navigation", "Task"], ["manipulation", "Task"]], "rel": [["semantic mapping", "Used-For", "domestic robot navigation"], ["semantic mapping", "Used-For", "manipulation"]], "rel_plus": [["semantic mapping:Task", "Used-For", "domestic robot navigation:Task"], ["semantic mapping:Task", "Used-For", "manipulation:Task"]]}
{"doc_id": "67855714", "sentence": "A common issue of deep neural networks - based methods for the problem of Single Image Super - Resolution ( SISR ) , is the recovery of finer texture details when super - resolving at large upscaling factors .", "ner": [["Single Image Super - Resolution", "Task"], ["SISR", "Task"]], "rel": [["SISR", "Synonym-Of", "Single Image Super - Resolution"]], "rel_plus": [["SISR:Task", "Synonym-Of", "Single Image Super - Resolution:Task"]]}
{"doc_id": "67855714", "sentence": "In particular , recent works proposed the use of a VGG loss which consists in minimizing the error between the generated high resolution images and ground - truth in the feature space of a Convolutional Neural Network ( VGG 1 9 ) , pre - trained on the very \" large \" ImageNet dataset .", "ner": [["VGG loss", "Method"], ["Convolutional Neural Network", "Method"], ["VGG 1 9", "Method"], ["ImageNet", "Dataset"]], "rel": [["VGG 1 9", "Synonym-Of", "Convolutional Neural Network"], ["VGG loss", "Part-Of", "Convolutional Neural Network"], ["Convolutional Neural Network", "Trained-With", "ImageNet"]], "rel_plus": [["VGG 1 9:Method", "Synonym-Of", "Convolutional Neural Network:Method"], ["VGG loss:Method", "Part-Of", "Convolutional Neural Network:Method"], ["Convolutional Neural Network:Method", "Trained-With", "ImageNet:Dataset"]]}
{"doc_id": "67855714", "sentence": "When considering the problem of super - resolving images with a distribution \" far \" from the ImageNet images distribution ( \\textit{e.g . , } satellite images ) , their proposed \\textit{fixed } VGG loss is no longer relevant .", "ner": [["ImageNet", "Dataset"], ["satellite images", "Dataset"], ["VGG loss", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "67855714", "sentence": "In this paper , we present a general framework named \\textit{Generative Collaborative Networks } ( GCN ) , where the idea consists in optimizing the \\textit{generator } ( the mapping of interest ) in the feature space of a \\textit{features extractor } network .", "ner": [["\\textit{Generative Collaborative Networks", "Method"], ["GCN", "Method"]], "rel": [["GCN", "Synonym-Of", "\\textit{Generative Collaborative Networks"]], "rel_plus": [["GCN:Method", "Synonym-Of", "\\textit{Generative Collaborative Networks:Method"]]}
{"doc_id": "67855714", "sentence": "We evaluate the GCN framework in the context of SISR , and we show that it results in a method that is adapted to super - resolution domains that are \" far \" from the ImageNet domain .", "ner": [["GCN", "Method"], ["SISR", "Task"], ["super - resolution", "Task"], ["ImageNet", "Dataset"]], "rel": [["GCN", "Used-For", "SISR"]], "rel_plus": [["GCN:Method", "Used-For", "SISR:Task"]]}
{"doc_id": "67855714", "sentence": "Generally , the considered optimization VGG loss irrelevant VGG loss relevant \u2260 Figure 1 : When super - resolving images from a different domain ( e.g. , satellite images on the right ) than the ImageNet domain ( e.g. , general objects on the left ) , the VGG loss introduced by [ 1 ] is no longer relevant .", "ner": [["VGG loss", "Method"], ["VGG loss", "Method"], ["satellite images", "Dataset"], ["ImageNet", "Dataset"], ["VGG loss", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "67855714", "sentence": "We propose a method that outperforms the SRGAN method [ 1 ] when super - resolving satellite images .", "ner": [["SRGAN", "Method"], ["super - resolving satellite images", "Task"]], "rel": [["SRGAN", "Used-For", "super - resolving satellite images"]], "rel_plus": [["SRGAN:Method", "Used-For", "super - resolving satellite images:Task"]]}
{"doc_id": "67855714", "sentence": "The former encourages solutions perceptually hard to distinguish from the HR groundtruth images , while the latter consists in using high - level feature maps of the VGG network [ 9 ] pre - trained on ImageNet [ 1 0 ] .", "ner": [["VGG", "Method"], ["ImageNet", "Dataset"]], "rel": [["VGG", "Trained-With", "ImageNet"]], "rel_plus": [["VGG:Method", "Trained-With", "ImageNet:Dataset"]]}
{"doc_id": "67855714", "sentence": "When considering the problem of super - resolving images from a target - domain different than ImageNet ( e.g. , satellite images ) , the features produced by the pre - trained VGG network on the source domain ( ImageNet ) are suboptimal and no longer relevant for the target domain .", "ner": [["ImageNet", "Dataset"], ["satellite images", "Dataset"], ["VGG", "Method"], ["ImageNet", "Dataset"]], "rel": [["VGG", "Trained-With", "ImageNet"]], "rel_plus": [["VGG:Method", "Trained-With", "ImageNet:Dataset"]]}
{"doc_id": "67855714", "sentence": "In this work , we present a general framework which we call Generative Collaborative Networks ( GCN ) , where the main idea consists in optimizing the generator ( i.e. , the mapping of interest ) in the feature space of a network which we shall refer to as a features extractor network .", "ner": [["Generative Collaborative Networks", "Method"], ["GCN", "Method"]], "rel": [["GCN", "Synonym-Of", "Generative Collaborative Networks"]], "rel_plus": [["GCN:Method", "Synonym-Of", "Generative Collaborative Networks:Method"]]}
{"doc_id": "67855714", "sentence": "In particular , we applied our framework to the problem of single image super - resolution , and we demonstrated that it results in a method that is more adapted ( compared to SRGAN [ 1 ] ) when super - resolving images from a domain that is \" far \" from the ImageNet domain .", "ner": [["single image super - resolution", "Task"], ["SRGAN", "Method"], ["ImageNet", "Dataset"]], "rel": [["SRGAN", "Used-For", "single image super - resolution"]], "rel_plus": [["SRGAN:Method", "Used-For", "single image super - resolution:Task"]]}
{"doc_id": "67855714", "sentence": "Similar approaches used Gaussian process regression [ 2 3 ] , trees [ 2 4 ] or Random Forests [ 2 5 ] to solve the regression problem introduced in [ 2 2 ] .", "ner": [["Gaussian process regression", "Method"], ["trees", "Method"], ["Random Forests", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "67855714", "sentence": "Convolutional neural networks (CNN) - based approaches outperformed other P sisr approaches , by showing excellent performance .", "ner": [["Convolutional neural networks", "Method"], ["(CNN)", "Method"]], "rel": [["(CNN)", "Synonym-Of", "Convolutional neural networks"]], "rel_plus": [["(CNN):Method", "Synonym-Of", "Convolutional neural networks:Method"]]}
{"doc_id": "67855714", "sentence": "The latter consists in minimizing the error between the recovered HR image and ground - truth in the high - level feature space of the pre - trained VGG network [ 9 ] on ImageNet [ 1 0 ] .", "ner": [["VGG", "Method"], ["ImageNet", "Dataset"]], "rel": [["VGG", "Trained-With", "ImageNet"]], "rel_plus": [["VGG:Method", "Trained-With", "ImageNet:Dataset"]]}
{"doc_id": "67855714", "sentence": "This method notably outperformed CNN - based methods for the problem P sisr .   Consider a problem P of learning a mapping function F , parameterized by \u03b8 F , that transforms images from a domain X to a domain Y , given a training set of N pairs { ( x i , y i ) } N i= 1 \u2208 X \u00d7 Y. Denote by p X and p Y the probability distributions respectively over X and Y. In addition , we introduce a given features extractor function denoted \u03a6 , parameterized by \u03b8 \u03a6 , that maps an image y \u2208 Y to a certain euclidean feature space S \u03a6 of dimensionality d. The mappings F and \u03a6 are typically feed - forward Convolutional Neural Networks .", "ner": [["CNN", "Method"], ["Convolutional Neural Networks", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "67855714", "sentence": "The Generative Collaborative Networks ( GCN ) framework consists in learning the mapping function F by minimizing a given loss function 1 in the space of features S \u03a6 , between the generated images ( through F ) and ground - truth .", "ner": [["Generative Collaborative Networks", "Method"], ["GCN", "Method"]], "rel": [["GCN", "Synonym-Of", "Generative Collaborative Networks"]], "rel_plus": [["GCN:Method", "Synonym-Of", "Generative Collaborative Networks:Method"]]}
{"doc_id": "67855714", "sentence": "We refer to this class of methods by P/mse . ( 1.b ) When \u03a6 corresponds to a random feature map neural network , that is to say , the weights \u03b8 \u03a6 are set randomly according to a given distribution \u00b5. We refer to this class of methods by P/ran . ( 1.c ) When \u03a6 is a part of a model that solves a reconstruction problem ( jointly with an auxiliary mapping function \u03a8 : S \u03a6 \u2192 Y ) , by minimizing the pixel - wise 2 -loss function between the reconstructed images ( through \u03a8 ) and ground - truth : Figure 2 : Overview of the GCN framework with examples of the two learning strategies .", "ner": [["neural network", "Method"], ["reconstruction", "Task"], ["GCN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "67855714", "sentence": "Notably , this strategy allows for the learning of reconstruction features which are different from classification - based features .", "ner": [["reconstruction", "Task"], ["classification", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "67855714", "sentence": "First methods used generative adversarial networks ( GANs ) for generating high perceptual quality images [ 3 6 , 3 7 ] , style transfer [ 3 8 ] and inpainting [ 3 9 ] , namely the class of methods P/adv with \u03bb 1 = 0 .", "ner": [["generative adversarial networks", "Method"], ["GANs", "Method"], ["generating high perceptual quality images", "Task"], ["style transfer", "Task"], ["inpainting", "Task"]], "rel": [["GANs", "Synonym-Of", "generative adversarial networks"], ["generative adversarial networks", "Used-For", "generating high perceptual quality images"], ["generative adversarial networks", "Used-For", "style transfer"], ["generative adversarial networks", "Used-For", "inpainting"]], "rel_plus": [["GANs:Method", "Synonym-Of", "generative adversarial networks:Method"], ["generative adversarial networks:Method", "Used-For", "generating high perceptual quality images:Task"], ["generative adversarial networks:Method", "Used-For", "style transfer:Task"], ["generative adversarial networks:Method", "Used-For", "inpainting:Task"]]}
{"doc_id": "67855714", "sentence": "Authors in [ 3 4 , 3 3 ] and in [ 4 1 ] used P/cla by considering respectively \u03a6 = VGG 1 9 and \u03a6 = AlexNet networks as fixed features extractors ( learned disjointly from the mapping of interest ) , which result in a more perceptually convincing results for both super - resolution and artistic style - transfer [ 4 2 , 4 3 ] .", "ner": [["VGG 1 9", "Method"], ["AlexNet", "Method"], ["super - resolution", "Task"], ["artistic style - transfer", "Task"]], "rel": [["VGG 1 9", "Used-For", "super - resolution"], ["AlexNet", "Used-For", "super - resolution"], ["VGG 1 9", "Used-For", "artistic style - transfer"], ["AlexNet", "Used-For", "artistic style - transfer"]], "rel_plus": [["VGG 1 9:Method", "Used-For", "super - resolution:Task"], ["AlexNet:Method", "Used-For", "super - resolution:Task"], ["VGG 1 9:Method", "Used-For", "artistic style - transfer:Task"], ["AlexNet:Method", "Used-For", "artistic style - transfer:Task"]]}
{"doc_id": "67855714", "sentence": "We particularly apply these strategies in the context of Single Image Super - Resolution , which results in methods that are more suitable ( comparing to the SRGAN method [ 1 ] ) to super - resolution domains that differ from the ImageNet domain .", "ner": [["Single Image Super - Resolution", "Task"], ["SRGAN", "Method"], ["super - resolution", "Task"], ["ImageNet", "Dataset"]], "rel": [["SRGAN", "Used-For", "Single Image Super - Resolution"], ["SRGAN", "Used-For", "super - resolution"], ["ImageNet", "Benchmark-For", "super - resolution"], ["SRGAN", "Evaluated-With", "ImageNet"]], "rel_plus": [["SRGAN:Method", "Used-For", "Single Image Super - Resolution:Task"], ["SRGAN:Method", "Used-For", "super - resolution:Task"], ["ImageNet:Dataset", "Benchmark-For", "super - resolution:Task"], ["SRGAN:Method", "Evaluated-With", "ImageNet:Dataset"]]}
{"doc_id": "67855714", "sentence": "In particular , we show on a dataset of satellite images ( different from the ImageNet domain ) that our method P sisr /adv , rec outperforms the SRGAN method [ 1 ] by a large margin on the considered domain .", "ner": [["satellite images", "Dataset"], ["ImageNet", "Dataset"], ["SRGAN", "Method"]], "rel": [["SRGAN", "Evaluated-With", "satellite images"]], "rel_plus": [["SRGAN:Method", "Evaluated-With", "satellite images:Dataset"]]}
{"doc_id": "67855714", "sentence": "Note that , as our goal is to show the irrelevance of the VGG loss for some visual domains ( different from ImageNet ) , we do not consider the well - known SR benchmarks ( e.g. , Set 5 , Set 1 4 , B 1 0 0 , Urban 1 0 0 ) for the evaluation , as these benchmarks are relatively close to the ImageNet domain .", "ner": [["VGG loss", "Method"], ["ImageNet", "Dataset"], ["SR", "Task"], ["Set 5", "Dataset"], ["Set 1 4", "Dataset"], ["B 1 0 0", "Dataset"], ["Urban 1 0 0", "Dataset"], ["ImageNet", "Dataset"]], "rel": [["Set 5", "Benchmark-For", "SR"], ["Set 1 4", "Benchmark-For", "SR"], ["B 1 0 0", "Benchmark-For", "SR"], ["Urban 1 0 0", "Benchmark-For", "SR"]], "rel_plus": [["Set 5:Dataset", "Benchmark-For", "SR:Task"], ["Set 1 4:Dataset", "Benchmark-For", "SR:Task"], ["B 1 0 0:Dataset", "Benchmark-For", "SR:Task"], ["Urban 1 0 0:Dataset", "Benchmark-For", "SR:Task"]]}
{"doc_id": "67855714", "sentence": "The evaluation of super - resolution methods ( more generally image regressionbased methods ) requires comparing visual patterns which remains an open problem in computer vision .", "ner": [["super - resolution", "Task"], ["computer vision", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "67855714", "sentence": "Zhang et al. [ 4 4 ] recently evaluated deep features across different architectures ( Squeeze [ 4 5 ] , AlexNet [ 4 6 ] and VGG [ 9 ] ) and tasks ( supervised , self - supervised and unsupervised networks ) and compared the resulting metrics with traditional ones .", "ner": [["Squeeze", "Method"], ["AlexNet", "Method"], ["VGG", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "67855714", "sentence": "Thus , we compute the perceptual error ( PE ) of a P sisr method ( a mapping F ) on a given test - set of N low - resolution images and their high - resolution as the mean distances between the generated images ( through F ) and ground - truth as follows : Note that we use the implementation of [ 4 4 ] to compute the perceptual distances d \u03a6 ( \u00b7 , \u00b7 ) using six variants which are based on the networks Squeeze [ 4 5 ] , AlexNet [ 4 6 ] and VGG [ 9 ] and their \" perceptual calibrated \" versions .", "ner": [["Squeeze", "Method"], ["AlexNet", "Method"], ["VGG", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "67855714", "sentence": "The best method is considered to be the one which minimizes the maximum amount of PEs across different networks \u03a6 \u2208 { Squ , Squ - l , Alex , Alex - l , VGG , VGG - l}. The overall goal of this section is to validate our statement about the relevance of the VGG loss when super - resolving images from a different domain than the ImageNet domain .", "ner": [["Squ", "Method"], ["Squ - l", "Method"], ["Alex", "Method"], ["Alex - l", "Method"], ["VGG", "Method"], ["VGG - l}.", "Method"], ["VGG loss", "Method"], ["ImageNet", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "67855714", "sentence": "Indeed , [ 1 ] defined a VGG loss on the feature map obtained by a specific layer of the pre - trained VGG 1 9 network and shows that it fixes the inherent problem of overly smooth results which comes with the pixel - wise loss .", "ner": [["VGG loss", "Method"], ["VGG 1 9", "Method"]], "rel": [["VGG loss", "Part-Of", "VGG 1 9"]], "rel_plus": [["VGG loss:Method", "Part-Of", "VGG 1 9:Method"]]}
{"doc_id": "67855714", "sentence": "Nevertheless , VGG 1 9 being trained on ImageNet , their method would not perform particularly well on different images , the distribution of which is far away from that of ImageNet .", "ner": [["VGG 1 9", "Method"], ["ImageNet", "Dataset"], ["ImageNet", "Dataset"]], "rel": [["VGG 1 9", "Trained-With", "ImageNet"]], "rel_plus": [["VGG 1 9:Method", "Trained-With", "ImageNet:Dataset"]]}
{"doc_id": "67855714", "sentence": "Since VGG 1 9 was trained on ImageNet for many ( more than 3 0 0 K ) iterations , we expect to have similar or worse results than the state - of - the - art method SRGAN from [ 1 ] on this database . \u2022 The Describable Textures Dataset ( DTD ) [ 4 7 ] , containing 5 , 6 0 0 images of textural patterns .", "ner": [["VGG 1 9", "Method"], ["ImageNet", "Dataset"], ["SRGAN", "Method"], ["Describable Textures Dataset", "Dataset"], ["DTD", "Dataset"]], "rel": [["VGG 1 9", "Trained-With", "ImageNet"], ["DTD", "Synonym-Of", "Describable Textures Dataset"]], "rel_plus": [["VGG 1 9:Method", "Trained-With", "ImageNet:Dataset"], ["DTD:Dataset", "Synonym-Of", "Describable Textures Dataset:Dataset"]]}
{"doc_id": "67855714", "sentence": "These data are relatively close to ImageNet and we show that our method gives convincing results relatively close to SRGAN . \u2022 A dataset containing satellite images 3 , which we generated by randomly cropping 2 5 6 \u00d7 2 5 6 images on a 7 2 0 5 \u00d7 7 2 0 5 satellite image which result in 2 3 5 , 1 8 3 images .", "ner": [["ImageNet", "Dataset"], ["SRGAN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "67855714", "sentence": "These blocks are made of two convolutional layers with 3 \u00d7 3 kernels and 6 4 features maps , each followed by batch normalization and PReLU as activation .", "ner": [["convolutional layers", "Method"], ["3 \u00d7 3 kernels", "Method"], ["batch normalization", "Method"], ["PReLU", "Method"]], "rel": [["3 \u00d7 3 kernels", "Part-Of", "convolutional layers"], ["batch normalization", "Part-Of", "convolutional layers"], ["PReLU", "Part-Of", "convolutional layers"]], "rel_plus": [["3 \u00d7 3 kernels:Method", "Part-Of", "convolutional layers:Method"], ["batch normalization:Method", "Part-Of", "convolutional layers:Method"], ["PReLU:Method", "Part-Of", "convolutional layers:Method"]]}
{"doc_id": "67855714", "sentence": "The architecture of all the used discriminators follows the guidelines of Radford et al. [ 4 8 ] as it is composed of convolutional layers , followed by a batch normalization and a LeakyReLU ( \u03b1 = 0. 2 ) activation .", "ner": [["discriminators", "Method"], ["convolutional layers", "Method"], ["batch normalization", "Method"], ["LeakyReLU", "Method"]], "rel": [["convolutional layers", "Part-Of", "discriminators"], ["batch normalization", "Part-Of", "discriminators"], ["LeakyReLU", "Part-Of", "discriminators"]], "rel_plus": [["convolutional layers:Method", "Part-Of", "discriminators:Method"], ["batch normalization:Method", "Part-Of", "discriminators:Method"], ["LeakyReLU:Method", "Part-Of", "discriminators:Method"]]}
{"doc_id": "67855714", "sentence": "Generally , the methods which were trained with an additional adversarial loss ( P sisr /adv and P sisr /adv , rec ) output images of higher quality ( on the datasets ImageNet and Sat ) as GANs were introduced to do just so : generate images that follow the distribution of the dataset .", "ner": [["ImageNet", "Dataset"], ["GANs", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "67855714", "sentence": "Among these two adversarial methods , it seems to us ( as suggested by the quantitative results of table 2 ) that P sisr /adv , rec ( column ( c ) of Figure 5 ) is able to detect and render more details , due to its ability to generate more relevant features as the features extractor \u03a6 is learned to solve a multi - task problem ; namely a discrimination and a reconstruction problem , in particular , this method allows for the learning of both classification and reconstruction - based features .", "ner": [["discrimination", "Task"], ["reconstruction", "Task"], ["classification", "Task"], ["reconstruction", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "67855714", "sentence": "As we can notice , the method P sisr /adv , rec outperforms the other methods in the datasets ImageNet and Sat , while P sisr /dis , rec gives the best results on DTD . 4. 3 . 5 .", "ner": [["ImageNet", "Dataset"], ["DTD", "Dataset"]], "rel": [["ImageNet", "Compare-With", "DTD"]], "rel_plus": [["ImageNet:Dataset", "Compare-With", "DTD:Dataset"]]}
{"doc_id": "67855714", "sentence": "P sisr /adv , rec against baseline methods on the satellite images domain Our main objective is to show that the VGG loss function ( namely , the SRGAN method [ 1 ] ) is no longer relevant when super - resolving images from a domain different than the ImageNet domain .", "ner": [["VGG loss", "Method"], ["SRGAN", "Method"], ["ImageNet", "Dataset"]], "rel": [["VGG loss", "Part-Of", "SRGAN"]], "rel_plus": [["VGG loss:Method", "Part-Of", "SRGAN:Method"]]}
{"doc_id": "67855714", "sentence": "In particular , by considering the satellite images domain , we show in this section that the selected method from the previous section ( P sisr /adv , rec ) outperforms some baselines , which are P sisr /mse ( pixel - wise MSE loss ) and P sisr /adv , mse ( pixel wise MSE loss combined with an adversarial loss ) , and the state - of - the - art super - resolution method , SRGAN [ 1 ] .", "ner": [["P sisr /mse", "Method"], ["pixel - wise MSE loss", "Method"], ["P sisr /adv , mse", "Method"], ["pixel wise MSE loss", "Method"], ["adversarial loss", "Method"], ["super - resolution", "Task"], ["SRGAN", "Method"]], "rel": [["pixel - wise MSE loss", "Synonym-Of", "P sisr /mse"], ["adversarial loss", "Part-Of", "P sisr /adv , mse"], ["pixel wise MSE loss", "Part-Of", "P sisr /adv , mse"], ["SRGAN", "Used-For", "super - resolution"]], "rel_plus": [["pixel - wise MSE loss:Method", "Synonym-Of", "P sisr /mse:Method"], ["adversarial loss:Method", "Part-Of", "P sisr /adv , mse:Method"], ["pixel wise MSE loss:Method", "Part-Of", "P sisr /adv , mse:Method"], ["SRGAN:Method", "Used-For", "super - resolution:Task"]]}
{"doc_id": "67855714", "sentence": "Our purpose being to show the relevance of the proposed method on a domain \" far \" from the ImageNet domain , we do not consider standard SR benchmarks , which are raltively \" close \" to the ImageNet domain .", "ner": [["ImageNet", "Dataset"], ["SR", "Method"], ["ImageNet", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "67855714", "sentence": "Note also that even if SRGAN [ 1 ] is optimized to minimize a VGG loss , it does not give the lowest perceptual errors in terms of the perceptual metrics VGG and VGG - l , this is due to the fact that the VGG features are not relevant for the satellite images domain .", "ner": [["SRGAN", "Method"], ["VGG loss", "Method"], ["VGG", "Method"], ["VGG - l", "Method"], ["VGG", "Method"]], "rel": [["VGG loss", "Part-Of", "SRGAN"]], "rel_plus": [["VGG loss:Method", "Part-Of", "SRGAN:Method"]]}
{"doc_id": "67855714", "sentence": "Qualitative results are provided P sisr /mse P sisr /adv , mse SRGAN [ 1 ] P sisr /rec P sisr /dis , rec P sisr /adv P sisr /adv , rec Figure 6 : Results of different P sisr methods on a patch of an image from the Sat dataset .", "ner": [["P sisr /mse P sisr /adv", "Method"], ["mse SRGAN", "Method"], ["sisr /rec P sisr /dis", "Method"], ["rec P sisr /adv P sisr /adv", "Method"], ["Sat", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "67855714", "sentence": "Table 4 : Comparison of our methods P sisr /adv , rec and P sisr /dis , rec with baselines and the SRGAN method [ 1 ] on the datasets ImageNet ( a subset of 2 0 0 , 0 0 0 randomely selected images ) and DTD , in terms of classical metrics ( L 2 and SSIM ) and perceptual metrics [ 4 4 ] . in figure 7 .", "ner": [["P sisr /adv", "Method"], ["rec and P sisr /dis", "Method"], ["SRGAN", "Method"], ["ImageNet", "Dataset"], ["DTD", "Dataset"]], "rel": [["P sisr /adv", "Compare-With", "rec and P sisr /dis"], ["P sisr /adv", "Compare-With", "SRGAN"], ["rec and P sisr /dis", "Compare-With", "SRGAN"], ["SRGAN", "Evaluated-With", "ImageNet"], ["rec and P sisr /dis", "Evaluated-With", "ImageNet"], ["P sisr /adv", "Evaluated-With", "ImageNet"], ["SRGAN", "Evaluated-With", "DTD"], ["rec and P sisr /dis", "Evaluated-With", "DTD"], ["P sisr /adv", "Evaluated-With", "DTD"]], "rel_plus": [["P sisr /adv:Method", "Compare-With", "rec and P sisr /dis:Method"], ["P sisr /adv:Method", "Compare-With", "SRGAN:Method"], ["rec and P sisr /dis:Method", "Compare-With", "SRGAN:Method"], ["SRGAN:Method", "Evaluated-With", "ImageNet:Dataset"], ["rec and P sisr /dis:Method", "Evaluated-With", "ImageNet:Dataset"], ["P sisr /adv:Method", "Evaluated-With", "ImageNet:Dataset"], ["SRGAN:Method", "Evaluated-With", "DTD:Dataset"], ["rec and P sisr /dis:Method", "Evaluated-With", "DTD:Dataset"], ["P sisr /adv:Method", "Evaluated-With", "DTD:Dataset"]]}
{"doc_id": "67855714", "sentence": "SRGAN performs better on ImageNet , which is not that surprising considering our features extractor was trained much less than VGG 1 9 used in [ 1 ] and the VGG features being more relevant for images from the ImageNet domain .", "ner": [["SRGAN", "Method"], ["ImageNet", "Dataset"], ["features extractor", "Method"], ["VGG 1 9", "Method"], ["VGG", "Method"], ["ImageNet", "Dataset"]], "rel": [["SRGAN", "Evaluated-With", "ImageNet"], ["features extractor", "Compare-With", "VGG 1 9"]], "rel_plus": [["SRGAN:Method", "Evaluated-With", "ImageNet:Dataset"], ["features extractor:Method", "Compare-With", "VGG 1 9:Method"]]}
{"doc_id": "67855714", "sentence": "On DTD though , we can see the benefit of our method over a pre - trained VGG loss .", "ner": [["DTD", "Dataset"], ["VGG loss", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "67855714", "sentence": "Our method generates super resolved images that are really close to the real high resolution images , while we can clearly see imperfections on SRGAN 's results because of VGG 1 9 which was not trained to detect perceptual features on satellite images .", "ner": [["SRGAN", "Method"], ["VGG 1 9", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "67855714", "sentence": "However , perceptual metrics agree with what we assess qualitatively : SRGAN performs best on ImageNet but not on Sat , the distribution of which is the farthest from ImageNet .", "ner": [["SRGAN", "Method"], ["ImageNet", "Dataset"], ["ImageNet", "Dataset"]], "rel": [["SRGAN", "Evaluated-With", "ImageNet"]], "rel_plus": [["SRGAN:Method", "Evaluated-With", "ImageNet:Dataset"]]}
{"doc_id": "67855714", "sentence": "Actually , SRGAN ranks third of all four methods on Sat , just before P sisr /adv , mse , while still performing best on DTD which still is pretty close to ImageNet .", "ner": [["SRGAN", "Method"], ["Sat", "Dataset"], ["P sisr /adv", "Method"], ["mse", "Method"], ["DTD", "Dataset"], ["ImageNet", "Dataset"]], "rel": [["SRGAN", "Evaluated-With", "Sat"], ["P sisr /adv", "Evaluated-With", "Sat"], ["mse", "Evaluated-With", "Sat"], ["SRGAN", "Evaluated-With", "DTD"], ["P sisr /adv", "Evaluated-With", "DTD"], ["mse", "Evaluated-With", "DTD"]], "rel_plus": [["SRGAN:Method", "Evaluated-With", "Sat:Dataset"], ["P sisr /adv:Method", "Evaluated-With", "Sat:Dataset"], ["mse:Method", "Evaluated-With", "Sat:Dataset"], ["SRGAN:Method", "Evaluated-With", "DTD:Dataset"], ["P sisr /adv:Method", "Evaluated-With", "DTD:Dataset"], ["mse:Method", "Evaluated-With", "DTD:Dataset"]]}
{"doc_id": "67855714", "sentence": "This shows that the VGG features become less and less relevant as the dataset 's distribution part from ImageNet .", "ner": [["VGG", "Method"], ["ImageNet", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "67855714", "sentence": "From these results , we make the following conclusions : \u2022 When the considered domain is far enough from the ImageNet domain , the VGG loss introduced by [ 1 ] is no longer relevant . \u2022 The VGG network can not be fine - tuned when considering a domain for which there is no available labels for the images ( e.g. , satellite images ) .", "ner": [["ImageNet", "Dataset"], ["VGG loss", "Method"], ["VGG", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "67855714", "sentence": "Thus , the SRGAN method can not be exploited efficiently in this case . \u2022 Our framework results in a method ( P sisr /adv , rec ) that outperforms some baselines and the SRGAN method on the satellite images domain . \u2022 Even on a domain close to the ImageNet domain ( e.g. , texture images ) , one can find within our framework methods which give almost similar results to the SRGAN method , while the later is based on VGG features and thus need to train the VGG network on the whole ImageNet dataset .", "ner": [["SRGAN", "Method"], ["SRGAN", "Method"], ["ImageNet", "Dataset"], ["SRGAN", "Method"], ["VGG", "Method"], ["VGG", "Method"], ["ImageNet", "Dataset"]], "rel": [["SRGAN", "Evaluated-With", "ImageNet"], ["VGG", "Part-Of", "SRGAN"], ["VGG", "Trained-With", "ImageNet"]], "rel_plus": [["SRGAN:Method", "Evaluated-With", "ImageNet:Dataset"], ["VGG:Method", "Part-Of", "SRGAN:Method"], ["VGG:Method", "Trained-With", "ImageNet:Dataset"]]}
{"doc_id": "67855714", "sentence": "In terms of perceptual metrics , the proposed P sisr methods rank in the second position after SRGAN [ 1 ] on the datasets ImageNet and DTD , while they outperform all the baselines on the satellite images domain which is far from the ImageNet domain .   In this paper , we propose a general framework named Generative Collaborative Networks ( GCN ) which generalizes the existing methods for the problem of learning a mapping between two domains .", "ner": [["SRGAN", "Method"], ["ImageNet", "Dataset"], ["DTD", "Dataset"], ["ImageNet", "Dataset"], ["Generative Collaborative Networks", "Method"], ["GCN", "Method"]], "rel": [["SRGAN", "Evaluated-With", "ImageNet"], ["SRGAN", "Evaluated-With", "DTD"], ["GCN", "Synonym-Of", "Generative Collaborative Networks"]], "rel_plus": [["SRGAN:Method", "Evaluated-With", "ImageNet:Dataset"], ["SRGAN:Method", "Evaluated-With", "DTD:Dataset"], ["GCN:Method", "Synonym-Of", "Generative Collaborative Networks:Method"]]}
{"doc_id": "67855714", "sentence": "The GCN framework was evaluated in the context of super - resolution on three datasets ( ImageNet [ 1 0 ] , DTD [ 4 7 ] and satellite images ) .", "ner": [["GCN", "Method"], ["super - resolution", "Task"], ["ImageNet", "Dataset"], ["DTD", "Dataset"], ["satellite images", "Dataset"]], "rel": [["GCN", "Used-For", "super - resolution"], ["GCN", "Evaluated-With", "ImageNet"], ["GCN", "Evaluated-With", "DTD"], ["GCN", "Evaluated-With", "satellite images"]], "rel_plus": [["GCN:Method", "Used-For", "super - resolution:Task"], ["GCN:Method", "Evaluated-With", "ImageNet:Dataset"], ["GCN:Method", "Evaluated-With", "DTD:Dataset"], ["GCN:Method", "Evaluated-With", "satellite images:Dataset"]]}
{"doc_id": "67855714", "sentence": "We have shown that the proposed joint - learning strategy leads to a method that outperforms the state of the art [ 1 ] which uses a pre - trained features extractor network ( VGG 1 9 on ImageNet ) .", "ner": [["VGG 1 9", "Method"], ["ImageNet", "Dataset"]], "rel": [["VGG 1 9", "Trained-With", "ImageNet"]], "rel_plus": [["VGG 1 9:Method", "Trained-With", "ImageNet:Dataset"]]}
{"doc_id": "67855714", "sentence": "However , note that even for domains close to the ImageNet domain , the proposed method gives convincing ( almost similar to [ 1 ] ) results without using the whole ImageNet dataset to learn the features extractor network ( as performed in [ 1 ] ) .", "ner": [["ImageNet", "Dataset"], ["ImageNet", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "67855714", "sentence": "More generally , the GCN framework offers a large vision on the wide variety of existing loss functions used in the literature of learning mappingsbased problems ( e.g. , super - resolution , image completion , artistic style transfer , etc . ) .", "ner": [["GCN", "Method"], ["super - resolution", "Task"], ["image completion", "Task"], ["artistic style transfer", "Task"]], "rel": [["GCN", "Used-For", "super - resolution"], ["GCN", "Used-For", "image completion"], ["GCN", "Used-For", "artistic style transfer"]], "rel_plus": [["GCN:Method", "Used-For", "super - resolution:Task"], ["GCN:Method", "Used-For", "image completion:Task"], ["GCN:Method", "Used-For", "artistic style transfer:Task"]]}
{"doc_id": "198897554", "sentence": "Recent progress of self - supervised visual representation learning has achieved remarkable success on many challenging computer vision benchmarks .", "ner": [["self - supervised visual representation learning", "Method"], ["computer vision", "Task"]], "rel": [["self - supervised visual representation learning", "Used-For", "computer vision"]], "rel_plus": [["self - supervised visual representation learning:Method", "Used-For", "computer vision:Task"]]}
{"doc_id": "198897554", "sentence": "In this work , we propose a generic method for self - supervised domain adaptation , using object recognition and semantic segmentation of urban scenes as use cases .", "ner": [["self - supervised domain adaptation", "Method"], ["object recognition", "Task"], ["semantic segmentation", "Task"]], "rel": [["self - supervised domain adaptation", "Used-For", "object recognition"], ["self - supervised domain adaptation", "Used-For", "semantic segmentation"]], "rel_plus": [["self - supervised domain adaptation:Method", "Used-For", "object recognition:Task"], ["self - supervised domain adaptation:Method", "Used-For", "semantic segmentation:Task"]]}
{"doc_id": "198897554", "sentence": "Focusing on simple pretext/auxiliary tasks ( e.g. image rotation prediction ) , we assess different learning strategies to improve domain adaptation effectiveness by self - supervision .", "ner": [["image rotation prediction", "Task"], ["domain adaptation effectiveness", "Task"], ["self - supervision", "Method"]], "rel": [["self - supervision", "Used-For", "domain adaptation effectiveness"]], "rel_plus": [["self - supervision:Method", "Used-For", "domain adaptation effectiveness:Task"]]}
{"doc_id": "198897554", "sentence": "Additionally , we propose two complementary strategies to further boost the domain adaptation accuracy on semantic segmentation within our method , consisting of prediction layer alignment and batch normalization calibration .", "ner": [["domain adaptation", "Method"], ["semantic segmentation", "Task"], ["batch normalization calibration", "Method"]], "rel": [["domain adaptation", "Used-For", "semantic segmentation"]], "rel_plus": [["domain adaptation:Method", "Used-For", "semantic segmentation:Task"]]}
{"doc_id": "198897554", "sentence": "The experimental results show adaptation levels comparable to most studied domain adaptation methods , thus , bringing self - supervision as a new alternative for reaching domain adaptation .", "ner": [["domain adaptation methods", "Method"], ["self - supervision", "Method"], ["domain adaptation", "Task"]], "rel": [["self - supervision", "Used-For", "domain adaptation"]], "rel_plus": [["self - supervision:Method", "Used-For", "domain adaptation:Task"]]}
{"doc_id": "198897554", "sentence": "Training an image or video classifier requires to associate some class or attributes to the whole image/video [ 1 , 2 , 3 , 4 ] , training an object detector requires manual drawing of object bounding boxes [ 5 , 6 ] , training a CNN for semantic segmentation requires the delineation of the borders between the considered classes [ 7 , 8 ] , etc .", "ner": [["CNN", "Method"], ["semantic segmentation", "Task"]], "rel": [["CNN", "Used-For", "semantic segmentation"]], "rel_plus": [["CNN:Method", "Used-For", "semantic segmentation:Task"]]}
{"doc_id": "198897554", "sentence": "Under this umbrella we find concepts such as active learning , self - labeling , transfer learning , domain adaptation , and self - supervision .", "ner": [["active learning", "Method"], ["self - labeling", "Method"], ["transfer learning", "Method"], ["domain adaptation", "Method"], ["self - supervision", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "198897554", "sentence": "The pretext and main task ( e.g. object recognition or semantic segmentation ) are learned jointly via multi - task learning .", "ner": [["object recognition", "Task"], ["semantic segmentation", "Task"], ["multi - task learning", "Method"]], "rel": [["multi - task learning", "Used-For", "object recognition"], ["multi - task learning", "Used-For", "semantic segmentation"]], "rel_plus": [["multi - task learning:Method", "Used-For", "object recognition:Task"], ["multi - task learning:Method", "Used-For", "semantic segmentation:Task"]]}
{"doc_id": "198897554", "sentence": "In transfer learning [ 1 5 , 1 6 ] , a model is trained to perform a visual task ( e.g. image classification ) but aiming at reusing it to perform a new task ( e.g. object detection ) in a way that we minimize the amount of labeled data required to train for the new task ( e.g. fine - tuning CNNs across tasks is a basic form of transfer learning ) .", "ner": [["transfer learning", "Method"], ["image classification", "Task"], ["object detection", "Task"], ["CNNs", "Method"], ["transfer learning", "Method"]], "rel": [["transfer learning", "Used-For", "image classification"], ["transfer learning", "Used-For", "object detection"], ["CNNs", "Used-For", "transfer learning"]], "rel_plus": [["transfer learning:Method", "Used-For", "image classification:Task"], ["transfer learning:Method", "Used-For", "object detection:Task"], ["CNNs:Method", "Used-For", "transfer learning:Method"]]}
{"doc_id": "198897554", "sentence": "In domain adaptation [ 1 7 , 1 8 , 1 9 , 2 0 ] , a model is trained to perform a visual task in a specific domain ( e.g. semantic segmentation in synthetic images ) , however , we need to apply it to perform the same task in a correlated , but significantly different , domain ( e.g. semantic segmentation in real - world images ) ; Finally , self - supervised learning [ 2 1 , 2 2 , 2 3 ] focuses on learning visual models without manual labeling ; more specifically , auxiliary relatively simple tasks , known as pretext tasks in this context , are created for training a generic visual model in the form of CNN .", "ner": [["domain adaptation", "Method"], ["semantic segmentation", "Task"], ["semantic segmentation", "Task"], ["self - supervised learning", "Method"], ["CNN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "198897554", "sentence": "This pretext CNN is then concatenated with another task - specific CNN .", "ner": [["CNN", "Method"], ["CNN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "198897554", "sentence": "Sometimes , both CNN blocks are fine - tuned [ 2 4 ] , and sometimes the pretext CNN block is frozen and only the task - specific CNN block is fine - tuned [ 2 3 ] .", "ner": [["CNN blocks", "Method"], ["CNN block", "Method"], ["CNN block", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "198897554", "sentence": "Active learning can be naturally combined with transfer learning or domain adaptation [ 2 5 ] .", "ner": [["Active learning", "Method"], ["transfer learning", "Method"], ["domain adaptation", "Method"]], "rel": [["transfer learning", "Part-Of", "Active learning"], ["domain adaptation", "Part-Of", "Active learning"]], "rel_plus": [["transfer learning:Method", "Part-Of", "Active learning:Method"], ["domain adaptation:Method", "Part-Of", "Active learning:Method"]]}
{"doc_id": "198897554", "sentence": "Self - labeling can also be combined with transfer learning or domain adaptation [ 1 2 ] .", "ner": [["Self - labeling", "Method"], ["transfer learning", "Method"], ["domain adaptation", "Method"]], "rel": [["transfer learning", "Part-Of", "Self - labeling"], ["domain adaptation", "Part-Of", "Self - labeling"]], "rel_plus": [["transfer learning:Method", "Part-Of", "Self - labeling:Method"], ["domain adaptation:Method", "Part-Of", "Self - labeling:Method"]]}
{"doc_id": "198897554", "sentence": "Selfsupervised learning , as usually performed , can be seen as a type of transfer learning ( from the pretext task to the main task ) .", "ner": [["Selfsupervised learning", "Method"], ["transfer learning", "Method"]], "rel": [["Selfsupervised learning", "SubClass-Of", "transfer learning"]], "rel_plus": [["Selfsupervised learning:Method", "SubClass-Of", "transfer learning:Method"]]}
{"doc_id": "198897554", "sentence": "What has not be explored , up to the best of our knowledge , is how self - supervised learning can support domain adaptation .", "ner": [["self - supervised learning", "Method"], ["domain adaptation", "Method"]], "rel": [["self - supervised learning", "Used-For", "domain adaptation"]], "rel_plus": [["self - supervised learning:Method", "Used-For", "domain adaptation:Method"]]}
{"doc_id": "198897554", "sentence": "In other words , via self - supervised learning , we perform unsupervised domain adaptation .", "ner": [["self - supervised learning", "Method"], ["unsupervised domain adaptation", "Task"]], "rel": [["self - supervised learning", "Used-For", "unsupervised domain adaptation"]], "rel_plus": [["self - supervised learning:Method", "Used-For", "unsupervised domain adaptation:Task"]]}
{"doc_id": "198897554", "sentence": "Accordingly , and using semantic segmentation of urban scenes as challenging main - task use case , the main contributions of this work are three - fold : \u2022 We proposed a generic method for domain adaptation with self - supervised visual representation learning . \u2022 Focusing on the image rotation prediction pretext learning task , we proposed several variations and studied their domain adaptation performance . \u2022 We proposed additional strategies to further boost the self - supervised domain adaptation , including prediction layer alignment and batch normalization calibration .", "ner": [["semantic segmentation", "Task"], ["domain adaptation", "Task"], ["self - supervised visual representation learning", "Method"], ["image rotation prediction", "Task"], ["domain adaptation", "Method"], ["self - supervised domain adaptation", "Method"], ["prediction layer", "Method"], ["batch normalization calibration", "Method"]], "rel": [["self - supervised visual representation learning", "Used-For", "domain adaptation"], ["domain adaptation", "Used-For", "image rotation prediction"], ["prediction layer", "Part-Of", "self - supervised domain adaptation"], ["batch normalization calibration", "Part-Of", "self - supervised domain adaptation"]], "rel_plus": [["self - supervised visual representation learning:Method", "Used-For", "domain adaptation:Task"], ["domain adaptation:Method", "Used-For", "image rotation prediction:Task"], ["prediction layer:Method", "Part-Of", "self - supervised domain adaptation:Method"], ["batch normalization calibration:Method", "Part-Of", "self - supervised domain adaptation:Method"]]}
{"doc_id": "198897554", "sentence": "In Section 2 , we review related self - supervised representation learning and domain adaptation methods .", "ner": [["self - supervised representation learning", "Method"], ["domain adaptation methods", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "198897554", "sentence": "In Section 4 , we conduct experiments on domain adaptation for object recognition as well as semantic segmentation , via our method .", "ner": [["domain adaptation", "Method"], ["object recognition", "Task"], ["semantic segmentation", "Task"]], "rel": [["domain adaptation", "Used-For", "object recognition"], ["domain adaptation", "Used-For", "semantic segmentation"]], "rel_plus": [["domain adaptation:Method", "Used-For", "object recognition:Task"], ["domain adaptation:Method", "Used-For", "semantic segmentation:Task"]]}
{"doc_id": "198897554", "sentence": "Other works incorporate image colorization [ 3 0 ] or image inpainting [ 3 1 ] as pretext tasks .", "ner": [["image colorization", "Task"], ["image inpainting", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "198897554", "sentence": "In [ 3 2 ] , relative depth prediction is used as a self - supervised proxy task , which has shown improvements to the downstream tasks , including semantic segmentation and car detection .", "ner": [["semantic segmentation", "Task"], ["car detection", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "198897554", "sentence": "There have been numerous domain adaptation methods proposed for object recognition since [ 3 3 ] .", "ner": [["domain adaptation methods", "Method"], ["object recognition", "Task"]], "rel": [["domain adaptation methods", "Used-For", "object recognition"]], "rel_plus": [["domain adaptation methods:Method", "Used-For", "object recognition:Task"]]}
{"doc_id": "198897554", "sentence": "Among existing domain adaptation methods , some try to align domains at input level , including GAN - based methods [ 3 4 ] and image stylization ones [ 3 5 , 3 6 , 3 7 ] .", "ner": [["domain adaptation methods", "Method"], ["GAN - based methods", "Method"], ["image stylization", "Method"]], "rel": [["GAN - based methods", "SubClass-Of", "domain adaptation methods"], ["image stylization", "SubClass-Of", "domain adaptation methods"]], "rel_plus": [["GAN - based methods:Method", "SubClass-Of", "domain adaptation methods:Method"], ["image stylization:Method", "SubClass-Of", "domain adaptation methods:Method"]]}
{"doc_id": "198897554", "sentence": "In [ 1 8 ] , a curriculum learning style is applied , where superpixels are computed in source and target domains and their distributions must match as auxiliary task during semantic segmentation training .", "ner": [["curriculum learning", "Method"], ["semantic segmentation", "Task"]], "rel": [["curriculum learning", "Used-For", "semantic segmentation"]], "rel_plus": [["curriculum learning:Method", "Used-For", "semantic segmentation:Task"]]}
{"doc_id": "198897554", "sentence": "Comparing to these work , our method is not specifically designed for semantic segmentation but generic for various computer vision tasks .", "ner": [["semantic segmentation", "Task"], ["computer vision", "Task"]], "rel": [["semantic segmentation", "SubTask-Of", "computer vision"]], "rel_plus": [["semantic segmentation:Task", "SubTask-Of", "computer vision:Task"]]}
{"doc_id": "198897554", "sentence": "In [ 4 5 ] , the self - supervised learning method jigsaw puzzle is used for object recognition domain generalization and adaptation .", "ner": [["self - supervised learning", "Method"], ["jigsaw puzzle", "Method"], ["object recognition domain generalization", "Task"]], "rel": [["jigsaw puzzle", "SubClass-Of", "self - supervised learning"], ["jigsaw puzzle", "Used-For", "object recognition domain generalization"]], "rel_plus": [["jigsaw puzzle:Method", "SubClass-Of", "self - supervised learning:Method"], ["jigsaw puzzle:Method", "Used-For", "object recognition domain generalization:Task"]]}
{"doc_id": "198897554", "sentence": "As we will see in the experimental section , our method outperforms the jigsaw puzzle based method on both object recognition and semantic segmentation tasks . .", "ner": [["jigsaw puzzle", "Method"], ["object recognition", "Task"], ["semantic segmentation", "Task"]], "rel": [["jigsaw puzzle", "Used-For", "object recognition"], ["jigsaw puzzle", "Used-For", "semantic segmentation"]], "rel_plus": [["jigsaw puzzle:Method", "Used-For", "object recognition:Task"], ["jigsaw puzzle:Method", "Used-For", "semantic segmentation:Task"]]}
{"doc_id": "198897554", "sentence": "The final semantic segmentation accuracy we obtain in target domain is superior to most of these methods , only behind [ 1 4 ] which is specific for semantic segmentation , and still not being far apart .", "ner": [["semantic segmentation", "Task"], ["semantic segmentation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "198897554", "sentence": "Finally , we introduce domain adaptation steps which complement self - supervision . 3. 1 Self - supervised domain adaptation Taking semantic segmentation as an example of main task , but without lose of generality , our method is shown in Figure 1 ; where E denotes an encoder network ( feature extractor ) and S a decoder network ( specific of the main task ) , so that E + S is a CNN for semantic segmentation .", "ner": [["domain adaptation", "Method"], ["self - supervision", "Method"], ["Self - supervised", "Method"], ["semantic segmentation", "Task"], ["encoder network", "Method"], ["feature extractor", "Method"], ["decoder network", "Method"], ["CNN", "Method"], ["semantic segmentation", "Task"]], "rel": [["Self - supervised", "Used-For", "semantic segmentation"], ["feature extractor", "Synonym-Of", "encoder network"], ["CNN", "Used-For", "semantic segmentation"]], "rel_plus": [["Self - supervised:Method", "Used-For", "semantic segmentation:Task"], ["feature extractor:Method", "Synonym-Of", "encoder network:Method"], ["CNN:Method", "Used-For", "semantic segmentation:Task"]]}
{"doc_id": "198897554", "sentence": "This model consists in the CNN E + P , where E is shared with the CNN of the main task .", "ner": [["CNN", "Method"], ["CNN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "198897554", "sentence": "Given a set of N s labeled training images from the source domain , the segmentation network takes as input the feature maps from E(x s i ) and outputs the segmentation predictions : H \u00d7 W \u00d7 C , where C is the number of semantic categories , H and W are the height and width of the output respectively , and \u03b8 e and \u03b8 s convey the parameters of E and S , respectively .", "ner": [["segmentation network", "Method"], ["segmentation", "Task"]], "rel": [["segmentation network", "Used-For", "segmentation"]], "rel_plus": [["segmentation network:Method", "Used-For", "segmentation:Task"]]}
{"doc_id": "198897554", "sentence": "The semantic segmentation training objective that we need to solve for E and S is : where the segmentation loss is the cross - entropy loss , defined as : With Eq. ( 1 ) and Eq. ( 3 ) , the objective function that selfsupervised domain adaptation must solve is : where \u03bb p is the weight to balance the two losses .", "ner": [["semantic segmentation", "Task"], ["segmentation loss", "Method"], ["cross - entropy loss", "Method"]], "rel": [["segmentation loss", "SubClass-Of", "cross - entropy loss"]], "rel_plus": [["segmentation loss:Method", "SubClass-Of", "cross - entropy loss:Method"]]}
{"doc_id": "198897554", "sentence": "In this section , we introduce two different strategies to complement self - supervised domain adaptation , including adversarial training for prediction layer alignment and batch normalization .   The proposed pretext task learning is able to perform domain adaptation at feature level , however , the predicted semantic labels may still not be well aligned .", "ner": [["self - supervised domain adaptation", "Method"], ["adversarial training", "Method"], ["batch normalization", "Method"], ["pretext task learning", "Method"], ["domain adaptation", "Task"]], "rel": [["adversarial training", "Part-Of", "self - supervised domain adaptation"], ["batch normalization", "Part-Of", "self - supervised domain adaptation"], ["pretext task learning", "Used-For", "domain adaptation"]], "rel_plus": [["adversarial training:Method", "Part-Of", "self - supervised domain adaptation:Method"], ["batch normalization:Method", "Part-Of", "self - supervised domain adaptation:Method"], ["pretext task learning:Method", "Used-For", "domain adaptation:Task"]]}
{"doc_id": "198897554", "sentence": "The batch normalization ( BN ) is originally designed to reduce the internal covariate shift and speedup the training of deep neural networks .", "ner": [["batch normalization", "Method"], ["BN", "Method"], ["deep neural networks", "Method"]], "rel": [["BN", "Synonym-Of", "batch normalization"], ["batch normalization", "Part-Of", "deep neural networks"]], "rel_plus": [["BN:Method", "Synonym-Of", "batch normalization:Method"], ["batch normalization:Method", "Part-Of", "deep neural networks:Method"]]}
{"doc_id": "198897554", "sentence": "Our BN calibration is similar to the AdaBN method [ 4 6 ] .", "ner": [["BN calibration", "Method"], ["AdaBN", "Method"]], "rel": [["BN calibration", "Compare-With", "AdaBN"]], "rel_plus": [["BN calibration:Method", "Compare-With", "AdaBN:Method"]]}
{"doc_id": "198897554", "sentence": "AdaBN is applied at the inference stage , i.e. to the testing images , while we use BN calibration as a post training process with target domain training images .", "ner": [["AdaBN", "Method"], ["BN calibration", "Method"]], "rel": [["AdaBN", "Compare-With", "BN calibration"]], "rel_plus": [["AdaBN:Method", "Compare-With", "BN calibration:Method"]]}
{"doc_id": "198897554", "sentence": "In this section , we conduct experiments to validate the proposed domain adaptation method for both object recognition and semantic segmentation .", "ner": [["domain adaptation method", "Method"], ["object recognition", "Task"], ["semantic segmentation", "Task"]], "rel": [["domain adaptation method", "Used-For", "object recognition"], ["domain adaptation method", "Used-For", "semantic segmentation"]], "rel_plus": [["domain adaptation method:Method", "Used-For", "object recognition:Task"], ["domain adaptation method:Method", "Used-For", "semantic segmentation:Task"]]}
{"doc_id": "198897554", "sentence": "We use   For object recognition , we evaluate on the multiple source domain adaptation dataset PACS [ 4 9 ] , which has 7 object categories and 4 domains ( Photo , Art Paintings , Cartoon and Sketches ) .", "ner": [["object recognition", "Task"], ["domain adaptation", "Method"], ["PACS", "Dataset"]], "rel": [["PACS", "Benchmark-For", "object recognition"], ["domain adaptation", "Used-For", "object recognition"], ["domain adaptation", "Evaluated-With", "PACS"]], "rel_plus": [["PACS:Dataset", "Benchmark-For", "object recognition:Task"], ["domain adaptation:Method", "Used-For", "object recognition:Task"], ["domain adaptation:Method", "Evaluated-With", "PACS:Dataset"]]}
{"doc_id": "198897554", "sentence": "Following [ 4 5 ] , we also compare to the domain discovery method DDiscovery [ 5 0 ] and Dial [ 5 1 ] .", "ner": [["DDiscovery", "Method"], ["Dial", "Method"]], "rel": [["DDiscovery", "Compare-With", "Dial"]], "rel_plus": [["DDiscovery:Method", "Compare-With", "Dial:Method"]]}
{"doc_id": "198897554", "sentence": "To make a fair comparison , we run jigsaw puzzle method with the same random seeds and denoted by Ours(jigsaw ) .", "ner": [["jigsaw puzzle", "Method"], ["Ours(jigsaw )", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "198897554", "sentence": "For semantic segmentation , we adapt semantic segmentation models from the source domain of synthetic images to the target domain of real - world images .", "ner": [["semantic segmentation", "Task"], ["semantic segmentation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "198897554", "sentence": "For the synthetic datasets , we use SYNTHIA [ 5 2 ] and GTA 5 [ 5 3 ] , and for the target domain , we use the Cityscapes dataset [ 7 ] .", "ner": [["SYNTHIA", "Dataset"], ["GTA 5", "Dataset"], ["Cityscapes", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "198897554", "sentence": "The GTA 5 [ 5 3 ] dataset is rendered from the Grand Theft Auto V video game .", "ner": [["GTA 5", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "198897554", "sentence": "For SYNTHIA dataset , we use the SYNTHIA - RAND - CITYSCAPES set [ 5 2 ] as the source domain training set , which contains 9 4 0 0 images .", "ner": [["SYNTHIA", "Dataset"], ["SYNTHIA - RAND - CITYSCAPES", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "198897554", "sentence": "We evaluate with the 1 6 common classes for SYNTHIA to Cityscapes domain adaptation .", "ner": [["SYNTHIA", "Dataset"], ["Cityscapes", "Dataset"], ["domain adaptation", "Method"]], "rel": [["domain adaptation", "Evaluated-With", "SYNTHIA"], ["domain adaptation", "Evaluated-With", "Cityscapes"]], "rel_plus": [["domain adaptation:Method", "Evaluated-With", "SYNTHIA:Dataset"], ["domain adaptation:Method", "Evaluated-With", "Cityscapes:Dataset"]]}
{"doc_id": "198897554", "sentence": "If not otherwise specified , all the experiments in this section use ResNet - 1 0 1 as backbone network and the domain adaptation is from GTA 5 to Cityscapes .", "ner": [["ResNet - 1 0 1", "Method"], ["domain adaptation", "Method"], ["GTA 5", "Dataset"], ["Cityscapes", "Dataset"]], "rel": [["ResNet - 1 0 1", "Part-Of", "domain adaptation"], ["domain adaptation", "Evaluated-With", "GTA 5"], ["domain adaptation", "Evaluated-With", "Cityscapes"]], "rel_plus": [["ResNet - 1 0 1:Method", "Part-Of", "domain adaptation:Method"], ["domain adaptation:Method", "Evaluated-With", "GTA 5:Dataset"], ["domain adaptation:Method", "Evaluated-With", "Cityscapes:Dataset"]]}
{"doc_id": "198897554", "sentence": "We also compare our method Rot to the jigsaw puzzle based self - supervision [ 4 5 ] .", "ner": [["jigsaw puzzle", "Method"], ["self - supervision", "Method"]], "rel": [["self - supervision", "Used-For", "jigsaw puzzle"]], "rel_plus": [["self - supervision:Method", "Used-For", "jigsaw puzzle:Method"]]}
{"doc_id": "198897554", "sentence": "The results are shown in Table 3 , where SYN 2 CS denotes SYNTHIA to Cityscapes domain adaptation and GTA 2 CS for GTA 5 to Cityscapes .", "ner": [["SYN 2 CS", "Dataset"], ["SYNTHIA", "Dataset"], ["Cityscapes", "Dataset"], ["domain adaptation", "Method"], ["GTA 2 CS", "Dataset"], ["GTA 5", "Dataset"], ["Cityscapes", "Dataset"]], "rel": [["domain adaptation", "Evaluated-With", "SYN 2 CS"], ["domain adaptation", "Evaluated-With", "SYNTHIA"], ["domain adaptation", "Evaluated-With", "Cityscapes"]], "rel_plus": [["domain adaptation:Method", "Evaluated-With", "SYN 2 CS:Dataset"], ["domain adaptation:Method", "Evaluated-With", "SYNTHIA:Dataset"], ["domain adaptation:Method", "Evaluated-With", "Cityscapes:Dataset"]]}
{"doc_id": "198897554", "sentence": "As , in this case , the decoder of the segmentation network is simply an up - sampling layer without any learnable parameter , the Final layer is actually the prediction layer of the segmentation network .", "ner": [["segmentation network", "Method"], ["up - sampling layer", "Method"], ["prediction layer", "Method"], ["segmentation network", "Method"]], "rel": [["up - sampling layer", "Part-Of", "segmentation network"], ["prediction layer", "Part-Of", "segmentation network"]], "rel_plus": [["up - sampling layer:Method", "Part-Of", "segmentation network:Method"], ["prediction layer:Method", "Part-Of", "segmentation network:Method"]]}
{"doc_id": "198897554", "sentence": "Figure 5 depicts the accuracy of pretext task vs. domain adaptation ( in the semantic segmentation task For a fixed model architecture , the best performing domain adaptation model does have relatively higher accuracy on pretext task , but the best pretext task accuracy does not indicate best domain adaptation model .", "ner": [["domain adaptation", "Method"], ["semantic segmentation", "Task"], ["domain adaptation model", "Method"], ["domain adaptation model", "Method"]], "rel": [["domain adaptation", "Used-For", "semantic segmentation"]], "rel_plus": [["domain adaptation:Method", "Used-For", "semantic segmentation:Task"]]}
{"doc_id": "198897554", "sentence": "We believe that how to design a pretext task to reliably estimate the accuracy of the main task in the target domain is an interesting and challenging future work . bined Adv and BN , we obtain the best results , improving Rot by 2. 1 percentage points .", "ner": [["Adv", "Method"], ["BN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "198897554", "sentence": "Tabel 5 shows more results with other architectures and datasets , where + Adv has consistent improvements to Rot but + Adv+BN gets saturated for the SYN 2 CS problem .", "ner": [["Adv", "Method"], ["Adv+BN", "Method"], ["SYN 2 CS", "Dataset"]], "rel": [["Adv", "Compare-With", "Adv+BN"], ["Adv", "Evaluated-With", "SYN 2 CS"], ["Adv+BN", "Evaluated-With", "SYN 2 CS"]], "rel_plus": [["Adv:Method", "Compare-With", "Adv+BN:Method"], ["Adv:Method", "Evaluated-With", "SYN 2 CS:Dataset"], ["Adv+BN:Method", "Evaluated-With", "SYN 2 CS:Dataset"]]}
{"doc_id": "198897554", "sentence": "To understand why BN does not have consistent improvements , we further conduct experiments using only BN calibration for domain adaptation .", "ner": [["BN", "Method"], ["BN calibration", "Method"], ["domain adaptation", "Method"]], "rel": [["BN calibration", "Part-Of", "domain adaptation"]], "rel_plus": [["BN calibration:Method", "Part-Of", "domain adaptation:Method"]]}
{"doc_id": "198897554", "sentence": "BN calibration alone achieves surprisingly good results , and the best domain adaptation gain even reaches 6. 8 percentage points .", "ner": [["BN calibration", "Method"], ["domain adaptation gain", "Task"]], "rel": [["BN calibration", "Used-For", "domain adaptation gain"]], "rel_plus": [["BN calibration:Method", "Used-For", "domain adaptation gain:Task"]]}
{"doc_id": "198897554", "sentence": "This might be because Rot and Rot+Adv have already learned domain invariant representation that effectively reduces the covariate sift and BN calibration could not contribute more to the adapted model .", "ner": [["BN calibration", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "198897554", "sentence": "The reason that Adv gives consistent rise to the base method is because Adv further aligns the predicted label distributions which is more complementary adaptation to the Rot than the provided by BN .", "ner": [["BN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "198897554", "sentence": "The compared methods cover large varieties of domain adaptation mechanisms , including input/feature/output level alignment methods , curriculum and self - labeling based methods .", "ner": [["domain adaptation mechanisms", "Method"], ["curriculum", "Method"], ["self - labeling", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "198897554", "sentence": "Looking at the absolute accuracy ( Adapt columns ) of the best performing architectures ( i.e. , ResNet based ones ) , only CBST is better than ours in SYN 2 CS and GTA 2 CS , and FCAN in GTA 2 CS .", "ner": [["ResNet", "Method"], ["CBST", "Method"], ["SYN 2 CS", "Dataset"], ["GTA 2 CS", "Dataset"], ["FCAN", "Method"], ["GTA 2 CS", "Dataset"]], "rel": [["CBST", "Evaluated-With", "SYN 2 CS"], ["CBST", "Evaluated-With", "GTA 2 CS"], ["FCAN", "Evaluated-With", "GTA 2 CS"]], "rel_plus": [["CBST:Method", "Evaluated-With", "SYN 2 CS:Dataset"], ["CBST:Method", "Evaluated-With", "GTA 2 CS:Dataset"], ["FCAN:Method", "Evaluated-With", "GTA 2 CS:Dataset"]]}
{"doc_id": "198897554", "sentence": "We also find that a deeper network ( ResNet - 1 0 1 ) can achieve better domain adaptation gain than the shallow one ( DRN - 2 6 ) .", "ner": [["ResNet - 1 0 1", "Method"], ["domain adaptation gain", "Task"], ["DRN - 2 6", "Method"]], "rel": [["ResNet - 1 0 1", "Used-For", "domain adaptation gain"], ["DRN - 2 6", "Used-For", "domain adaptation gain"], ["ResNet - 1 0 1", "Compare-With", "DRN - 2 6"]], "rel_plus": [["ResNet - 1 0 1:Method", "Used-For", "domain adaptation gain:Task"], ["DRN - 2 6:Method", "Used-For", "domain adaptation gain:Task"], ["ResNet - 1 0 1:Method", "Compare-With", "DRN - 2 6:Method"]]}
{"doc_id": "198897554", "sentence": "The only method that outperforms ours systematically is CBST , which is specifically designed for semantic segmentation on urban scenarios .", "ner": [["CBST", "Method"], ["semantic segmentation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "198897554", "sentence": "In this work , we have explored self - supervised learning for domain adaptation .", "ner": [["self - supervised learning", "Task"], ["domain adaptation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "198897554", "sentence": "Taking object recognition and semantic segmentation of urban scenes as relevant use cases , we have performed an ablative analysis of the different components included in our overall domain adaptation procedure .", "ner": [["object recognition", "Task"], ["semantic segmentation", "Task"], ["domain adaptation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "202719032", "sentence": "To understand the above issues , experiments are performed on the state - of - the - art detectors : Faster R - CNN , R - FCN , Mask R - CNN and SSD .", "ner": [["Faster R - CNN", "Method"], ["R - FCN", "Method"], ["Mask R - CNN", "Method"], ["SSD", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202719032", "sentence": "Critical findings are observed : ( 1 ) The best balance between detection accuracy , detection speed and file size is achieved at 8 times downsampling captured with a $ 4 0 \\times$ objective ; ( 2 ) compression which reduces the file size dramatically , does not necessarily have an adverse effect on overall accuracy ; ( 3 ) reducing the amount of training data to some extents causes a drop in precision but has a negligible impact on the recall ; ( 4 ) in most cases , Faster R - CNN achieves the best accuracy in the glomerulus detection task .", "ner": [["detection", "Task"], ["detection", "Task"], ["Faster R - CNN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202719032", "sentence": "It is impossible to directly feed such large images to Computer Aided Diagnostic ( CAD ) systems based on Convolutional Neural Networks ( CNNs ) .", "ner": [["Computer Aided Diagnostic", "Method"], ["CAD", "Method"], ["Convolutional Neural Networks", "Method"], ["CNNs", "Method"]], "rel": [["CAD", "Synonym-Of", "Computer Aided Diagnostic"], ["CNNs", "Synonym-Of", "Convolutional Neural Networks"]], "rel_plus": [["CAD:Method", "Synonym-Of", "Computer Aided Diagnostic:Method"], ["CNNs:Method", "Synonym-Of", "Convolutional Neural Networks:Method"]]}
{"doc_id": "202719032", "sentence": "After that , the patches are fed into the CNN detector to get detection result .", "ner": [["CNN", "Method"], ["detection", "Task"]], "rel": [["CNN", "Used-For", "detection"]], "rel_plus": [["CNN:Method", "Used-For", "detection:Task"]]}
{"doc_id": "202719032", "sentence": "For instance , reducing the training data by 2 0 % only reduces Faster R - CNN detection performance from 0. 7 8 1 mAP to 0. 7 3 4 mAP at an Intersection of Union ( IoU ) threshold of 0. 5 .", "ner": [["Faster R - CNN", "Method"], ["detection", "Task"]], "rel": [["Faster R - CNN", "Used-For", "detection"]], "rel_plus": [["Faster R - CNN:Method", "Used-For", "detection:Task"]]}
{"doc_id": "202719032", "sentence": "Note that , this finding does not apply to SSD [ 1 8 ] which is also the worst performing method in the study . \u2022 Of the four object detectors : Faster R - CNN [ 2 4 ] , Mask R - CNN [ 1 0 ] , R - FCN [ 4 ] and SSD [ 1 8 ] , Faster R - CNN has the best performance of 0. 7 8 1 mAP at an IoU of 0. 5 .", "ner": [["SSD", "Method"], ["object detectors", "Method"], ["Faster R - CNN", "Method"], ["Mask R - CNN", "Method"], ["R - FCN", "Method"], ["SSD", "Method"], ["Faster R - CNN", "Method"]], "rel": [["Faster R - CNN", "SubClass-Of", "object detectors"], ["Mask R - CNN", "SubClass-Of", "object detectors"], ["R - FCN", "SubClass-Of", "object detectors"], ["SSD", "SubClass-Of", "object detectors"], ["Faster R - CNN", "SubClass-Of", "object detectors"]], "rel_plus": [["Faster R - CNN:Method", "SubClass-Of", "object detectors:Method"], ["Mask R - CNN:Method", "SubClass-Of", "object detectors:Method"], ["R - FCN:Method", "SubClass-Of", "object detectors:Method"], ["SSD:Method", "SubClass-Of", "object detectors:Method"], ["Faster R - CNN:Method", "SubClass-Of", "object detectors:Method"]]}
{"doc_id": "202719032", "sentence": "Although pre - processing and the amount of training data will strongly affect detection accuracy for automatic glomerular analysis , most literature about glomerulus classification and detection mention little about these problems .", "ner": [["detection", "Task"], ["classification", "Task"], ["detection", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "202719032", "sentence": "Simon et al. [ 2 7 ] used local binary patterns ( LBPs ) image feature vector to train a support vector machine ( SVM ) model to classify glomeruli on light microscopy ( LM ) renal images .", "ner": [["local binary patterns", "Method"], ["LBPs", "Method"], ["support vector machine", "Method"], ["SVM", "Method"]], "rel": [["LBPs", "Synonym-Of", "local binary patterns"], ["SVM", "Synonym-Of", "support vector machine"]], "rel_plus": [["LBPs:Method", "Synonym-Of", "local binary patterns:Method"], ["SVM:Method", "Synonym-Of", "support vector machine:Method"]]}
{"doc_id": "202719032", "sentence": "Since the CNN was first proposed for object detection task as Region - based CNN ( R - CNN ) by Girshick et al. [ 9 ] , various CNN - based models have produced impressive object detection performance .", "ner": [["CNN", "Method"], ["object detection", "Task"], ["Region - based CNN", "Method"], ["R - CNN", "Method"], ["CNN - based models", "Method"], ["object detection", "Task"]], "rel": [["Region - based CNN", "SubClass-Of", "CNN"], ["CNN", "Used-For", "object detection"], ["R - CNN", "Synonym-Of", "Region - based CNN"], ["CNN - based models", "Used-For", "object detection"]], "rel_plus": [["Region - based CNN:Method", "SubClass-Of", "CNN:Method"], ["CNN:Method", "Used-For", "object detection:Task"], ["R - CNN:Method", "Synonym-Of", "Region - based CNN:Method"], ["CNN - based models:Method", "Used-For", "object detection:Task"]]}
{"doc_id": "202719032", "sentence": "Several works have applied CNN methods to the computational pathology domain , such as cancer detection [ 5 ] , [ 1 2 ] , [ 2 8 ] , organ segmentation on CT and MRI images [ 2 ] , [ 2 1 ] and cell classification [ 3 ] , [ 7 ] .", "ner": [["CNN", "Method"], ["cancer detection", "Task"], ["organ segmentation on CT and MRI images", "Task"], ["cell classification", "Task"]], "rel": [["CNN", "Used-For", "cancer detection"], ["CNN", "Used-For", "organ segmentation on CT and MRI images"], ["CNN", "Used-For", "cell classification"]], "rel_plus": [["CNN:Method", "Used-For", "cancer detection:Task"], ["CNN:Method", "Used-For", "organ segmentation on CT and MRI images:Task"], ["CNN:Method", "Used-For", "cell classification:Task"]]}
{"doc_id": "202719032", "sentence": "Modern CNN based object detectors can be roughly categorized into two categories : the two - stage detectors , such as Faster R - CNN [ 2 4 ] , Mask R - CNN [ 1 0 ] , and R - FCN [ 4 ] , and the one - stage detectors , such as YOLO [ 2 2 ] , YOLOv 2 [ 2 3 ] , SSD [ 1 8 ] and RetinaNet [ 1 6 ] .", "ner": [["CNN based object detectors", "Method"], ["Faster R - CNN", "Method"], ["Mask R - CNN", "Method"], ["R - FCN", "Method"], ["YOLO", "Method"], ["YOLOv 2", "Method"], ["SSD", "Method"], ["RetinaNet", "Method"]], "rel": [["Faster R - CNN", "SubClass-Of", "CNN based object detectors"], ["Mask R - CNN", "SubClass-Of", "CNN based object detectors"], ["R - FCN", "SubClass-Of", "CNN based object detectors"], ["YOLO", "SubClass-Of", "CNN based object detectors"], ["YOLOv 2", "SubClass-Of", "CNN based object detectors"], ["SSD", "SubClass-Of", "CNN based object detectors"], ["RetinaNet", "SubClass-Of", "CNN based object detectors"]], "rel_plus": [["Faster R - CNN:Method", "SubClass-Of", "CNN based object detectors:Method"], ["Mask R - CNN:Method", "SubClass-Of", "CNN based object detectors:Method"], ["R - FCN:Method", "SubClass-Of", "CNN based object detectors:Method"], ["YOLO:Method", "SubClass-Of", "CNN based object detectors:Method"], ["YOLOv 2:Method", "SubClass-Of", "CNN based object detectors:Method"], ["SSD:Method", "SubClass-Of", "CNN based object detectors:Method"], ["RetinaNet:Method", "SubClass-Of", "CNN based object detectors:Method"]]}
{"doc_id": "202719032", "sentence": "In the two - stage detector approach , the input image is first fed to a Region Proposal Network ( RPN ) to generate a sparse set of candidate boxes .", "ner": [["Region Proposal Network", "Method"], ["RPN", "Method"]], "rel": [["RPN", "Synonym-Of", "Region Proposal Network"]], "rel_plus": [["RPN:Method", "Synonym-Of", "Region Proposal Network:Method"]]}
{"doc_id": "202719032", "sentence": "We employed four object detectors for our experiments : Faster R - CNN , R - FCN , Mask R - CNN and SSD .", "ner": [["Faster R - CNN", "Method"], ["R - FCN", "Method"], ["Mask R - CNN", "Method"], ["SSD", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202719032", "sentence": "Faster R - CNN [ 2 4 ] is an improved version of R - CNN [ 9 ] and Fast R - CNN [ 8 ] .", "ner": [["Faster R - CNN", "Method"], ["R - CNN", "Method"], ["Fast R - CNN", "Method"]], "rel": [["Faster R - CNN", "SubClass-Of", "R - CNN"], ["Faster R - CNN", "SubClass-Of", "Fast R - CNN"]], "rel_plus": [["Faster R - CNN:Method", "SubClass-Of", "R - CNN:Method"], ["Faster R - CNN:Method", "SubClass-Of", "Fast R - CNN:Method"]]}
{"doc_id": "202719032", "sentence": "Inspired by image classification , R - CNN directly applies a CNN based image classifier on a set of generated region proposals [ 1 3 ] .", "ner": [["image classification", "Task"], ["R - CNN", "Method"], ["CNN based image classifier", "Method"]], "rel": [["CNN based image classifier", "Part-Of", "R - CNN"]], "rel_plus": [["CNN based image classifier:Method", "Part-Of", "R - CNN:Method"]]}
{"doc_id": "202719032", "sentence": "Although R - CNN improves the state - of - the - art detection accuracy , the proposal features are calculated multiple times which leads to large run time [ 1 3 ] .", "ner": [["R - CNN", "Method"], ["detection", "Task"]], "rel": [["R - CNN", "Used-For", "detection"]], "rel_plus": [["R - CNN:Method", "Used-For", "detection:Task"]]}
{"doc_id": "202719032", "sentence": "Fast R - CNN alleviates this problem by making all region proposal features share one - time generated feature extraction [ 1 3 ] .", "ner": [["Fast R - CNN", "Method"], ["feature extraction", "Method"]], "rel": [["feature extraction", "Part-Of", "Fast R - CNN"]], "rel_plus": [["feature extraction:Method", "Part-Of", "Fast R - CNN:Method"]]}
{"doc_id": "202719032", "sentence": "However , both R - CNN and Fast R - CNN depend on external proposal generators which then become the new bottleneck as everything apart from the regional proposal generator runs in the GPU .", "ner": [["R - CNN", "Method"], ["Fast R - CNN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202719032", "sentence": "Faster R - CNN solves this problem by using a neural network called RPN to generate the candidate anchors and it is then able to be trained end - to - end .", "ner": [["Faster R - CNN", "Method"], ["neural network", "Method"], ["RPN", "Method"]], "rel": [["RPN", "Part-Of", "Faster R - CNN"], ["RPN", "SubClass-Of", "neural network"]], "rel_plus": [["RPN:Method", "Part-Of", "Faster R - CNN:Method"], ["RPN:Method", "SubClass-Of", "neural network:Method"]]}
{"doc_id": "202719032", "sentence": "R - FCN [ 4 ] is proposed based on Faster R - CNN .", "ner": [["R - FCN", "Method"], ["Faster R - CNN", "Method"]], "rel": [["R - FCN", "SubClass-Of", "Faster R - CNN"]], "rel_plus": [["R - FCN:Method", "SubClass-Of", "Faster R - CNN:Method"]]}
{"doc_id": "202719032", "sentence": "R - FCN modifies the backbone network used for feature extraction in Faster R - CNN .", "ner": [["R - FCN", "Method"], ["Faster R - CNN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202719032", "sentence": "Mask R - CNN [ 1 0 ] is mainly targeted to address the instance segmentation problem .", "ner": [["Mask R - CNN", "Method"], ["instance segmentation", "Task"]], "rel": [["Mask R - CNN", "Used-For", "instance segmentation"]], "rel_plus": [["Mask R - CNN:Method", "Used-For", "instance segmentation:Task"]]}
{"doc_id": "202719032", "sentence": "However , there are several improvements that allow Mask R - CNN to outperform Faster R - CNN .", "ner": [["Mask R - CNN", "Method"], ["Faster R - CNN", "Method"]], "rel": [["Mask R - CNN", "Compare-With", "Faster R - CNN"]], "rel_plus": [["Mask R - CNN:Method", "Compare-With", "Faster R - CNN:Method"]]}
{"doc_id": "202719032", "sentence": "In addition , unlike Faster R - CNN , Mask RCNN uses a Feature Pyramid Network ( FPN ) [ 1 5 ] with ResNet as its backbone .", "ner": [["Faster R - CNN", "Method"], ["Mask RCNN", "Method"], ["Feature Pyramid Network", "Method"], ["FPN", "Method"], ["ResNet", "Method"]], "rel": [["Mask RCNN", "Compare-With", "Faster R - CNN"], ["Feature Pyramid Network", "Part-Of", "Mask RCNN"], ["FPN", "Synonym-Of", "Feature Pyramid Network"], ["ResNet", "Part-Of", "Feature Pyramid Network"]], "rel_plus": [["Mask RCNN:Method", "Compare-With", "Faster R - CNN:Method"], ["Feature Pyramid Network:Method", "Part-Of", "Mask RCNN:Method"], ["FPN:Method", "Synonym-Of", "Feature Pyramid Network:Method"], ["ResNet:Method", "Part-Of", "Feature Pyramid Network:Method"]]}
{"doc_id": "202719032", "sentence": "SSD is similar to RPN , since both provide the detection results in one step .", "ner": [["SSD", "Method"], ["RPN", "Method"], ["detection", "Task"]], "rel": [["SSD", "Compare-With", "RPN"], ["SSD", "Used-For", "detection"], ["RPN", "Used-For", "detection"]], "rel_plus": [["SSD:Method", "Compare-With", "RPN:Method"], ["SSD:Method", "Used-For", "detection:Task"], ["RPN:Method", "Used-For", "detection:Task"]]}
{"doc_id": "202719032", "sentence": "The difference is that whilst RPN provides object/non - object classification , SSD provides class - level classification .", "ner": [["RPN", "Method"], ["object/non - object classification", "Task"], ["SSD", "Method"], ["class - level classification", "Task"]], "rel": [["RPN", "Used-For", "object/non - object classification"], ["RPN", "Compare-With", "SSD"], ["SSD", "Used-For", "class - level classification"]], "rel_plus": [["RPN:Method", "Used-For", "object/non - object classification:Task"], ["RPN:Method", "Compare-With", "SSD:Method"], ["SSD:Method", "Used-For", "class - level classification:Task"]]}
{"doc_id": "202719032", "sentence": "In our experiments , Faster R - CNN , Mask R - CNN and R - FCN used Resnet 1 0 1 [ 1 1 ] as the backbone network .", "ner": [["Faster R - CNN", "Method"], ["Mask R - CNN", "Method"], ["R - FCN", "Method"], ["Resnet 1 0 1", "Method"]], "rel": [["Resnet 1 0 1", "Part-Of", "Faster R - CNN"], ["Resnet 1 0 1", "Part-Of", "Mask R - CNN"], ["Resnet 1 0 1", "Part-Of", "R - FCN"]], "rel_plus": [["Resnet 1 0 1:Method", "Part-Of", "Faster R - CNN:Method"], ["Resnet 1 0 1:Method", "Part-Of", "Mask R - CNN:Method"], ["Resnet 1 0 1:Method", "Part-Of", "R - FCN:Method"]]}
{"doc_id": "202719032", "sentence": "SSD used Mobilenet v 2 [ 2 6 ] as the backbone .", "ner": [["SSD", "Method"], ["Mobilenet v 2", "Method"]], "rel": [["Mobilenet v 2", "Part-Of", "SSD"]], "rel_plus": [["Mobilenet v 2:Method", "Part-Of", "SSD:Method"]]}
{"doc_id": "202719032", "sentence": "Faster R - CNN , Mask R - CNN and R - FCN used a batch size of 1 and the Stochastic Gradient Descent ( SGD ) with momentum value of 0. 9 as optimizer .", "ner": [["Faster R - CNN", "Method"], ["Mask R - CNN", "Method"], ["R - FCN", "Method"], ["Stochastic Gradient Descent", "Method"], ["SGD", "Method"], ["momentum", "Method"]], "rel": [["Stochastic Gradient Descent", "Part-Of", "Faster R - CNN"], ["Stochastic Gradient Descent", "Part-Of", "Mask R - CNN"], ["Stochastic Gradient Descent", "Part-Of", "R - FCN"], ["SGD", "Synonym-Of", "Stochastic Gradient Descent"], ["momentum", "Part-Of", "Stochastic Gradient Descent"]], "rel_plus": [["Stochastic Gradient Descent:Method", "Part-Of", "Faster R - CNN:Method"], ["Stochastic Gradient Descent:Method", "Part-Of", "Mask R - CNN:Method"], ["Stochastic Gradient Descent:Method", "Part-Of", "R - FCN:Method"], ["SGD:Method", "Synonym-Of", "Stochastic Gradient Descent:Method"], ["momentum:Method", "Part-Of", "Stochastic Gradient Descent:Method"]]}
{"doc_id": "202719032", "sentence": "SSD used a batch size of 3 2 and RMSprop [ 2 9 ] with momentum value of 0. 9 as optimizer .", "ner": [["SSD", "Method"], ["RMSprop", "Method"], ["momentum", "Method"]], "rel": [["RMSprop", "Part-Of", "SSD"], ["momentum", "Part-Of", "RMSprop"]], "rel_plus": [["RMSprop:Method", "Part-Of", "SSD:Method"], ["momentum:Method", "Part-Of", "RMSprop:Method"]]}
{"doc_id": "202719032", "sentence": "The input image size for Faster R - CNN , Mask R - CNN and R - FCN was 1 0 2 4 \u00d7 1 0 2 4 pixels .", "ner": [["Faster R - CNN", "Method"], ["Mask R - CNN", "Method"], ["R - FCN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202719032", "sentence": "The patches were further resized to 3 0 0 \u00d7 3 0 0 pixels for SSD in order to align with SSD pre - trained models .", "ner": [["SSD", "Method"], ["SSD", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202719032", "sentence": "In contrast to the two - stage detectors , SSD has much lower detection accuracy .", "ner": [["SSD", "Method"], ["detection", "Task"]], "rel": [["SSD", "Used-For", "detection"]], "rel_plus": [["SSD:Method", "Used-For", "detection:Task"]]}
{"doc_id": "202719032", "sentence": "Due to its single - shot detection , SSD suffers from a strong false negative problem as shown in Figure 1 4 .", "ner": [["detection", "Task"], ["SSD", "Method"]], "rel": [["SSD", "Used-For", "detection"]], "rel_plus": [["SSD:Method", "Used-For", "detection:Task"]]}
{"doc_id": "202719032", "sentence": "This makes SSD can make better detection .", "ner": [["SSD", "Method"], ["detection", "Task"]], "rel": [["SSD", "Used-For", "detection"]], "rel_plus": [["SSD:Method", "Used-For", "detection:Task"]]}
{"doc_id": "202719032", "sentence": "In addition , according to the results in Figure 1 2 , there is less than a 0.0 1 mAP accuracy decrease for both Faster R - CNN and Mask R - CNN .", "ner": [["Faster R - CNN", "Method"], ["Mask R - CNN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202719032", "sentence": "SSD presents a pulse at 6 0 % training data , we conjecture SSD 's unstable performance is due to its single shot manner which makes it easier to be affected by background noise .", "ner": [["SSD", "Method"], ["SSD", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202719032", "sentence": "Fewer training data to some extent leads to worse precision but has little effect towards recall .   By observing the performance of the selected four detectors in Figure 8 , Figures 1 2 and 1 5 , we find that at an IoU threshold of 0. 5 , Faster R - CNN always gets the best performance followed by R - FCN , Mask R - CNN , and SSD .", "ner": [["Faster R - CNN", "Method"], ["R - FCN", "Method"], ["Mask R - CNN", "Method"], ["SSD", "Method"]], "rel": [["Faster R - CNN", "Compare-With", "R - FCN"], ["Faster R - CNN", "Compare-With", "Mask R - CNN"], ["Faster R - CNN", "Compare-With", "SSD"]], "rel_plus": [["Faster R - CNN:Method", "Compare-With", "R - FCN:Method"], ["Faster R - CNN:Method", "Compare-With", "Mask R - CNN:Method"], ["Faster R - CNN:Method", "Compare-With", "SSD:Method"]]}
{"doc_id": "202719032", "sentence": "One reason why Mask R - CNN has lower performance than Faster R - CNN and R - FCN may be because the ground truth masks fed into Mask R - CNN have background noise .", "ner": [["Mask R - CNN", "Method"], ["Faster R - CNN", "Method"], ["R - FCN", "Method"], ["Mask R - CNN", "Method"]], "rel": [["Mask R - CNN", "Compare-With", "Faster R - CNN"], ["Mask R - CNN", "Compare-With", "R - FCN"]], "rel_plus": [["Mask R - CNN:Method", "Compare-With", "Faster R - CNN:Method"], ["Mask R - CNN:Method", "Compare-With", "R - FCN:Method"]]}
{"doc_id": "202719032", "sentence": "For all Mask R - CNN experiments , we use ground truth bounding boxes as ground truth masks which hampers the Mask R - CNN performance and leads to high false positive ( low precision ) .", "ner": [["Mask R - CNN", "Method"], ["Mask R - CNN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202719032", "sentence": "Therefore , although for generic object detection task , such as COCO dataset [ 1 7 ] , Mask R - CNN has good performance , in our scenario , Faster R - CNN is more recommendable .", "ner": [["object detection", "Task"], ["COCO", "Dataset"], ["Mask R - CNN", "Method"], ["Faster R - CNN", "Method"]], "rel": [["COCO", "Benchmark-For", "object detection"], ["Mask R - CNN", "Used-For", "object detection"], ["Mask R - CNN", "Evaluated-With", "COCO"], ["Mask R - CNN", "Compare-With", "Faster R - CNN"]], "rel_plus": [["COCO:Dataset", "Benchmark-For", "object detection:Task"], ["Mask R - CNN:Method", "Used-For", "object detection:Task"], ["Mask R - CNN:Method", "Evaluated-With", "COCO:Dataset"], ["Mask R - CNN:Method", "Compare-With", "Faster R - CNN:Method"]]}
{"doc_id": "202719032", "sentence": "Due to its single - shot approach , SSD suffers from false negative problem and has much lower detection accuracy compared to two - stage detectors , but it has the fastest detection speed .", "ner": [["SSD", "Method"], ["detection", "Task"], ["detection", "Task"]], "rel": [["SSD", "Used-For", "detection"]], "rel_plus": [["SSD:Method", "Used-For", "detection:Task"]]}
{"doc_id": "202719032", "sentence": "Within the three two - stage detectors , R - FCN has the fastest detection speed .", "ner": [["R - FCN", "Method"], ["detection", "Task"]], "rel": [["R - FCN", "Used-For", "detection"]], "rel_plus": [["R - FCN:Method", "Used-For", "detection:Task"]]}
{"doc_id": "202719032", "sentence": "The main difference between Faster R - CNN and R - FCN is the depth of the RoI sub - network .", "ner": [["Faster R - CNN", "Method"], ["R - FCN", "Method"]], "rel": [["Faster R - CNN", "Compare-With", "R - FCN"]], "rel_plus": [["Faster R - CNN:Method", "Compare-With", "R - FCN:Method"]]}
{"doc_id": "202719032", "sentence": "R - FCN extracts features from the final convolutional layer of ResNet 1 0 1 network and uses position sensitive score maps and position sensitive RoI Pooling to get location information .", "ner": [["R - FCN", "Method"], ["convolutional layer", "Method"], ["ResNet 1 0 1", "Method"], ["position sensitive RoI Pooling", "Method"]], "rel": [["position sensitive RoI Pooling", "Part-Of", "R - FCN"], ["ResNet 1 0 1", "Part-Of", "R - FCN"], ["convolutional layer", "Part-Of", "ResNet 1 0 1"]], "rel_plus": [["position sensitive RoI Pooling:Method", "Part-Of", "R - FCN:Method"], ["ResNet 1 0 1:Method", "Part-Of", "R - FCN:Method"], ["convolutional layer:Method", "Part-Of", "ResNet 1 0 1:Method"]]}
{"doc_id": "202719032", "sentence": "Thus R - FCN has a shallower RoI sub - network than Faster R - CNN which helps to increase speed . [ 1 0 ] 1 0 3 6 . 1 7 ms Faster R - CNN ( ResNet 1 0 1 ) [ 2 4 ] 8 6 5 . 3 3 ms R - FCN ( ResNet 1 0 1 ) [ 4 ] 8 1 0 . 2 1 ms SSD ( MobileNet v 2 ) [ 1 8 ] 7 4 5 . 4 6 ms V. CONCLUSION For general object detection , the most important elements are detection accuracy and speed .", "ner": [["R - FCN", "Method"], ["Faster R - CNN", "Method"], ["Faster R - CNN", "Method"], ["ResNet 1 0 1", "Method"], ["R - FCN", "Method"], ["ResNet 1 0 1", "Method"], ["SSD", "Method"], ["MobileNet v 2", "Method"], ["object detection", "Task"], ["detection", "Task"]], "rel": [["R - FCN", "Compare-With", "Faster R - CNN"], ["ResNet 1 0 1", "Part-Of", "Faster R - CNN"], ["ResNet 1 0 1", "Part-Of", "R - FCN"], ["MobileNet v 2", "Part-Of", "SSD"], ["object detection", "SubTask-Of", "detection"]], "rel_plus": [["R - FCN:Method", "Compare-With", "Faster R - CNN:Method"], ["ResNet 1 0 1:Method", "Part-Of", "Faster R - CNN:Method"], ["ResNet 1 0 1:Method", "Part-Of", "R - FCN:Method"], ["MobileNet v 2:Method", "Part-Of", "SSD:Method"], ["object detection:Task", "SubTask-Of", "detection:Task"]]}
{"doc_id": "202719032", "sentence": "Finally , when investigating different CNN models , since detection accuracy instead of speed is our primary concern , accurate two - stage detectors are preferred .", "ner": [["CNN", "Method"], ["detection", "Task"]], "rel": [["CNN", "Used-For", "detection"]], "rel_plus": [["CNN:Method", "Used-For", "detection:Task"]]}
{"doc_id": "202719032", "sentence": "Although Mask R - CNN and R - FCN add novel modifications to Faster R - CNN and achieve exciting performance on the COCO dataset , due to the nature of renal DIF data , these modifications do not lead to better performance on the glomerulus detection task .", "ner": [["Mask R - CNN", "Method"], ["R - FCN", "Method"], ["Faster R - CNN", "Method"], ["COCO", "Dataset"]], "rel": [["Mask R - CNN", "Compare-With", "Faster R - CNN"], ["R - FCN", "Compare-With", "Faster R - CNN"], ["R - FCN", "Evaluated-With", "COCO"], ["Mask R - CNN", "Evaluated-With", "COCO"]], "rel_plus": [["Mask R - CNN:Method", "Compare-With", "Faster R - CNN:Method"], ["R - FCN:Method", "Compare-With", "Faster R - CNN:Method"], ["R - FCN:Method", "Evaluated-With", "COCO:Dataset"], ["Mask R - CNN:Method", "Evaluated-With", "COCO:Dataset"]]}
{"doc_id": "202734254", "sentence": "With a lot of work about context - free question answering systems , there is an emerging trend of conversational question answering models in the natural language processing field .", "ner": [["context - free question answering", "Task"], ["conversational question answering", "Task"], ["natural language processing", "Task"]], "rel": [["conversational question answering", "SubTask-Of", "natural language processing"], ["context - free question answering", "SubTask-Of", "natural language processing"]], "rel_plus": [["conversational question answering:Task", "SubTask-Of", "natural language processing:Task"], ["context - free question answering:Task", "SubTask-Of", "natural language processing:Task"]]}
{"doc_id": "202734254", "sentence": "Thanks to the recently collected datasets , including QuAC and CoQA , there has been more work on conversational question answering , and recent work has achieved competitive performance on both datasets .", "ner": [["QuAC", "Dataset"], ["CoQA", "Dataset"], ["conversational question answering", "Task"]], "rel": [["CoQA", "Benchmark-For", "conversational question answering"], ["QuAC", "Benchmark-For", "conversational question answering"]], "rel_plus": [["CoQA:Dataset", "Benchmark-For", "conversational question answering:Task"], ["QuAC:Dataset", "Benchmark-For", "conversational question answering:Task"]]}
{"doc_id": "202734254", "sentence": "To investigate these questions , we design different training settings , testing settings , as well as an attack to verify the models ' capability of content understanding on QuAC and CoQA .", "ner": [["QuAC", "Dataset"], ["CoQA", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "202734254", "sentence": "The experimental results indicate some potential hazards in the benchmark datasets , QuAC and CoQA , for conversational comprehension research .", "ner": [["QuAC", "Dataset"], ["CoQA", "Dataset"], ["conversational comprehension", "Task"]], "rel": [["CoQA", "Benchmark-For", "conversational comprehension"], ["QuAC", "Benchmark-For", "conversational comprehension"]], "rel_plus": [["CoQA:Dataset", "Benchmark-For", "conversational comprehension:Task"], ["QuAC:Dataset", "Benchmark-For", "conversational comprehension:Task"]]}
{"doc_id": "202734254", "sentence": "There are two benchmark conversational question answering datasets , QuAC ( Choi et al. , 2 0 1 8) and CoQA ( Reddy et al. , 2 0 1 9 ) .", "ner": [["conversational question answering", "Task"], ["QuAC", "Dataset"], ["CoQA", "Dataset"]], "rel": [["QuAC", "Benchmark-For", "conversational question answering"], ["CoQA", "Benchmark-For", "conversational question answering"]], "rel_plus": [["QuAC:Dataset", "Benchmark-For", "conversational question answering:Task"], ["CoQA:Dataset", "Benchmark-For", "conversational question answering:Task"]]}
{"doc_id": "202734254", "sentence": "Different from traditional machine reading comprehension ( Rajpurkar et al. , 2 0 1 6 ; Nguyen et al. , 2 0 1 6 ; Rajpurkar et al. , 2 0 1 8 ) whose questions are context - free , questions and answers in QuAC and CoQA are collected in a conversational manner .", "ner": [["machine reading comprehension", "Task"], ["QuAC", "Dataset"], ["CoQA", "Dataset"]], "rel": [["QuAC", "Benchmark-For", "machine reading comprehension"], ["CoQA", "Benchmark-For", "machine reading comprehension"]], "rel_plus": [["QuAC:Dataset", "Benchmark-For", "machine reading comprehension:Task"], ["CoQA:Dataset", "Benchmark-For", "machine reading comprehension:Task"]]}
{"doc_id": "202734254", "sentence": "QuAC and CoQA also feature many linguistic phenomena unique to conversations , so they are believed to be important materials for research about conversational question answering .", "ner": [["QuAC", "Dataset"], ["CoQA", "Dataset"], ["conversational question answering", "Task"]], "rel": [["CoQA", "Benchmark-For", "conversational question answering"], ["QuAC", "Benchmark-For", "conversational question answering"]], "rel_plus": [["CoQA:Dataset", "Benchmark-For", "conversational question answering:Task"], ["QuAC:Dataset", "Benchmark-For", "conversational question answering:Task"]]}
{"doc_id": "202734254", "sentence": "It is motivated by the fact that the position of the answer to the previous question is widely used in many of previous conversational question answering models , such as BiDAF - with - ctx ( Seo et al. , 2 0 1 6 ) and FlowQA ( Huang et al. , 2 0 1 9 ) .", "ner": [["conversational question answering", "Task"], ["BiDAF - with - ctx", "Method"], ["FlowQA", "Method"]], "rel": [["BiDAF - with - ctx", "Used-For", "conversational question answering"], ["FlowQA", "Used-For", "conversational question answering"]], "rel_plus": [["BiDAF - with - ctx:Method", "Used-For", "conversational question answering:Task"], ["FlowQA:Method", "Used-For", "conversational question answering:Task"]]}
{"doc_id": "202734254", "sentence": "We focus on recent open - sourced models , including FlowQA ( Huang et al. , 2 0 1 9 ) , BERT ( Devlin et al. , 2 0 1 9 ) , and SDNet .", "ner": [["FlowQA", "Method"], ["BERT", "Method"], ["SDNet", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202734254", "sentence": "The results indicate some concerns about the conversational QA models : 1 ) Higher performance on QuAC and CoQA does not necessarily imply better content comprehension . 2 ) Models trained on QuAC show tendency to rely heavier on the previous answers ' positions rather than their textual content .", "ner": [["conversational QA", "Task"], ["QuAC", "Dataset"], ["CoQA", "Dataset"], ["QuAC", "Dataset"]], "rel": [["CoQA", "Benchmark-For", "conversational QA"], ["QuAC", "Benchmark-For", "conversational QA"]], "rel_plus": [["CoQA:Dataset", "Benchmark-For", "conversational QA:Task"], ["QuAC:Dataset", "Benchmark-For", "conversational QA:Task"]]}
{"doc_id": "202734254", "sentence": "Yatskar ( 2 0 1 9 ) compared CoQA , SQuAD 2. 0 and QuAC qualitatively , there was no any investigation on what conversational question answering models capture .", "ner": [["CoQA", "Dataset"], ["SQuAD 2. 0", "Dataset"], ["QuAC", "Dataset"], ["conversational question answering", "Task"]], "rel": [["QuAC", "Benchmark-For", "conversational question answering"], ["SQuAD 2. 0", "Benchmark-For", "conversational question answering"], ["CoQA", "Benchmark-For", "conversational question answering"]], "rel_plus": [["QuAC:Dataset", "Benchmark-For", "conversational question answering:Task"], ["SQuAD 2. 0:Dataset", "Benchmark-For", "conversational question answering:Task"], ["CoQA:Dataset", "Benchmark-For", "conversational question answering:Task"]]}
{"doc_id": "202734254", "sentence": "Similar phenomena happened in the computer vision area , where Geirhos et al. ( 2 0 1 9 ) indicated that CNN trained on ImageNet relied much on textual information rather than shape information , and Brendel and Bethge ( 2 0 1 9 ) further showed that only textual information can achieve very high accuracy on ImageNet .", "ner": [["computer vision", "Task"], ["CNN", "Method"], ["ImageNet", "Dataset"], ["ImageNet", "Dataset"]], "rel": [["CNN", "Used-For", "computer vision"], ["CNN", "Trained-With", "ImageNet"], ["CNN", "Evaluated-With", "ImageNet"]], "rel_plus": [["CNN:Method", "Used-For", "computer vision:Task"], ["CNN:Method", "Trained-With", "ImageNet:Dataset"], ["CNN:Method", "Evaluated-With", "ImageNet:Dataset"]]}
{"doc_id": "202734254", "sentence": "There are two main datasets for conversational question answering , QuAC ( Choi et al. , 2 0 1 8) and CoQA ( Reddy et al. , 2 0 1 9 ) .", "ner": [["conversational question answering", "Task"], ["QuAC", "Dataset"], ["CoQA", "Dataset"]], "rel": [["QuAC", "Benchmark-For", "conversational question answering"], ["CoQA", "Benchmark-For", "conversational question answering"]], "rel_plus": [["QuAC:Dataset", "Benchmark-For", "conversational question answering:Task"], ["CoQA:Dataset", "Benchmark-For", "conversational question answering:Task"]]}
{"doc_id": "202734254", "sentence": "Even though both datasets are collected for conversational question answering , they have several different properties . \u2022 Answer format Answers in QuAC are always the text spans in the given passage , while answers in CoQA are free texts similar to some spans in the passage .", "ner": [["conversational question answering", "Task"], ["QuAC", "Dataset"], ["CoQA", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "202734254", "sentence": "Answers in QuAC is generally longer than answers in CoQA shown in Figure 2 , where the distribution implies that QuAC is more realistic than CoQA .", "ner": [["QuAC", "Dataset"], ["CoQA", "Dataset"], ["QuAC", "Dataset"], ["CoQA", "Dataset"]], "rel": [["QuAC", "Compare-With", "CoQA"], ["QuAC", "Compare-With", "CoQA"]], "rel_plus": [["QuAC:Dataset", "Compare-With", "CoQA:Dataset"], ["QuAC:Dataset", "Compare-With", "CoQA:Dataset"]]}
{"doc_id": "202734254", "sentence": "Note that the evidence span ( span in the passage that supports the answer ) is provided in CoQA , so the previous answers ' position information is still available . \u2022 Dataset collection process The Amazon mechanical turkers who generated the CoQA dataset have full access to the passage .", "ner": [["CoQA", "Dataset"], ["CoQA", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "202734254", "sentence": "We consider models including FlowQA , BERT , and SDNet , as they are the only publicly available models till now .", "ner": [["FlowQA", "Method"], ["BERT", "Method"], ["SDNet", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202734254", "sentence": "For FlowQA and SDNet , we use the code released by the authors .", "ner": [["FlowQA", "Method"], ["SDNet", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202734254", "sentence": "All models in the experiments are mainly based on those designed for single - turn reading comprehension tasks , so this section focuses on describing the modification for each method in order to handle understanding in conversational question answering .", "ner": [["reading comprehension", "Task"], ["conversational question answering", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "202734254", "sentence": "This model significantly improved conversational machine comprehension tasks for both QuAC and CoQA data .", "ner": [["conversational machine comprehension", "Task"], ["QuAC", "Dataset"], ["CoQA", "Dataset"]], "rel": [["CoQA", "Benchmark-For", "conversational machine comprehension"], ["QuAC", "Benchmark-For", "conversational machine comprehension"]], "rel_plus": [["CoQA:Dataset", "Benchmark-For", "conversational machine comprehension:Task"], ["QuAC:Dataset", "Benchmark-For", "conversational machine comprehension:Task"]]}
{"doc_id": "202734254", "sentence": "In the current state - of - the - art question answering models , most models leverage the benefits from BERT ( Devlin et al. , 2 0 1 9 ) to advance the task .", "ner": [["question answering", "Task"], ["BERT", "Method"]], "rel": [["BERT", "Used-For", "question answering"]], "rel_plus": [["BERT:Method", "Used-For", "question answering:Task"]]}
{"doc_id": "202734254", "sentence": "We apply BERT on QuAC by converting the task into a single - turn machine reading comprehension task such as SQuAD ( Rajpurkar et al. , 2 0 1 6 ) .", "ner": [["BERT", "Method"], ["QuAC", "Dataset"], ["machine reading comprehension", "Task"], ["SQuAD", "Dataset"]], "rel": [["BERT", "Evaluated-With", "QuAC"], ["BERT", "Used-For", "machine reading comprehension"], ["SQuAD", "Benchmark-For", "machine reading comprehension"]], "rel_plus": [["BERT:Method", "Evaluated-With", "QuAC:Dataset"], ["BERT:Method", "Used-For", "machine reading comprehension:Task"], ["SQuAD:Dataset", "Benchmark-For", "machine reading comprehension:Task"]]}
{"doc_id": "202734254", "sentence": "Then we follow the procedure of applying BERT to SQuAD that concatenates the extended and the context to form the input and uses the context output representations from BERT to predict the start and the end of the answer span ( Devlin et al. , 2 0 1 9 ) .", "ner": [["BERT", "Method"], ["SQuAD", "Dataset"], ["BERT", "Method"]], "rel": [["BERT", "Used-For", "SQuAD"]], "rel_plus": [["BERT:Method", "Used-For", "SQuAD:Dataset"]]}
{"doc_id": "202734254", "sentence": "For the input representation for both context and question words , they used BERT as contextualized embeddings along with GloVe .", "ner": [["BERT", "Method"], ["contextualized embeddings", "Method"], ["GloVe", "Method"]], "rel": [["BERT", "SubClass-Of", "contextualized embeddings"]], "rel_plus": [["BERT:Method", "SubClass-Of", "contextualized embeddings:Method"]]}
{"doc_id": "202734254", "sentence": "This section attempts at investigating how well the performance reflects the capability of comprehension on QuAC and CoQA ?", "ner": [["comprehension", "Task"], ["QuAC", "Dataset"], ["CoQA", "Dataset"]], "rel": [["QuAC", "Benchmark-For", "comprehension"], ["CoQA", "Benchmark-For", "comprehension"]], "rel_plus": [["QuAC:Dataset", "Benchmark-For", "comprehension:Task"], ["CoQA:Dataset", "Benchmark-For", "comprehension:Task"]]}
{"doc_id": "202734254", "sentence": "Especially , for QuAC , since answer spans are typically as long as sentences , masked language models like BERT can in no way recover the masked part .", "ner": [["QuAC", "Dataset"], ["masked language models", "Method"], ["BERT", "Method"]], "rel": [["BERT", "SubClass-Of", "masked language models"]], "rel_plus": [["BERT:Method", "SubClass-Of", "masked language models:Method"]]}
{"doc_id": "202734254", "sentence": "For both QuAC and CoQA , models trained without access to conversation content can achieve performance significantly better than models trained without access to any conversation history .", "ner": [["QuAC", "Dataset"], ["CoQA", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "202734254", "sentence": "For CoQA , since its answer span is generally much shorter than QuAC , we repeat sentences that contain the answer span .", "ner": [["CoQA", "Dataset"], ["QuAC", "Dataset"]], "rel": [["CoQA", "Compare-With", "QuAC"]], "rel_plus": [["CoQA:Dataset", "Compare-With", "QuAC:Dataset"]]}
{"doc_id": "202734254", "sentence": "For QuAC , the results shown in Table 2 indicates that both FlowQA and BERT are sensitive to the distance between consecutive answers .", "ner": [["QuAC", "Dataset"], ["FlowQA", "Method"], ["BERT", "Method"]], "rel": [["FlowQA", "Evaluated-With", "QuAC"], ["BERT", "Evaluated-With", "QuAC"]], "rel_plus": [["FlowQA:Method", "Evaluated-With", "QuAC:Dataset"], ["BERT:Method", "Evaluated-With", "QuAC:Dataset"]]}
{"doc_id": "202734254", "sentence": "Furthermore , we conduct the same experiments on FlowQA and BERT trained without using position information of the previous answers , and find that although they perform much worse than the full models , they are more robust against the attack .", "ner": [["FlowQA", "Method"], ["BERT", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202734254", "sentence": "We find that both BERT and FlowQA predict answers more accurately when the current answer has short distance to the previous answer ( blue and green lines ) .", "ner": [["BERT", "Method"], ["FlowQA", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202734254", "sentence": "Unlike on QuAC , -position models are not more robust than the original models as obviously as on QuAC .", "ner": [["QuAC", "Dataset"], ["QuAC", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "202734254", "sentence": "Particularly , for FlowQA , the content information of the previous answer may be flowed along with the flow structure , and the RNN mem - Distance to previous answer in words .", "ner": [["FlowQA", "Method"], ["RNN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202734254", "sentence": "Figure 6 : F 1 change between before and after repeat attack in terms of the answer 's distance to its previous answers on QuAC ( left : FlowQA ; right : BERT ) .", "ner": [["QuAC", "Dataset"], ["FlowQA", "Method"], ["BERT", "Method"]], "rel": [["FlowQA", "Evaluated-With", "QuAC"], ["BERT", "Evaluated-With", "QuAC"]], "rel_plus": [["FlowQA:Method", "Evaluated-With", "QuAC:Dataset"], ["BERT:Method", "Evaluated-With", "QuAC:Dataset"]]}
{"doc_id": "202734254", "sentence": "Meanwhile , as expected , the models except SDNet -position trained without the position information almost drop to the performance of ones trained without any conversation history Table 4 : F 1 results on the validation sets of QuAC and CoQA .", "ner": [["SDNet", "Method"], ["QuAC", "Dataset"], ["CoQA", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "202734254", "sentence": "However , it is surprising that FlowQA on QuAC can still keep the performance up to 6 0 % F 1 , implying that FlowQA may rely on position information much more than the semantic information .", "ner": [["FlowQA", "Method"], ["QuAC", "Dataset"], ["FlowQA", "Method"]], "rel": [["FlowQA", "Evaluated-With", "QuAC"]], "rel_plus": [["FlowQA:Method", "Evaluated-With", "QuAC:Dataset"]]}
{"doc_id": "202734254", "sentence": "The results are shown in Table 5 : F 1 score on the validation sets of QuAC and CoQA .", "ner": [["QuAC", "Dataset"], ["CoQA", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "202734254", "sentence": "On the other hand , it is interesting to see that FlowQA trained on CoQA rely little position information of the previous answers .", "ner": [["FlowQA", "Method"], ["CoQA", "Dataset"]], "rel": [["FlowQA", "Trained-With", "CoQA"]], "rel_plus": [["FlowQA:Method", "Trained-With", "CoQA:Dataset"]]}
{"doc_id": "202734254", "sentence": "To further investigate the performance difference between the models trained on QuAC and CoQA , two questions are focused here .", "ner": [["QuAC", "Dataset"], ["CoQA", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "202734254", "sentence": "Figure 2 shows that answers in CoQA are much shorter than in QuAC , so if we normalize the distance to the previous answer related to the length of the answers , the average distance to the previous answer in CoQA would be much longer than QuAC .", "ner": [["CoQA", "Dataset"], ["QuAC", "Dataset"], ["CoQA", "Dataset"], ["QuAC", "Dataset"]], "rel": [["CoQA", "Compare-With", "QuAC"]], "rel_plus": [["CoQA:Dataset", "Compare-With", "QuAC:Dataset"]]}
{"doc_id": "202734254", "sentence": "Though there is a high percentage of the questions containing pronouns in QuAC , they do not necessarily force the model to learn coreference resolution either .", "ner": [["QuAC", "Dataset"], ["coreference resolution", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "202734254", "sentence": "The experiments shows concerns 1 ) Performance on QuAC and CoQA does not well reflect model 's comprehension on conversation content . 2 ) The model trained on QuAC does not necessarily learn conversation comprehension . 3 ) In CoQA , crosssentence information is not that important for cur - rent model .", "ner": [["QuAC", "Dataset"], ["CoQA", "Dataset"], ["QuAC", "Dataset"], ["CoQA", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "202734254", "sentence": "Origin FlowQA prediction : ( F 1 0. 9 4 ) He stated that while he would expect tests on the efficacy of prayers to be negative , he would not rule out a priori the possibility Origin BERT prediction : ( F 1 1. 0 ) He stated that while he would expect tests on the efficacy of prayers to be negative , Attacked FlowQA prediction : ( F 1 0. 2 1 ) he regarded parapsychology and other research into the paranormal as tantamount to \" tempting God \" and seeking \" signs and wonders \" .", "ner": [["FlowQA", "Method"], ["BERT", "Method"], ["FlowQA", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "22825560", "sentence": "Unsupervised domain adaptation has emerged as an alternative approach that does not require as much annotated data , prior evaluations of domain adaptation approaches have been limited to relatively similar datasets , e.g source and target domains are samples captured by different cameras .", "ner": [["Unsupervised domain adaptation", "Method"], ["domain adaptation", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "22825560", "sentence": "We also propose a new domain adaptation network called \" Deep MagNet \" that effectively transfers knowledge for cross - modality domain adaptation problems .", "ner": [["domain adaptation network", "Method"], ["Deep MagNet", "Method"], ["cross - modality domain adaptation", "Task"]], "rel": [["Deep MagNet", "SubClass-Of", "domain adaptation network"], ["Deep MagNet", "Used-For", "cross - modality domain adaptation"]], "rel_plus": [["Deep MagNet:Method", "SubClass-Of", "domain adaptation network:Method"], ["Deep MagNet:Method", "Used-For", "cross - modality domain adaptation:Task"]]}
{"doc_id": "22825560", "sentence": "Convolutional Neural Networks ( CNNs ) have been successfully applied to almost every problem in computer vision , from interest point description [ 1 ] to stereo [ 2 ] , object detection [ 3 ] , semantic segmentation [ 4 ] , action recognition [ 5 ] and Structure - from - Motion [ 6 ] .", "ner": [["Convolutional Neural Networks", "Method"], ["CNNs", "Method"], ["computer vision", "Task"], ["object detection", "Task"], ["semantic segmentation", "Task"], ["action recognition", "Task"], ["Structure - from - Motion", "Task"]], "rel": [["CNNs", "Synonym-Of", "Convolutional Neural Networks"], ["Convolutional Neural Networks", "Used-For", "computer vision"], ["object detection", "SubTask-Of", "computer vision"], ["semantic segmentation", "SubTask-Of", "computer vision"], ["action recognition", "SubTask-Of", "computer vision"], ["Structure - from - Motion", "SubTask-Of", "computer vision"], ["Convolutional Neural Networks", "Used-For", "object detection"], ["Convolutional Neural Networks", "Used-For", "semantic segmentation"], ["Convolutional Neural Networks", "Used-For", "action recognition"], ["Convolutional Neural Networks", "Used-For", "Structure - from - Motion"]], "rel_plus": [["CNNs:Method", "Synonym-Of", "Convolutional Neural Networks:Method"], ["Convolutional Neural Networks:Method", "Used-For", "computer vision:Task"], ["object detection:Task", "SubTask-Of", "computer vision:Task"], ["semantic segmentation:Task", "SubTask-Of", "computer vision:Task"], ["action recognition:Task", "SubTask-Of", "computer vision:Task"], ["Structure - from - Motion:Task", "SubTask-Of", "computer vision:Task"], ["Convolutional Neural Networks:Method", "Used-For", "object detection:Task"], ["Convolutional Neural Networks:Method", "Used-For", "semantic segmentation:Task"], ["Convolutional Neural Networks:Method", "Used-For", "action recognition:Task"], ["Convolutional Neural Networks:Method", "Used-For", "Structure - from - Motion:Task"]]}
{"doc_id": "22825560", "sentence": "While it is not uncommon to leverage transfer learning in CNNs when a labeled dataset of appropriate size is available for the target task , only a few datasets and approaches have been proposed for the challenging case of an unlabeled target dataset .", "ner": [["transfer learning", "Task"], ["CNNs", "Method"]], "rel": [["CNNs", "Used-For", "transfer learning"]], "rel_plus": [["CNNs:Method", "Used-For", "transfer learning:Task"]]}
{"doc_id": "22825560", "sentence": "Today , unsupervised domain adaptation today is at the stage object recognition was , 1 5 years ago , with Caltech 1 0 1 [ 8 ] .", "ner": [["unsupervised domain adaptation", "Method"], ["object recognition", "Task"], ["Caltech 1 0 1", "Dataset"]], "rel": [["unsupervised domain adaptation", "Used-For", "object recognition"], ["Caltech 1 0 1", "Benchmark-For", "object recognition"]], "rel_plus": [["unsupervised domain adaptation:Method", "Used-For", "object recognition:Task"], ["Caltech 1 0 1:Dataset", "Benchmark-For", "object recognition:Task"]]}
{"doc_id": "22825560", "sentence": "We also discuss the performance of Deep MagNet against other state of the art methods and demonstrate its superior performance with respect to cross - modality domain adaptation tasks .", "ner": [["Deep MagNet", "Method"], ["cross - modality domain adaptation", "Task"]], "rel": [["Deep MagNet", "Used-For", "cross - modality domain adaptation"]], "rel_plus": [["Deep MagNet:Method", "Used-For", "cross - modality domain adaptation:Task"]]}
{"doc_id": "22825560", "sentence": "Our main contributions can be summarized as follows : \u2022 We propose a new dataset suite with the following pairs of source and target sets : ( i ) randomly 3 0 % and 7 0 % samples of real scenes Pascal 3d+ [ 9 ] ( ii ) renderings of CAD models , which are instances of 1 2 classes obtained from ShapeNet [ 1 0 ] ; and real scenes from Pascal 3D+ containing the 1 2 common categories , ( iii ) human - drawn coarse sketches , Sketch - 2 5 0 [ 1 1 ] has 8 7 common categories in Caltech 2 5 6 [ 1 2 ] . \u2022 We propose a novel domain adaptation network that outperforms on multimodal domain adaptation problems .", "ner": [["Pascal 3d+", "Dataset"], ["ShapeNet", "Dataset"], ["Pascal 3D+", "Dataset"], ["Sketch - 2 5 0", "Dataset"], ["Caltech 2 5 6", "Dataset"], ["domain adaptation network", "Method"], ["multimodal domain adaptation problems", "Task"]], "rel": [["domain adaptation network", "Used-For", "multimodal domain adaptation problems"]], "rel_plus": [["domain adaptation network:Method", "Used-For", "multimodal domain adaptation problems:Task"]]}
{"doc_id": "22825560", "sentence": "In particular , We thoroughly evaluate and extend the Residual Transfer Network ( RTN ) [ 1 3 ] , which applies the MMD loss on classification layer outputs , and also uses an entropy minimization loss on the target domain predictions as an additional regularizer .", "ner": [["Residual Transfer Network", "Method"], ["RTN", "Method"], ["MMD loss", "Method"], ["entropy minimization loss", "Method"]], "rel": [["RTN", "Synonym-Of", "Residual Transfer Network"], ["MMD loss", "Part-Of", "Residual Transfer Network"], ["entropy minimization loss", "Part-Of", "Residual Transfer Network"]], "rel_plus": [["RTN:Method", "Synonym-Of", "Residual Transfer Network:Method"], ["MMD loss:Method", "Part-Of", "Residual Transfer Network:Method"], ["entropy minimization loss:Method", "Part-Of", "Residual Transfer Network:Method"]]}
{"doc_id": "22825560", "sentence": "On the other hand , we employ densely connected convolutional networks ( DenseNets ) [ 2 7 ] applying the MMD loss not only at the level of classifier activations but also on the convolutional feature learning layers .", "ner": [["densely connected convolutional networks", "Method"], ["DenseNets", "Method"], ["MMD loss", "Method"]], "rel": [["DenseNets", "Synonym-Of", "densely connected convolutional networks"], ["MMD loss", "Part-Of", "densely connected convolutional networks"]], "rel_plus": [["DenseNets:Method", "Synonym-Of", "densely connected convolutional networks:Method"], ["MMD loss:Method", "Part-Of", "densely connected convolutional networks:Method"]]}
{"doc_id": "22825560", "sentence": "Generative approaches : Recently , multiple works [ 1 9 ] , [ 2 0 ] , [ 2 1 ] leverage conditional Generative Adversarial Networks ( cGANs ) to convert a labeled source image to a target domain image such that it is indistinguishable from a real target image to a CNN - based discriminator while maintaining the image attributes relevant to the label .", "ner": [["conditional Generative Adversarial Networks", "Method"], ["cGANs", "Method"], ["CNN - based discriminator", "Method"]], "rel": [["cGANs", "Synonym-Of", "conditional Generative Adversarial Networks"], ["CNN - based discriminator", "Part-Of", "conditional Generative Adversarial Networks"]], "rel_plus": [["cGANs:Method", "Synonym-Of", "conditional Generative Adversarial Networks:Method"], ["CNN - based discriminator:Method", "Part-Of", "conditional Generative Adversarial Networks:Method"]]}
{"doc_id": "22825560", "sentence": "Non - connectionist approaches : Significant literature [ 2 8 ] , [ 2 9 ] exists that does not follow the uniform CNN framework for domain adaptation .", "ner": [["uniform CNN", "Method"], ["domain adaptation", "Task"]], "rel": [["uniform CNN", "Used-For", "domain adaptation"]], "rel_plus": [["uniform CNN:Method", "Used-For", "domain adaptation:Task"]]}
{"doc_id": "22825560", "sentence": "Some of these papers introduced important domain transfer datasets , including Office - 3 1 [ 2 4 ] and Office - Caltech [ 3 0 ] that we utilize .", "ner": [["Office - 3 1", "Dataset"], ["Office - Caltech", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "22825560", "sentence": "We propose a network \" Deep MagNet \" that is composed of dense blocks , a residual block ( Res Block ) and multiple MMD loss functions .", "ner": [["Deep MagNet", "Method"], ["dense blocks", "Method"], ["residual block", "Method"], ["Res Block", "Method"], ["MMD loss", "Method"]], "rel": [["MMD loss", "Part-Of", "Deep MagNet"], ["dense blocks", "Part-Of", "Deep MagNet"], ["residual block", "Part-Of", "Deep MagNet"], ["Res Block", "Synonym-Of", "residual block"]], "rel_plus": [["MMD loss:Method", "Part-Of", "Deep MagNet:Method"], ["dense blocks:Method", "Part-Of", "Deep MagNet:Method"], ["residual block:Method", "Part-Of", "Deep MagNet:Method"], ["Res Block:Method", "Synonym-Of", "residual block:Method"]]}
{"doc_id": "22825560", "sentence": "The DenseNet we employed in this network is DenseNet 1 2 1 which is one variation of DenseNet .", "ner": [["DenseNet", "Method"], ["DenseNet 1 2 1", "Method"], ["DenseNet", "Method"]], "rel": [["DenseNet 1 2 1", "SubClass-Of", "DenseNet"]], "rel_plus": [["DenseNet 1 2 1:Method", "SubClass-Of", "DenseNet:Method"]]}
{"doc_id": "22825560", "sentence": "Transition layers are consistent through the network , which means if we adopt transition layer A in one application then all the transition layers in the network diagram are of type A. Transition layer A involves bottleneck and fully connected layers , which is used on texture informative datasets like Office - 3 1 and Office - Caltech .", "ner": [["Transition layers", "Method"], ["Transition layer A", "Method"], ["bottleneck", "Method"], ["fully connected layers", "Method"], ["Office - 3 1", "Dataset"], ["Office - Caltech", "Dataset"]], "rel": [["bottleneck", "Part-Of", "Transition layer A"], ["fully connected layers", "Part-Of", "Transition layer A"], ["Office - Caltech", "Used-For", "Transition layer A"], ["Office - 3 1", "Used-For", "Transition layer A"]], "rel_plus": [["bottleneck:Method", "Part-Of", "Transition layer A:Method"], ["fully connected layers:Method", "Part-Of", "Transition layer A:Method"], ["Office - Caltech:Dataset", "Used-For", "Transition layer A:Method"], ["Office - 3 1:Dataset", "Used-For", "Transition layer A:Method"]]}
{"doc_id": "22825560", "sentence": "The first term of our loss function is the MMD loss , which is widely used in domain adaptation tasks .", "ner": [["MMD loss", "Method"], ["domain adaptation", "Method"]], "rel": [["MMD loss", "Part-Of", "domain adaptation"]], "rel_plus": [["MMD loss:Method", "Part-Of", "domain adaptation:Method"]]}
{"doc_id": "22825560", "sentence": "The features in standard CNNs tend to transition from generic to specific across different layers of the network , and the transferability of features and classifiers across domains will tend to decrease as the discrepancy measure increases . in our design , we use multiple MMD loss functions at different levels of the network to ensure the features between the source and target domains have minimum discrepancy at all level .", "ner": [["CNNs", "Method"], ["MMD loss", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "22825560", "sentence": "As shown in the network architecture , MMD loss functions are applied at each intermediate layers between dense blocks and one more MMD loss function is applied at the end of the network .", "ner": [["MMD loss", "Method"], ["MMD loss", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "22825560", "sentence": "We validate our method on benchmark datasets like Office - 3 1 and Office - Caltech , then we implement our network and other baselines on the new dataset suite .", "ner": [["Office - 3 1", "Dataset"], ["Office - Caltech", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "22825560", "sentence": "More specifically , we fine - tuned Deep MagNet from the DenseNet 1 2 1 pre - trained model on all experiments .", "ner": [["Deep MagNet", "Method"], ["DenseNet 1 2 1", "Method"]], "rel": [["DenseNet 1 2 1", "Part-Of", "Deep MagNet"]], "rel_plus": [["DenseNet 1 2 1:Method", "Part-Of", "Deep MagNet:Method"]]}
{"doc_id": "22825560", "sentence": "We fed two type of features into MMD loss layers in order to accommodate different domain adaptation problems .", "ner": [["MMD loss", "Method"], ["domain adaptation problems", "Task"]], "rel": [["MMD loss", "Used-For", "domain adaptation problems"]], "rel_plus": [["MMD loss:Method", "Used-For", "domain adaptation problems:Task"]]}
{"doc_id": "22825560", "sentence": "A. Real to Real Images 1 ) Office - 3 1 and Office - Caltech Experiments : We first evaluate our network with two benchmark datasets : Office - 3 1 and Office - Caltech .", "ner": [["Office - 3 1", "Dataset"], ["Office - Caltech", "Dataset"], ["Office - 3 1", "Dataset"], ["Office - Caltech", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "22825560", "sentence": "Office - 3 1 is a standard benchmark dataset of domain adaptation problems , including 4, 6 5 2 images of 3 1 categories collected from three different domains : Amazon ( A ) , which contains images downloaded from amazon.com , Webcam ( W ) , which includes images taken by web camera , DSLR ( D ) , which holds images taken by a digital SLR camera .", "ner": [["Office - 3 1", "Dataset"], ["domain adaptation problems", "Task"]], "rel": [["Office - 3 1", "Benchmark-For", "domain adaptation problems"]], "rel_plus": [["Office - 3 1:Dataset", "Benchmark-For", "domain adaptation problems:Task"]]}
{"doc_id": "22825560", "sentence": "Office - Caltech is an extension of the Office - 3 1 dataset and it is built by selecting the 1 0 common categories shared between Office - 3 1 and Caltech - 2 5 6 ( C ) .", "ner": [["Office - Caltech", "Dataset"], ["Office - 3 1", "Dataset"], ["Office - 3 1", "Dataset"], ["Caltech - 2 5 6 ( C )", "Dataset"]], "rel": [["Office - Caltech", "SubClass-Of", "Office - 3 1"]], "rel_plus": [["Office - Caltech:Dataset", "SubClass-Of", "Office - 3 1:Dataset"]]}
{"doc_id": "22825560", "sentence": "Office - Caltech has one more domain than Office - 3 1 but much fewer images in each domain .", "ner": [["Office - Caltech", "Dataset"], ["Office - 3 1", "Dataset"]], "rel": [["Office - Caltech", "Compare-With", "Office - 3 1"]], "rel_plus": [["Office - Caltech:Dataset", "Compare-With", "Office - 3 1:Dataset"]]}
{"doc_id": "22825560", "sentence": "Office - Caltech is still a great alternative to check the robustness of the domain adaptation algorithms .", "ner": [["Office - Caltech", "Dataset"], ["domain adaptation", "Method"]], "rel": [["domain adaptation", "Evaluated-With", "Office - Caltech"]], "rel_plus": [["domain adaptation:Method", "Evaluated-With", "Office - Caltech:Dataset"]]}
{"doc_id": "22825560", "sentence": "We build transfer tasks between all domains for both datasets . 2 ) Pascal 3D+ Experiments : Even though Office - 3 1 and Office - Caltech are widely used , they all have certain limitations that can not fully evaluate real to real images problems .", "ner": [["Pascal 3D+", "Dataset"], ["Office - 3 1", "Dataset"], ["Office - Caltech", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "22825560", "sentence": "Due to the reasons above , we introduce a new experiment in this section to complement the weakness of Office - 3 1 and Office - Caltech .", "ner": [["Office - 3 1", "Dataset"], ["Office - Caltech", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "22825560", "sentence": "The idea behind this experiment is to test the performance of networks on real images from a different perspective since Office - 3 1 and Office - Caltech datasets are not considered as natural images .", "ner": [["Office - 3 1", "Dataset"], ["Office - Caltech", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "22825560", "sentence": "We also conduct a reverse experiment that performs domain adaptation task : Pascal 3D+ \u2192 ShapeNet .", "ner": [["domain adaptation task", "Task"], ["Pascal 3D+", "Dataset"], ["ShapeNet", "Dataset"]], "rel": [["Pascal 3D+", "Benchmark-For", "domain adaptation task"], ["ShapeNet", "Benchmark-For", "domain adaptation task"]], "rel_plus": [["Pascal 3D+:Dataset", "Benchmark-For", "domain adaptation task:Task"], ["ShapeNet:Dataset", "Benchmark-For", "domain adaptation task:Task"]]}
{"doc_id": "22825560", "sentence": "Caltech - 2 5 6 is a publicly available dataset that has 2 5 6 categories of daily objects and it shares 8 7 common categories with Sketch - 5 0 .", "ner": [["Caltech - 2 5 6", "Dataset"], ["Sketch - 5 0", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "22825560", "sentence": "We build the transfer task with Sketch - 2 5 0 as source domain and Caltech - 2 5 6 as target domain .", "ner": [["Sketch - 2 5 0", "Dataset"], ["Caltech - 2 5 6", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "22825560", "sentence": "The source domain , Sketch - 2 5 0 has 6 9 6 0 images of 8 7 categories and the target domain , Caltech - 2 5 6 has 1 1 7 1 2 images of 8 7 categories .", "ner": [["Sketch - 2 5 0", "Dataset"], ["Caltech - 2 5 6", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "22825560", "sentence": "We have accomplished the Table I .   We compare against some conventional methods , like TCA [ 2 8 ] , GFK [ 2 9 ] , DDC [ 3 7 ] and DAN [ 1 5 ] , as well as state of the art methods like RTN , CORAL , Compact DNN [ 3 8 ] and JAN [ 1 7 ] on Office - 3 1 and Office - Caltech datasets .", "ner": [["TCA", "Method"], ["GFK", "Method"], ["DDC", "Method"], ["DAN", "Method"], ["RTN", "Method"], ["CORAL", "Method"], ["Compact DNN", "Method"], ["JAN", "Method"], ["Office - 3 1", "Dataset"], ["Office - Caltech", "Dataset"]], "rel": [["TCA", "Evaluated-With", "Office - 3 1"], ["GFK", "Evaluated-With", "Office - 3 1"], ["DDC", "Evaluated-With", "Office - 3 1"], ["DAN", "Evaluated-With", "Office - 3 1"], ["RTN", "Evaluated-With", "Office - 3 1"], ["CORAL", "Evaluated-With", "Office - 3 1"], ["Compact DNN", "Evaluated-With", "Office - 3 1"], ["JAN", "Evaluated-With", "Office - 3 1"], ["JAN", "Evaluated-With", "Office - Caltech"], ["Compact DNN", "Evaluated-With", "Office - Caltech"], ["CORAL", "Evaluated-With", "Office - Caltech"], ["RTN", "Evaluated-With", "Office - Caltech"], ["DAN", "Evaluated-With", "Office - Caltech"], ["DDC", "Evaluated-With", "Office - Caltech"], ["GFK", "Evaluated-With", "Office - Caltech"], ["TCA", "Evaluated-With", "Office - Caltech"]], "rel_plus": [["TCA:Method", "Evaluated-With", "Office - 3 1:Dataset"], ["GFK:Method", "Evaluated-With", "Office - 3 1:Dataset"], ["DDC:Method", "Evaluated-With", "Office - 3 1:Dataset"], ["DAN:Method", "Evaluated-With", "Office - 3 1:Dataset"], ["RTN:Method", "Evaluated-With", "Office - 3 1:Dataset"], ["CORAL:Method", "Evaluated-With", "Office - 3 1:Dataset"], ["Compact DNN:Method", "Evaluated-With", "Office - 3 1:Dataset"], ["JAN:Method", "Evaluated-With", "Office - 3 1:Dataset"], ["JAN:Method", "Evaluated-With", "Office - Caltech:Dataset"], ["Compact DNN:Method", "Evaluated-With", "Office - Caltech:Dataset"], ["CORAL:Method", "Evaluated-With", "Office - Caltech:Dataset"], ["RTN:Method", "Evaluated-With", "Office - Caltech:Dataset"], ["DAN:Method", "Evaluated-With", "Office - Caltech:Dataset"], ["DDC:Method", "Evaluated-With", "Office - Caltech:Dataset"], ["GFK:Method", "Evaluated-With", "Office - Caltech:Dataset"], ["TCA:Method", "Evaluated-With", "Office - Caltech:Dataset"]]}
{"doc_id": "22825560", "sentence": "We first compare against classic neural networks trained on ImageNet , like AlexNet , ResNet and DenseNet on new dataset suite .", "ner": [["neural networks", "Method"], ["ImageNet", "Dataset"], ["AlexNet", "Method"], ["ResNet", "Method"], ["DenseNet", "Method"]], "rel": [["DenseNet", "SubClass-Of", "neural networks"], ["ResNet", "SubClass-Of", "neural networks"], ["AlexNet", "SubClass-Of", "neural networks"], ["neural networks", "Trained-With", "ImageNet"], ["AlexNet", "Trained-With", "ImageNet"], ["ResNet", "Trained-With", "ImageNet"], ["DenseNet", "Trained-With", "ImageNet"]], "rel_plus": [["DenseNet:Method", "SubClass-Of", "neural networks:Method"], ["ResNet:Method", "SubClass-Of", "neural networks:Method"], ["AlexNet:Method", "SubClass-Of", "neural networks:Method"], ["neural networks:Method", "Trained-With", "ImageNet:Dataset"], ["AlexNet:Method", "Trained-With", "ImageNet:Dataset"], ["ResNet:Method", "Trained-With", "ImageNet:Dataset"], ["DenseNet:Method", "Trained-With", "ImageNet:Dataset"]]}
{"doc_id": "22825560", "sentence": "Moreover , we also include other state of the art domain adaptation methods into our comparison in crossmodality transfer tasks , e.g. RTN , CORAL , Compact DNN and JAN .", "ner": [["domain adaptation", "Method"], ["RTN", "Method"], ["CORAL", "Method"], ["Compact DNN", "Method"], ["JAN", "Method"]], "rel": [["RTN", "SubClass-Of", "domain adaptation"], ["CORAL", "SubClass-Of", "domain adaptation"], ["JAN", "SubClass-Of", "domain adaptation"], ["Compact DNN", "SubClass-Of", "domain adaptation"]], "rel_plus": [["RTN:Method", "SubClass-Of", "domain adaptation:Method"], ["CORAL:Method", "SubClass-Of", "domain adaptation:Method"], ["JAN:Method", "SubClass-Of", "domain adaptation:Method"], ["Compact DNN:Method", "SubClass-Of", "domain adaptation:Method"]]}
{"doc_id": "22825560", "sentence": "The location and scale of objects in the images are highly unpredictable , which makes this transfer task harder than Office - 3 1 and Office - Caltech datasets .", "ner": [["Office - 3 1", "Dataset"], ["Office - Caltech", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "22825560", "sentence": "Methods that heavily rely on texture information are very likely to achieve great performance on Office - 3 1 and Office - Caltech but not on Pascal 3D+ .", "ner": [["Office - 3 1", "Dataset"], ["Office - Caltech", "Dataset"], ["Pascal 3D+", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "22825560", "sentence": "The images in Pascal 3D+ require more varied features to achieve better classification results , which fully demonstrates the diverse features learned by Deep MagNet .", "ner": [["Pascal 3D+", "Dataset"], ["classification", "Task"], ["Deep MagNet", "Method"]], "rel": [["Deep MagNet", "Evaluated-With", "Pascal 3D+"], ["Deep MagNet", "Used-For", "classification"], ["Pascal 3D+", "Benchmark-For", "classification"]], "rel_plus": [["Deep MagNet:Method", "Evaluated-With", "Pascal 3D+:Dataset"], ["Deep MagNet:Method", "Used-For", "classification:Task"], ["Pascal 3D+:Dataset", "Benchmark-For", "classification:Task"]]}
{"doc_id": "22825560", "sentence": "The performance of Deep MagNet on this task is better than on real to real transfer tasks on Office - 3 1 and Office - Caltech datasets .", "ner": [["Deep MagNet", "Method"], ["Office - 3 1", "Dataset"], ["Office - Caltech", "Dataset"]], "rel": [["Deep MagNet", "Evaluated-With", "Office - 3 1"], ["Deep MagNet", "Evaluated-With", "Office - Caltech"]], "rel_plus": [["Deep MagNet:Method", "Evaluated-With", "Office - 3 1:Dataset"], ["Deep MagNet:Method", "Evaluated-With", "Office - Caltech:Dataset"]]}
{"doc_id": "22825560", "sentence": "The experimental results also prove this idea since the average classification accuracy of Sketch - 2 5 0 and Caltech - 2 5 6 is the lowest among all experiments .", "ner": [["classification", "Task"], ["Sketch - 2 5 0", "Dataset"], ["Caltech - 2 5 6", "Dataset"]], "rel": [["Sketch - 2 5 0", "Benchmark-For", "classification"], ["Caltech - 2 5 6", "Benchmark-For", "classification"]], "rel_plus": [["Sketch - 2 5 0:Dataset", "Benchmark-For", "classification:Task"], ["Caltech - 2 5 6:Dataset", "Benchmark-For", "classification:Task"]]}
{"doc_id": "22825560", "sentence": "Deep MagNet has the best performance on this extremely hard domain adaptation tasks .", "ner": [["Deep MagNet", "Method"], ["domain adaptation tasks", "Task"]], "rel": [["Deep MagNet", "Used-For", "domain adaptation tasks"]], "rel_plus": [["Deep MagNet:Method", "Used-For", "domain adaptation tasks:Task"]]}
{"doc_id": "22825560", "sentence": "We can observe that the average classification results of AlexNet , DenseNet - 1 2 1 and ResNet - 1 5 2 on the new dataset suite meet our expectation .", "ner": [["classification", "Task"], ["AlexNet", "Method"], ["DenseNet - 1 2 1", "Method"], ["ResNet - 1 5 2", "Method"]], "rel": [["AlexNet", "Used-For", "classification"], ["DenseNet - 1 2 1", "Used-For", "classification"], ["ResNet - 1 5 2", "Used-For", "classification"]], "rel_plus": [["AlexNet:Method", "Used-For", "classification:Task"], ["DenseNet - 1 2 1:Method", "Used-For", "classification:Task"], ["ResNet - 1 5 2:Method", "Used-For", "classification:Task"]]}
{"doc_id": "22825560", "sentence": "When we compare other state of the art domain adaptation methods , like RTN , CORAL and JAN , it is easy to observe that they have better results than networks that are trained on the ImageNet , e.g. AlexNet , ResNet and DenseNet .", "ner": [["domain adaptation", "Method"], ["RTN", "Method"], ["CORAL", "Method"], ["JAN", "Method"], ["ImageNet", "Dataset"], ["AlexNet", "Method"], ["ResNet", "Method"], ["DenseNet", "Method"]], "rel": [["RTN", "SubClass-Of", "domain adaptation"], ["CORAL", "SubClass-Of", "domain adaptation"], ["JAN", "SubClass-Of", "domain adaptation"], ["AlexNet", "Trained-With", "ImageNet"], ["ResNet", "Trained-With", "ImageNet"], ["DenseNet", "Trained-With", "ImageNet"], ["RTN", "Compare-With", "AlexNet"], ["CORAL", "Compare-With", "AlexNet"], ["JAN", "Compare-With", "AlexNet"], ["RTN", "Compare-With", "ResNet"], ["CORAL", "Compare-With", "ResNet"], ["JAN", "Compare-With", "ResNet"], ["RTN", "Compare-With", "DenseNet"], ["CORAL", "Compare-With", "DenseNet"], ["JAN", "Compare-With", "DenseNet"]], "rel_plus": [["RTN:Method", "SubClass-Of", "domain adaptation:Method"], ["CORAL:Method", "SubClass-Of", "domain adaptation:Method"], ["JAN:Method", "SubClass-Of", "domain adaptation:Method"], ["AlexNet:Method", "Trained-With", "ImageNet:Dataset"], ["ResNet:Method", "Trained-With", "ImageNet:Dataset"], ["DenseNet:Method", "Trained-With", "ImageNet:Dataset"], ["RTN:Method", "Compare-With", "AlexNet:Method"], ["CORAL:Method", "Compare-With", "AlexNet:Method"], ["JAN:Method", "Compare-With", "AlexNet:Method"], ["RTN:Method", "Compare-With", "ResNet:Method"], ["CORAL:Method", "Compare-With", "ResNet:Method"], ["JAN:Method", "Compare-With", "ResNet:Method"], ["RTN:Method", "Compare-With", "DenseNet:Method"], ["CORAL:Method", "Compare-With", "DenseNet:Method"], ["JAN:Method", "Compare-With", "DenseNet:Method"]]}
{"doc_id": "22825560", "sentence": "Deep MagNet has significantly better transfer ability and generalization ability than RTN .", "ner": [["Deep MagNet", "Method"], ["RTN", "Method"]], "rel": [["Deep MagNet", "Compare-With", "RTN"]], "rel_plus": [["Deep MagNet:Method", "Compare-With", "RTN:Method"]]}
{"doc_id": "22825560", "sentence": "RTN shows great performance on Office - 3 1 and Office - Caltech datasets while it struggles on the newly proposed dataset suites .", "ner": [["RTN", "Method"], ["Office - 3 1", "Dataset"], ["Office - Caltech", "Dataset"]], "rel": [["RTN", "Evaluated-With", "Office - 3 1"], ["RTN", "Evaluated-With", "Office - Caltech"]], "rel_plus": [["RTN:Method", "Evaluated-With", "Office - 3 1:Dataset"], ["RTN:Method", "Evaluated-With", "Office - Caltech:Dataset"]]}
{"doc_id": "22825560", "sentence": "RTN employs MMD loss functions that compare the source features and target features at the end of the network , which provides less flexibility in some cases .", "ner": [["RTN", "Method"], ["MMD loss", "Method"]], "rel": [["MMD loss", "Part-Of", "RTN"]], "rel_plus": [["MMD loss:Method", "Part-Of", "RTN:Method"]]}
{"doc_id": "22825560", "sentence": "Office - 3 1 and OfficeCaltech datasets have well - defined spatial information , therefore feed high - level features into MMD loss functions boost the performance .", "ner": [["Office - 3 1", "Dataset"], ["OfficeCaltech", "Dataset"], ["MMD loss", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "22825560", "sentence": "Our method extends RTN and overcomes the over - reliance on the pixel level information , which proves the advantages of having multiple MMD loss functions and transition layers .", "ner": [["RTN", "Method"], ["MMD loss", "Method"]], "rel": [["MMD loss", "Part-Of", "RTN"]], "rel_plus": [["MMD loss:Method", "Part-Of", "RTN:Method"]]}
{"doc_id": "22825560", "sentence": "Additionally , we attempted to replace the pre - trained models of RTN , e.g. replace AlexNet or ResNet with DenseNet while keeping everything else same .", "ner": [["RTN", "Method"], ["AlexNet", "Method"], ["ResNet", "Method"], ["DenseNet", "Method"]], "rel": [["AlexNet", "Part-Of", "RTN"], ["ResNet", "Part-Of", "RTN"], ["DenseNet", "Part-Of", "RTN"]], "rel_plus": [["AlexNet:Method", "Part-Of", "RTN:Method"], ["ResNet:Method", "Part-Of", "RTN:Method"], ["DenseNet:Method", "Part-Of", "RTN:Method"]]}
{"doc_id": "210860760", "sentence": "In this paper , we propose a graph embedding method , called ExEm , that uses dominating - set theory and deep learning approaches to capture node representations .", "ner": [["graph embedding method", "Method"], ["ExEm", "Method"], ["deep learning", "Method"]], "rel": [["ExEm", "SubClass-Of", "graph embedding method"]], "rel_plus": [["ExEm:Method", "SubClass-Of", "graph embedding method:Method"]]}
{"doc_id": "210860760", "sentence": "To learn the node embeddings , ExEm exploits three embedding methods including Word 2 vec , fastText and the concatenation of these two .", "ner": [["ExEm", "Method"], ["Word 2 vec", "Method"], ["fastText", "Method"]], "rel": [["Word 2 vec", "Part-Of", "ExEm"], ["fastText", "Part-Of", "ExEm"]], "rel_plus": [["Word 2 vec:Method", "Part-Of", "ExEm:Method"], ["fastText:Method", "Part-Of", "ExEm:Method"]]}
{"doc_id": "210860760", "sentence": "At the end , we conduct extensive experiments to validate the effectiveness of ExEm through assessing its performance over the multi - label classification , link prediction , and recommendation tasks on common datasets and our collected data formed by crawling the vast author Scopus profiles .", "ner": [["ExEm", "Method"], ["multi - label classification", "Task"], ["link prediction", "Task"], ["recommendation", "Task"]], "rel": [["ExEm", "Used-For", "multi - label classification"], ["ExEm", "Used-For", "link prediction"], ["ExEm", "Used-For", "recommendation"]], "rel_plus": [["ExEm:Method", "Used-For", "multi - label classification:Task"], ["ExEm:Method", "Used-For", "link prediction:Task"], ["ExEm:Method", "Used-For", "recommendation:Task"]]}
{"doc_id": "210860760", "sentence": "This information can be derived from the structure and content of social networks for different tasks such as classification , link prediction and recommendation [ 8 ] .", "ner": [["classification", "Task"], ["link prediction", "Task"], ["recommendation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "210860760", "sentence": "Here we propose a graph embedding method that using dominating - set theory and neural models , called \" ExEm \" to convert a graph to a low - dimensional vector space .", "ner": [["graph embedding method", "Method"], ["dominating - set theory", "Method"], ["neural models", "Method"], ["ExEm", "Method"]], "rel": [["neural models", "Part-Of", "graph embedding method"], ["dominating - set theory", "Part-Of", "graph embedding method"], ["ExEm", "SubClass-Of", "graph embedding method"]], "rel_plus": [["neural models:Method", "Part-Of", "graph embedding method:Method"], ["dominating - set theory:Method", "Part-Of", "graph embedding method:Method"], ["ExEm:Method", "SubClass-Of", "graph embedding method:Method"]]}
{"doc_id": "210860760", "sentence": "In order to train this neural network , Word 2 vec and fastText and their combination are utilized .", "ner": [["neural network", "Method"], ["Word 2 vec", "Method"], ["fastText", "Method"]], "rel": [["Word 2 vec", "Used-For", "neural network"], ["fastText", "Used-For", "neural network"]], "rel_plus": [["Word 2 vec:Method", "Used-For", "neural network:Method"], ["fastText:Method", "Used-For", "neural network:Method"]]}
{"doc_id": "210860760", "sentence": "Does data gathered from Scopus provide a suitable real dataset for the different tasks such as classification , link prediction , recommendation and so on ? \u2022 Q 2 .", "ner": [["classification", "Task"], ["link prediction", "Task"], ["recommendation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "210860760", "sentence": "Thus , the incorporation of graph - structured data and the deep learning model results in an outstanding feature learning technique , called graph embedding .", "ner": [["deep learning", "Method"], ["feature learning technique", "Method"], ["graph embedding", "Method"]], "rel": [["graph embedding", "SubClass-Of", "feature learning technique"]], "rel_plus": [["graph embedding:Method", "SubClass-Of", "feature learning technique:Method"]]}
{"doc_id": "210860760", "sentence": "TriDNR [ 1 4 ] utilized node structure , node content , and node labels for the graph embedding .", "ner": [["TriDNR", "Method"], ["graph embedding", "Method"]], "rel": [["TriDNR", "Used-For", "graph embedding"]], "rel_plus": [["TriDNR:Method", "Used-For", "graph embedding:Method"]]}
{"doc_id": "210860760", "sentence": "Mahmood et al. [ 1 7 ] have proposed a geodesic density gradient ( GDG ) algorithm that is divided a network into a series of relatively small communities [ 1 8 , 1 9 ] .", "ner": [["geodesic density gradient", "Method"], ["GDG", "Method"]], "rel": [["GDG", "Synonym-Of", "geodesic density gradient"]], "rel_plus": [["GDG:Method", "Synonym-Of", "geodesic density gradient:Method"]]}
{"doc_id": "210860760", "sentence": "DNGR [ 2 0 ] is based on a deep learning approach that comprises three steps .", "ner": [["DNGR", "Method"], ["deep learning", "Method"]], "rel": [["deep learning", "Used-For", "DNGR"]], "rel_plus": [["deep learning:Method", "Used-For", "DNGR:Method"]]}
{"doc_id": "210860760", "sentence": "HOPE [ 2 1 ] is considered as a matrix factorization based method .", "ner": [["HOPE", "Method"], ["matrix factorization", "Method"]], "rel": [["HOPE", "Part-Of", "matrix factorization"]], "rel_plus": [["HOPE:Method", "Part-Of", "matrix factorization:Method"]]}
{"doc_id": "210860760", "sentence": "Then , it applied generalized Singular Value Decomposition ( SVD ) to the general formulation to capture the embedding successfully [ 7 ] .", "ner": [["Singular Value Decomposition", "Method"], ["SVD", "Method"]], "rel": [["SVD", "Synonym-Of", "Singular Value Decomposition"]], "rel_plus": [["SVD:Method", "Synonym-Of", "Singular Value Decomposition:Method"]]}
{"doc_id": "210860760", "sentence": "SDNE [ 2 2 ] is a deep learning based graph embedding that applied an auto - encoder on the whole graph for social network embedding purpose .", "ner": [["SDNE", "Method"], ["deep learning", "Method"], ["graph embedding", "Method"], ["auto - encoder", "Method"]], "rel": [["auto - encoder", "Part-Of", "SDNE"], ["SDNE", "SubClass-Of", "graph embedding"], ["deep learning", "Used-For", "graph embedding"]], "rel_plus": [["auto - encoder:Method", "Part-Of", "SDNE:Method"], ["SDNE:Method", "SubClass-Of", "graph embedding:Method"], ["deep learning:Method", "Used-For", "graph embedding:Method"]]}
{"doc_id": "210860760", "sentence": "Goyal et al. [ 2 7 ] proposed a deep learning model to capture temporal patterns in dynamic networks for the link prediction task .", "ner": [["deep learning", "Method"], ["link prediction", "Task"]], "rel": [["deep learning", "Used-For", "link prediction"]], "rel_plus": [["deep learning:Method", "Used-For", "link prediction:Task"]]}
{"doc_id": "210860760", "sentence": "This study introduced three different architectures using an autoencoder , LSTM , and combination of autoencoder and LSTM .", "ner": [["autoencoder", "Method"], ["LSTM", "Method"], ["autoencoder", "Method"], ["LSTM", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210860760", "sentence": "Cai et al. [ 6 ] summarized the researches into five categories : matrix factorization , deep learning , edge reconstruction , graph kernel , and generative model .", "ner": [["matrix factorization", "Method"], ["deep learning", "Method"], ["edge reconstruction", "Method"], ["graph kernel", "Method"], ["generative model", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210860760", "sentence": "In this study , deep learning - based graph embedding is divided into two groups , deep learning graph embedding with and without a random walk .", "ner": [["deep learning", "Method"], ["graph embedding", "Method"], ["deep learning graph embedding", "Method"], ["random walk", "Method"]], "rel": [["deep learning", "Used-For", "graph embedding"], ["random walk", "Part-Of", "deep learning graph embedding"]], "rel_plus": [["deep learning:Method", "Used-For", "graph embedding:Method"], ["random walk:Method", "Part-Of", "deep learning graph embedding:Method"]]}
{"doc_id": "210860760", "sentence": "Moreover , deep learning based graph embedding techniques with random walks represent a graph as a set of random walks and these random walks are fed into a deep learning method like Skip - Gram to optimize their neighborhood preserving likelihood objectives .", "ner": [["deep learning", "Method"], ["graph embedding", "Method"], ["random walks", "Method"], ["deep learning", "Method"], ["Skip - Gram", "Method"]], "rel": [["deep learning", "Used-For", "graph embedding"], ["random walks", "Part-Of", "graph embedding"], ["Skip - Gram", "SubClass-Of", "deep learning"]], "rel_plus": [["deep learning:Method", "Used-For", "graph embedding:Method"], ["random walks:Method", "Part-Of", "graph embedding:Method"], ["Skip - Gram:Method", "SubClass-Of", "deep learning:Method"]]}
{"doc_id": "210860760", "sentence": "In comparison , deep learning based graph embedding methods without random walks apply an autoencoder or deep neural network such as Convolutional Neural Network , on the whole graph .", "ner": [["deep learning", "Method"], ["graph embedding", "Method"], ["random walks", "Method"], ["autoencoder", "Method"], ["deep neural network", "Method"], ["Convolutional Neural Network", "Method"]], "rel": [["deep learning", "Used-For", "graph embedding"], ["autoencoder", "Part-Of", "graph embedding"], ["deep neural network", "Part-Of", "graph embedding"], ["Convolutional Neural Network", "SubClass-Of", "deep neural network"]], "rel_plus": [["deep learning:Method", "Used-For", "graph embedding:Method"], ["autoencoder:Method", "Part-Of", "graph embedding:Method"], ["deep neural network:Method", "Part-Of", "graph embedding:Method"], ["Convolutional Neural Network:Method", "SubClass-Of", "deep neural network:Method"]]}
{"doc_id": "210860760", "sentence": "Moreover , this survey summarized the existing approaches into five types matrix factorization , random walk , edge modeling , deep learning and hybrid from methodology perspective .", "ner": [["matrix factorization", "Method"], ["random walk", "Method"], ["edge modeling", "Method"], ["deep learning", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210860760", "sentence": "Goyal et al. [ 7 ] and Cui [ 1 2 ] presented the graph embedding techniques in three categories : factorization based , random walk based and deep learning based .", "ner": [["graph embedding", "Method"], ["factorization based", "Method"], ["random walk based", "Method"], ["deep learning based", "Method"]], "rel": [["random walk based", "SubClass-Of", "graph embedding"], ["factorization based", "SubClass-Of", "graph embedding"], ["deep learning based", "SubClass-Of", "graph embedding"]], "rel_plus": [["random walk based:Method", "SubClass-Of", "graph embedding:Method"], ["factorization based:Method", "SubClass-Of", "graph embedding:Method"], ["deep learning based:Method", "SubClass-Of", "graph embedding:Method"]]}
{"doc_id": "210860760", "sentence": "ExEm solves the graph embedding problem by considering multi - label classification , link prediction , and node recommendation tasks .", "ner": [["ExEm", "Method"], ["graph embedding problem", "Task"], ["multi - label classification", "Task"], ["link prediction", "Task"], ["node recommendation", "Task"]], "rel": [["multi - label classification", "Used-For", "graph embedding problem"], ["link prediction", "Used-For", "graph embedding problem"], ["node recommendation", "Used-For", "graph embedding problem"], ["ExEm", "Used-For", "graph embedding problem"]], "rel_plus": [["multi - label classification:Task", "Used-For", "graph embedding problem:Task"], ["link prediction:Task", "Used-For", "graph embedding problem:Task"], ["node recommendation:Task", "Used-For", "graph embedding problem:Task"], ["ExEm:Method", "Used-For", "graph embedding problem:Task"]]}
{"doc_id": "210860760", "sentence": "In this study to train the SKIP - GRAM model , we use two state - of - the - art word embedding methods , \" Word 2 Vec \" [ 4 0 ] and \" fastText \" [ 4 1 ] .", "ner": [["SKIP - GRAM", "Method"], ["word embedding methods", "Method"], ["Word 2 Vec", "Method"], ["fastText", "Method"]], "rel": [["word embedding methods", "Part-Of", "SKIP - GRAM"], ["Word 2 Vec", "Part-Of", "SKIP - GRAM"], ["fastText", "Part-Of", "SKIP - GRAM"], ["Word 2 Vec", "SubClass-Of", "word embedding methods"], ["fastText", "SubClass-Of", "word embedding methods"]], "rel_plus": [["word embedding methods:Method", "Part-Of", "SKIP - GRAM:Method"], ["Word 2 Vec:Method", "Part-Of", "SKIP - GRAM:Method"], ["fastText:Method", "Part-Of", "SKIP - GRAM:Method"], ["Word 2 Vec:Method", "SubClass-Of", "word embedding methods:Method"], ["fastText:Method", "SubClass-Of", "word embedding methods:Method"]]}
{"doc_id": "210860760", "sentence": "ExEm(fastText ) , ExEm(Word 2 vec ) and ExEm(fastText+Word 2 vec ) respectively demonstrate the implementation of ExEm with \" fastText \" , \" Word 2 Vec \" , and combination of \" fastText \" and \" Word 2 Vec \" .", "ner": [["ExEm(fastText )", "Method"], ["ExEm(Word 2 vec )", "Method"], ["ExEm(fastText+Word 2 vec )", "Method"], ["ExEm", "Method"], ["fastText", "Method"], ["Word 2 Vec", "Method"], ["fastText", "Method"], ["Word 2 Vec", "Method"]], "rel": [["fastText", "Part-Of", "ExEm"], ["Word 2 Vec", "Part-Of", "ExEm"]], "rel_plus": [["fastText:Method", "Part-Of", "ExEm:Method"], ["Word 2 Vec:Method", "Part-Of", "ExEm:Method"]]}
{"doc_id": "210860760", "sentence": "Furthermore , we solve the optimization problem by performing Stochastic gradient descent ( SGD ) parameters similar to the method proposed by [ 4 2 ] .", "ner": [["Stochastic gradient descent", "Method"], ["SGD", "Method"]], "rel": [["SGD", "Synonym-Of", "Stochastic gradient descent"]], "rel_plus": [["SGD:Method", "Synonym-Of", "Stochastic gradient descent:Method"]]}
{"doc_id": "210860760", "sentence": "The feature representations obtained through graph embedding techniques are evaluated on learning tasks such as multi - label classification .", "ner": [["graph embedding", "Method"], ["multi - label classification", "Task"]], "rel": [["graph embedding", "Used-For", "multi - label classification"]], "rel_plus": [["graph embedding:Method", "Used-For", "multi - label classification:Task"]]}
{"doc_id": "210860760", "sentence": "Karate , BlogCatalog , Wikipedia , and Protein - Protein Interactions(PPI ) are the most used labeled datasets to estimate the efficiency of a proposed graph embedding method .", "ner": [["Karate", "Dataset"], ["BlogCatalog", "Dataset"], ["Wikipedia", "Dataset"], ["Protein - Protein Interactions(PPI )", "Dataset"], ["graph embedding", "Method"]], "rel": [["graph embedding", "Evaluated-With", "Karate"], ["graph embedding", "Evaluated-With", "BlogCatalog"], ["graph embedding", "Evaluated-With", "Wikipedia"], ["graph embedding", "Evaluated-With", "Protein - Protein Interactions(PPI )"]], "rel_plus": [["graph embedding:Method", "Evaluated-With", "Karate:Dataset"], ["graph embedding:Method", "Evaluated-With", "BlogCatalog:Dataset"], ["graph embedding:Method", "Evaluated-With", "Wikipedia:Dataset"], ["graph embedding:Method", "Evaluated-With", "Protein - Protein Interactions(PPI ):Dataset"]]}
{"doc_id": "210860760", "sentence": "Although , it is possible to discover datasets for the collaboration network of authors such as Arxiv AstroPhysics or Arxiv High Energy Physics Theory , but these is a demand for a labeled collaborative network dataset for the mentioned goals .", "ner": [["Arxiv AstroPhysics", "Dataset"], ["Arxiv High Energy Physics Theory", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "210860760", "sentence": "In other words , usage of the collected dataset can be enumerated as : \u2022 Evaluation of graph embedding techniques performance \u2022 Multi - label classification task \u2022 Link prediction task \u2022 Community detection task What are the differences between this dataset and previous ones ?", "ner": [["graph embedding", "Method"], ["Multi - label classification", "Task"], ["Link prediction", "Task"], ["Community detection", "Task"]], "rel": [["graph embedding", "Used-For", "Multi - label classification"], ["graph embedding", "Used-For", "Link prediction"], ["graph embedding", "Used-For", "Community detection"]], "rel_plus": [["graph embedding:Method", "Used-For", "Multi - label classification:Task"], ["graph embedding:Method", "Used-For", "Link prediction:Task"], ["graph embedding:Method", "Used-For", "Community detection:Task"]]}
{"doc_id": "210860760", "sentence": "Further , Scopus has Python library , called Scopus , to extract data from the Scopus database .", "ner": [["Scopus database", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "210860760", "sentence": "Additionally , authors with publications indexed in Scopus have their profiles and are automatically assigned as a unique Scopus author identifier .", "ner": [], "rel": [], "rel_plus": []}
{"doc_id": "210860760", "sentence": "Then , the corpus is fed as input into the SKIP - GRAM model of fastText and Word 2 Vec .", "ner": [["SKIP - GRAM", "Method"], ["fastText", "Method"], ["Word 2 Vec", "Method"]], "rel": [["fastText", "Part-Of", "SKIP - GRAM"], ["Word 2 Vec", "Part-Of", "SKIP - GRAM"]], "rel_plus": [["fastText:Method", "Part-Of", "SKIP - GRAM:Method"], ["Word 2 Vec:Method", "Part-Of", "SKIP - GRAM:Method"]]}
{"doc_id": "210860760", "sentence": "The labels of nodes represent blogger interests . \u2022 Protein - Protein interactions(PPI ) [ 5 3 ] : This is a subgraph of the biological network .", "ner": [["Protein - Protein interactions(PPI )", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "210860760", "sentence": "The labels are shown the Part - of - Speech ( POS ) tags . \u2022 arXiv(Astro - PH ) [ 5 5 ] : This is a collaboration network that is constructed from the collaborations between authors ' papers submitted to the e - print arXiv and Astro Physics category .", "ner": [["Part - of - Speech", "Method"], ["POS", "Method"], ["arXiv(Astro - PH )", "Dataset"]], "rel": [["POS", "Synonym-Of", "Part - of - Speech"]], "rel_plus": [["POS:Method", "Synonym-Of", "Part - of - Speech:Method"]]}
{"doc_id": "210860760", "sentence": "The details of tasks are represented in 6. 2 subsection . \u2022 Multi - label classification : One of the tasks that increasingly used by modern applications is multi - label classification .", "ner": [["Multi - label classification", "Task"], ["multi - label classification", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "210860760", "sentence": "In this task , nodes in a graph are associated with a set of target labels [ 5 6 ] . \u2022 Link prediction : The low - dimensional vectors of nodes encode rich information about the network structure and have application in various information processing tasks such as link prediction [ 6 ] .", "ner": [["Link prediction", "Task"], ["link prediction", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "210860760", "sentence": "To approve the performance of Expert 2 ve we compare it against the following baselines algorithms : \u2022 DeepWalk [ 1 5 ] : DeepWalk represents a graph as a set of simple unbiased random walks and predicts the local neighborhood of nodes via these random walks .", "ner": [["Expert 2 ve", "Method"], ["DeepWalk", "Method"], ["DeepWalk", "Method"], ["unbiased random walks", "Method"]], "rel": [["unbiased random walks", "Part-Of", "DeepWalk"]], "rel_plus": [["unbiased random walks:Method", "Part-Of", "DeepWalk:Method"]]}
{"doc_id": "210860760", "sentence": "Hierarchical softmax with a binary - tree structure operates over the full vertex set to approximate the normalizing factor [ 5 7 ] . \u2022 Node 2 vec [ 5 8 ] : Node 2 vec is the extended version of DeepWalk with a more elaborate random walk .", "ner": [["Hierarchical softmax", "Method"], ["Node 2 vec", "Method"], ["Node 2 vec", "Method"], ["DeepWalk", "Method"], ["random walk", "Method"]], "rel": [["random walk", "Part-Of", "Node 2 vec"], ["Node 2 vec", "SubClass-Of", "DeepWalk"]], "rel_plus": [["random walk:Method", "Part-Of", "Node 2 vec:Method"], ["Node 2 vec:Method", "SubClass-Of", "DeepWalk:Method"]]}
{"doc_id": "210860760", "sentence": "It insinuates breadth - first sampling ( BFS ) and depth - first sampling ( DFS ) for biased random walks . it is noted that negative sampling is applied to a set of random samples to compute the normalizing factor .", "ner": [["breadth - first sampling", "Method"], ["BFS", "Method"], ["depth - first sampling", "Method"], ["DFS", "Method"]], "rel": [["BFS", "Synonym-Of", "breadth - first sampling"], ["DFS", "Synonym-Of", "depth - first sampling"]], "rel_plus": [["BFS:Method", "Synonym-Of", "breadth - first sampling:Method"], ["DFS:Method", "Synonym-Of", "depth - first sampling:Method"]]}
{"doc_id": "210860760", "sentence": "That means a multi - label classification task is one of the ways for evaluating the performance of a graph embedding approach .", "ner": [["multi - label classification", "Task"], ["graph embedding", "Method"]], "rel": [["graph embedding", "Evaluated-With", "multi - label classification"]], "rel_plus": [["graph embedding:Method", "Evaluated-With", "multi - label classification:Task"]]}
{"doc_id": "210860760", "sentence": "Hence , we compare the effectiveness of ExEm method with the two other relevant approaches under the multi - label classification task .", "ner": [["ExEm", "Method"], ["multi - label classification", "Task"]], "rel": [["ExEm", "Used-For", "multi - label classification"]], "rel_plus": [["ExEm:Method", "Used-For", "multi - label classification:Task"]]}
{"doc_id": "210860760", "sentence": "Figure 2 0 presents the results of Micro - F 1 and Macro - F 1 scores for different approaches under BlogCatalog , Scopus , PPI and Wikipedia .", "ner": [["BlogCatalog", "Dataset"], ["Scopus", "Dataset"], ["PPI", "Dataset"], ["Wikipedia", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "210860760", "sentence": "For the BlogCatalog dataset , ExEm presents significant improvements over Micro - F 1 .", "ner": [["BlogCatalog dataset", "Dataset"], ["ExEm", "Method"]], "rel": [["ExEm", "Evaluated-With", "BlogCatalog dataset"]], "rel_plus": [["ExEm:Method", "Evaluated-With", "BlogCatalog dataset:Dataset"]]}
{"doc_id": "210860760", "sentence": "DeepWalk and Node 2 vec do not provide good performance because they only need local information means the adjacent neighbors .", "ner": [["DeepWalk", "Method"], ["Node 2 vec", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210860760", "sentence": "It can also be concluded that ExEm(Word 2 vec ) achieves the lowest results than ExEm(fastText ) and ExEm(fastText+Word 2 vec ) .", "ner": [["ExEm(Word 2 vec )", "Method"], ["ExEm(fastText )", "Method"], ["ExEm(fastText+Word 2 vec )", "Method"]], "rel": [["ExEm(Word 2 vec )", "Compare-With", "ExEm(fastText )"], ["ExEm(Word 2 vec )", "Compare-With", "ExEm(fastText+Word 2 vec )"]], "rel_plus": [["ExEm(Word 2 vec ):Method", "Compare-With", "ExEm(fastText ):Method"], ["ExEm(Word 2 vec ):Method", "Compare-With", "ExEm(fastText+Word 2 vec ):Method"]]}
{"doc_id": "210860760", "sentence": "In the case of the Scopus network , as the results present , both evaluation metrics have more value than the BlogCatalog network because the Scopus network has the highest density in comparison to three other datasets .", "ner": [], "rel": [], "rel_plus": []}
{"doc_id": "210860760", "sentence": "ExEm has significant improvements in the Macro - F 1 score over Node 2 vec and DeepWalk .", "ner": [["ExEm", "Method"], ["Node 2 vec", "Method"], ["DeepWalk", "Method"]], "rel": [["ExEm", "Compare-With", "Node 2 vec"], ["ExEm", "Compare-With", "DeepWalk"]], "rel_plus": [["ExEm:Method", "Compare-With", "Node 2 vec:Method"], ["ExEm:Method", "Compare-With", "DeepWalk:Method"]]}
{"doc_id": "210860760", "sentence": "For Micro - F 1 value , ExEm(fastText+Word 2 vec ) and ExEm(fastText ) display better results , respectively .", "ner": [["ExEm(fastText+Word 2 vec )", "Method"], ["ExEm(fastText )", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210860760", "sentence": "When training data is 8 0 % and 9 0 % , Node 2 vec obtains higher consequences than ExEm(Word 2 vec ) .", "ner": [["Node 2 vec", "Method"], ["ExEm(Word 2 vec )", "Method"]], "rel": [["Node 2 vec", "Compare-With", "ExEm(Word 2 vec )"]], "rel_plus": [["Node 2 vec:Method", "Compare-With", "ExEm(Word 2 vec ):Method"]]}
{"doc_id": "210860760", "sentence": "For Wikipedia dataset , the results indicate that ExEm outperformed baseline algorithms , considering both Micro and Macro - f 1 .", "ner": [["Wikipedia dataset", "Dataset"], ["ExEm", "Method"]], "rel": [["ExEm", "Evaluated-With", "Wikipedia dataset"]], "rel_plus": [["ExEm:Method", "Evaluated-With", "Wikipedia dataset:Dataset"]]}
{"doc_id": "210860760", "sentence": "While DeepWalk and Node 2 vec suffer from local structure information .", "ner": [["DeepWalk", "Method"], ["Node 2 vec", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210860760", "sentence": "It is important to note that the Wikipedia dataset is denser than BlogCatalog and PPI [ 8 ] .", "ner": [["Wikipedia dataset", "Dataset"], ["BlogCatalog", "Dataset"], ["PPI", "Dataset"]], "rel": [["Wikipedia dataset", "Compare-With", "BlogCatalog"], ["Wikipedia dataset", "Compare-With", "PPI"]], "rel_plus": [["Wikipedia dataset:Dataset", "Compare-With", "BlogCatalog:Dataset"], ["Wikipedia dataset:Dataset", "Compare-With", "PPI:Dataset"]]}
{"doc_id": "210860760", "sentence": "That means the results of multi - label classification indicate how effective are our graph embedding approach , especially for higher density networks .", "ner": [["multi - label classification", "Task"], ["graph embedding", "Method"]], "rel": [["graph embedding", "Used-For", "multi - label classification"]], "rel_plus": [["graph embedding:Method", "Used-For", "multi - label classification:Task"]]}
{"doc_id": "210860760", "sentence": "These operators are defined by the following equations [ 8 , 5 8 , 6 1 ] : Figure 2 0 : Performance comparison of 5 methods ( DeepWalk , Node 2 Vec , ExEm(Word 2 vec ) , ExEm(fastText ) , and ExEm(fastText+Word 2 vec ) ) ) by considering their achieved values of Micro - F 1 and Macro - F 1 for various datasets under different train - test split ratio for multi - label classification task ( dimension of embedding is 1 2 8)   As investigated in the study [ 6 2 ] , link prediction can be addressed as a binary classification problem .", "ner": [["DeepWalk", "Method"], ["Node 2 Vec", "Method"], ["ExEm(Word 2 vec )", "Method"], ["ExEm(fastText )", "Method"], ["ExEm(fastText+Word 2 vec )", "Method"], ["multi - label classification", "Task"], ["link prediction", "Task"], ["binary classification", "Task"]], "rel": [["DeepWalk", "Used-For", "multi - label classification"], ["Node 2 Vec", "Used-For", "multi - label classification"], ["ExEm(Word 2 vec )", "Used-For", "multi - label classification"], ["ExEm(fastText )", "Used-For", "multi - label classification"], ["ExEm(fastText+Word 2 vec )", "Used-For", "multi - label classification"], ["link prediction", "SubTask-Of", "binary classification"]], "rel_plus": [["DeepWalk:Method", "Used-For", "multi - label classification:Task"], ["Node 2 Vec:Method", "Used-For", "multi - label classification:Task"], ["ExEm(Word 2 vec ):Method", "Used-For", "multi - label classification:Task"], ["ExEm(fastText ):Method", "Used-For", "multi - label classification:Task"], ["ExEm(fastText+Word 2 vec ):Method", "Used-For", "multi - label classification:Task"], ["link prediction:Task", "SubTask-Of", "binary classification:Task"]]}
{"doc_id": "210860760", "sentence": "We have compared ExEm against well - known baselines that have the best results on link prediction .", "ner": [["ExEm", "Method"], ["link prediction", "Task"]], "rel": [["ExEm", "Used-For", "link prediction"]], "rel_plus": [["ExEm:Method", "Used-For", "link prediction:Task"]]}
{"doc_id": "210860760", "sentence": "Though DeepWalk and Node 2 vec are random walk based methods their walks do not provide enough information about nodes .", "ner": [["DeepWalk", "Method"], ["Node 2 vec", "Method"], ["random walk based methods", "Method"]], "rel": [["DeepWalk", "SubClass-Of", "random walk based methods"], ["Node 2 vec", "SubClass-Of", "random walk based methods"]], "rel_plus": [["DeepWalk:Method", "SubClass-Of", "random walk based methods:Method"], ["Node 2 vec:Method", "SubClass-Of", "random walk based methods:Method"]]}
{"doc_id": "210860760", "sentence": "It should be considered that ExEm(fastText ) and ExEm(fastText+Word 2 vec ) have better results than ExEm(Word 2 vec ) in most cases .", "ner": [["ExEm(fastText )", "Method"], ["ExEm(fastText+Word 2 vec )", "Method"], ["ExEm(Word 2 vec )", "Method"]], "rel": [["ExEm(fastText )", "Compare-With", "ExEm(Word 2 vec )"], ["ExEm(fastText+Word 2 vec )", "Compare-With", "ExEm(Word 2 vec )"]], "rel_plus": [["ExEm(fastText ):Method", "Compare-With", "ExEm(Word 2 vec ):Method"], ["ExEm(fastText+Word 2 vec ):Method", "Compare-With", "ExEm(Word 2 vec ):Method"]]}
{"doc_id": "210860760", "sentence": "Figure 2 1 : The value of F 1 - score which is achieved by ExEm along with the other two related approaches on Scopus dataset for recommendation task for three topics IE , NLP and ML .", "ner": [["ExEm", "Method"], ["Scopus dataset", "Dataset"], ["recommendation", "Task"], ["IE", "Task"], ["NLP", "Task"], ["ML", "Task"]], "rel": [["ExEm", "Evaluated-With", "Scopus dataset"], ["ExEm", "Used-For", "recommendation"], ["Scopus dataset", "Benchmark-For", "recommendation"]], "rel_plus": [["ExEm:Method", "Evaluated-With", "Scopus dataset:Dataset"], ["ExEm:Method", "Used-For", "recommendation:Task"], ["Scopus dataset:Dataset", "Benchmark-For", "recommendation:Task"]]}
{"doc_id": "210860760", "sentence": "Additionally , the detailed AUC score achieved by DeepWalk , Node 2 vec and ExEm for the Scopus dataset is exhibited in table 9 .", "ner": [["DeepWalk", "Method"], ["Node 2 vec", "Method"], ["ExEm", "Method"], ["Scopus dataset", "Dataset"]], "rel": [["DeepWalk", "Evaluated-With", "Scopus dataset"], ["Node 2 vec", "Evaluated-With", "Scopus dataset"], ["ExEm", "Evaluated-With", "Scopus dataset"]], "rel_plus": [["DeepWalk:Method", "Evaluated-With", "Scopus dataset:Dataset"], ["Node 2 vec:Method", "Evaluated-With", "Scopus dataset:Dataset"], ["ExEm:Method", "Evaluated-With", "Scopus dataset:Dataset"]]}
{"doc_id": "210860760", "sentence": "In this case study , first , we select three topics : information extraction ( IE ) , natural language processing ( NLP ) , and machine learning ( ML ) from Arnetminer as query input .", "ner": [["information extraction", "Task"], ["IE", "Task"], ["natural language processing", "Task"], ["NLP", "Task"], ["machine learning", "Task"], ["ML", "Task"]], "rel": [["IE", "Synonym-Of", "information extraction"], ["NLP", "Synonym-Of", "natural language processing"], ["ML", "Synonym-Of", "machine learning"]], "rel_plus": [["IE:Task", "Synonym-Of", "information extraction:Task"], ["NLP:Task", "Synonym-Of", "natural language processing:Task"], ["ML:Task", "Synonym-Of", "machine learning:Task"]]}
{"doc_id": "210860760", "sentence": "Moreover , the related people lists of these topics are used to construct the ground truth for the evaluation of the recommendation task on the Scopus dataset .", "ner": [["recommendation", "Task"], ["Scopus dataset", "Dataset"]], "rel": [["Scopus dataset", "Benchmark-For", "recommendation"]], "rel_plus": [["Scopus dataset:Dataset", "Benchmark-For", "recommendation:Task"]]}
{"doc_id": "210860760", "sentence": "ExEm(Word 2 vec ) takes the second - ranking position and provides better performance in comparison to ExEm(fastText ) in the recommendation task .", "ner": [["ExEm(Word 2 vec )", "Method"], ["ExEm(fastText )", "Method"], ["recommendation", "Task"]], "rel": [["ExEm(Word 2 vec )", "Compare-With", "ExEm(fastText )"], ["ExEm(Word 2 vec )", "Used-For", "recommendation"], ["ExEm(fastText )", "Used-For", "recommendation"]], "rel_plus": [["ExEm(Word 2 vec ):Method", "Compare-With", "ExEm(fastText ):Method"], ["ExEm(Word 2 vec ):Method", "Used-For", "recommendation:Task"], ["ExEm(fastText ):Method", "Used-For", "recommendation:Task"]]}
{"doc_id": "210860760", "sentence": "DeepWalk and Node 2 vec represent similar achievements but the lowest F 1 - score compared with ExEm .", "ner": [["DeepWalk", "Method"], ["Node 2 vec", "Method"], ["ExEm", "Method"]], "rel": [["Node 2 vec", "Compare-With", "ExEm"], ["DeepWalk", "Compare-With", "ExEm"]], "rel_plus": [["Node 2 vec:Method", "Compare-With", "ExEm:Method"], ["DeepWalk:Method", "Compare-With", "ExEm:Method"]]}
{"doc_id": "210860760", "sentence": "Moreover , the results of experiments in Figure 2 2 : The value of F 1 - score which is achieved by ExEm along with the other two related approaches on Scopus dataset for recommendation task for three topics IE , NLP and M. section of 7 is an important evidence for usefulness of the prepared data for multi - label classification , link prediction and expert recommendation tasks . \u2022 A 2 .", "ner": [["ExEm", "Method"], ["Scopus dataset", "Dataset"], ["recommendation", "Task"], ["IE", "Task"], ["NLP", "Task"], ["multi - label classification", "Task"], ["link prediction", "Task"], ["recommendation", "Task"]], "rel": [["ExEm", "Evaluated-With", "Scopus dataset"], ["Scopus dataset", "Benchmark-For", "recommendation"], ["ExEm", "Used-For", "recommendation"]], "rel_plus": [["ExEm:Method", "Evaluated-With", "Scopus dataset:Dataset"], ["Scopus dataset:Dataset", "Benchmark-For", "recommendation:Task"], ["ExEm:Method", "Used-For", "recommendation:Task"]]}
{"doc_id": "210860760", "sentence": "Word 2 Vec , fastText and their combination are employed to train the neural network of the SKIP - GRAM model .", "ner": [["Word 2 Vec", "Method"], ["fastText", "Method"], ["neural network", "Method"], ["SKIP - GRAM", "Method"]], "rel": [["fastText", "Part-Of", "neural network"], ["Word 2 Vec", "Part-Of", "neural network"], ["fastText", "Part-Of", "SKIP - GRAM"], ["Word 2 Vec", "Part-Of", "SKIP - GRAM"], ["neural network", "Part-Of", "SKIP - GRAM"]], "rel_plus": [["fastText:Method", "Part-Of", "neural network:Method"], ["Word 2 Vec:Method", "Part-Of", "neural network:Method"], ["fastText:Method", "Part-Of", "SKIP - GRAM:Method"], ["Word 2 Vec:Method", "Part-Of", "SKIP - GRAM:Method"], ["neural network:Method", "Part-Of", "SKIP - GRAM:Method"]]}
{"doc_id": "210860760", "sentence": "Furthermore , the DeepWalk and Node 2 Vec approaches were used in order to compare them with the ExEm method .", "ner": [["DeepWalk", "Method"], ["Node 2 Vec", "Method"], ["ExEm", "Method"]], "rel": [["DeepWalk", "Compare-With", "ExEm"], ["Node 2 Vec", "Compare-With", "ExEm"]], "rel_plus": [["DeepWalk:Method", "Compare-With", "ExEm:Method"], ["Node 2 Vec:Method", "Compare-With", "ExEm:Method"]]}
{"doc_id": "210860760", "sentence": "Considering the efficiency of the embedding vectors in the experiments of multi - label classification , link prediction and recommendation tasks , ExEm performs better than the other 2 relevant methods .", "ner": [["multi - label classification", "Task"], ["link prediction", "Task"], ["recommendation", "Task"], ["ExEm", "Method"]], "rel": [["ExEm", "Used-For", "multi - label classification"], ["ExEm", "Used-For", "link prediction"], ["ExEm", "Used-For", "recommendation"]], "rel_plus": [["ExEm:Method", "Used-For", "multi - label classification:Task"], ["ExEm:Method", "Used-For", "link prediction:Task"], ["ExEm:Method", "Used-For", "recommendation:Task"]]}
{"doc_id": "102351044", "sentence": "Convolutional Neural networks ( CNNs ) based applications have become ubiquitous , where proper regularization is greatly needed .", "ner": [["Convolutional Neural networks", "Method"], ["CNNs", "Method"]], "rel": [["CNNs", "Synonym-Of", "Convolutional Neural networks"]], "rel_plus": [["CNNs:Method", "Synonym-Of", "Convolutional Neural networks:Method"]]}
{"doc_id": "102351044", "sentence": "However , many recent works show that the standard dropout is ineffective or even detrimental to the training of CNNs .", "ner": [["dropout", "Method"], ["CNNs", "Method"]], "rel": [["dropout", "Part-Of", "CNNs"]], "rel_plus": [["dropout:Method", "Part-Of", "CNNs:Method"]]}
{"doc_id": "102351044", "sentence": "In this paper , we revisit this issue and examine various dropout variants in an attempt to improve existing dropout - based regularization techniques for CNNs .", "ner": [["dropout", "Method"], ["dropout - based regularization", "Method"], ["CNNs", "Method"]], "rel": [["dropout", "SubClass-Of", "dropout - based regularization"], ["dropout - based regularization", "Part-Of", "CNNs"]], "rel_plus": [["dropout:Method", "SubClass-Of", "dropout - based regularization:Method"], ["dropout - based regularization:Method", "Part-Of", "CNNs:Method"]]}
{"doc_id": "102351044", "sentence": "We attribute the failure of standard dropout to the conflict between the stochasticity of dropout and its following Batch Normalization ( BN ) , and propose to reduce the conflict by placing dropout operations right before the convolutional operation instead of BN , or totally address this issue by replacing BN with Group Normalization ( GN ) .", "ner": [["dropout", "Method"], ["dropout", "Method"], ["Batch Normalization", "Method"], ["BN", "Method"], ["dropout", "Method"], ["convolutional operation", "Method"], ["BN", "Method"], ["BN", "Method"], ["Group Normalization", "Method"], ["GN", "Method"]], "rel": [["BN", "Synonym-Of", "Batch Normalization"], ["GN", "Synonym-Of", "Group Normalization"]], "rel_plus": [["BN:Method", "Synonym-Of", "Batch Normalization:Method"], ["GN:Method", "Synonym-Of", "Group Normalization:Method"]]}
{"doc_id": "102351044", "sentence": "We further introduce a structurally more suited dropout variant Drop - Conv 2 d , which provides more efficient and effective regularization for deep CNNs .", "ner": [["dropout", "Method"], ["Drop - Conv 2 d", "Method"], ["CNNs", "Method"]], "rel": [["Drop - Conv 2 d", "SubClass-Of", "dropout"], ["Drop - Conv 2 d", "Part-Of", "CNNs"]], "rel_plus": [["Drop - Conv 2 d:Method", "SubClass-Of", "dropout:Method"], ["Drop - Conv 2 d:Method", "Part-Of", "CNNs:Method"]]}
{"doc_id": "102351044", "sentence": "These dropout variants can be readily integrated into the building blocks of CNNs and implemented in existing deep learning platforms .", "ner": [["dropout", "Method"], ["CNNs", "Method"], ["deep learning", "Method"]], "rel": [["dropout", "Part-Of", "CNNs"], ["dropout", "Part-Of", "deep learning"]], "rel_plus": [["dropout:Method", "Part-Of", "CNNs:Method"], ["dropout:Method", "Part-Of", "deep learning:Method"]]}
{"doc_id": "102351044", "sentence": "Extensive experiments on benchmark datasets including CIFAR , SVHN and ImageNet are conducted to compare the existing building blocks and the proposed ones with dropout training .", "ner": [["CIFAR", "Dataset"], ["SVHN", "Dataset"], ["ImageNet", "Dataset"], ["dropout training", "Method"]], "rel": [["dropout training", "Evaluated-With", "CIFAR"], ["dropout training", "Evaluated-With", "SVHN"], ["dropout training", "Evaluated-With", "ImageNet"]], "rel_plus": [["dropout training:Method", "Evaluated-With", "CIFAR:Dataset"], ["dropout training:Method", "Evaluated-With", "SVHN:Dataset"], ["dropout training:Method", "Evaluated-With", "ImageNet:Dataset"]]}
{"doc_id": "102351044", "sentence": "Recently , with big data and the proliferation of machine learning , especially deep learning , there is a surge of machinelearning - based data - driven [ 2 5 ] applications including deeplearning - based video stream analysis [ 1 6 ] , health - care analysis [ 3 ] and database system optimization [ 3 2 , 2 2 ] .", "ner": [["machine learning", "Method"], ["deep learning", "Method"], ["deeplearning", "Method"], ["video stream analysis", "Task"], ["health - care analysis", "Task"], ["database system optimization", "Task"]], "rel": [["deep learning", "SubClass-Of", "machine learning"], ["deeplearning", "Used-For", "video stream analysis"], ["deeplearning", "Used-For", "health - care analysis"], ["deeplearning", "Used-For", "database system optimization"]], "rel_plus": [["deep learning:Method", "SubClass-Of", "machine learning:Method"], ["deeplearning:Method", "Used-For", "video stream analysis:Task"], ["deeplearning:Method", "Used-For", "health - care analysis:Task"], ["deeplearning:Method", "Used-For", "database system optimization:Task"]]}
{"doc_id": "102351044", "sentence": "For example , deep convolutional neural networks ( CNNs ) have led to a series of breakthroughs on a variety of tasks [ 1 9 , 6 , 1 2 ] .", "ner": [["convolutional neural networks", "Method"], ["CNNs", "Method"]], "rel": [["CNNs", "Synonym-Of", "convolutional neural networks"]], "rel_plus": [["CNNs:Method", "Synonym-Of", "convolutional neural networks:Method"]]}
{"doc_id": "102351044", "sentence": "To alleviate overfitting , many explicit and implicit regularization methods have been proposed , including early stopping , weight decay , data augmentation etc .", "ner": [["explicit and implicit regularization", "Method"], ["early stopping", "Method"], ["weight decay", "Method"], ["data augmentation", "Method"]], "rel": [["early stopping", "SubClass-Of", "explicit and implicit regularization"], ["weight decay", "SubClass-Of", "explicit and implicit regularization"], ["data augmentation", "SubClass-Of", "explicit and implicit regularization"]], "rel_plus": [["early stopping:Method", "SubClass-Of", "explicit and implicit regularization:Method"], ["weight decay:Method", "SubClass-Of", "explicit and implicit regularization:Method"], ["data augmentation:Method", "SubClass-Of", "explicit and implicit regularization:Method"]]}
{"doc_id": "102351044", "sentence": "Recent applications of dropout in convolutional neural networks [ 7 , 3 7 , 1 3 ] fail to obtain significant performance improvement .", "ner": [["dropout", "Method"], ["convolutional neural networks", "Method"]], "rel": [["dropout", "Part-Of", "convolutional neural networks"]], "rel_plus": [["dropout:Method", "Part-Of", "convolutional neural networks:Method"]]}
{"doc_id": "102351044", "sentence": "Dropout [ 2 8 , 9 ] is initially introduced in fully connected layers [ 1 9 ] of neural networks .", "ner": [["Dropout", "Method"], ["fully connected layers", "Method"], ["neural networks", "Method"]], "rel": [["Dropout", "Part-Of", "fully connected layers"], ["fully connected layers", "Part-Of", "neural networks"]], "rel_plus": [["Dropout:Method", "Part-Of", "fully connected layers:Method"], ["fully connected layers:Method", "Part-Of", "neural networks:Method"]]}
{"doc_id": "102351044", "sentence": "Recent CNN models replace the fully connected layers with a global average pooling layer [ 2 1 ] .", "ner": [["CNN", "Method"], ["fully connected layers", "Method"], ["global average pooling", "Method"]], "rel": [["global average pooling", "Part-Of", "CNN"]], "rel_plus": [["global average pooling:Method", "Part-Of", "CNN:Method"]]}
{"doc_id": "102351044", "sentence": "Many CNNs have tried to apply dropout to convolution layers .", "ner": [["CNNs", "Method"], ["dropout", "Method"], ["convolution", "Method"]], "rel": [["convolution", "Part-Of", "CNNs"], ["dropout", "Part-Of", "convolution"]], "rel_plus": [["convolution:Method", "Part-Of", "CNNs:Method"], ["dropout:Method", "Part-Of", "convolution:Method"]]}
{"doc_id": "102351044", "sentence": "For instance , WRN [ 3 7 ] applies a dropout layer between the convolution layers in each residual convolutional building block .", "ner": [["WRN", "Method"], ["dropout", "Method"], ["convolution", "Method"], ["residual convolutional building block", "Method"]], "rel": [["dropout", "Part-Of", "WRN"], ["convolution", "Part-Of", "WRN"], ["residual convolutional building block", "Part-Of", "WRN"], ["convolution", "Part-Of", "residual convolutional building block"], ["dropout", "Part-Of", "residual convolutional building block"]], "rel_plus": [["dropout:Method", "Part-Of", "WRN:Method"], ["convolution:Method", "Part-Of", "WRN:Method"], ["residual convolutional building block:Method", "Part-Of", "WRN:Method"], ["convolution:Method", "Part-Of", "residual convolutional building block:Method"], ["dropout:Method", "Part-Of", "residual convolutional building block:Method"]]}
{"doc_id": "102351044", "sentence": "Dropout used in these CNNs is still on the neuron level , which turns out to be less effective .", "ner": [["Dropout", "Method"], ["CNNs", "Method"]], "rel": [["Dropout", "Part-Of", "CNNs"]], "rel_plus": [["Dropout:Method", "Part-Of", "CNNs:Method"]]}
{"doc_id": "102351044", "sentence": "For example , even negative effects are observed [ 8 ] when applying dropout in the identity mapping part of the residual block in ResNet [ 7 ] .", "ner": [["dropout", "Method"], ["residual block", "Method"], ["ResNet", "Method"]], "rel": [["dropout", "Part-Of", "residual block"], ["residual block", "Part-Of", "ResNet"]], "rel_plus": [["dropout:Method", "Part-Of", "residual block:Method"], ["residual block:Method", "Part-Of", "ResNet:Method"]]}
{"doc_id": "102351044", "sentence": "The effect of dropout in CNNs is further diminished by the introduction of other regularization techniques such as data augmentation and batch normalization [ 1 5 ] .", "ner": [["dropout", "Method"], ["CNNs", "Method"], ["regularization techniques", "Method"], ["data augmentation", "Method"], ["batch normalization", "Method"]], "rel": [["dropout", "Part-Of", "CNNs"], ["data augmentation", "SubClass-Of", "regularization techniques"], ["batch normalization", "SubClass-Of", "regularization techniques"]], "rel_plus": [["dropout:Method", "Part-Of", "CNNs:Method"], ["data augmentation:Method", "SubClass-Of", "regularization techniques:Method"], ["batch normalization:Method", "SubClass-Of", "regularization techniques:Method"]]}
{"doc_id": "102351044", "sentence": "To better integrate dropout into CNNs , we revisit the existing dropout methods applied at different structural levels , namely neuron , channel , path and layer level .", "ner": [["dropout", "Method"], ["CNNs", "Method"], ["dropout", "Method"]], "rel": [["dropout", "Part-Of", "CNNs"]], "rel_plus": [["dropout:Method", "Part-Of", "CNNs:Method"]]}
{"doc_id": "102351044", "sentence": "By analogy with standard dropout in the neuron level , we present a unified framework to analyze the four dropout methods .", "ner": [["dropout", "Method"], ["dropout", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "102351044", "sentence": "They are denoted as drop - neuron , drop - channel , drop - path and drop - layer for the neuron , channel , path and layer level dropout respectively .", "ner": [["drop - neuron", "Method"], ["drop - channel", "Method"], ["drop - path", "Method"], ["drop - layer", "Method"], ["dropout", "Method"]], "rel": [["drop - neuron", "SubClass-Of", "dropout"], ["drop - channel", "SubClass-Of", "dropout"], ["drop - path", "SubClass-Of", "dropout"], ["drop - layer", "SubClass-Of", "dropout"]], "rel_plus": [["drop - neuron:Method", "SubClass-Of", "dropout:Method"], ["drop - channel:Method", "SubClass-Of", "dropout:Method"], ["drop - path:Method", "SubClass-Of", "dropout:Method"], ["drop - layer:Method", "SubClass-Of", "dropout:Method"]]}
{"doc_id": "102351044", "sentence": "To appreciate the reason why existing dropout methods fail when they are applied to convolutional layers of CNNs , we investigate the interaction between dropout and other techniques involved in training CNNs , including data augmentation and batch normalization [ 1 5 ] .", "ner": [["dropout", "Method"], ["convolutional layers", "Method"], ["CNNs", "Method"], ["dropout", "Method"], ["CNNs", "Method"], ["data augmentation", "Method"], ["batch normalization", "Method"]], "rel": [["dropout", "Part-Of", "convolutional layers"], ["convolutional layers", "Part-Of", "CNNs"], ["data augmentation", "Used-For", "CNNs"], ["batch normalization", "Part-Of", "CNNs"]], "rel_plus": [["dropout:Method", "Part-Of", "convolutional layers:Method"], ["convolutional layers:Method", "Part-Of", "CNNs:Method"], ["data augmentation:Method", "Used-For", "CNNs:Method"], ["batch normalization:Method", "Part-Of", "CNNs:Method"]]}
{"doc_id": "102351044", "sentence": "We attribute the failure of standard dropouts to the increase of variance from random deactivation of the basic components , e.g. neurons in drop - neuron and channels in drop - channel , which conflicts with the essence of the batch normalization layer following each convolutional layer .", "ner": [["dropouts", "Method"], ["drop - neuron", "Method"], ["drop - channel", "Method"], ["batch normalization", "Method"], ["convolutional layer", "Method"]], "rel": [["batch normalization", "Part-Of", "convolutional layer"]], "rel_plus": [["batch normalization:Method", "Part-Of", "convolutional layer:Method"]]}
{"doc_id": "102351044", "sentence": "We therefore propose to reorder the dropout and batch normalization in the convolutional building blocks to address this problem .", "ner": [["dropout", "Method"], ["batch normalization", "Method"], ["convolutional building blocks", "Method"]], "rel": [["dropout", "Part-Of", "convolutional building blocks"], ["batch normalization", "Part-Of", "convolutional building blocks"]], "rel_plus": [["dropout:Method", "Part-Of", "convolutional building blocks:Method"], ["batch normalization:Method", "Part-Of", "convolutional building blocks:Method"]]}
{"doc_id": "102351044", "sentence": "A convolutional building block contains a sequence of layers , including dropout layer , convolution layer , batch normalization layer , etc .", "ner": [["convolutional building block", "Method"], ["dropout", "Method"], ["convolution", "Method"], ["batch normalization", "Method"]], "rel": [["dropout", "Part-Of", "convolutional building block"], ["convolution", "Part-Of", "convolutional building block"], ["batch normalization", "Part-Of", "convolutional building block"]], "rel_plus": [["dropout:Method", "Part-Of", "convolutional building block:Method"], ["convolution:Method", "Part-Of", "convolutional building block:Method"], ["batch normalization:Method", "Part-Of", "convolutional building block:Method"]]}
{"doc_id": "102351044", "sentence": "First , the proposed dropout building blocks are generally applicable to existing CNN architectures .", "ner": [["dropout building blocks", "Method"], ["CNN", "Method"]], "rel": [["dropout building blocks", "Part-Of", "CNN"]], "rel_plus": [["dropout building blocks:Method", "Part-Of", "CNN:Method"]]}
{"doc_id": "102351044", "sentence": "Third , the introduction of different levels of dropouts to convolutional layers of CNNs , especially drop - channel , provides a more general and effective regularization for CNNs which achieves stateof - the - art performance in a wide range of tasks , e.g. CIFAR - 1 0 , CIFAR - 1 0 0 and SVHN .", "ner": [["dropouts", "Method"], ["convolutional layers", "Method"], ["CNNs", "Method"], ["drop - channel", "Method"], ["CNNs", "Method"], ["CIFAR - 1 0", "Dataset"], ["CIFAR - 1 0 0", "Dataset"], ["SVHN", "Dataset"]], "rel": [["drop - channel", "SubClass-Of", "dropouts"], ["dropouts", "Part-Of", "convolutional layers"], ["convolutional layers", "Part-Of", "CNNs"], ["drop - channel", "Part-Of", "CNNs"], ["CNNs", "Evaluated-With", "CIFAR - 1 0"], ["CNNs", "Evaluated-With", "CIFAR - 1 0 0"], ["CNNs", "Evaluated-With", "SVHN"]], "rel_plus": [["drop - channel:Method", "SubClass-Of", "dropouts:Method"], ["dropouts:Method", "Part-Of", "convolutional layers:Method"], ["convolutional layers:Method", "Part-Of", "CNNs:Method"], ["drop - channel:Method", "Part-Of", "CNNs:Method"], ["CNNs:Method", "Evaluated-With", "CIFAR - 1 0:Dataset"], ["CNNs:Method", "Evaluated-With", "CIFAR - 1 0 0:Dataset"], ["CNNs:Method", "Evaluated-With", "SVHN:Dataset"]]}
{"doc_id": "102351044", "sentence": "We adopt widely benchmarked datasets CIFAR - 1 0 , CIFAR - 1 0 0 , SVHN and ImageNet where significant improvement in terms of accuracy is observed even with the presence of extensive data augmentation and batch normalization .", "ner": [["CIFAR - 1 0", "Dataset"], ["CIFAR - 1 0 0", "Dataset"], ["SVHN", "Dataset"], ["ImageNet", "Dataset"], ["data augmentation", "Method"], ["batch normalization", "Method"]], "rel": [["data augmentation", "Evaluated-With", "CIFAR - 1 0"], ["batch normalization", "Evaluated-With", "CIFAR - 1 0"], ["data augmentation", "Evaluated-With", "CIFAR - 1 0 0"], ["batch normalization", "Evaluated-With", "CIFAR - 1 0 0"], ["data augmentation", "Evaluated-With", "SVHN"], ["batch normalization", "Evaluated-With", "SVHN"], ["data augmentation", "Evaluated-With", "ImageNet"], ["batch normalization", "Evaluated-With", "ImageNet"]], "rel_plus": [["data augmentation:Method", "Evaluated-With", "CIFAR - 1 0:Dataset"], ["batch normalization:Method", "Evaluated-With", "CIFAR - 1 0:Dataset"], ["data augmentation:Method", "Evaluated-With", "CIFAR - 1 0 0:Dataset"], ["batch normalization:Method", "Evaluated-With", "CIFAR - 1 0 0:Dataset"], ["data augmentation:Method", "Evaluated-With", "SVHN:Dataset"], ["batch normalization:Method", "Evaluated-With", "SVHN:Dataset"], ["data augmentation:Method", "Evaluated-With", "ImageNet:Dataset"], ["batch normalization:Method", "Evaluated-With", "ImageNet:Dataset"]]}
{"doc_id": "102351044", "sentence": "With our proposed convolutional building blocks , we achieve significant improvement over state - of - the - art CNNs by 3. 1 7 % on CIFAR - 1 0 , 1 6 . 1 5 % on CIFAR - 1 0 0 , and 1. 4 4 % on SVHN .", "ner": [["convolutional building blocks", "Method"], ["CNNs", "Method"], ["CIFAR - 1 0", "Dataset"], ["CIFAR - 1 0 0", "Dataset"], ["SVHN", "Dataset"]], "rel": [["convolutional building blocks", "Part-Of", "CNNs"], ["CNNs", "Evaluated-With", "CIFAR - 1 0"], ["CNNs", "Evaluated-With", "CIFAR - 1 0 0"], ["CNNs", "Evaluated-With", "SVHN"]], "rel_plus": [["convolutional building blocks:Method", "Part-Of", "CNNs:Method"], ["CNNs:Method", "Evaluated-With", "CIFAR - 1 0:Dataset"], ["CNNs:Method", "Evaluated-With", "CIFAR - 1 0 0:Dataset"], ["CNNs:Method", "Evaluated-With", "SVHN:Dataset"]]}
{"doc_id": "102351044", "sentence": "The main contributions of this paper are : \u2022 We present a unified framework for analyzing dropout methods in CNNs .", "ner": [["dropout", "Method"], ["CNNs", "Method"]], "rel": [["dropout", "Part-Of", "CNNs"]], "rel_plus": [["dropout:Method", "Part-Of", "CNNs:Method"]]}
{"doc_id": "102351044", "sentence": "Specifically , we investigate the failure of two types of dropouts , which is mainly due to the incorrect placement of them in the convolutional building blocks . \u2022 We propose new convolutional building blocks supporting dropout training mechanisms to better integrate dropouts , which are readily applicable to existing CNN architectures and harness the benefits of the regularization and model ensemble effect . \u2022 Extensive experiments are conducted to compare different dropout methods and test the effectiveness of the proposed dropout convolutional building blocks as illustrated in Figure 1 , with which we achieve significant improvement over state - of - the - art CNNs .", "ner": [["dropouts", "Method"], ["convolutional building blocks", "Method"], ["convolutional building blocks", "Method"], ["dropout training mechanisms", "Method"], ["dropouts", "Method"], ["CNN", "Method"], ["dropout", "Method"], ["dropout convolutional building blocks", "Method"], ["CNNs", "Method"]], "rel": [["dropouts", "Part-Of", "convolutional building blocks"], ["dropouts", "Part-Of", "convolutional building blocks"], ["dropout training mechanisms", "Used-For", "convolutional building blocks"], ["convolutional building blocks", "Part-Of", "CNN"], ["dropout", "Part-Of", "dropout convolutional building blocks"], ["dropout convolutional building blocks", "Part-Of", "CNNs"]], "rel_plus": [["dropouts:Method", "Part-Of", "convolutional building blocks:Method"], ["dropouts:Method", "Part-Of", "convolutional building blocks:Method"], ["dropout training mechanisms:Method", "Used-For", "convolutional building blocks:Method"], ["convolutional building blocks:Method", "Part-Of", "CNN:Method"], ["dropout:Method", "Part-Of", "dropout convolutional building blocks:Method"], ["dropout convolutional building blocks:Method", "Part-Of", "CNNs:Method"]]}
{"doc_id": "102351044", "sentence": "Section 2 introduces the background , mainly focusing on the development of convolutional neural networks and dropout for deep neural networks .", "ner": [["convolutional neural networks", "Method"], ["dropout", "Method"], ["deep neural networks", "Method"]], "rel": [["convolutional neural networks", "Part-Of", "deep neural networks"], ["dropout", "Part-Of", "deep neural networks"]], "rel_plus": [["convolutional neural networks:Method", "Part-Of", "deep neural networks:Method"], ["dropout:Method", "Part-Of", "deep neural networks:Method"]]}
{"doc_id": "102351044", "sentence": "Finally , we conclude the paper in Section 5 .   The development of convolutional neural networks ( CNNs ) in recent years mainly comes from the architecture engineering .", "ner": [["convolutional neural networks", "Method"], ["CNNs", "Method"]], "rel": [["CNNs", "Synonym-Of", "convolutional neural networks"]], "rel_plus": [["CNNs:Method", "Synonym-Of", "convolutional neural networks:Method"]]}
{"doc_id": "102351044", "sentence": "AlexNet [ 1 9 ] wins the 2 0 1 2 ILSVRC with an 8 layers CNN model .", "ner": [["AlexNet", "Method"], ["2 0 1 2 ILSVRC", "Dataset"], ["8 layers CNN", "Method"]], "rel": [["8 layers CNN", "Part-Of", "AlexNet"], ["AlexNet", "Evaluated-With", "2 0 1 2 ILSVRC"]], "rel_plus": [["8 layers CNN:Method", "Part-Of", "AlexNet:Method"], ["AlexNet:Method", "Evaluated-With", "2 0 1 2 ILSVRC:Dataset"]]}
{"doc_id": "102351044", "sentence": "In 2 0 1 4 , VGG [ 2 6 ] and GoogLeNet [ 3 0 ] push the depth of CNNs to 1 9 and 2 2 respectively by stacking the basic convolutional building blocks , e.g. Inception module in GoogLeNet .", "ner": [["VGG", "Method"], ["GoogLeNet", "Method"], ["CNNs", "Method"], ["convolutional building blocks", "Method"], ["Inception module", "Method"], ["GoogLeNet", "Method"]], "rel": [["GoogLeNet", "SubClass-Of", "CNNs"], ["VGG", "SubClass-Of", "CNNs"], ["Inception module", "SubClass-Of", "convolutional building blocks"], ["Inception module", "Part-Of", "GoogLeNet"]], "rel_plus": [["GoogLeNet:Method", "SubClass-Of", "CNNs:Method"], ["VGG:Method", "SubClass-Of", "CNNs:Method"], ["Inception module:Method", "SubClass-Of", "convolutional building blocks:Method"], ["Inception module:Method", "Part-Of", "GoogLeNet:Method"]]}
{"doc_id": "102351044", "sentence": "With the shortcut of identity mapping , ResNet enables the training of extremely deep CNNs over 1 0 0 0 layers and wins the competition with an unprecedentedly 1 5 2 layers deep CNN .", "ner": [["ResNet", "Method"], ["CNNs", "Method"], ["1 5 2 layers deep CNN", "Method"]], "rel": [["ResNet", "Part-Of", "CNNs"], ["1 5 2 layers deep CNN", "SubClass-Of", "CNNs"]], "rel_plus": [["ResNet:Method", "Part-Of", "CNNs:Method"], ["1 5 2 layers deep CNN:Method", "SubClass-Of", "CNNs:Method"]]}
{"doc_id": "102351044", "sentence": "Deeper CNNs undoubtedly provides larger representational capacity , while widening each layer can lead to better representation complementarily . [ 2 1 ] replaces the filter kernel of the convolutional transformation with multilayer perceptron .", "ner": [["CNNs", "Method"], ["filter kernel", "Method"], ["convolutional transformation", "Method"], ["multilayer perceptron", "Method"]], "rel": [["convolutional transformation", "Part-Of", "CNNs"], ["multilayer perceptron", "Part-Of", "convolutional transformation"]], "rel_plus": [["convolutional transformation:Method", "Part-Of", "CNNs:Method"], ["multilayer perceptron:Method", "Part-Of", "convolutional transformation:Method"]]}
{"doc_id": "102351044", "sentence": "This Network In Network ( NIN ) structure allows complex and learnable interactions between input channels .", "ner": [["Network In Network", "Method"], ["NIN", "Method"]], "rel": [["NIN", "Synonym-Of", "Network In Network"]], "rel_plus": [["NIN:Method", "Synonym-Of", "Network In Network:Method"]]}
{"doc_id": "102351044", "sentence": "Similar to NIN , CNNs such as Inception series [ 3 0 , 2 9 ] and ResNeXt [ 3 6 ] explore group convolution [ 1 9 ] , using multibranches convolutional operation based on the idea of splitting , transforming and aggregating combination .", "ner": [["NIN", "Method"], ["CNNs", "Method"], ["Inception", "Method"], ["ResNeXt", "Method"], ["group convolution", "Method"], ["multibranches convolutional operation", "Method"]], "rel": [["Inception", "SubClass-Of", "CNNs"], ["ResNeXt", "SubClass-Of", "CNNs"], ["group convolution", "SubClass-Of", "CNNs"], ["NIN", "Compare-With", "CNNs"], ["multibranches convolutional operation", "Used-For", "group convolution"]], "rel_plus": [["Inception:Method", "SubClass-Of", "CNNs:Method"], ["ResNeXt:Method", "SubClass-Of", "CNNs:Method"], ["group convolution:Method", "SubClass-Of", "CNNs:Method"], ["NIN:Method", "Compare-With", "CNNs:Method"], ["multibranches convolutional operation:Method", "Used-For", "group convolution:Method"]]}
{"doc_id": "102351044", "sentence": "Besides the exploration of deeper and wider CNNs , new building blocks of CNNs have also been proposed , the stacking of which leads to state - of - the - art CNN architectures .", "ner": [["CNNs", "Method"], ["CNNs", "Method"], ["CNN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "102351044", "sentence": "For instance , the new convolutional layer NIN [ 2 1 ] and Inception modules [ 3 0 ] enable more complicated feature extractions with minimum additional parameters .", "ner": [["convolutional layer", "Method"], ["NIN", "Method"], ["Inception", "Method"]], "rel": [["NIN", "SubClass-Of", "convolutional layer"], ["Inception", "SubClass-Of", "convolutional layer"]], "rel_plus": [["NIN:Method", "SubClass-Of", "convolutional layer:Method"], ["Inception:Method", "SubClass-Of", "convolutional layer:Method"]]}
{"doc_id": "102351044", "sentence": "Many regularization methods , mainly introduced during training , have been proposed to this end , including early stopping , weight decay , data augmentation , batch normalization [ 1 5 ] etc .", "ner": [["early stopping", "Method"], ["weight decay", "Method"], ["data augmentation", "Method"], ["batch normalization", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "102351044", "sentence": "To explain the effect of dropout , [ 3 4 ] shows that for generalized linear models , dropout performs a form of adaptive regularization .", "ner": [["dropout", "Method"], ["dropout", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "102351044", "sentence": "Theoretically , the dropout regularizer is first - order equivalent to an L 2 regularizer . [ 1 ] also shows that dropout provides immediately the magnitude of the regularization term which is adaptively scaled by the inputs and the variance of the dropout variables . [ 5 , 1 7 ] instead formulate neural networks trained with dropout in the Bayesian inference framework , providing tools to model uncertainty with dropout training .", "ner": [["dropout", "Method"], ["L 2 regularizer", "Method"], ["dropout", "Method"], ["dropout", "Method"], ["neural networks", "Method"], ["dropout", "Method"], ["Bayesian inference", "Method"], ["dropout training", "Method"]], "rel": [["dropout", "Compare-With", "L 2 regularizer"], ["dropout", "Part-Of", "neural networks"], ["dropout training", "Part-Of", "Bayesian inference"], ["neural networks", "Part-Of", "Bayesian inference"]], "rel_plus": [["dropout:Method", "Compare-With", "L 2 regularizer:Method"], ["dropout:Method", "Part-Of", "neural networks:Method"], ["dropout training:Method", "Part-Of", "Bayesian inference:Method"], ["neural networks:Method", "Part-Of", "Bayesian inference:Method"]]}
{"doc_id": "102351044", "sentence": "Swapout generalizes dropout with a new stochastic training method , whose training process can be viewed as sampling from a rich archi - tecture including dropout , stochastic depth [ 1 4 ] and residual architecture [ 7 ] .", "ner": [["dropout", "Method"], ["dropout", "Method"], ["stochastic depth", "Method"], ["residual architecture", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "102351044", "sentence": "Unlike deactivating neurons in dropout , another work DropConnect [ 3 5 ] introduces randomness to connections and sets a randomly selected subset of weights within the network to zero during training .", "ner": [["dropout", "Method"], ["DropConnect", "Method"]], "rel": [["DropConnect", "Compare-With", "dropout"]], "rel_plus": [["DropConnect:Method", "Compare-With", "dropout:Method"]]}
{"doc_id": "102351044", "sentence": "Different levels of dropout have also been studied and introduced to deep convolutional neural networks .", "ner": [["dropout", "Method"], ["convolutional neural networks", "Method"]], "rel": [["dropout", "Part-Of", "convolutional neural networks"]], "rel_plus": [["dropout:Method", "Part-Of", "convolutional neural networks:Method"]]}
{"doc_id": "102351044", "sentence": "In [ 3 1 ] , SpatialDropout shows that adding one additional layer with dropout applied to channels can achieve better performance in the specific task of object localization .", "ner": [["SpatialDropout", "Method"], ["dropout", "Method"], ["object localization", "Task"]], "rel": [["SpatialDropout", "SubClass-Of", "dropout"], ["SpatialDropout", "Used-For", "object localization"]], "rel_plus": [["SpatialDropout:Method", "SubClass-Of", "dropout:Method"], ["SpatialDropout:Method", "Used-For", "object localization:Task"]]}
{"doc_id": "102351044", "sentence": "In the path level , Drop - path proposed in FractalNet [ 2 0 ] randomly drops individual paths during training , preventing co - adaptation of parallel paths .", "ner": [["Drop - path", "Method"], ["FractalNet", "Method"]], "rel": [["Drop - path", "Part-Of", "FractalNet"]], "rel_plus": [["Drop - path:Method", "Part-Of", "FractalNet:Method"]]}
{"doc_id": "102351044", "sentence": "This can be interpreted as layer - wise dropout , which achieves an ensemble of ResNets implicitly [ 3 3 ] .", "ner": [["layer - wise dropout", "Method"], ["ResNets", "Method"]], "rel": [["layer - wise dropout", "Part-Of", "ResNets"]], "rel_plus": [["layer - wise dropout:Method", "Part-Of", "ResNets:Method"]]}
{"doc_id": "102351044", "sentence": "These dropout variants in convolutional neural networks apply dropout to basic units of CNNs , harnessing both the regularization and model ensemble benefits .", "ner": [["dropout", "Method"], ["convolutional neural networks", "Method"], ["dropout", "Method"], ["CNNs", "Method"]], "rel": [["dropout", "Part-Of", "convolutional neural networks"], ["dropout", "Part-Of", "CNNs"]], "rel_plus": [["dropout:Method", "Part-Of", "convolutional neural networks:Method"], ["dropout:Method", "Part-Of", "CNNs:Method"]]}
{"doc_id": "102351044", "sentence": "Based on such formulation , we introduce general training mechanisms with drop - operations of different structural level deriving from dropout for deep CNNs .", "ner": [["dropout", "Method"], ["CNNs", "Method"]], "rel": [["dropout", "Part-Of", "CNNs"]], "rel_plus": [["dropout:Method", "Part-Of", "CNNs:Method"]]}
{"doc_id": "102351044", "sentence": "Broadly speaking , the topology of neural networks , including vanilla neural networks , recurrent neural networks [ 2 , 1 0 ] and convolutional neural networks [ 1 9 , 7 , 1 3 ] , can be represented precisely by a set of connections among neurons , where the information flow from input neurons to output neurons is regulated by learnable weights coupled with each connection .", "ner": [["neural networks", "Method"], ["vanilla neural networks", "Method"], ["recurrent neural networks", "Method"], ["convolutional neural networks", "Method"]], "rel": [["vanilla neural networks", "SubClass-Of", "neural networks"], ["recurrent neural networks", "SubClass-Of", "neural networks"], ["convolutional neural networks", "SubClass-Of", "neural networks"]], "rel_plus": [["vanilla neural networks:Method", "SubClass-Of", "neural networks:Method"], ["recurrent neural networks:Method", "SubClass-Of", "neural networks:Method"], ["convolutional neural networks:Method", "SubClass-Of", "neural networks:Method"]]}
{"doc_id": "102351044", "sentence": "The most fundamental operation in CNNs comes from the convolutional layer which can be constructed to represent any given transformation Fconv : X \u2192 Y , where X \u2208 R C in \u00d7W in \u00d7H in is the input with Cin channels of size Win \u00d7 Hin , Y \u2208 R C out \u00d7W out \u00d7H out the output likewise .", "ner": [["CNNs", "Method"], ["convolutional layer", "Method"]], "rel": [["convolutional layer", "Part-Of", "CNNs"]], "rel_plus": [["convolutional layer:Method", "Part-Of", "CNNs:Method"]]}
{"doc_id": "102351044", "sentence": "It worth noting that group convolution [ 1 9 ] and depth - wise convolution [ 1 1 ] can also be represented under such formulation with customized constraints on the connection between channels .", "ner": [["group convolution", "Method"], ["depth - wise convolution", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "102351044", "sentence": "For instance , NIN [ 2 1 ] lengthens Fconv by following the traditional filter kernel with two layers of multilayer perceptron transformation , which is structurally equivalent to appending it with two convolutional layers with 1 \u00d7 1 filter .", "ner": [["NIN", "Method"], ["filter kernel", "Method"], ["multilayer perceptron", "Method"], ["convolutional layers", "Method"], ["1 \u00d7 1 filter", "Method"]], "rel": [["filter kernel", "Part-Of", "multilayer perceptron"], ["1 \u00d7 1 filter", "Part-Of", "convolutional layers"]], "rel_plus": [["filter kernel:Method", "Part-Of", "multilayer perceptron:Method"], ["1 \u00d7 1 filter:Method", "Part-Of", "convolutional layers:Method"]]}
{"doc_id": "102351044", "sentence": "DenseNet [ 1 3 ] also proposes direct feature reusage by forwarding and appending input channels X directly to output channels Y , specifically Dropout [ 2 8 , 9 ] has been empirically shown to be an effective method of regularization and an economical way of model ensemble for deep neural networks .", "ner": [["DenseNet", "Method"], ["Dropout", "Method"], ["deep neural networks", "Method"]], "rel": [["Dropout", "Part-Of", "DenseNet"]], "rel_plus": [["Dropout:Method", "Part-Of", "DenseNet:Method"]]}
{"doc_id": "102351044", "sentence": "As is illustrated in Figure 2 , the standard neuron level dropout , which we name it drop - neuron here to differentiate canonical dropout with other higher structural level of dropouts , introduces randomness to the training process which forces each neuron to learn more robust representations that are effective even with different random subsets of other neurons , thus improves generalization .", "ner": [["neuron level dropout", "Method"], ["drop - neuron", "Method"], ["dropout", "Method"], ["dropouts", "Method"]], "rel": [["drop - neuron", "Synonym-Of", "neuron level dropout"], ["drop - neuron", "Compare-With", "dropout"]], "rel_plus": [["drop - neuron:Method", "Synonym-Of", "neuron level dropout:Method"], ["drop - neuron:Method", "Compare-With", "dropout:Method"]]}
{"doc_id": "102351044", "sentence": "More importantly , after training , rescaling each wj with 1 \u2212 pj [ 9 , 2 8 , 1 ] , the resulting network then can be regarded as the ensemble network of many dropout subnets and therefore be used during inference time directly without dropout .", "ner": [["dropout", "Method"], ["dropout", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "102351044", "sentence": "For instance , dropout prevents overfitting significantly in the fully connected layers in the ILSVRC - 2 0 1 2 [ 4 ] winning CNN model AlexNet [ 1 9 ] , and also recurrent layers in Neural Network Language Modeling models [ 2 3 , 3 8 ] .", "ner": [["dropout", "Method"], ["fully connected layers", "Method"], ["ILSVRC - 2 0 1 2", "Dataset"], ["CNN", "Method"], ["AlexNet", "Method"], ["Neural Network Language Modeling", "Method"]], "rel": [["dropout", "Part-Of", "fully connected layers"], ["AlexNet", "Evaluated-With", "ILSVRC - 2 0 1 2"], ["AlexNet", "SubClass-Of", "CNN"], ["fully connected layers", "Part-Of", "AlexNet"], ["dropout", "Part-Of", "Neural Network Language Modeling"]], "rel_plus": [["dropout:Method", "Part-Of", "fully connected layers:Method"], ["AlexNet:Method", "Evaluated-With", "ILSVRC - 2 0 1 2:Dataset"], ["AlexNet:Method", "SubClass-Of", "CNN:Method"], ["fully connected layers:Method", "Part-Of", "AlexNet:Method"], ["dropout:Method", "Part-Of", "Neural Network Language Modeling:Method"]]}
{"doc_id": "102351044", "sentence": "However , recent stateof - the - art CNN models [ 7 , 1 3 , 1 2 ] no longer contains fullyconnected layers , which are instead replaced by global average pooling [ 2 1 ] .", "ner": [["CNN", "Method"], ["fullyconnected layers", "Method"], ["global average pooling", "Method"]], "rel": [["global average pooling", "Part-Of", "CNN"]], "rel_plus": [["global average pooling:Method", "Part-Of", "CNN:Method"]]}
{"doc_id": "102351044", "sentence": "We conjecture that this is because structurally in CNNs , feature extractions are conducted channel - wisely during convolutional operation , thus neuron level dropout can hardly enjoy the ensemble benefits .", "ner": [["CNNs", "Method"], ["convolutional operation", "Method"], ["neuron level dropout", "Method"]], "rel": [["convolutional operation", "Part-Of", "CNNs"], ["neuron level dropout", "Part-Of", "CNNs"]], "rel_plus": [["convolutional operation:Method", "Part-Of", "CNNs:Method"], ["neuron level dropout:Method", "Part-Of", "CNNs:Method"]]}
{"doc_id": "102351044", "sentence": "Dropout , therefore , can only improve performance with the regularization effect , whose contribution is quite finite in CNNs with extensive data augmentation .   The channel level dropout , drop - channel , is inspired by the observation that there exists a close structural correspondence between channels in convolutional layer and neurons in vanilla neural networks , which is formulated formally in Equation 1 , Equation 2 and illustrated in Figure 2 , Figure 3 .", "ner": [["Dropout", "Method"], ["CNNs", "Method"], ["data augmentation", "Method"], ["channel level dropout", "Method"], ["drop - channel", "Method"], ["convolutional layer", "Method"], ["neurons", "Method"], ["vanilla neural networks", "Method"]], "rel": [["data augmentation", "Used-For", "CNNs"], ["Dropout", "Part-Of", "CNNs"], ["drop - channel", "Synonym-Of", "channel level dropout"], ["convolutional layer", "Part-Of", "vanilla neural networks"], ["neurons", "Part-Of", "vanilla neural networks"]], "rel_plus": [["data augmentation:Method", "Used-For", "CNNs:Method"], ["Dropout:Method", "Part-Of", "CNNs:Method"], ["drop - channel:Method", "Synonym-Of", "channel level dropout:Method"], ["convolutional layer:Method", "Part-Of", "vanilla neural networks:Method"], ["neurons:Method", "Part-Of", "vanilla neural networks:Method"]]}
{"doc_id": "102351044", "sentence": "The idea of channel level dropout is first introduced in an object localization model named SpatialDropout [ 3 1 ] .", "ner": [["channel level dropout", "Method"], ["object localization", "Task"], ["SpatialDropout", "Method"]], "rel": [["channel level dropout", "Used-For", "object localization"], ["SpatialDropout", "Used-For", "object localization"]], "rel_plus": [["channel level dropout:Method", "Used-For", "object localization:Task"], ["SpatialDropout:Method", "Used-For", "object localization:Task"]]}
{"doc_id": "102351044", "sentence": "However , experiments in [ 3 1 ] only show that SpatialDropout improve CNN models over a small dataset and the effectiveness and interaction of this channel level dropout technique in effect with other training techniques , e.g. data augmentation and batch normalization [ 1 5 ] , are not properly examined .", "ner": [["SpatialDropout", "Method"], ["CNN", "Method"], ["channel level dropout", "Method"], ["data augmentation", "Method"], ["batch normalization", "Method"]], "rel": [["SpatialDropout", "Part-Of", "CNN"], ["channel level dropout", "Part-Of", "CNN"], ["data augmentation", "Used-For", "CNN"], ["batch normalization", "Part-Of", "CNN"]], "rel_plus": [["SpatialDropout:Method", "Part-Of", "CNN:Method"], ["channel level dropout:Method", "Part-Of", "CNN:Method"], ["data augmentation:Method", "Used-For", "CNN:Method"], ["batch normalization:Method", "Part-Of", "CNN:Method"]]}
{"doc_id": "102351044", "sentence": "To exploit both regularization and ensemble effects to the largest extent , we investigate further in the complicated interaction between drop - channel and other training techniques widely used in state - of - the - art CNNs .", "ner": [["drop - channel", "Method"], ["CNNs", "Method"]], "rel": [["drop - channel", "Part-Of", "CNNs"]], "rel_plus": [["drop - channel:Method", "Part-Of", "CNNs:Method"]]}
{"doc_id": "102351044", "sentence": "Based on our observation and empirical evaluation , we propose general convolutional building blocks for the training of deep Each convolutional layer of state - of - the - art CNN models is typically coupled with a batch normalization layer ( BN ) [ 1 5 ] to normalize inputs batch - wisely , which stabilizes mean and variance of the input channels X received by each output channel yi .", "ner": [["convolutional building blocks", "Method"], ["convolutional layer", "Method"], ["CNN", "Method"], ["batch normalization", "Method"], ["BN", "Method"]], "rel": [["convolutional building blocks", "Part-Of", "convolutional layer"], ["convolutional layer", "Part-Of", "CNN"], ["batch normalization", "Part-Of", "CNN"], ["BN", "Synonym-Of", "batch normalization"]], "rel_plus": [["convolutional building blocks:Method", "Part-Of", "convolutional layer:Method"], ["convolutional layer:Method", "Part-Of", "CNN:Method"], ["batch normalization:Method", "Part-Of", "CNN:Method"], ["BN:Method", "Synonym-Of", "batch normalization:Method"]]}
{"doc_id": "102351044", "sentence": "Take traditional pre - activation convolutional layers [ 8 , 1 3 , 1 2 ] for example , the convolutional transformation follows a BN \u2212 ReLU \u2212 Conv pipeline , which is illustrated in Figure 5a .", "ner": [["pre - activation convolutional layers", "Method"], ["convolutional transformation", "Method"], ["BN \u2212 ReLU \u2212 Conv", "Method"]], "rel": [["BN \u2212 ReLU \u2212 Conv", "Part-Of", "pre - activation convolutional layers"], ["convolutional transformation", "Part-Of", "pre - activation convolutional layers"]], "rel_plus": [["BN \u2212 ReLU \u2212 Conv:Method", "Part-Of", "pre - activation convolutional layers:Method"], ["convolutional transformation:Method", "Part-Of", "pre - activation convolutional layers:Method"]]}
{"doc_id": "102351044", "sentence": "We argue that the drop - operation , including drop - neuron and drop - channel , is not incorporated into the convolutional transformation properly , which is either neglected by discarding this technique totally or used in an erroneous way .", "ner": [["drop - operation", "Method"], ["drop - neuron", "Method"], ["drop - channel", "Method"], ["convolutional transformation", "Method"]], "rel": [["drop - neuron", "SubClass-Of", "drop - operation"], ["drop - channel", "SubClass-Of", "drop - operation"]], "rel_plus": [["drop - neuron:Method", "SubClass-Of", "drop - operation:Method"], ["drop - channel:Method", "SubClass-Of", "drop - operation:Method"]]}
{"doc_id": "102351044", "sentence": "However , drop - operation is traditionally introduced right after convolutional layer and before BN layer , which leads to violent fluctuation of the mean and variance of inputs received by BN layers , especially for drop - channel .", "ner": [["drop - operation", "Method"], ["convolutional layer", "Method"], ["BN", "Method"], ["BN", "Method"], ["drop - channel", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "102351044", "sentence": "We attribute the failure of the standard dropout to the incorrect placement of drop - operations and propose general convolutional building blocks with drop - operations incorporated right before each convolutional layer in Figure 5b , BOTH FOR drop - channel and drop - neuron .", "ner": [["dropout", "Method"], ["drop - operations", "Method"], ["convolutional building blocks", "Method"], ["drop - operations", "Method"], ["convolutional layer", "Method"], ["drop - channel", "Method"], ["drop - neuron", "Method"]], "rel": [["drop - operations", "Part-Of", "convolutional building blocks"], ["drop - channel", "Part-Of", "convolutional building blocks"], ["drop - neuron", "Part-Of", "convolutional building blocks"], ["convolutional layer", "Part-Of", "convolutional building blocks"], ["drop - neuron", "SubClass-Of", "drop - operations"], ["drop - channel", "SubClass-Of", "drop - operations"]], "rel_plus": [["drop - operations:Method", "Part-Of", "convolutional building blocks:Method"], ["drop - channel:Method", "Part-Of", "convolutional building blocks:Method"], ["drop - neuron:Method", "Part-Of", "convolutional building blocks:Method"], ["convolutional layer:Method", "Part-Of", "convolutional building blocks:Method"], ["drop - neuron:Method", "SubClass-Of", "drop - operations:Method"], ["drop - channel:Method", "SubClass-Of", "drop - operations:Method"]]}
{"doc_id": "102351044", "sentence": "Path level ( drop - path ) and layer level ( drop - layer ) dropout are proposed in FractalNet [ 2 0 ] and ResNet with Stochastic Depth [ 1 4 ] respectively .", "ner": [["drop - path", "Method"], ["drop - layer", "Method"], ["dropout", "Method"], ["FractalNet", "Method"], ["ResNet", "Method"], ["Stochastic Depth", "Method"]], "rel": [["drop - layer", "SubClass-Of", "dropout"], ["drop - path", "SubClass-Of", "dropout"], ["dropout", "Part-Of", "FractalNet"], ["dropout", "Part-Of", "ResNet"], ["dropout", "Part-Of", "Stochastic Depth"]], "rel_plus": [["drop - layer:Method", "SubClass-Of", "dropout:Method"], ["drop - path:Method", "SubClass-Of", "dropout:Method"], ["dropout:Method", "Part-Of", "FractalNet:Method"], ["dropout:Method", "Part-Of", "ResNet:Method"], ["dropout:Method", "Part-Of", "Stochastic Depth:Method"]]}
{"doc_id": "102351044", "sentence": "Although these two higher level dropouts are effective in regularizing CNNs , they are highly dependent on CNN architectures .", "ner": [["dropouts", "Method"], ["CNNs", "Method"], ["CNN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "102351044", "sentence": "Specifically , drop - path requires CNN contains multiple paths of Fconv in process 4 and drop - layer demands shortcut connection of process 5 illustrated in Figure 4 .", "ner": [["drop - path", "Method"], ["CNN", "Method"], ["drop - layer", "Method"]], "rel": [["drop - layer", "Part-Of", "CNN"]], "rel_plus": [["drop - layer:Method", "Part-Of", "CNN:Method"]]}
{"doc_id": "102351044", "sentence": "In FractalNet , the original drop - path is applied to the fractal architecture , where paths are heterogeneous and deviate from conventional CNN architectures .", "ner": [["FractalNet", "Method"], ["drop - path", "Method"], ["CNN", "Method"]], "rel": [["drop - path", "Part-Of", "FractalNet"]], "rel_plus": [["drop - path:Method", "Part-Of", "FractalNet:Method"]]}
{"doc_id": "102351044", "sentence": "To make drop - path more applicable , we reincarnate drop - path as general convolutional building blocks , mainly inspired by the bottleneck structure [ 7 ] and group convolution [ 1 9 ] .", "ner": [["drop - path", "Method"], ["drop - path", "Method"], ["convolutional building blocks", "Method"], ["group convolution", "Method"]], "rel": [["drop - path", "Part-Of", "convolutional building blocks"], ["group convolution", "Part-Of", "convolutional building blocks"]], "rel_plus": [["drop - path:Method", "Part-Of", "convolutional building blocks:Method"], ["group convolution:Method", "Part-Of", "convolutional building blocks:Method"]]}
{"doc_id": "102351044", "sentence": "The building block is resting on the parameter efficient bottleneck structure of one 3 \u00d7 3 convolution surrounded by dimensionality reducing and expanding with 1 \u00d7 1 convolution , namely conv 1 \u00d7 1 \u2212 conv 3 \u00d7 3 \u2212 conv 1 \u00d7 1 .", "ner": [["3 \u00d7 3 convolution", "Method"], ["1 \u00d7 1 convolution", "Method"], ["conv 1 \u00d7 1 \u2212 conv 3 \u00d7 3 \u2212 conv 1 \u00d7 1", "Method"]], "rel": [["3 \u00d7 3 convolution", "Part-Of", "conv 1 \u00d7 1 \u2212 conv 3 \u00d7 3 \u2212 conv 1 \u00d7 1"], ["1 \u00d7 1 convolution", "Part-Of", "conv 1 \u00d7 1 \u2212 conv 3 \u00d7 3 \u2212 conv 1 \u00d7 1"]], "rel_plus": [["3 \u00d7 3 convolution:Method", "Part-Of", "conv 1 \u00d7 1 \u2212 conv 3 \u00d7 3 \u2212 conv 1 \u00d7 1:Method"], ["1 \u00d7 1 convolution:Method", "Part-Of", "conv 1 \u00d7 1 \u2212 conv 3 \u00d7 3 \u2212 conv 1 \u00d7 1:Method"]]}
{"doc_id": "102351044", "sentence": "To support drop - path , we introduce group convolution to the inner 3 \u00d7 3 convolutional layer with P groups .", "ner": [["drop - path", "Method"], ["group convolution", "Method"], ["inner 3 \u00d7 3 convolutional layer", "Method"]], "rel": [["group convolution", "Part-Of", "inner 3 \u00d7 3 convolutional layer"]], "rel_plus": [["group convolution:Method", "Part-Of", "inner 3 \u00d7 3 convolutional layer:Method"]]}
{"doc_id": "102351044", "sentence": "Then structurally , the bottleneck building block contains P independent paths of homogeneous transformations , each of which first collapses C channels input into d channels by 1 \u00d7 1 convolution , then transforms by inner 3 \u00d7 3 convolution within each path and finally expand back to C channels altogether by 1 \u00d7 1 convolution .", "ner": [["bottleneck building block", "Method"], ["1 \u00d7 1 convolution", "Method"], ["3 \u00d7 3 convolution", "Method"], ["1 \u00d7 1 convolution", "Method"]], "rel": [["1 \u00d7 1 convolution", "Part-Of", "bottleneck building block"], ["3 \u00d7 3 convolution", "Part-Of", "bottleneck building block"]], "rel_plus": [["1 \u00d7 1 convolution:Method", "Part-Of", "bottleneck building block:Method"], ["3 \u00d7 3 convolution:Method", "Part-Of", "bottleneck building block:Method"]]}
{"doc_id": "102351044", "sentence": "In the implementation , the transformation of this building block is equivalent to the original bottleneck with the introduction of group convolution to the inner 3 \u00d7 3 convolution , as is illustrated in the right panel of Figure 6 .", "ner": [["group convolution", "Method"], ["inner 3 \u00d7 3 convolution", "Method"]], "rel": [["group convolution", "Part-Of", "inner 3 \u00d7 3 convolution"]], "rel_plus": [["group convolution:Method", "Part-Of", "inner 3 \u00d7 3 convolution:Method"]]}
{"doc_id": "102351044", "sentence": "For the layer level dropout , we also examine various architectures , e.g. randomly bypassing each convolutional layer with 1 \u00d7 1 shortcut transformation .", "ner": [["layer level dropout", "Method"], ["convolutional layer", "Method"], ["1 \u00d7 1 shortcut transformation", "Method"]], "rel": [["layer level dropout", "Part-Of", "convolutional layer"], ["1 \u00d7 1 shortcut transformation", "Part-Of", "convolutional layer"]], "rel_plus": [["layer level dropout:Method", "Part-Of", "convolutional layer:Method"], ["1 \u00d7 1 shortcut transformation:Method", "Part-Of", "convolutional layer:Method"]]}
{"doc_id": "102351044", "sentence": "However , we find out that drop - layer performs best when applied to the shortcut connection of identity mapping [ 8 , 3 7 ] , namely Y = Fconv(X)+X , which is already analyzed extensively in ResNet with Stochastic Depth [ 1 4 ] .", "ner": [["drop - layer", "Method"], ["ResNet", "Method"], ["Stochastic Depth", "Method"]], "rel": [["Stochastic Depth", "Part-Of", "ResNet"], ["drop - layer", "Part-Of", "ResNet"]], "rel_plus": [["Stochastic Depth:Method", "Part-Of", "ResNet:Method"], ["drop - layer:Method", "Part-Of", "ResNet:Method"]]}
{"doc_id": "102351044", "sentence": "In the above subsections , we have already introduced and examined four different structural levels of dropout , namely drop - neuron , drop - channel , drop - path and drop - layer .", "ner": [["dropout", "Method"], ["drop - neuron", "Method"], ["drop - channel", "Method"], ["drop - path", "Method"], ["drop - layer", "Method"]], "rel": [["drop - neuron", "SubClass-Of", "dropout"], ["drop - channel", "SubClass-Of", "dropout"], ["drop - path", "SubClass-Of", "dropout"], ["drop - layer", "SubClass-Of", "dropout"]], "rel_plus": [["drop - neuron:Method", "SubClass-Of", "dropout:Method"], ["drop - channel:Method", "SubClass-Of", "dropout:Method"], ["drop - path:Method", "SubClass-Of", "dropout:Method"], ["drop - layer:Method", "SubClass-Of", "dropout:Method"]]}
{"doc_id": "102351044", "sentence": "However , there is one exception for drop - neuron , mainly because in CNNs , the neuron is not the basic unit participates in the convolutional transformation , therefore , the ensemble effect of it is less significant thus its effectiveness diminishes . dropout level neuron channel path layer granularity The applicability is another dimension when applying these drop - operations to CNNs .", "ner": [["drop - neuron", "Method"], ["CNNs", "Method"], ["convolutional transformation", "Method"], ["dropout level neuron", "Method"], ["drop - operations", "Method"], ["CNNs", "Method"]], "rel": [["drop - neuron", "Part-Of", "CNNs"], ["drop - operations", "Part-Of", "CNNs"]], "rel_plus": [["drop - neuron:Method", "Part-Of", "CNNs:Method"], ["drop - operations:Method", "Part-Of", "CNNs:Method"]]}
{"doc_id": "102351044", "sentence": "As we have already discussed along with the introduction of the four levels of dropout techniques , drop - neuron and drop - channel are readily applicable to existing CNNs with minor modification of placing the drop - op right before each convolutional transformation , and we have also proposed general convolutional building blocks in Section 3. 2 . 4 which support drop - path regularization .", "ner": [["dropout", "Method"], ["drop - neuron", "Method"], ["drop - channel", "Method"], ["CNNs", "Method"], ["convolutional building blocks", "Method"], ["drop - path", "Method"]], "rel": [["drop - neuron", "SubClass-Of", "dropout"], ["drop - channel", "SubClass-Of", "dropout"], ["drop - channel", "Part-Of", "CNNs"], ["drop - neuron", "Part-Of", "CNNs"], ["drop - path", "Part-Of", "convolutional building blocks"]], "rel_plus": [["drop - neuron:Method", "SubClass-Of", "dropout:Method"], ["drop - channel:Method", "SubClass-Of", "dropout:Method"], ["drop - channel:Method", "Part-Of", "CNNs:Method"], ["drop - neuron:Method", "Part-Of", "CNNs:Method"], ["drop - path:Method", "Part-Of", "convolutional building blocks:Method"]]}
{"doc_id": "102351044", "sentence": "We finally point out that these general dropout training mechanisms can be easily introduced to existing deep CNNs with the replacement of original convolutional transformation to our proposed convolutional building blocks support corresponding levels of dropout operations and share the only hyper - parameter dropout rate p among all the dropout components in the network for the desired regularization strength .", "ner": [["dropout training mechanisms", "Method"], ["CNNs", "Method"], ["convolutional transformation", "Method"], ["convolutional building blocks", "Method"], ["dropout operations", "Method"], ["dropout", "Method"], ["dropout", "Method"]], "rel": [["dropout training mechanisms", "Part-Of", "CNNs"], ["convolutional building blocks", "Part-Of", "CNNs"], ["dropout operations", "Part-Of", "convolutional building blocks"]], "rel_plus": [["dropout training mechanisms:Method", "Part-Of", "CNNs:Method"], ["convolutional building blocks:Method", "Part-Of", "CNNs:Method"], ["dropout operations:Method", "Part-Of", "convolutional building blocks:Method"]]}
{"doc_id": "102351044", "sentence": "That is , they can co - exist in the same network whenever the CNN architecture allows for it , exploiting the benefits of dropout to the largest extent .", "ner": [["CNN", "Method"], ["dropout", "Method"]], "rel": [["dropout", "Part-Of", "CNN"]], "rel_plus": [["dropout:Method", "Part-Of", "CNN:Method"]]}
{"doc_id": "102351044", "sentence": "It also worth noting that applying these drop - operations to CNNs introduces no additional model parameter , adds negligible computational cost for drop - neuron and drop - channel , and even reduces training time considerably for drop - path and drop - layer [ 1 4 ] .", "ner": [["drop - operations", "Method"], ["CNNs", "Method"], ["drop - neuron", "Method"], ["drop - channel", "Method"], ["drop - path", "Method"], ["drop - layer", "Method"]], "rel": [["drop - operations", "Part-Of", "CNNs"]], "rel_plus": [["drop - operations:Method", "Part-Of", "CNNs:Method"]]}
{"doc_id": "102351044", "sentence": "We evaluate the four levels of dropouts on representative stat - of - the - art CNNs on widely compared benchmark datasets , including CIFAR , SVHN and ImageNet .", "ner": [["CNNs", "Method"], ["CIFAR", "Dataset"], ["SVHN", "Dataset"], ["ImageNet", "Dataset"]], "rel": [["CNNs", "Evaluated-With", "CIFAR"], ["CNNs", "Evaluated-With", "SVHN"], ["CNNs", "Evaluated-With", "ImageNet"]], "rel_plus": [["CNNs:Method", "Evaluated-With", "CIFAR:Dataset"], ["CNNs:Method", "Evaluated-With", "SVHN:Dataset"], ["CNNs:Method", "Evaluated-With", "ImageNet:Dataset"]]}
{"doc_id": "102351044", "sentence": "Secondly , we evaluate the effectiveness of convolutional building blocks with proposed drop - operations of drop - neuron and dropchannel , which is the foundation for the new state - of - theart results on CIFAR and SVHN datasets .", "ner": [["convolutional building blocks", "Method"], ["drop - operations", "Method"], ["drop - neuron", "Method"], ["dropchannel", "Method"], ["CIFAR", "Dataset"], ["SVHN", "Dataset"]], "rel": [["drop - neuron", "Part-Of", "convolutional building blocks"], ["dropchannel", "Part-Of", "convolutional building blocks"], ["drop - operations", "Part-Of", "convolutional building blocks"], ["drop - neuron", "SubClass-Of", "drop - operations"], ["dropchannel", "SubClass-Of", "drop - operations"], ["convolutional building blocks", "Evaluated-With", "CIFAR"], ["convolutional building blocks", "Evaluated-With", "SVHN"]], "rel_plus": [["drop - neuron:Method", "Part-Of", "convolutional building blocks:Method"], ["dropchannel:Method", "Part-Of", "convolutional building blocks:Method"], ["drop - operations:Method", "Part-Of", "convolutional building blocks:Method"], ["drop - neuron:Method", "SubClass-Of", "drop - operations:Method"], ["dropchannel:Method", "SubClass-Of", "drop - operations:Method"], ["convolutional building blocks:Method", "Evaluated-With", "CIFAR:Dataset"], ["convolutional building blocks:Method", "Evaluated-With", "SVHN:Dataset"]]}
{"doc_id": "102351044", "sentence": "Then we compare drop - neuron , drop - channel , drop - path and drop - layer together with their combinations on representative CNN architectures , based on which we propose enhancement for existing best models on CIFAR and SVHN datasets and achieve a significant better results .   The performance of dropout training mechanisms are evaluated on benchmark image classification datasets CIFAR [ 1 8 ] , SVHN [ 2 4 ] and ImageNet [ 4 ] .", "ner": [["drop - neuron", "Method"], ["drop - channel", "Method"], ["drop - path", "Method"], ["drop - layer", "Method"], ["CNN", "Method"], ["CIFAR", "Dataset"], ["SVHN", "Dataset"], ["dropout training mechanisms", "Method"], ["image classification", "Task"], ["CIFAR", "Dataset"], ["SVHN", "Dataset"], ["ImageNet", "Dataset"]], "rel": [["drop - neuron", "Part-Of", "CNN"], ["drop - channel", "Part-Of", "CNN"], ["drop - path", "Part-Of", "CNN"], ["drop - layer", "Part-Of", "CNN"], ["CNN", "Evaluated-With", "CIFAR"], ["CNN", "Evaluated-With", "SVHN"], ["dropout training mechanisms", "Used-For", "image classification"], ["CIFAR", "Benchmark-For", "image classification"], ["SVHN", "Benchmark-For", "image classification"], ["ImageNet", "Benchmark-For", "image classification"], ["dropout training mechanisms", "Evaluated-With", "CIFAR"], ["dropout training mechanisms", "Evaluated-With", "SVHN"], ["dropout training mechanisms", "Evaluated-With", "ImageNet"]], "rel_plus": [["drop - neuron:Method", "Part-Of", "CNN:Method"], ["drop - channel:Method", "Part-Of", "CNN:Method"], ["drop - path:Method", "Part-Of", "CNN:Method"], ["drop - layer:Method", "Part-Of", "CNN:Method"], ["CNN:Method", "Evaluated-With", "CIFAR:Dataset"], ["CNN:Method", "Evaluated-With", "SVHN:Dataset"], ["dropout training mechanisms:Method", "Used-For", "image classification:Task"], ["CIFAR:Dataset", "Benchmark-For", "image classification:Task"], ["SVHN:Dataset", "Benchmark-For", "image classification:Task"], ["ImageNet:Dataset", "Benchmark-For", "image classification:Task"], ["dropout training mechanisms:Method", "Evaluated-With", "CIFAR:Dataset"], ["dropout training mechanisms:Method", "Evaluated-With", "SVHN:Dataset"], ["dropout training mechanisms:Method", "Evaluated-With", "ImageNet:Dataset"]]}
{"doc_id": "102351044", "sentence": "CIFAR - 1 0 ( C 1 0 ) consists of images drawn from 1 0 classes and CIFAR - 1 0 0 ( C 1 0 0 ) from 1 0 0 classes .", "ner": [["CIFAR - 1 0", "Dataset"], ["C 1 0", "Dataset"], ["CIFAR - 1 0 0", "Dataset"], ["C 1 0 0", "Dataset"]], "rel": [["C 1 0", "Synonym-Of", "CIFAR - 1 0"], ["C 1 0 0", "Synonym-Of", "CIFAR - 1 0 0"]], "rel_plus": [["C 1 0:Dataset", "Synonym-Of", "CIFAR - 1 0:Dataset"], ["C 1 0 0:Dataset", "Synonym-Of", "CIFAR - 1 0 0:Dataset"]]}
{"doc_id": "102351044", "sentence": "Following the standard data augmentation scheme [ 7 , 1 4 , 1 3 ] , each image is first zeropadded with 4 pixels on each side , then randomly cropped to produce 3 2 \u00d7 3 2 images again , followed by a random horizontal flip .", "ner": [["data augmentation", "Method"], ["random horizontal flip", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "102351044", "sentence": "We denote the datasets with data augmentation by \" + \" behind the dataset names ( e.g. , C 1 0 + ) .", "ner": [["data augmentation", "Method"], ["C 1 0 +", "Dataset"]], "rel": [["data augmentation", "Used-For", "C 1 0 +"]], "rel_plus": [["data augmentation:Method", "Used-For", "C 1 0 +:Dataset"]]}
{"doc_id": "102351044", "sentence": "As summarized in Table 1 , drop - neuron and drop - channel are applicable to general CNNs while the applicability of drop - path and drop - layer are dependent on the detailed CNN architectures .", "ner": [["drop - neuron", "Method"], ["drop - channel", "Method"], ["CNNs", "Method"], ["drop - path", "Method"], ["drop - layer", "Method"], ["CNN", "Method"]], "rel": [["drop - neuron", "Part-Of", "CNNs"], ["drop - channel", "Part-Of", "CNNs"], ["drop - neuron", "Compare-With", "drop - path"], ["drop - channel", "Compare-With", "drop - path"], ["drop - neuron", "Compare-With", "drop - layer"], ["drop - channel", "Compare-With", "drop - layer"], ["drop - path", "Part-Of", "CNN"], ["drop - layer", "Part-Of", "CNN"]], "rel_plus": [["drop - neuron:Method", "Part-Of", "CNNs:Method"], ["drop - channel:Method", "Part-Of", "CNNs:Method"], ["drop - neuron:Method", "Compare-With", "drop - path:Method"], ["drop - channel:Method", "Compare-With", "drop - path:Method"], ["drop - neuron:Method", "Compare-With", "drop - layer:Method"], ["drop - channel:Method", "Compare-With", "drop - layer:Method"], ["drop - path:Method", "Part-Of", "CNN:Method"], ["drop - layer:Method", "Part-Of", "CNN:Method"]]}
{"doc_id": "102351044", "sentence": "To evaluate the effectiveness of the four levels of dropout training mechanisms , we therefore adopt CNN architectures with representative convolutional transformation .", "ner": [["dropout training mechanisms", "Method"], ["CNN", "Method"], ["convolutional transformation", "Method"]], "rel": [["convolutional transformation", "Part-Of", "CNN"], ["dropout training mechanisms", "Part-Of", "CNN"]], "rel_plus": [["convolutional transformation:Method", "Part-Of", "CNN:Method"], ["dropout training mechanisms:Method", "Part-Of", "CNN:Method"]]}
{"doc_id": "102351044", "sentence": "Specifically , we first evaluate and compare drop - neuron and drop - channel in our proposed convolutional building blocks illustrated in Figure 5b on VGG [ 2 6 ] , whose convolutional layer is a plain 3 \u00d7 3 conv following process 2 and 4 of Figure 4 .", "ner": [["drop - neuron", "Method"], ["drop - channel", "Method"], ["convolutional building blocks", "Method"], ["VGG", "Method"], ["convolutional layer", "Method"], ["3 \u00d7 3 conv", "Method"]], "rel": [["drop - neuron", "Compare-With", "drop - channel"], ["drop - neuron", "Part-Of", "convolutional building blocks"], ["drop - channel", "Part-Of", "convolutional building blocks"], ["convolutional building blocks", "Part-Of", "VGG"], ["convolutional layer", "Part-Of", "VGG"], ["3 \u00d7 3 conv", "Part-Of", "VGG"], ["3 \u00d7 3 conv", "SubClass-Of", "convolutional layer"]], "rel_plus": [["drop - neuron:Method", "Compare-With", "drop - channel:Method"], ["drop - neuron:Method", "Part-Of", "convolutional building blocks:Method"], ["drop - channel:Method", "Part-Of", "convolutional building blocks:Method"], ["convolutional building blocks:Method", "Part-Of", "VGG:Method"], ["convolutional layer:Method", "Part-Of", "VGG:Method"], ["3 \u00d7 3 conv:Method", "Part-Of", "VGG:Method"], ["3 \u00d7 3 conv:Method", "SubClass-Of", "convolutional layer:Method"]]}
{"doc_id": "102351044", "sentence": "Then for drop - path of the new convolutional building block proposed in Section 3. 2 . 4 and drop - layer , we test their effectiveness in comparison with drop - channel and drop - neuron on ResNeXt [ 3 6 ] with multiple paths and residual connection , Wide Residual Networks [ 3 7 ] ( WRN ) with wider convolutional layer and residual connection and DenseNet [ 1 3 ] with shortcut connection .", "ner": [["drop - path", "Method"], ["convolutional building block", "Method"], ["drop - layer", "Method"], ["drop - channel", "Method"], ["drop - neuron", "Method"], ["ResNeXt", "Method"], ["residual connection", "Method"], ["Wide Residual Networks", "Method"], ["WRN", "Method"], ["convolutional layer", "Method"], ["residual connection", "Method"], ["DenseNet", "Method"], ["shortcut connection", "Method"]], "rel": [["drop - path", "Part-Of", "convolutional building block"], ["drop - layer", "Compare-With", "drop - channel"], ["drop - path", "Compare-With", "drop - channel"], ["drop - layer", "Compare-With", "drop - neuron"], ["drop - path", "Compare-With", "drop - neuron"], ["drop - neuron", "Part-Of", "ResNeXt"], ["drop - channel", "Part-Of", "ResNeXt"], ["residual connection", "Part-Of", "ResNeXt"], ["Wide Residual Networks", "Part-Of", "ResNeXt"], ["DenseNet", "Part-Of", "ResNeXt"], ["WRN", "Synonym-Of", "Wide Residual Networks"], ["convolutional layer", "Part-Of", "Wide Residual Networks"], ["residual connection", "Part-Of", "Wide Residual Networks"], ["shortcut connection", "Part-Of", "DenseNet"]], "rel_plus": [["drop - path:Method", "Part-Of", "convolutional building block:Method"], ["drop - layer:Method", "Compare-With", "drop - channel:Method"], ["drop - path:Method", "Compare-With", "drop - channel:Method"], ["drop - layer:Method", "Compare-With", "drop - neuron:Method"], ["drop - path:Method", "Compare-With", "drop - neuron:Method"], ["drop - neuron:Method", "Part-Of", "ResNeXt:Method"], ["drop - channel:Method", "Part-Of", "ResNeXt:Method"], ["residual connection:Method", "Part-Of", "ResNeXt:Method"], ["Wide Residual Networks:Method", "Part-Of", "ResNeXt:Method"], ["DenseNet:Method", "Part-Of", "ResNeXt:Method"], ["WRN:Method", "Synonym-Of", "Wide Residual Networks:Method"], ["convolutional layer:Method", "Part-Of", "Wide Residual Networks:Method"], ["residual connection:Method", "Part-Of", "Wide Residual Networks:Method"], ["shortcut connection:Method", "Part-Of", "DenseNet:Method"]]}
{"doc_id": "102351044", "sentence": "We denote these models adopted with WRN - depth - k , ResNeXtdepth - P - d and DenseNet - L - K , with k , P , d , L , K being widening factor [ 3 7 ] , the number of paths , the channel width of each path [ 3 6 ] , the number of layers ( depth ) and K the growth rate [ 1 3 ] respectively following the convention .", "ner": [["WRN - depth - k", "Method"], ["ResNeXtdepth - P - d", "Method"], ["DenseNet - L - K", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "102351044", "sentence": "The building blocks adopted for WRN , ResNeXt and DenseNet are basic block ( Block ) with two consecutive 3 \u00d7 3 conv , bottleneck block ( B - Block ) proposed in 3. 2 . 4 and DenseNet bottleneck block ( D - Block ) [ 1 3 ] with dimensionality reduction of 1 \u00d7 1 conv and a following transformation of 3 \u00d7 3 conv .", "ner": [["WRN", "Method"], ["ResNeXt", "Method"], ["DenseNet", "Method"], ["basic block", "Method"], ["Block", "Method"], ["3 \u00d7 3 conv", "Method"], ["bottleneck block", "Method"], ["B - Block", "Method"], ["DenseNet bottleneck block", "Method"], ["D - Block", "Method"], ["dimensionality reduction of 1 \u00d7 1 conv", "Method"], ["transformation of 3 \u00d7 3 conv", "Method"]], "rel": [["basic block", "Part-Of", "WRN"], ["DenseNet bottleneck block", "Part-Of", "WRN"], ["basic block", "Part-Of", "ResNeXt"], ["DenseNet bottleneck block", "Part-Of", "ResNeXt"], ["basic block", "Part-Of", "DenseNet"], ["DenseNet bottleneck block", "Part-Of", "DenseNet"], ["Block", "Synonym-Of", "basic block"], ["3 \u00d7 3 conv", "Part-Of", "basic block"], ["bottleneck block", "Part-Of", "basic block"], ["B - Block", "Synonym-Of", "bottleneck block"], ["D - Block", "Synonym-Of", "DenseNet bottleneck block"], ["transformation of 3 \u00d7 3 conv", "Part-Of", "DenseNet bottleneck block"], ["dimensionality reduction of 1 \u00d7 1 conv", "Part-Of", "DenseNet bottleneck block"]], "rel_plus": [["basic block:Method", "Part-Of", "WRN:Method"], ["DenseNet bottleneck block:Method", "Part-Of", "WRN:Method"], ["basic block:Method", "Part-Of", "ResNeXt:Method"], ["DenseNet bottleneck block:Method", "Part-Of", "ResNeXt:Method"], ["basic block:Method", "Part-Of", "DenseNet:Method"], ["DenseNet bottleneck block:Method", "Part-Of", "DenseNet:Method"], ["Block:Method", "Synonym-Of", "basic block:Method"], ["3 \u00d7 3 conv:Method", "Part-Of", "basic block:Method"], ["bottleneck block:Method", "Part-Of", "basic block:Method"], ["B - Block:Method", "Synonym-Of", "bottleneck block:Method"], ["D - Block:Method", "Synonym-Of", "DenseNet bottleneck block:Method"], ["transformation of 3 \u00d7 3 conv:Method", "Part-Of", "DenseNet bottleneck block:Method"], ["dimensionality reduction of 1 \u00d7 1 conv:Method", "Part-Of", "DenseNet bottleneck block:Method"]]}
{"doc_id": "102351044", "sentence": "The detailed architectures and configurations of representative convolutional neural networks for CIFAR - 1 0 / 1 0 0 , SVHN and ImageNet datasets are provided in Table 2 and   Table 3 respectively .", "ner": [["convolutional neural networks", "Method"], ["CIFAR - 1 0 / 1 0 0", "Dataset"], ["SVHN", "Dataset"], ["ImageNet", "Dataset"]], "rel": [["convolutional neural networks", "Evaluated-With", "CIFAR - 1 0 / 1 0 0"], ["convolutional neural networks", "Evaluated-With", "SVHN"], ["convolutional neural networks", "Evaluated-With", "ImageNet"]], "rel_plus": [["convolutional neural networks:Method", "Evaluated-With", "CIFAR - 1 0 / 1 0 0:Dataset"], ["convolutional neural networks:Method", "Evaluated-With", "SVHN:Dataset"], ["convolutional neural networks:Method", "Evaluated-With", "ImageNet:Dataset"]]}
{"doc_id": "102351044", "sentence": "For CNNs with drop - neuron and drop - channel , each convolutional layer is replaced with our proposed convolutional building block in Section 3. 2 . 4 , where the drop - operations of drop - neuron and drop - channel are incorporated into the transformation right before the convolutional operation .", "ner": [["CNNs", "Method"], ["drop - neuron", "Method"], ["drop - channel", "Method"], ["convolutional layer", "Method"], ["convolutional building block", "Method"], ["drop - operations", "Method"], ["drop - neuron", "Method"], ["drop - channel", "Method"], ["convolutional operation", "Method"]], "rel": [["drop - neuron", "Part-Of", "CNNs"], ["drop - channel", "Part-Of", "CNNs"], ["convolutional building block", "Part-Of", "CNNs"], ["drop - operations", "Part-Of", "convolutional building block"], ["convolutional operation", "Part-Of", "convolutional building block"], ["drop - neuron", "Part-Of", "convolutional building block"], ["drop - channel", "Part-Of", "convolutional building block"], ["drop - neuron", "SubClass-Of", "drop - operations"], ["drop - channel", "SubClass-Of", "drop - operations"]], "rel_plus": [["drop - neuron:Method", "Part-Of", "CNNs:Method"], ["drop - channel:Method", "Part-Of", "CNNs:Method"], ["convolutional building block:Method", "Part-Of", "CNNs:Method"], ["drop - operations:Method", "Part-Of", "convolutional building block:Method"], ["convolutional operation:Method", "Part-Of", "convolutional building block:Method"], ["drop - neuron:Method", "Part-Of", "convolutional building block:Method"], ["drop - channel:Method", "Part-Of", "convolutional building block:Method"], ["drop - neuron:Method", "SubClass-Of", "drop - operations:Method"], ["drop - channel:Method", "SubClass-Of", "drop - operations:Method"]]}
{"doc_id": "102351044", "sentence": "While for CNNs with drop - path , we again replace each convolutional layer with the proposed general convolutional building block in Section 3. 2 . 4 supporting drop - path with bottleneck and group convolution .", "ner": [["CNNs", "Method"], ["drop - path", "Method"], ["convolutional layer", "Method"], ["general convolutional building block", "Method"], ["drop - path", "Method"], ["bottleneck", "Method"], ["group convolution", "Method"]], "rel": [["drop - path", "Part-Of", "CNNs"], ["general convolutional building block", "Part-Of", "CNNs"], ["drop - path", "Part-Of", "general convolutional building block"], ["group convolution", "Part-Of", "general convolutional building block"], ["bottleneck", "Part-Of", "drop - path"]], "rel_plus": [["drop - path:Method", "Part-Of", "CNNs:Method"], ["general convolutional building block:Method", "Part-Of", "CNNs:Method"], ["drop - path:Method", "Part-Of", "general convolutional building block:Method"], ["group convolution:Method", "Part-Of", "general convolutional building block:Method"], ["bottleneck:Method", "Part-Of", "drop - path:Method"]]}
{"doc_id": "102351044", "sentence": "We train the networks with SGD + Nesterov momentum and cross - entropy loss , and we adopt weight initialization introduced by [ 6 ] .", "ner": [["SGD + Nesterov momentum", "Method"], ["cross - entropy loss", "Method"], ["weight initialization", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "102351044", "sentence": "For CIFAR datasets , we train 3 0 0 epochs on VGG - 1 1 , ResNeXt - 2 9 - P 6 4 - d 4 , and DenseNet - L 1 9 0 - K 4 0 , 2 0 0 epochs on WRN - 4 0 - 4 .", "ner": [["CIFAR", "Dataset"], ["VGG - 1 1", "Method"], ["ResNeXt - 2 9 - P 6 4 - d 4", "Method"], ["DenseNet - L 1 9 0 - K 4 0", "Method"], ["WRN - 4 0 - 4", "Method"]], "rel": [["VGG - 1 1", "Trained-With", "CIFAR"], ["ResNeXt - 2 9 - P 6 4 - d 4", "Trained-With", "CIFAR"], ["DenseNet - L 1 9 0 - K 4 0", "Trained-With", "CIFAR"], ["WRN - 4 0 - 4", "Trained-With", "CIFAR"]], "rel_plus": [["VGG - 1 1:Method", "Trained-With", "CIFAR:Dataset"], ["ResNeXt - 2 9 - P 6 4 - d 4:Method", "Trained-With", "CIFAR:Dataset"], ["DenseNet - L 1 9 0 - K 4 0:Method", "Trained-With", "CIFAR:Dataset"], ["WRN - 4 0 - 4:Method", "Trained-With", "CIFAR:Dataset"]]}
{"doc_id": "102351044", "sentence": "For SVHN , we train 1 6 0 epochs on WRN - 1 6 - 8 .", "ner": [["SVHN", "Dataset"], ["WRN - 1 6 - 8", "Method"]], "rel": [["WRN - 1 6 - 8", "Trained-With", "SVHN"]], "rel_plus": [["WRN - 1 6 - 8:Method", "Trained-With", "SVHN:Dataset"]]}
{"doc_id": "102351044", "sentence": "The initial learning rate is set to 0. 1 , weight decay 0.0 0 0 1 , dampening 0 , momentum 0. 9 and mini - batch size 1 2 8 for CIFAR and SVHN datasets .", "ner": [["weight decay", "Method"], ["dampening", "Method"], ["momentum", "Method"], ["CIFAR", "Dataset"], ["SVHN", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "102351044", "sentence": "For ImageNet , we train 9 0 epochs on VGG - 1 6 , WRN - 5 0 - 2 , DenseNet - L 1 6 9 - K 3 2 with a mini - batch size of 2 5 6 .", "ner": [["ImageNet", "Dataset"], ["VGG - 1 6", "Method"], ["WRN - 5 0 - 2", "Method"], ["DenseNet - L 1 6 9 - K 3 2", "Method"]], "rel": [["VGG - 1 6", "Trained-With", "ImageNet"], ["WRN - 5 0 - 2", "Trained-With", "ImageNet"], ["DenseNet - L 1 6 9 - K 3 2", "Trained-With", "ImageNet"]], "rel_plus": [["VGG - 1 6:Method", "Trained-With", "ImageNet:Dataset"], ["WRN - 5 0 - 2:Method", "Trained-With", "ImageNet:Dataset"], ["DenseNet - L 1 6 9 - K 3 2:Method", "Trained-With", "ImageNet:Dataset"]]}
{"doc_id": "102351044", "sentence": "Other training details follow their original settings .   In this subsection , we validate the effectiveness of our proposed convolutional building blocks supporting drop - operation of drop - neuron and drop - channel .", "ner": [["convolutional building blocks", "Method"], ["drop - operation", "Method"], ["drop - neuron", "Method"], ["drop - channel", "Method"]], "rel": [["drop - operation", "Part-Of", "convolutional building blocks"], ["drop - channel", "Part-Of", "convolutional building blocks"], ["drop - neuron", "Part-Of", "convolutional building blocks"], ["drop - neuron", "SubClass-Of", "drop - operation"], ["drop - channel", "SubClass-Of", "drop - operation"]], "rel_plus": [["drop - operation:Method", "Part-Of", "convolutional building blocks:Method"], ["drop - channel:Method", "Part-Of", "convolutional building blocks:Method"], ["drop - neuron:Method", "Part-Of", "convolutional building blocks:Method"], ["drop - neuron:Method", "SubClass-Of", "drop - operation:Method"], ["drop - channel:Method", "SubClass-Of", "drop - operation:Method"]]}
{"doc_id": "102351044", "sentence": "The CNN architectures adopted include VGG - 1 1 , WRN - 4 0 - 4 and DenseNet - L 1 0 0 - K 1 2 , whose architectures follows the notation in Section 4. 2 and The results are summarized in Table 4 , where we report the results of networks trained without dropout ( original ) , with original drop - neuron and drop - channel ( DN and DC ) , and with our proposed drop - neuron and drop - channel ( DN+ and DC+ ) respectively .", "ner": [["CNN", "Method"], ["VGG - 1 1", "Method"], ["WRN - 4 0 - 4", "Method"], ["DenseNet - L 1 0 0 - K 1 2", "Method"], ["dropout", "Method"], ["drop - neuron", "Method"], ["drop - channel", "Method"], ["DN", "Method"], ["DC", "Method"], ["drop - neuron", "Method"], ["drop - channel", "Method"], ["DN+", "Method"], ["DC+", "Method"]], "rel": [["VGG - 1 1", "SubClass-Of", "CNN"], ["WRN - 4 0 - 4", "SubClass-Of", "CNN"], ["DenseNet - L 1 0 0 - K 1 2", "SubClass-Of", "CNN"], ["DN", "Synonym-Of", "drop - neuron"], ["DC", "Synonym-Of", "drop - channel"], ["DN+", "Synonym-Of", "drop - neuron"], ["DC+", "Synonym-Of", "drop - channel"]], "rel_plus": [["VGG - 1 1:Method", "SubClass-Of", "CNN:Method"], ["WRN - 4 0 - 4:Method", "SubClass-Of", "CNN:Method"], ["DenseNet - L 1 0 0 - K 1 2:Method", "SubClass-Of", "CNN:Method"], ["DN:Method", "Synonym-Of", "drop - neuron:Method"], ["DC:Method", "Synonym-Of", "drop - channel:Method"], ["DN+:Method", "Synonym-Of", "drop - neuron:Method"], ["DC+:Method", "Synonym-Of", "drop - channel:Method"]]}
{"doc_id": "102351044", "sentence": "Another important finding is that comparing to their original results , networks trained with drop - neuron and drop - channel achieve significantly better performance , which demonstrates that dropout technique is effective in regularizing CNNs if applied properly .", "ner": [["drop - neuron", "Method"], ["drop - channel", "Method"], ["dropout", "Method"], ["CNNs", "Method"]], "rel": [["drop - channel", "Part-Of", "CNNs"], ["drop - neuron", "Part-Of", "CNNs"], ["dropout", "Part-Of", "CNNs"]], "rel_plus": [["drop - channel:Method", "Part-Of", "CNNs:Method"], ["drop - neuron:Method", "Part-Of", "CNNs:Method"], ["dropout:Method", "Part-Of", "CNNs:Method"]]}
{"doc_id": "102351044", "sentence": "For instance , the introduction of drop - channel alone achieves absolute reduction of error rate 0. 4 2 % , 0. 6 6 % and 0. 2 5 % improvement of accuracy on VGG - 1 1 , WRN - 4 0 - 4 and DenseNet - L 1 0 0 - K 1 2 respectively .", "ner": [["drop - channel", "Method"], ["VGG - 1 1", "Method"], ["WRN - 4 0 - 4", "Method"], ["DenseNet - L 1 0 0 - K 1 2", "Method"]], "rel": [["drop - channel", "Part-Of", "VGG - 1 1"], ["drop - channel", "Part-Of", "WRN - 4 0 - 4"], ["drop - channel", "Part-Of", "DenseNet - L 1 0 0 - K 1 2"]], "rel_plus": [["drop - channel:Method", "Part-Of", "VGG - 1 1:Method"], ["drop - channel:Method", "Part-Of", "WRN - 4 0 - 4:Method"], ["drop - channel:Method", "Part-Of", "DenseNet - L 1 0 0 - K 1 2:Method"]]}
{"doc_id": "102351044", "sentence": "In this subsection , we test out drop - operations of dropneuron and drop - channel for the training of CNNs .", "ner": [["drop - operations", "Method"], ["dropneuron", "Method"], ["drop - channel", "Method"], ["CNNs", "Method"]], "rel": [["dropneuron", "SubClass-Of", "drop - operations"], ["drop - channel", "SubClass-Of", "drop - operations"], ["drop - operations", "Part-Of", "CNNs"], ["dropneuron", "Part-Of", "CNNs"], ["drop - channel", "Part-Of", "CNNs"]], "rel_plus": [["dropneuron:Method", "SubClass-Of", "drop - operations:Method"], ["drop - channel:Method", "SubClass-Of", "drop - operations:Method"], ["drop - operations:Method", "Part-Of", "CNNs:Method"], ["dropneuron:Method", "Part-Of", "CNNs:Method"], ["drop - channel:Method", "Part-Of", "CNNs:Method"]]}
{"doc_id": "102351044", "sentence": "The results are reported on VGG - 1 1 ( see Table 2 ) trained with CIFAR - 1 0 datasets , whose error rates and learning curves are illustrated in Figure 7 and Figure 8 .", "ner": [["VGG - 1 1", "Method"], ["CIFAR - 1 0", "Dataset"]], "rel": [["VGG - 1 1", "Trained-With", "CIFAR - 1 0"]], "rel_plus": [["VGG - 1 1:Method", "Trained-With", "CIFAR - 1 0:Dataset"]]}
{"doc_id": "102351044", "sentence": "We denote VGG networks trained without dropout , with drop - neuron and drop - channel as VGG - 1 1 , drop - neuron and drop - channel respectively , and the network trained with standard data augmentation is marked with a suffix + .", "ner": [["VGG", "Method"], ["dropout", "Method"], ["drop - neuron", "Method"], ["drop - channel", "Method"], ["VGG - 1 1", "Method"], ["drop - neuron", "Method"], ["drop - channel", "Method"], ["data augmentation", "Method"], ["suffix +", "Method"]], "rel": [["data augmentation", "Used-For", "suffix +"]], "rel_plus": [["data augmentation:Method", "Used-For", "suffix +:Method"]]}
{"doc_id": "102351044", "sentence": "The results demonstrate that firstly , data augmentation is of vital importance for CNNs , without whose regularization effect , the performance deteriorates around 3% .", "ner": [["data augmentation", "Method"], ["CNNs", "Method"]], "rel": [["data augmentation", "Used-For", "CNNs"]], "rel_plus": [["data augmentation:Method", "Used-For", "CNNs:Method"]]}
{"doc_id": "102351044", "sentence": "Secondly , drop - neuron and drop - channel can improve over the performance both with and without data augmentation .", "ner": [["drop - neuron", "Method"], ["drop - channel", "Method"], ["data augmentation", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "102351044", "sentence": "Combined with data augmentation , drop - channel achieves the best result of 4. 6 2 % from 5. 0 9 % , 0. 4 7 % improvement , meanwhile without data augmentation , drop - neuron achieves the best accuracy of 6. 8 5 % from 8. 2 4 % , 1. 3 9 % improvement .", "ner": [["data augmentation", "Method"], ["drop - channel", "Method"], ["data augmentation", "Method"], ["drop - neuron", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "102351044", "sentence": "The learning curves corroborate the aforementioned findings and further demonstrate that drop - operations of drop - neuron and dropchannel are effective in regularizing CNNs .", "ner": [["drop - neuron", "Method"], ["dropchannel", "Method"], ["CNNs", "Method"]], "rel": [["dropchannel", "Part-Of", "CNNs"], ["drop - neuron", "Part-Of", "CNNs"]], "rel_plus": [["dropchannel:Method", "Part-Of", "CNNs:Method"], ["drop - neuron:Method", "Part-Of", "CNNs:Method"]]}
{"doc_id": "102351044", "sentence": "Furthermore , for state - of - the - art CNNs which are typically trained with extensive data augmentation , drop - channel is a powerful and general technique to improve the performance by a significant margin .", "ner": [["CNNs", "Method"], ["data augmentation", "Method"], ["drop - channel", "Method"]], "rel": [["drop - channel", "Part-Of", "CNNs"], ["data augmentation", "Used-For", "CNNs"]], "rel_plus": [["drop - channel:Method", "Part-Of", "CNNs:Method"], ["data augmentation:Method", "Used-For", "CNNs:Method"]]}
{"doc_id": "102351044", "sentence": "In this subsection , we introduce drop - path to CNNs , specifically ResNeXt - 2 9 - 6 4 - 4 from Table 2 , and study the effect of this path level dropout in the training process alone and in combination with finer - grained drop - operations .", "ner": [["drop - path", "Method"], ["CNNs", "Method"], ["ResNeXt - 2 9 - 6 4 - 4", "Method"], ["path level dropout", "Method"], ["drop - operations", "Method"]], "rel": [["drop - path", "Part-Of", "CNNs"], ["ResNeXt - 2 9 - 6 4 - 4", "SubClass-Of", "CNNs"]], "rel_plus": [["drop - path:Method", "Part-Of", "CNNs:Method"], ["ResNeXt - 2 9 - 6 4 - 4:Method", "SubClass-Of", "CNNs:Method"]]}
{"doc_id": "102351044", "sentence": "The results of ResNeXt trained with drop - neuron , dropchannel , drop - path and the combination of drop - path and drop - channel ( drop - path - channel , results are reported with the same dropout rate from 0.0 5 to 0. 1 5 for both drop - path and drop - channel ) are summarized in Figure 9 .", "ner": [["ResNeXt", "Method"], ["drop - neuron", "Method"], ["dropchannel", "Method"], ["drop - path", "Method"], ["drop - path", "Method"], ["drop - channel", "Method"], ["drop - path - channel", "Method"], ["dropout", "Method"], ["drop - path", "Method"], ["drop - channel", "Method"]], "rel": [["drop - neuron", "Part-Of", "ResNeXt"], ["dropchannel", "Part-Of", "ResNeXt"], ["drop - path", "Part-Of", "ResNeXt"], ["drop - path - channel", "Part-Of", "ResNeXt"], ["drop - channel", "Part-Of", "drop - path - channel"], ["drop - path", "Part-Of", "drop - path - channel"]], "rel_plus": [["drop - neuron:Method", "Part-Of", "ResNeXt:Method"], ["dropchannel:Method", "Part-Of", "ResNeXt:Method"], ["drop - path:Method", "Part-Of", "ResNeXt:Method"], ["drop - path - channel:Method", "Part-Of", "ResNeXt:Method"], ["drop - channel:Method", "Part-Of", "drop - path - channel:Method"], ["drop - path:Method", "Part-Of", "drop - path - channel:Method"]]}
{"doc_id": "102351044", "sentence": "We can notice that drop - neuron improves the performance mildly from 5. 1 0 % to 4. 8 7 % , and drop - channel outperforms dropneuron with better error rate 4. 7 2 % .", "ner": [["drop - neuron", "Method"], ["drop - channel", "Method"], ["dropneuron", "Method"]], "rel": [["drop - channel", "Compare-With", "dropneuron"]], "rel_plus": [["drop - channel:Method", "Compare-With", "dropneuron:Method"]]}
{"doc_id": "102351044", "sentence": "With our proposed drop - path building block , ResNeXt achieves even better result of 4. 6 2 % , which is 0. 4 8 % relative improvement over the network trained without any dropout .", "ner": [["drop - path building block", "Method"], ["ResNeXt", "Method"], ["dropout", "Method"]], "rel": [["drop - path building block", "Part-Of", "ResNeXt"]], "rel_plus": [["drop - path building block:Method", "Part-Of", "ResNeXt:Method"]]}
{"doc_id": "102351044", "sentence": "More interestingly , as we have discussed in Section 3. 2 . 5 , path level dropout can be applied in combination with finer - To understand the impact of these dropouts on the training of CNNs , we plot the learning curves trained with dropouts of corresponding best dropout rates in Figure 1 0 .", "ner": [["path level dropout", "Method"], ["CNNs", "Method"], ["dropout", "Method"]], "rel": [["path level dropout", "Part-Of", "CNNs"]], "rel_plus": [["path level dropout:Method", "Part-Of", "CNNs:Method"]]}
{"doc_id": "102351044", "sentence": "We conjecture that this is mostly due to the fact that drop - path is a more radical regularization technique , where the whole path is dropped for those paths randomly being chosen , thus more susceptible to the removal of basic components from dropout .", "ner": [["drop - path", "Method"], ["dropout", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "102351044", "sentence": "Therefore , a smaller dropout rate is preferable for higher level of dropouts , namely drop - path and drop - layer .", "ner": [["dropout", "Method"], ["drop - path", "Method"], ["drop - layer", "Method"]], "rel": [["drop - layer", "SubClass-Of", "dropout"], ["drop - path", "SubClass-Of", "dropout"]], "rel_plus": [["drop - layer:Method", "SubClass-Of", "dropout:Method"], ["drop - path:Method", "SubClass-Of", "dropout:Method"]]}
{"doc_id": "102351044", "sentence": "As we have discussed in Section 3. 2 . 4 and Section 3. 2 . 5 , the effect of drop - layer is extensively studied in ResNet with Stochastic Depth [ 1 4 ] and [ 3 3 ] .", "ner": [["drop - layer", "Method"], ["ResNet", "Method"], ["Stochastic Depth", "Method"]], "rel": [["drop - layer", "Part-Of", "ResNet"], ["Stochastic Depth", "Part-Of", "ResNet"]], "rel_plus": [["drop - layer:Method", "Part-Of", "ResNet:Method"], ["Stochastic Depth:Method", "Part-Of", "ResNet:Method"]]}
{"doc_id": "102351044", "sentence": "To generalize drop - layer , we test out various design choices and find out that this layer level dropout is highly dependent on the residual connection of identity mapping introduced in [ 7 , 8 ] which is later widely adopted in CNNs with residual connections [ 3 7 , 3 6 ] .", "ner": [["drop - layer", "Method"], ["layer level dropout", "Method"], ["residual connection", "Method"], ["CNNs", "Method"], ["residual connections", "Method"]], "rel": [["residual connections", "Part-Of", "CNNs"]], "rel_plus": [["residual connections:Method", "Part-Of", "CNNs:Method"]]}
{"doc_id": "102351044", "sentence": "In this subsection , we focus mainly on the comparison of drop - layer with finer - grained levels of dropouts , namely drop - neuron and drop - channel , and the combination effect .", "ner": [["drop - layer", "Method"], ["drop - neuron", "Method"], ["drop - channel", "Method"]], "rel": [["drop - layer", "Compare-With", "drop - neuron"], ["drop - layer", "Compare-With", "drop - channel"]], "rel_plus": [["drop - layer:Method", "Compare-With", "drop - neuron:Method"], ["drop - layer:Method", "Compare-With", "drop - channel:Method"]]}
{"doc_id": "102351044", "sentence": "We adopt WRN - 4 0 - 4 ( see Table 2 ) and test out different dropout techniques with dropout rate ranging from 0.0 to 0. 4 0 in every 0.0 5 .", "ner": [["WRN - 4 0 - 4", "Method"], ["dropout", "Method"], ["dropout", "Method"]], "rel": [["dropout", "Part-Of", "WRN - 4 0 - 4"]], "rel_plus": [["dropout:Method", "Part-Of", "WRN - 4 0 - 4:Method"]]}
{"doc_id": "102351044", "sentence": "When combined with drop - channel , droplayer improves the performance slightly but the overall best result 4. 3 1 % is comparable to the effect of drop - channel alone .", "ner": [["drop - channel", "Method"], ["drop - channel", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "102351044", "sentence": "We therefore conclude that drop - channel is a favorable choice over drop - layer , which achieves better results with more stable training process .", "ner": [["drop - channel", "Method"], ["drop - layer", "Method"]], "rel": [["drop - channel", "Compare-With", "drop - layer"]], "rel_plus": [["drop - channel:Method", "Compare-With", "drop - layer:Method"]]}
{"doc_id": "102351044", "sentence": "So far we have introduced the convolutional building blocks of different levels of dropouts for the enhancement of existing convolutional neural networks , meanwhile extensive experiments are conducted on these dropouts , namely dropneuron , drop - channel , drop - path and drop - layer to validate their effectiveness .", "ner": [["convolutional building blocks", "Method"], ["dropouts", "Method"], ["convolutional neural networks", "Method"], ["dropouts", "Method"], ["dropneuron", "Method"], ["drop - channel", "Method"], ["drop - path", "Method"], ["drop - layer", "Method"]], "rel": [["dropouts", "Part-Of", "convolutional building blocks"], ["convolutional building blocks", "Part-Of", "convolutional neural networks"], ["dropneuron", "SubClass-Of", "dropouts"], ["drop - channel", "SubClass-Of", "dropouts"], ["drop - path", "SubClass-Of", "dropouts"], ["drop - layer", "SubClass-Of", "dropouts"]], "rel_plus": [["dropouts:Method", "Part-Of", "convolutional building blocks:Method"], ["convolutional building blocks:Method", "Part-Of", "convolutional neural networks:Method"], ["dropneuron:Method", "SubClass-Of", "dropouts:Method"], ["drop - channel:Method", "SubClass-Of", "dropouts:Method"], ["drop - path:Method", "SubClass-Of", "dropouts:Method"], ["drop - layer:Method", "SubClass-Of", "dropouts:Method"]]}
{"doc_id": "102351044", "sentence": "The overall experimental results of CIFARs and SVHN datasets are summarized in Table 5 .", "ner": [["CIFARs", "Dataset"], ["SVHN", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "102351044", "sentence": "For SVHN , the current best reported result of 1. 5 4 % error rate is achieved by WRN - 1 6 - 8 ( introduce our proposed drop - neuron and drop - channel convolutional building blocks to this network and the results are summarized in Figure 1 3 and Figure 1 4 .", "ner": [["SVHN", "Dataset"], ["WRN - 1 6 - 8", "Method"], ["drop - neuron", "Method"], ["drop - channel", "Method"]], "rel": [["WRN - 1 6 - 8", "Evaluated-With", "SVHN"], ["drop - neuron", "Part-Of", "WRN - 1 6 - 8"], ["drop - channel", "Part-Of", "WRN - 1 6 - 8"]], "rel_plus": [["WRN - 1 6 - 8:Method", "Evaluated-With", "SVHN:Dataset"], ["drop - neuron:Method", "Part-Of", "WRN - 1 6 - 8:Method"], ["drop - channel:Method", "Part-Of", "WRN - 1 6 - 8:Method"]]}
{"doc_id": "102351044", "sentence": "The results in Figure 1 3 corroborate that drop - neuron is more effective in regularizing the network trained without data augmentation .", "ner": [["drop - neuron", "Method"], ["data augmentation", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "102351044", "sentence": "Replacing conventional convolutional layers with our drop - neuron convolutional building blocks in WRN - 1 6 - 8 , we achieve a better error rate of 1. 4 4 % on SVHN over the original state - of - the - art model .", "ner": [["convolutional layers", "Method"], ["drop - neuron convolutional building blocks", "Method"], ["WRN - 1 6 - 8", "Method"], ["SVHN", "Dataset"]], "rel": [["drop - neuron convolutional building blocks", "Part-Of", "WRN - 1 6 - 8"], ["WRN - 1 6 - 8", "Evaluated-With", "SVHN"]], "rel_plus": [["drop - neuron convolutional building blocks:Method", "Part-Of", "WRN - 1 6 - 8:Method"], ["WRN - 1 6 - 8:Method", "Evaluated-With", "SVHN:Dataset"]]}
{"doc_id": "102351044", "sentence": "For CIFAR datasets , DenseNet - L 1 9 0 - K 4 0 2 is the stateof - the - art model obtaining 4. 3 6 % and 1 7 . 1 8 % error rates on CIFAR - 1 0 and CIFAR - 1 0 0 respectively .", "ner": [["CIFAR", "Dataset"], ["DenseNet - L 1 9 0 - K 4 0", "Method"], ["CIFAR - 1 0", "Dataset"], ["CIFAR - 1 0 0", "Dataset"]], "rel": [["DenseNet - L 1 9 0 - K 4 0", "Evaluated-With", "CIFAR"], ["DenseNet - L 1 9 0 - K 4 0", "Evaluated-With", "CIFAR - 1 0"], ["DenseNet - L 1 9 0 - K 4 0", "Evaluated-With", "CIFAR - 1 0 0"]], "rel_plus": [["DenseNet - L 1 9 0 - K 4 0:Method", "Evaluated-With", "CIFAR:Dataset"], ["DenseNet - L 1 9 0 - K 4 0:Method", "Evaluated-With", "CIFAR - 1 0:Dataset"], ["DenseNet - L 1 9 0 - K 4 0:Method", "Evaluated-With", "CIFAR - 1 0 0:Dataset"]]}
{"doc_id": "102351044", "sentence": "We apply dropneuron and drop - channel convolutional building blocks in replacement of convolutional layers in the model and with dropout rate 0. 1 , significantly better results are achieved with 3. 1 7 % and 1 6 . 1 5 % error rates , 0. 2 9 % and 1. 0 3 % relative improvement respectively .", "ner": [["dropneuron", "Method"], ["drop - channel convolutional building blocks", "Method"], ["convolutional layers", "Method"], ["dropout", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "102351044", "sentence": "Depth Params ImageNet VGG - 1 6 [ The overall experimental results on ImageNet dataset are summarized in Table 6 .", "ner": [["ImageNet", "Dataset"], ["VGG - 1 6", "Method"], ["ImageNet", "Dataset"]], "rel": [["VGG - 1 6", "Trained-With", "ImageNet"], ["VGG - 1 6", "Evaluated-With", "ImageNet"]], "rel_plus": [["VGG - 1 6:Method", "Trained-With", "ImageNet:Dataset"], ["VGG - 1 6:Method", "Evaluated-With", "ImageNet:Dataset"]]}
{"doc_id": "102351044", "sentence": "Three representative CNN architectures are adopted for this large dataset , specifically VGG - 1 6 [ 2 6 ] with plain convolutional operation , WRN - 5 0 - 2 [ 3 7 ] with residual connection and DenseNet - L 1 6 9 - K 3 2 [ 1 3 ] with dense connection between layers .", "ner": [["CNN", "Method"], ["VGG - 1 6", "Method"], ["convolutional operation", "Method"], ["WRN - 5 0 - 2", "Method"], ["residual connection", "Method"], ["DenseNet - L 1 6 9 - K 3 2", "Method"], ["dense connection", "Method"]], "rel": [["VGG - 1 6", "SubClass-Of", "CNN"], ["WRN - 5 0 - 2", "SubClass-Of", "CNN"], ["DenseNet - L 1 6 9 - K 3 2", "SubClass-Of", "CNN"], ["convolutional operation", "Part-Of", "VGG - 1 6"], ["residual connection", "Part-Of", "WRN - 5 0 - 2"], ["dense connection", "Part-Of", "DenseNet - L 1 6 9 - K 3 2"]], "rel_plus": [["VGG - 1 6:Method", "SubClass-Of", "CNN:Method"], ["WRN - 5 0 - 2:Method", "SubClass-Of", "CNN:Method"], ["DenseNet - L 1 6 9 - K 3 2:Method", "SubClass-Of", "CNN:Method"], ["convolutional operation:Method", "Part-Of", "VGG - 1 6:Method"], ["residual connection:Method", "Part-Of", "WRN - 5 0 - 2:Method"], ["dense connection:Method", "Part-Of", "DenseNet - L 1 6 9 - K 3 2:Method"]]}
{"doc_id": "102351044", "sentence": "For this large image classification dataset , we test the combination of drop - layer and drop - channel for WRN - 5 0 - 2 , meanwhile also the combination of drop - path and drop - channel for DenseNet - L 1 6 9 - K 3 2 .", "ner": [["image classification", "Task"], ["drop - layer", "Method"], ["drop - channel", "Method"], ["WRN - 5 0 - 2", "Method"], ["drop - path", "Method"], ["drop - channel", "Method"], ["DenseNet - L 1 6 9 - K 3 2", "Method"]], "rel": [["WRN - 5 0 - 2", "Used-For", "image classification"], ["DenseNet - L 1 6 9 - K 3 2", "Used-For", "image classification"], ["drop - layer", "Part-Of", "WRN - 5 0 - 2"], ["drop - channel", "Part-Of", "WRN - 5 0 - 2"], ["drop - channel", "Part-Of", "DenseNet - L 1 6 9 - K 3 2"], ["drop - path", "Part-Of", "DenseNet - L 1 6 9 - K 3 2"]], "rel_plus": [["WRN - 5 0 - 2:Method", "Used-For", "image classification:Task"], ["DenseNet - L 1 6 9 - K 3 2:Method", "Used-For", "image classification:Task"], ["drop - layer:Method", "Part-Of", "WRN - 5 0 - 2:Method"], ["drop - channel:Method", "Part-Of", "WRN - 5 0 - 2:Method"], ["drop - channel:Method", "Part-Of", "DenseNet - L 1 6 9 - K 3 2:Method"], ["drop - path:Method", "Part-Of", "DenseNet - L 1 6 9 - K 3 2:Method"]]}
{"doc_id": "102351044", "sentence": "For the plain CNN VGG - 1 6 , we obtain 0. 2 8 % accuracy improvement with drop - channel .", "ner": [["CNN", "Method"], ["VGG - 1 6", "Method"], ["drop - channel", "Method"]], "rel": [["VGG - 1 6", "SubClass-Of", "CNN"], ["drop - channel", "Part-Of", "VGG - 1 6"]], "rel_plus": [["VGG - 1 6:Method", "SubClass-Of", "CNN:Method"], ["drop - channel:Method", "Part-Of", "VGG - 1 6:Method"]]}
{"doc_id": "102351044", "sentence": "For WRN - 5 0 - 2 , we observe larger improvement of 0. 4 4 % with the introduction of the combination of drop - layer and drop - channel .", "ner": [["WRN - 5 0 - 2", "Method"], ["drop - layer", "Method"], ["drop - channel", "Method"]], "rel": [["drop - layer", "Part-Of", "WRN - 5 0 - 2"], ["drop - channel", "Part-Of", "WRN - 5 0 - 2"]], "rel_plus": [["drop - layer:Method", "Part-Of", "WRN - 5 0 - 2:Method"], ["drop - channel:Method", "Part-Of", "WRN - 5 0 - 2:Method"]]}
{"doc_id": "102351044", "sentence": "Lastly , with drop - path and drop - channel mechanisms introduced , DenseNet - L 1 6 9 - K 3 2 achieves 0. 4 3 % error rate drop .", "ner": [["drop - path", "Method"], ["drop - channel", "Method"], ["DenseNet - L 1 6 9 - K 3 2", "Method"]], "rel": [["drop - path", "Part-Of", "DenseNet - L 1 6 9 - K 3 2"], ["drop - channel", "Part-Of", "DenseNet - L 1 6 9 - K 3 2"]], "rel_plus": [["drop - path:Method", "Part-Of", "DenseNet - L 1 6 9 - K 3 2:Method"], ["drop - channel:Method", "Part-Of", "DenseNet - L 1 6 9 - K 3 2:Method"]]}
{"doc_id": "102351044", "sentence": "The results of the three architectures on ImageNet further corroborate that the dropout training mechanisms , the application of the four levels of dropouts alone or their combination , can significantly improve the performance if adopted properly .", "ner": [["ImageNet", "Dataset"], ["dropout training mechanisms", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "102351044", "sentence": "The improvement demonstrates that our proposed dropout convolutional building blocks are also beneficial for large image classification dataset .", "ner": [["dropout convolutional building blocks", "Method"], ["image classification", "Task"]], "rel": [["dropout convolutional building blocks", "Used-For", "image classification"]], "rel_plus": [["dropout convolutional building blocks:Method", "Used-For", "image classification:Task"]]}
{"doc_id": "102351044", "sentence": "To illustrate the difference between the regularization effect of drop - neuron and drop - channel , we plot training curves of the 1 9 0 layer DenseNet on CIFAR - 1 0 0 + with these two dropouts in Figure 1 5 .", "ner": [["drop - neuron", "Method"], ["drop - channel", "Method"], ["DenseNet", "Method"], ["CIFAR - 1 0 0 +", "Dataset"]], "rel": [["drop - neuron", "Part-Of", "DenseNet"], ["drop - channel", "Part-Of", "DenseNet"], ["DenseNet", "Trained-With", "CIFAR - 1 0 0 +"]], "rel_plus": [["drop - neuron:Method", "Part-Of", "DenseNet:Method"], ["drop - channel:Method", "Part-Of", "DenseNet:Method"], ["DenseNet:Method", "Trained-With", "CIFAR - 1 0 0 +:Dataset"]]}
{"doc_id": "102351044", "sentence": "Compared to drop - neuron , however , dropchannel regularizes the model effectively thus achieves significantly better performance .", "ner": [["drop - neuron", "Method"], ["dropchannel", "Method"]], "rel": [["dropchannel", "Compare-With", "drop - neuron"]], "rel_plus": [["dropchannel:Method", "Compare-With", "drop - neuron:Method"]]}
{"doc_id": "102351044", "sentence": "Furthermore , DenseNet regularized by drop - channel learns with higher training loss yet much lower test error , indicating that drop - channel prevents overfitting effectively .", "ner": [["DenseNet", "Method"], ["drop - channel", "Method"], ["drop - channel", "Method"]], "rel": [["drop - channel", "Part-Of", "DenseNet"]], "rel_plus": [["drop - channel:Method", "Part-Of", "DenseNet:Method"]]}
{"doc_id": "102351044", "sentence": "In this paper , we examine the four structural levels of dropout training mechanisms in a unified convolutional transformation framework , including drop - neuron , drop - channel , drop - path and drop - layer .", "ner": [["dropout training mechanisms", "Method"], ["convolutional transformation framework", "Method"], ["drop - neuron", "Method"], ["drop - channel", "Method"], ["drop - path", "Method"], ["drop - layer", "Method"]], "rel": [["drop - neuron", "SubClass-Of", "dropout training mechanisms"], ["drop - channel", "SubClass-Of", "dropout training mechanisms"], ["drop - path", "SubClass-Of", "dropout training mechanisms"], ["drop - layer", "SubClass-Of", "dropout training mechanisms"], ["dropout training mechanisms", "Part-Of", "convolutional transformation framework"], ["drop - neuron", "Part-Of", "convolutional transformation framework"], ["drop - channel", "Part-Of", "convolutional transformation framework"], ["drop - path", "Part-Of", "convolutional transformation framework"], ["drop - layer", "Part-Of", "convolutional transformation framework"]], "rel_plus": [["drop - neuron:Method", "SubClass-Of", "dropout training mechanisms:Method"], ["drop - channel:Method", "SubClass-Of", "dropout training mechanisms:Method"], ["drop - path:Method", "SubClass-Of", "dropout training mechanisms:Method"], ["drop - layer:Method", "SubClass-Of", "dropout training mechanisms:Method"], ["dropout training mechanisms:Method", "Part-Of", "convolutional transformation framework:Method"], ["drop - neuron:Method", "Part-Of", "convolutional transformation framework:Method"], ["drop - channel:Method", "Part-Of", "convolutional transformation framework:Method"], ["drop - path:Method", "Part-Of", "convolutional transformation framework:Method"], ["drop - layer:Method", "Part-Of", "convolutional transformation framework:Method"]]}
{"doc_id": "102351044", "sentence": "We attribute the failure of standard dropout to the incorrect placement within the convolutional layer , which causes great training instability .", "ner": [["dropout", "Method"], ["convolutional layer", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "102351044", "sentence": "Specifically , drop - neuron and drop - channel are widely applicable to existing CNNs while drop - path and drop - layer are highly dependent on the network architecture .", "ner": [["drop - neuron", "Method"], ["drop - channel", "Method"], ["CNNs", "Method"], ["drop - path", "Method"], ["drop - layer", "Method"]], "rel": [["drop - channel", "Part-Of", "CNNs"], ["drop - neuron", "Part-Of", "CNNs"], ["drop - neuron", "Compare-With", "drop - path"], ["drop - channel", "Compare-With", "drop - path"], ["drop - neuron", "Compare-With", "drop - layer"], ["drop - channel", "Compare-With", "drop - layer"]], "rel_plus": [["drop - channel:Method", "Part-Of", "CNNs:Method"], ["drop - neuron:Method", "Part-Of", "CNNs:Method"], ["drop - neuron:Method", "Compare-With", "drop - path:Method"], ["drop - channel:Method", "Compare-With", "drop - path:Method"], ["drop - neuron:Method", "Compare-With", "drop - layer:Method"], ["drop - channel:Method", "Compare-With", "drop - layer:Method"]]}
{"doc_id": "102351044", "sentence": "Moreover , drop - neuron and drop - channel can be applied in combination with higher levels of dropouts , which can further stabilize the training process .", "ner": [["drop - neuron", "Method"], ["drop - channel", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "102351044", "sentence": "On the other hand though , drop - neuron outperforms drop - channel in the network trained without data augmentation , as is shown in Section 4. 4 . 2 .", "ner": [["drop - neuron", "Method"], ["drop - channel", "Method"], ["data augmentation", "Method"]], "rel": [["drop - neuron", "Compare-With", "drop - channel"]], "rel_plus": [["drop - neuron:Method", "Compare-With", "drop - channel:Method"]]}
{"doc_id": "102351044", "sentence": "With our proposed convolutional building blocks specially designed for dropout training mechanisms , we achieve significant improvement over state - of - the - art CNNs on CIFAR - 1 0 / 1 0 0 , SVHN and ImageNet datasets .", "ner": [["convolutional building blocks", "Method"], ["dropout training mechanisms", "Method"], ["CNNs", "Method"], ["CIFAR - 1 0 / 1 0 0", "Dataset"], ["SVHN", "Dataset"], ["ImageNet", "Dataset"]], "rel": [["convolutional building blocks", "Used-For", "dropout training mechanisms"], ["convolutional building blocks", "Part-Of", "CNNs"], ["CNNs", "Evaluated-With", "CIFAR - 1 0 / 1 0 0"], ["CNNs", "Evaluated-With", "SVHN"], ["CNNs", "Evaluated-With", "ImageNet"]], "rel_plus": [["convolutional building blocks:Method", "Used-For", "dropout training mechanisms:Method"], ["convolutional building blocks:Method", "Part-Of", "CNNs:Method"], ["CNNs:Method", "Evaluated-With", "CIFAR - 1 0 / 1 0 0:Dataset"], ["CNNs:Method", "Evaluated-With", "SVHN:Dataset"], ["CNNs:Method", "Evaluated-With", "ImageNet:Dataset"]]}
{"doc_id": "102351044", "sentence": "Given the generality and flexibility , these dropout training mechanisms would be useful for improving the performance for a wide range of deep CNNs .", "ner": [["dropout training mechanisms", "Method"], ["CNNs", "Method"]], "rel": [["dropout training mechanisms", "Used-For", "CNNs"]], "rel_plus": [["dropout training mechanisms:Method", "Used-For", "CNNs:Method"]]}
{"doc_id": "211010786", "sentence": "Granger causality is a fundamental technique for causal inference in time series data , commonly used in the social and biological sciences .", "ner": [["Granger causality", "Method"], ["causal inference", "Task"]], "rel": [["Granger causality", "Used-For", "causal inference"]], "rel_plus": [["Granger causality:Method", "Used-For", "causal inference:Task"]]}
{"doc_id": "211010786", "sentence": "The assumption of fixed time delay also exists in Transfer Entropy , which is considered to be a non - linear version of Granger causality .", "ner": [["Transfer Entropy", "Method"], ["Granger causality", "Method"]], "rel": [["Transfer Entropy", "SubClass-Of", "Granger causality"]], "rel_plus": [["Transfer Entropy:Method", "SubClass-Of", "Granger causality:Method"]]}
{"doc_id": "211010786", "sentence": "To address this issue , we develop Variable - lag Granger causality and Variable - lag Transfer Entropy , generalizations of both Granger causality and Transfer Entropy that relax the assumption of the fixed time delay and allow causes to influence effects with arbitrary time delays .", "ner": [["Variable - lag Granger causality", "Method"], ["Variable - lag Transfer Entropy", "Method"], ["Granger causality", "Method"], ["Transfer Entropy", "Method"]], "rel": [["Variable - lag Granger causality", "SubClass-Of", "Granger causality"], ["Variable - lag Transfer Entropy", "SubClass-Of", "Transfer Entropy"]], "rel_plus": [["Variable - lag Granger causality:Method", "SubClass-Of", "Granger causality:Method"], ["Variable - lag Transfer Entropy:Method", "SubClass-Of", "Transfer Entropy:Method"]]}
{"doc_id": "211010786", "sentence": "In addition , we propose methods for inferring both variable - lag Granger causality and Transfer Entropy relations .", "ner": [["variable - lag Granger causality", "Method"], ["Transfer Entropy", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "211010786", "sentence": "In our approaches , we utilize an optimal warping path of Dynamic Time Warping ( DTW ) to infer variable - lag causal relations .", "ner": [["Dynamic Time Warping", "Method"], ["DTW", "Method"]], "rel": [["DTW", "Synonym-Of", "Dynamic Time Warping"]], "rel_plus": [["DTW:Method", "Synonym-Of", "Dynamic Time Warping:Method"]]}
{"doc_id": "211010786", "sentence": "In this work , when we refer to causality , we mean speci cally the predictive causality de ned by Granger causality . e key assumptions of Granger causality are that 1 ) the process of e ect generation can be explained by a set of structural equations , and 2 ) the current realization of the e ect at any time point is in uenced by a set of causes in the past .", "ner": [["Granger causality", "Method"], ["Granger causality", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "211010786", "sentence": "Similar to other causal inference methods , Granger causality assumes unconfoundedness and that all relevant variables are included in the analysis . ere are several studies that have been developed based on Granger causality [ 7 , 2 4 , 2 9 ] . e typical operational de nitions [ 7 ] and inference methods for inferring Granger causality , including the common so ware implementation packages [ 1 , 2 ] , assume that the e ect is in uenced by the cause with a xed and constant time delay .", "ner": [["causal inference methods", "Method"], ["Granger causality", "Method"], ["Granger causality", "Method"], ["Granger causality", "Method"]], "rel": [["Granger causality", "SubClass-Of", "causal inference methods"]], "rel_plus": [["Granger causality:Method", "SubClass-Of", "causal inference methods:Method"]]}
{"doc_id": "211010786", "sentence": "Hence , Transfer Entropy has been developed to be a non - linear extension of Granger causality [ 9 , 2 3 ] .", "ner": [["Transfer Entropy", "Method"], ["Granger causality", "Method"]], "rel": [["Transfer Entropy", "SubClass-Of", "Granger causality"]], "rel_plus": [["Transfer Entropy:Method", "SubClass-Of", "Granger causality:Method"]]}
{"doc_id": "211010786", "sentence": "To address the remaining gap , we introduce the concept Variable - lag Granger causality and Variable - lag Transfer Entropy and methods to infer them in time series data .", "ner": [["Variable - lag Granger causality", "Method"], ["Variable - lag Transfer Entropy", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "211010786", "sentence": "We prove that our denitions and the proposed inference methods can address the arbitrary - time - lag in uence between cause and e ect , while the traditional operationalizations of Granger causality , transfer entropy , and their corresponding inference methods can not .", "ner": [["Granger causality", "Method"], ["transfer entropy", "Method"]], "rel": [["transfer entropy", "SubClass-Of", "Granger causality"]], "rel_plus": [["transfer entropy:Method", "SubClass-Of", "Granger causality:Method"]]}
{"doc_id": "211010786", "sentence": "We use Dynamic Time Warping ( DTW ) [ 3 1 ] to align the cause X to the e ect time series Y while leveraging the power of Granger causality and transfer entropy .", "ner": [["Dynamic Time Warping", "Method"], ["DTW", "Method"], ["Granger causality", "Method"], ["transfer entropy", "Method"]], "rel": [["DTW", "Synonym-Of", "Dynamic Time Warping"], ["Granger causality", "Used-For", "Dynamic Time Warping"], ["transfer entropy", "Used-For", "Dynamic Time Warping"]], "rel_plus": [["DTW:Method", "Synonym-Of", "Dynamic Time Warping:Method"], ["Granger causality:Method", "Used-For", "Dynamic Time Warping:Method"], ["transfer entropy:Method", "Used-For", "Dynamic Time Warping:Method"]]}
{"doc_id": "211010786", "sentence": "In the literature , there are many clustering - based Granger causality methods that use DTW to cluster time series and perform Granger causality only for time series within the same clusters [ 2 8 , 4 6 ] .", "ner": [["clustering - based Granger causality", "Method"], ["DTW", "Method"], ["Granger causality", "Method"]], "rel": [["DTW", "Used-For", "clustering - based Granger causality"], ["DTW", "Used-For", "Granger causality"]], "rel_plus": [["DTW:Method", "Used-For", "clustering - based Granger causality:Method"], ["DTW:Method", "Used-For", "Granger causality:Method"]]}
{"doc_id": "211010786", "sentence": "Previous work on inferring causal relations using both Granger causality and DTW has the assumption that the smaller warping distance between two time series , the stronger the causal relation is [ 4 0 ] .", "ner": [["Granger causality", "Method"], ["DTW", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "211010786", "sentence": "However , their work assumes that Granger causality and DTW should run independently .", "ner": [["Granger causality", "Method"], ["DTW", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "211010786", "sentence": "In contrast , our method formalizes the integration of Granger causality and DTW by generalizing the de nition of Granger causality itself and using DTW as an instantiation of the optimal alignment requirement of the time series .", "ner": [["Granger causality", "Method"], ["DTW", "Method"], ["Granger causality", "Method"], ["DTW", "Method"]], "rel": [["DTW", "Used-For", "Granger causality"]], "rel_plus": [["DTW:Method", "Used-For", "Granger causality:Method"]]}
{"doc_id": "211010786", "sentence": "Speci cally , knowing the cause X = x never reveals information about the structural function f ( X ) and vice versa . is idea has been used in the context of times series data [ 3 5 ] by relying on the concept of Spectral Independence Criterion ( SIC ) .", "ner": [["Spectral Independence Criterion", "Method"], ["SIC", "Method"]], "rel": [["SIC", "Synonym-Of", "Spectral Independence Criterion"]], "rel_plus": [["SIC:Method", "Synonym-Of", "Spectral Independence Criterion:Method"]]}
{"doc_id": "211010786", "sentence": "Recent work on Granger causality has focused on various generalizations for it , including ones based on information theory , such as transfer entropy [ 3 3 , 3 8 ] and directed information graphs [ 3 0 ] .", "ner": [["Granger causality", "Method"], ["information theory", "Method"], ["transfer entropy", "Method"], ["directed information graphs", "Method"]], "rel": [["transfer entropy", "SubClass-Of", "information theory"], ["directed information graphs", "SubClass-Of", "information theory"]], "rel_plus": [["transfer entropy:Method", "SubClass-Of", "information theory:Method"], ["directed information graphs:Method", "SubClass-Of", "information theory:Method"]]}
{"doc_id": "211010786", "sentence": "However , none of them study tests for variable - lag Granger causality , as we propose in this work . ere is a framework of causal inference in [ 2 5 ] based on conditional independence tests on time series generated from some discrete - time stochastic processes that allows unknown latent variables .", "ner": [["variable - lag Granger causality", "Method"], ["causal inference", "Task"]], "rel": [["variable - lag Granger causality", "Used-For", "causal inference"]], "rel_plus": [["variable - lag Granger causality:Method", "Used-For", "causal inference:Task"]]}
{"doc_id": "211010786", "sentence": "However , the approach in [ 2 5 ] still assumes that data points at any time step have been generated from some structural vector autoregression ( SVAR ) . e recent work in [ 1 9 ] models causal relation between time series as a form of polynomial function and uses a stochastic block model to nd a causal graph .", "ner": [["structural vector autoregression", "Method"], ["SVAR", "Method"], ["stochastic block model", "Task"]], "rel": [["SVAR", "Synonym-Of", "structural vector autoregression"]], "rel_plus": [["SVAR:Method", "Synonym-Of", "structural vector autoregression:Method"]]}
{"doc_id": "211010786", "sentence": "Moreover , Transfer Entropy , which is considered to be a non - linear extension of Granger causality [ 9 , 2 3 ] , still has the xed - lag assumption .", "ner": [["Transfer Entropy", "Method"], ["Granger causality", "Method"]], "rel": [["Transfer Entropy", "SubClass-Of", "Granger causality"]], "rel_plus": [["Transfer Entropy:Method", "SubClass-Of", "Granger causality:Method"]]}
{"doc_id": "211010786", "sentence": "Here , we propose the concept of variable - lag Granger causality , VL - Granger causality for short , which generalizes the Granger causal relation of De nition 4. 1 in a way that addresses the xed - lag limitation .", "ner": [["variable - lag Granger causality", "Method"], ["VL - Granger causality", "Method"]], "rel": [["VL - Granger causality", "Synonym-Of", "variable - lag Granger causality"]], "rel_plus": [["VL - Granger causality:Method", "Synonym-Of", "variable - lag Granger causality:Method"]]}
{"doc_id": "211010786", "sentence": "Before de ning this approach , we show that VL - Granger causality is the proper generalization of the traditional operational de nition of Granger causality stated in De nition 4. 1 .", "ner": [["VL - Granger causality", "Method"], ["Granger causality", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "211010786", "sentence": "Transfer Entropy has been shown to be a non - linear extension of Granger causality [ 9 , 2 3 ] .", "ner": [["Transfer Entropy", "Method"], ["Granger causality", "Method"]], "rel": [["Transfer Entropy", "SubClass-Of", "Granger causality"]], "rel_plus": [["Transfer Entropy:Method", "SubClass-Of", "Granger causality:Method"]]}
{"doc_id": "211010786", "sentence": "In this section , we generalize our concept of VL - Granger causality to cover the transfer entropy concept .", "ner": [["VL - Granger causality", "Method"], ["transfer entropy", "Method"]], "rel": [["transfer entropy", "Used-For", "VL - Granger causality"]], "rel_plus": [["transfer entropy:Method", "Used-For", "VL - Granger causality:Method"]]}
{"doc_id": "211010786", "sentence": "Hence , Variable - lag Transfer Entropy function generalizes the transfer entropy function .", "ner": [["Variable - lag Transfer Entropy", "Method"], ["transfer entropy", "Method"]], "rel": [["transfer entropy", "Used-For", "Variable - lag Transfer Entropy"]], "rel_plus": [["transfer entropy:Method", "Used-For", "Variable - lag Transfer Entropy:Method"]]}
{"doc_id": "211010786", "sentence": "In Algorithm 1 line 1 - 2 , we have a x - lag parameter FixLa that controls whether we choose to compute the normal Granger causality ( FixLa = true ) or VL - Granger causality ( FixLa = f alse ) .", "ner": [["Granger causality", "Method"], ["VL - Granger causality", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "211010786", "sentence": "Otherwise , we consider VL - Granger causality ( lines 3 - 5 ) by computing the emulation relation between X DTW and Y where X DTW is a version of X that is reconstructed through DTW and is most similar to Y , captured by DTW ReconstructionFunction(X , Y ) which we explain in Section 7. 3 .", "ner": [["VL - Granger causality", "Method"], ["X DTW", "Method"], ["X DTW", "Method"], ["DTW", "Method"], ["DTW ReconstructionFunction(X , Y )", "Method"]], "rel": [["DTW", "Used-For", "VL - Granger causality"], ["DTW", "Used-For", "X DTW"]], "rel_plus": [["DTW:Method", "Used-For", "VL - Granger causality:Method"], ["DTW:Method", "Used-For", "X DTW:Method"]]}
{"doc_id": "211010786", "sentence": "E cient algorithms for computing DTW ( X , Y ) exist and they can incorporate various kernels between points [ 2 6 , 3 1 ] . en , we useP to construct X DTW where X DT W ( t ) = X ( t \u2212 \u2206 t ) .", "ner": [["DTW ( X , Y )", "Method"], ["X DTW", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "211010786", "sentence": "First , we compute Transfer Entropy causality ( line 1 in Algorithm 4 ) . e ag T ransEResult = true if X Transfer - Entropy - causes Y , otherwise T ransEResult = f alse .", "ner": [["Transfer Entropy causality", "Method"], ["Transfer - Entropy", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "211010786", "sentence": "To determine whether X Transfer - Entropy - causes Y , we can use the Transfer Entropy Ratio . e VL - Transfer Entropy Ratio is de ned below : e value T ( X , Y ) ratio is greater than one implies that X Transfer - Entropy - causes Y . e higher T ( X , Y ) ratio implies the higher strength of X causes Y . e same is true for T VL ( X , Y ) ratio . input :X , Y , \u03b4 max output : XT rans f er Entrop CausesY A er we got the results of both T ransEResult and V LT ransEResult , then we proceed to report the conclusion of causal relation between X and Y w.r.t . the following four conditions . \u2022 If both T ransEResult and V LT ransEResult are true , then we compare the T ( X , Y ) ratio with T VL ( X , Y ) ratio .", "ner": [["Transfer - Entropy", "Method"], ["Transfer Entropy", "Method"], ["VL - Transfer Entropy", "Method"], ["Transfer - Entropy", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "211010786", "sentence": "We compute the VL - transfer entropy ( line 3 ) using RTransferEntropy(X DTW , Y ) .", "ner": [["VL - transfer entropy", "Method"], ["RTransferEntropy(X DTW , Y )", "Method"]], "rel": [["RTransferEntropy(X DTW , Y )", "Used-For", "VL - transfer entropy"]], "rel_plus": [["RTransferEntropy(X DTW , Y ):Method", "Used-For", "VL - transfer entropy:Method"]]}
{"doc_id": "211010786", "sentence": "We compared our methods , VL - Granger causality ( VL - G ) and VL - Transfer entropy ( VL - TE ) , with several existing methods : Granger causality with F - test ( G ) [ 7 ] , Copula - Granger method ( CG ) [ 2 4 ] , Spectral Independence Criterion method ( SIC ) [ 3 5 ] , and transfer entropy ( TE ) [ 1 0 ] .", "ner": [["VL - Granger causality", "Method"], ["VL - G", "Method"], ["VL - Transfer entropy", "Method"], ["VL - TE", "Method"], ["Granger causality", "Method"], ["F - test ( G )", "Method"], ["Copula - Granger", "Method"], ["CG", "Method"], ["Spectral Independence Criterion", "Method"], ["SIC", "Method"], ["transfer entropy", "Method"], ["TE", "Method"]], "rel": [["VL - G", "Synonym-Of", "VL - Granger causality"], ["VL - TE", "Synonym-Of", "VL - Transfer entropy"], ["F - test ( G )", "Part-Of", "Granger causality"], ["CG", "Synonym-Of", "Copula - Granger"], ["SIC", "Synonym-Of", "Spectral Independence Criterion"], ["TE", "Synonym-Of", "transfer entropy"]], "rel_plus": [["VL - G:Method", "Synonym-Of", "VL - Granger causality:Method"], ["VL - TE:Method", "Synonym-Of", "VL - Transfer entropy:Method"], ["F - test ( G ):Method", "Part-Of", "Granger causality:Method"], ["CG:Method", "Synonym-Of", "Copula - Granger:Method"], ["SIC:Method", "Synonym-Of", "Spectral Independence Criterion:Method"], ["TE:Method", "Synonym-Of", "transfer entropy:Method"]]}
{"doc_id": "211010786", "sentence": "First , we generated X either by drawing the value of each time step from a normal distribution with zero mean and a constant variance ( X ( t ) \u223c N ) or by Auto - Regressive Moving Average model ( ARMA or A. ) with X ( t ) = 0. 9 \u00b5 + 0. 1 X ( t \u2212 1 ) . e rst set we generated was of explicitly related pairs of time series X and Y , where Y emulates X with some time lag \u2206 \u2208 [ 1 , 2 0 ] ( X \u227a Y ) .", "ner": [["Auto - Regressive Moving Average model", "Method"], ["ARMA", "Method"]], "rel": [["ARMA", "Synonym-Of", "Auto - Regressive Moving Average model"]], "rel_plus": [["ARMA:Method", "Synonym-Of", "Auto - Regressive Moving Average model:Method"]]}
{"doc_id": "211010786", "sentence": "Hence , the time complexity of VL - G is O(T \u03b4 max ) . e time complexity of TE can be at most O(T 3 ) [ 3 7 ] , which makes VL - TE has the same time complexity .", "ner": [["VL - G", "Method"], ["TE", "Method"], ["VL - TE", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "211010786", "sentence": "VL - TE also performed be er than TE .", "ner": [["VL - TE", "Method"], ["TE", "Method"]], "rel": [["VL - TE", "Compare-With", "TE"]], "rel_plus": [["VL - TE:Method", "Compare-With", "TE:Method"]]}
{"doc_id": "211010786", "sentence": "Table 4 shows the result of causal graph inference . e VL - G performed the best overall with the highest F 1 score .   is result re ects the fact that our approaches can handle complicated time series in causal inference task be er than the rest of other methods .", "ner": [["VL - G", "Method"], ["causal inference", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "211010786", "sentence": "VL - TE also performed be er than VL - TE .", "ner": [["VL - TE", "Method"], ["VL - TE", "Method"]], "rel": [["VL - TE", "Compare-With", "VL - TE"]], "rel_plus": [["VL - TE:Method", "Compare-With", "VL - TE:Method"]]}
{"doc_id": "211010786", "sentence": "Group , then we measured the ability of methods to infer that X is a cause of Y . e results , which are in the \" Group : X \u227a Y \" column in Table 4 , show that VL - G , G , TE and VL - TE performed well in this task , while CG and SIC failed to infer causal relations .", "ner": [["VL - G", "Method"], ["G", "Method"], ["TE", "Method"], ["VL - TE", "Method"], ["CG", "Method"], ["SIC", "Method"]], "rel": [["VL - G", "Compare-With", "CG"], ["G", "Compare-With", "CG"], ["TE", "Compare-With", "CG"], ["VL - TE", "Compare-With", "CG"], ["VL - G", "Compare-With", "SIC"], ["G", "Compare-With", "SIC"], ["TE", "Compare-With", "SIC"], ["VL - TE", "Compare-With", "SIC"]], "rel_plus": [["VL - G:Method", "Compare-With", "CG:Method"], ["G:Method", "Compare-With", "CG:Method"], ["TE:Method", "Compare-With", "CG:Method"], ["VL - TE:Method", "Compare-With", "CG:Method"], ["VL - G:Method", "Compare-With", "SIC:Method"], ["G:Method", "Compare-With", "SIC:Method"], ["TE:Method", "Compare-With", "SIC:Method"], ["VL - TE:Method", "Compare-With", "SIC:Method"]]}
{"doc_id": "211010786", "sentence": "For CG , SIC , and TE , they failed in one dataset each . is implies that some dataset that a speci c approach failed to detect a causal relation has broke some assumption of a speci c approach .", "ner": [["CG", "Method"], ["SIC", "Method"], ["TE", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "211010786", "sentence": "For the old faithful geyser dataset , both G and TE failed to detect a causal relation while both VL - G and VL - TE successfully inferred a causal relation . is implies that this dataset has a high - level of variable lags that broke a x - lag assumption of G and TE . 9. 4 Variable lags vs. fixed lag 9. 4 . 1 VL - Granger causality .", "ner": [["G", "Method"], ["TE", "Method"], ["VL - G", "Method"], ["VL - TE", "Method"], ["TE", "Method"], ["VL - Granger causality", "Method"]], "rel": [["G", "Compare-With", "VL - G"], ["TE", "Compare-With", "VL - G"], ["G", "Compare-With", "VL - TE"], ["TE", "Compare-With", "VL - TE"]], "rel_plus": [["G:Method", "Compare-With", "VL - G:Method"], ["TE:Method", "Compare-With", "VL - G:Method"], ["G:Method", "Compare-With", "VL - TE:Method"], ["TE:Method", "Compare-With", "VL - TE:Method"]]}
{"doc_id": "211010786", "sentence": "Fig. 9 shows the results of BIC di erence ratio for VL - G and G. Obviously , VL - G has a higher BIC di erence ratio than G 's . is suggests that VL - G was able to capture stronger signal of X causes Y . 9. 4 . 2 VL - Transfer Entropy .", "ner": [["VL - G", "Method"], ["G.", "Method"], ["VL - G", "Method"], ["G", "Method"], ["VL - G", "Method"], ["VL - Transfer Entropy", "Method"]], "rel": [["VL - G", "Compare-With", "G"]], "rel_plus": [["VL - G:Method", "Compare-With", "G:Method"]]}
{"doc_id": "211010786", "sentence": "To compare the performance of VL - TE and TE , we also simulated 1 0 0 datasets of X \u227a Y with variable lags .", "ner": [["VL - TE", "Method"], ["TE", "Method"]], "rel": [["VL - TE", "Compare-With", "TE"]], "rel_plus": [["VL - TE:Method", "Compare-With", "TE:Method"]]}
{"doc_id": "211010786", "sentence": "Fig. 1 0 shows the results of transfer entropy ratio for VL - TE and TE .", "ner": [["transfer entropy", "Method"], ["VL - TE", "Method"], ["TE", "Method"]], "rel": [["transfer entropy", "Part-Of", "VL - TE"], ["transfer entropy", "Part-Of", "TE"]], "rel_plus": [["transfer entropy:Method", "Part-Of", "VL - TE:Method"], ["transfer entropy:Method", "Part-Of", "TE:Method"]]}
{"doc_id": "211010786", "sentence": "Obviously , VL - TE has a higher transfer entropy ratio than TE 's . is suggests that VL - TE was able to capture stronger signal of X causes Y .   In this work , we proposed a method to infer Granger and transfer entropy causal relations in time series where the causes in uence e ects with arbitrary time delays , which can change dynamically .", "ner": [["VL - TE", "Method"], ["TE", "Method"], ["VL - TE", "Method"], ["Granger and transfer entropy", "Method"]], "rel": [["VL - TE", "Compare-With", "TE"]], "rel_plus": [["VL - TE:Method", "Compare-With", "TE:Method"]]}
{"doc_id": "211010786", "sentence": "We formalized a new Granger causal relation and a new transfer entropy causal relation , proving that they are true generalizations of the traditional Granger causality and transfer entropy respectively .", "ner": [["Granger causal relation", "Method"], ["transfer entropy", "Method"], ["Granger causality", "Method"], ["transfer entropy", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "211010786", "sentence": "We demonstrated on both carefully designed synthetic datasets and noisy real - world datasets that the new causal relations can address the arbitrary - time - lag in uence between cause and e ect , while the traditional Granger causality and transfer entropy can not .", "ner": [["Granger causality", "Method"], ["transfer entropy", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "211010786", "sentence": "Moreover , in addition to improving and extending Granger causality and transfer entropy , our approach can be applied to infer leader - follower relations , as well as the dependency property between cause and e ect .", "ner": [["Granger causality", "Method"], ["transfer entropy", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "211010786", "sentence": "We have shown that , in many situations , the causal relations between time series do not have a lock - step connection of a xed lag that the traditional Granger causality and transfer entropy assume .", "ner": [["Granger causality", "Method"], ["transfer entropy", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "211010786", "sentence": "Hence , traditional Granger causality and transfer entropy missed true existing causal relations in such cases , while our methods correctly inferred them .", "ner": [["Granger causality", "Method"], ["transfer entropy", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210164517", "sentence": "More specifically , here we have proposed an Inception module based Convolutional Neural Network Architecture which has achieved improved accuracy of upto 6% improvement over the existing network architecture for emotion classification when combinedly tested over multiple datasets when tried over humanoid robots in real - time .", "ner": [["Inception module", "Method"], ["Convolutional Neural Network", "Method"], ["emotion classification", "Task"]], "rel": [["Convolutional Neural Network", "Part-Of", "Inception module"], ["Convolutional Neural Network", "Used-For", "emotion classification"]], "rel_plus": [["Convolutional Neural Network:Method", "Part-Of", "Inception module:Method"], ["Convolutional Neural Network:Method", "Used-For", "emotion classification:Task"]]}
{"doc_id": "210164517", "sentence": "Our proposed model is reducing the trainable Hyperparameters to an extent of 9 4 % as compared to vanilla CNN model which clearly indicates that it can be used in real time based application such as human robot interaction .", "ner": [["CNN", "Method"], ["human robot interaction", "Task"]], "rel": [["CNN", "Used-For", "human robot interaction"]], "rel_plus": [["CNN:Method", "Used-For", "human robot interaction:Task"]]}
{"doc_id": "210164517", "sentence": "To take up this challenge , in this paper , we have proposed a model to make emotion detection more accurate over many variations of the dataset and also more real times implementable with less time complexity over existing models and hence have implemented a humanoid robot NAO to be able to predict human emotion while it is talking to the person .", "ner": [["emotion detection", "Task"], ["predict human emotion", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "210164517", "sentence": "Section 4 has proposed methodlogy , it explains Inception module , feature extraction , facial emotion recognition details .", "ner": [["Inception module", "Method"], ["feature extraction", "Method"], ["facial emotion recognition", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "210164517", "sentence": "Researcher in [ 8 ] have used Support Vector Machine ( SVM ) to classify seven different emotion using Gabor wavelet .", "ner": [["Support Vector Machine", "Method"], ["SVM", "Method"], ["Gabor wavelet", "Method"]], "rel": [["SVM", "Synonym-Of", "Support Vector Machine"], ["Gabor wavelet", "Part-Of", "Support Vector Machine"]], "rel_plus": [["SVM:Method", "Synonym-Of", "Support Vector Machine:Method"], ["Gabor wavelet:Method", "Part-Of", "Support Vector Machine:Method"]]}
{"doc_id": "210164517", "sentence": "To optimize complexity of a CNN for image classification task , authors in [ 9 ] have build a classifer using Principal Component Analysis ( PCA ) , to reduce the dimensionality and then CNN to extract the features .", "ner": [["CNN", "Method"], ["image classification", "Task"], ["Principal Component Analysis", "Method"], ["PCA", "Method"], ["CNN", "Method"]], "rel": [["CNN", "Used-For", "image classification"], ["PCA", "Synonym-Of", "Principal Component Analysis"]], "rel_plus": [["CNN:Method", "Used-For", "image classification:Task"], ["PCA:Method", "Synonym-Of", "Principal Component Analysis:Method"]]}
{"doc_id": "210164517", "sentence": "In [ 1 0 ] also , authors have built a model to predict human emotion using PCA + Artificial Neural Network ( ANN ) .", "ner": [["predict human emotion", "Task"], ["PCA", "Method"], ["Artificial Neural Network", "Method"], ["ANN", "Method"]], "rel": [["Artificial Neural Network", "Used-For", "predict human emotion"], ["PCA", "Used-For", "predict human emotion"], ["ANN", "Synonym-Of", "Artificial Neural Network"]], "rel_plus": [["Artificial Neural Network:Method", "Used-For", "predict human emotion:Task"], ["PCA:Method", "Used-For", "predict human emotion:Task"], ["ANN:Method", "Synonym-Of", "Artificial Neural Network:Method"]]}
{"doc_id": "210164517", "sentence": "When analysed over L 1 and L 2 regularization to extract image features is done in [ 1 1 ] .", "ner": [["L 1", "Method"], ["L 2 regularization", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210164517", "sentence": "Shima et . al. in [ 1 2 ] have tried to reduce the overfitting of the model by introducing dropout , batch normalization and L 2 regularization in CNN and have verified the feature extraction gives the same effect when extracted only by CNN or when Histogram of Oriented Gradients ( HOG ) features are used in parallel with CNN .", "ner": [["dropout", "Method"], ["batch normalization", "Method"], ["L 2 regularization", "Method"], ["CNN", "Method"], ["feature extraction", "Method"], ["CNN", "Method"], ["Histogram of Oriented Gradients", "Method"], ["HOG", "Method"], ["CNN", "Method"]], "rel": [["L 2 regularization", "Part-Of", "CNN"], ["batch normalization", "Part-Of", "CNN"], ["dropout", "Part-Of", "CNN"], ["HOG", "Synonym-Of", "Histogram of Oriented Gradients"], ["Histogram of Oriented Gradients", "Part-Of", "CNN"]], "rel_plus": [["L 2 regularization:Method", "Part-Of", "CNN:Method"], ["batch normalization:Method", "Part-Of", "CNN:Method"], ["dropout:Method", "Part-Of", "CNN:Method"], ["HOG:Method", "Synonym-Of", "Histogram of Oriented Gradients:Method"], ["Histogram of Oriented Gradients:Method", "Part-Of", "CNN:Method"]]}
{"doc_id": "210164517", "sentence": "Researchers in [ 1 3 ] , have improved on AlexNet performace to predict human emotion by varying the number of convolution layers as per the application need .", "ner": [["AlexNet", "Method"], ["predict human emotion", "Task"], ["convolution layers", "Method"]], "rel": [["convolution layers", "Part-Of", "AlexNet"], ["AlexNet", "Used-For", "predict human emotion"]], "rel_plus": [["convolution layers:Method", "Part-Of", "AlexNet:Method"], ["AlexNet:Method", "Used-For", "predict human emotion:Task"]]}
{"doc_id": "210164517", "sentence": "In [ 1 4 ] , authors have added SVM for classification purposes of features extracted via AlexNet model .", "ner": [["SVM", "Method"], ["classification", "Task"], ["AlexNet", "Method"]], "rel": [["AlexNet", "Used-For", "SVM"], ["SVM", "Used-For", "classification"]], "rel_plus": [["AlexNet:Method", "Used-For", "SVM:Method"], ["SVM:Method", "Used-For", "classification:Task"]]}
{"doc_id": "210164517", "sentence": "Representational autoencoders and CNN model is built in [ 1 5 ] to predict human emotion .", "ner": [["autoencoders", "Method"], ["CNN", "Method"], ["predict human emotion", "Task"]], "rel": [["autoencoders", "Used-For", "predict human emotion"], ["CNN", "Used-For", "predict human emotion"]], "rel_plus": [["autoencoders:Method", "Used-For", "predict human emotion:Task"], ["CNN:Method", "Used-For", "predict human emotion:Task"]]}
{"doc_id": "210164517", "sentence": "Authors in [ 1 6 ] have used CNN to extract out features and then implemented SVM for classification over translation invariant features and not over hand crafted features as used in earlier research .", "ner": [["CNN", "Method"], ["SVM", "Method"], ["classification", "Task"]], "rel": [["CNN", "Used-For", "SVM"], ["SVM", "Used-For", "classification"]], "rel_plus": [["CNN:Method", "Used-For", "SVM:Method"], ["SVM:Method", "Used-For", "classification:Task"]]}
{"doc_id": "210164517", "sentence": "Recently , EEG dataset is also used to predict human emotion in [ 1 7 ] using DEAP dataset .", "ner": [["predict human emotion", "Task"], ["DEAP", "Dataset"]], "rel": [["DEAP", "Benchmark-For", "predict human emotion"]], "rel_plus": [["DEAP:Dataset", "Benchmark-For", "predict human emotion:Task"]]}
{"doc_id": "210164517", "sentence": "Here , deep neural network and CNN model are built and analysed over EEG data for emotion prediction rather than facial images .", "ner": [["deep neural network", "Method"], ["CNN", "Method"], ["emotion prediction", "Task"]], "rel": [["CNN", "Used-For", "emotion prediction"], ["deep neural network", "Used-For", "emotion prediction"]], "rel_plus": [["CNN:Method", "Used-For", "emotion prediction:Task"], ["deep neural network:Method", "Used-For", "emotion prediction:Task"]]}
{"doc_id": "210164517", "sentence": "Further in [ 1 8 ] paper , authors have built a model using genetic algorithm ( GA ) + SVM for EEG signals dataset .", "ner": [["genetic algorithm", "Method"], ["GA", "Method"], ["SVM", "Method"]], "rel": [["GA", "Synonym-Of", "genetic algorithm"]], "rel_plus": [["GA:Method", "Synonym-Of", "genetic algorithm:Method"]]}
{"doc_id": "210164517", "sentence": "Electrodemal Activity [ EDA ] signals are also used for emotion classification as in [ 1 9 ] .", "ner": [["Electrodemal Activity", "Method"], ["EDA", "Method"], ["emotion classification", "Task"]], "rel": [["EDA", "Synonym-Of", "Electrodemal Activity"], ["Electrodemal Activity", "Used-For", "emotion classification"]], "rel_plus": [["EDA:Method", "Synonym-Of", "Electrodemal Activity:Method"], ["Electrodemal Activity:Method", "Used-For", "emotion classification:Task"]]}
{"doc_id": "210164517", "sentence": "Classification here is done using SVM classifier .", "ner": [["Classification", "Task"], ["SVM", "Method"]], "rel": [["SVM", "Used-For", "Classification"]], "rel_plus": [["SVM:Method", "Used-For", "Classification:Task"]]}
{"doc_id": "210164517", "sentence": "This review focusses on various hybrid deep learning approaches for emotion classification problem involving CNN , Recurrent Neural Network ( RNN ) especially LSTM to capture sequential frames of images to predict emotion .", "ner": [["deep learning", "Method"], ["emotion classification", "Task"], ["CNN", "Method"], ["Recurrent Neural Network", "Method"], ["RNN", "Method"], ["LSTM", "Method"], ["predict emotion", "Task"]], "rel": [["CNN", "Used-For", "emotion classification"], ["Recurrent Neural Network", "Used-For", "emotion classification"], ["LSTM", "Used-For", "emotion classification"], ["deep learning", "Used-For", "emotion classification"], ["RNN", "Synonym-Of", "Recurrent Neural Network"], ["LSTM", "SubClass-Of", "Recurrent Neural Network"], ["LSTM", "Used-For", "predict emotion"], ["Recurrent Neural Network", "Used-For", "predict emotion"], ["CNN", "Used-For", "predict emotion"]], "rel_plus": [["CNN:Method", "Used-For", "emotion classification:Task"], ["Recurrent Neural Network:Method", "Used-For", "emotion classification:Task"], ["LSTM:Method", "Used-For", "emotion classification:Task"], ["deep learning:Method", "Used-For", "emotion classification:Task"], ["RNN:Method", "Synonym-Of", "Recurrent Neural Network:Method"], ["LSTM:Method", "SubClass-Of", "Recurrent Neural Network:Method"], ["LSTM:Method", "Used-For", "predict emotion:Task"], ["Recurrent Neural Network:Method", "Used-For", "predict emotion:Task"], ["CNN:Method", "Used-For", "predict emotion:Task"]]}
{"doc_id": "210164517", "sentence": "Spatial representations are extracted using CNN model and temporal represenations are computed using Long Short Term Memory ( LSTM ) model .", "ner": [["CNN", "Method"], ["Long Short Term Memory", "Method"], ["LSTM", "Method"]], "rel": [["LSTM", "Synonym-Of", "Long Short Term Memory"]], "rel_plus": [["LSTM:Method", "Synonym-Of", "Long Short Term Memory:Method"]]}
{"doc_id": "210164517", "sentence": "Here , LSTMs are stacked on top of CNN to extract temporal representation after pulling spatial representations .", "ner": [["LSTMs", "Method"], ["CNN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210164517", "sentence": "In [ 2 5 ] , authors have further tried to improve upon the network by adding non - linearity in the system and worked upon the complexity by added global average pooling in place of fully connected layers .", "ner": [["global average pooling", "Method"], ["fully connected layers", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210164517", "sentence": "To solve the real time implementational need for a social personal robot in order to predict human emotion for collaborative behavior , we have proposed a CNN model based on Inception modeule concept and have verified its real time complexity .", "ner": [["predict human emotion", "Task"], ["CNN", "Method"], ["Inception modeule", "Method"]], "rel": [["Inception modeule", "Part-Of", "CNN"]], "rel_plus": [["Inception modeule:Method", "Part-Of", "CNN:Method"]]}
{"doc_id": "210164517", "sentence": "We have tested our proposed model on seven different datasets namely , Fer 2 0 1 3 , JAFFE , CK dataset , CFD , Impa - face 3 D dataset , Affectnet and a custom dataset built in our lab .", "ner": [["Fer 2 0 1 3", "Dataset"], ["JAFFE", "Dataset"], ["CK", "Dataset"], ["CFD", "Dataset"], ["Impa - face 3 D", "Dataset"], ["Affectnet", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "210164517", "sentence": "To optimize the model , we have worked on batch normalization [ 2 6 ] , 1 \u00d7 1 convolution [ 2 6 ] , depthwise separable convolutions and global average pooling ( GAP ) [ 2 6 ] .", "ner": [["batch normalization", "Method"], ["1 \u00d7 1 convolution", "Method"], ["depthwise separable convolutions", "Method"], ["global average pooling", "Method"], ["GAP", "Method"]], "rel": [["GAP", "Synonym-Of", "global average pooling"]], "rel_plus": [["GAP:Method", "Synonym-Of", "global average pooling:Method"]]}
{"doc_id": "210164517", "sentence": "Finally the network is tested over humanoid robot , NAO along with vanilla CNN model with almost same number of layers and performances of both situations are compared and analyzed .   Inception module [ 2 6 ] helps reduce the complexity as it uses 1 \u00d7 1 kernal size before using any bigger size kernel and also reduces channels thereby reducing the computation .", "ner": [["CNN", "Method"], ["Inception module", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210164517", "sentence": "Along with separable convolution , 1X 1 convolution is added with the result as residue as shown in the architecture .", "ner": [["separable convolution", "Method"], ["1X 1 convolution", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210164517", "sentence": "After feature extraction in Convolution layers , classification task is performed .", "ner": [["feature extraction", "Task"], ["Convolution", "Method"], ["classification", "Task"]], "rel": [["Convolution", "Used-For", "feature extraction"]], "rel_plus": [["Convolution:Method", "Used-For", "feature extraction:Task"]]}
{"doc_id": "210164517", "sentence": "For classification , instead of Fully Connected layers , we have used Global Average Pooling which globally extracts out the average of the features .", "ner": [["classification", "Task"], ["Fully Connected layers", "Method"], ["Global Average Pooling", "Method"]], "rel": [["Global Average Pooling", "Used-For", "classification"]], "rel_plus": [["Global Average Pooling:Method", "Used-For", "classification:Task"]]}
{"doc_id": "210164517", "sentence": "So average of the best features is highlighted out to perform the classification using Softmax function .", "ner": [["classification", "Task"], ["Softmax", "Method"]], "rel": [["Softmax", "Used-For", "classification"]], "rel_plus": [["Softmax:Method", "Used-For", "classification:Task"]]}
{"doc_id": "210164517", "sentence": "Global average pooling averages the 7 feature map and Softmax then classifies it to the most dominant emotion .", "ner": [["Global average pooling", "Method"], ["Softmax", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210164517", "sentence": "Global average pooling averages the 7 feature map and Softmax then classifies it to the most dominant emotion .", "ner": [["Global average pooling", "Method"], ["Softmax", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210164517", "sentence": "The model is based on inception module [ 2 6 ] , involving 1X 1 Con - volutions and Global Average Pooling to reduce the number of trainable hyperparameters .", "ner": [["inception module", "Method"], ["1X 1 Con - volutions", "Method"], ["Global Average Pooling", "Method"]], "rel": [["1X 1 Con - volutions", "Part-Of", "inception module"], ["Global Average Pooling", "Part-Of", "inception module"]], "rel_plus": [["1X 1 Con - volutions:Method", "Part-Of", "inception module:Method"], ["Global Average Pooling:Method", "Part-Of", "inception module:Method"]]}
{"doc_id": "210164517", "sentence": "In this network , separable Convolutions are used to reduce the hyperparameter requirement . 1X 1 Convolutions are used to further reduce complexity and fully connected layer is replaced with global average pooling to further reduce most of the trainable parameters in order to make it real time implementable .", "ner": [["Convolutions", "Method"], ["1X 1 Convolutions", "Method"], ["fully connected layer", "Method"], ["global average pooling", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210164517", "sentence": "In that figure , first row has samples of Fer 2 0 1 3 dataset , second row has CFD dataset , third row is sample from AffectNet and finally samples of JAFFE dataset are shown .", "ner": [["Fer 2 0 1 3", "Dataset"], ["CFD", "Dataset"], ["AffectNet", "Dataset"], ["JAFFE", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "210164517", "sentence": "As the fully connected layer in a CNN model has mostly 9 0 % of the parameters of the entire network , so using Global Average Pooling [ 2 6 ] has reduced the excess parameters .", "ner": [["fully connected layer", "Method"], ["CNN", "Method"], ["Global Average Pooling", "Method"]], "rel": [["fully connected layer", "Part-Of", "CNN"], ["Global Average Pooling", "Part-Of", "CNN"]], "rel_plus": [["fully connected layer:Method", "Part-Of", "CNN:Method"], ["Global Average Pooling:Method", "Part-Of", "CNN:Method"]]}
{"doc_id": "210164517", "sentence": "Further parameters are more reduced by adding 1X 1 convolutions which can drastically reduce the parameter requirement and using separable convolutions which further reduces the parameters to be trained .", "ner": [["1X 1 convolutions", "Method"], ["convolutions", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210164517", "sentence": "It works as convolving as two sequential layers ( on the name of one Convolution ) , namely depth - wise convolution and point - wise convolution as shown in Fig 3 .", "ner": [["Convolution", "Method"], ["depth - wise convolution", "Method"], ["point - wise convolution", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210164517", "sentence": "Say if we have input of 1 6 channels and Convolution operation is performed using 3 2 3X 3 filters , in that case all the 1 6 channels are traversed by every 3 2 channels of 3X 3 size , so the number of parameters are : 1 6 X 3 2 X 3X 3 = 4 6 0 8 While when separable convolution is used , it traverses 1 6 channels by 1 3X 3 filter and also 1 6 channels by 3 2 1X 1 filters , thereby resultant number of parameters becomes : Hence the number of required parameters reduces to a great extent performing the same task achieving desired accuracy .", "ner": [["Convolution", "Method"], ["separable convolution", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210164517", "sentence": "Comparison on confusion matrix for JAFFE dataset is shown in Table 1 , and 2 of Fig. 7 for Vanilla CNN , and our model respectively .", "ner": [["JAFFE", "Dataset"], ["CNN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210164517", "sentence": "These tables show that Vanilla CNN ( a variant of AlexNet ) lagged in prediction of angry and disgust emotion at all and also percentage accuracy for other emotions were also lesser as compared to our model .", "ner": [["CNN", "Method"], ["AlexNet", "Method"]], "rel": [["AlexNet", "SubClass-Of", "CNN"]], "rel_plus": [["AlexNet:Method", "SubClass-Of", "CNN:Method"]]}
{"doc_id": "210164517", "sentence": "To create similar conditions for comparison , both the models were created with global average pooling for classification purpose hence we donnot need to compare fully connected layer parameters .", "ner": [["global average pooling", "Method"], ["classification", "Task"], ["fully connected layer", "Method"]], "rel": [["global average pooling", "Used-For", "classification"]], "rel_plus": [["global average pooling:Method", "Used-For", "classification:Task"]]}
{"doc_id": "210164517", "sentence": "Also use of Depthwise Separable Convolution in our model rather than Convolution reduces the number of multiplication as explained in fig 3 .", "ner": [["Depthwise Separable Convolution", "Method"], ["Convolution", "Method"]], "rel": [["Depthwise Separable Convolution", "Compare-With", "Convolution"]], "rel_plus": [["Depthwise Separable Convolution:Method", "Compare-With", "Convolution:Method"]]}
{"doc_id": "210164517", "sentence": "The Vanilla CNN model is framed with 1 2 layers of Convolution operation with batch normalization and ReLU activation function .", "ner": [["CNN", "Method"], ["Convolution", "Method"], ["batch normalization", "Method"], ["ReLU activation", "Method"]], "rel": [["Convolution", "Part-Of", "CNN"], ["batch normalization", "Part-Of", "Convolution"], ["ReLU activation", "Part-Of", "Convolution"]], "rel_plus": [["Convolution:Method", "Part-Of", "CNN:Method"], ["batch normalization:Method", "Part-Of", "Convolution:Method"], ["ReLU activation:Method", "Part-Of", "Convolution:Method"]]}
{"doc_id": "210164517", "sentence": "The fully connected layers are replaced with global average pooling so that all we need to compare is just convolution operation and its complexity .", "ner": [["fully connected layers", "Method"], ["global average pooling", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210839545", "sentence": "In this paper , we develop a novel network named Gated Path Selection Network ( GPSNet ) , which aims to learn adaptive receptive fields .", "ner": [["Gated Path Selection Network", "Method"], ["GPSNet", "Method"]], "rel": [["GPSNet", "Synonym-Of", "Gated Path Selection Network"]], "rel_plus": [["GPSNet:Method", "Synonym-Of", "Gated Path Selection Network:Method"]]}
{"doc_id": "210839545", "sentence": "In GPSNet , we first design a two - dimensional multi - scale network - SuperNet , which densely incorporates features from growing receptive fields .", "ner": [["GPSNet", "Method"], ["two - dimensional multi - scale network", "Method"], ["SuperNet", "Method"]], "rel": [["SuperNet", "Part-Of", "GPSNet"], ["SuperNet", "SubClass-Of", "two - dimensional multi - scale network"]], "rel_plus": [["SuperNet:Method", "Part-Of", "GPSNet:Method"], ["SuperNet:Method", "SubClass-Of", "two - dimensional multi - scale network:Method"]]}
{"doc_id": "210839545", "sentence": "On two representative semantic segmentation datasets , i.e. , Cityscapes , and ADE 2 0 K , we show that the proposed approach consistently outperforms previous methods and achieves competitive performance without bells and whistles .", "ner": [["semantic segmentation", "Task"], ["Cityscapes", "Dataset"], ["ADE 2 0 K", "Dataset"]], "rel": [["Cityscapes", "Benchmark-For", "semantic segmentation"], ["ADE 2 0 K", "Benchmark-For", "semantic segmentation"]], "rel_plus": [["Cityscapes:Dataset", "Benchmark-For", "semantic segmentation:Task"], ["ADE 2 0 K:Dataset", "Benchmark-For", "semantic segmentation:Task"]]}
{"doc_id": "210839545", "sentence": "Recent stateof - the - art semantic segmentation approaches [ 1 7 , 2 , 3 , 4 , 6 , 3 3 , 2 8 ] are typically based on the Fully Convolutional Networks ( FCNs ) [ 1 7 ] .", "ner": [["semantic segmentation", "Task"], ["Fully Convolutional Networks", "Method"], ["FCNs", "Method"]], "rel": [["Fully Convolutional Networks", "Used-For", "semantic segmentation"], ["FCNs", "Synonym-Of", "Fully Convolutional Networks"]], "rel_plus": [["Fully Convolutional Networks:Method", "Used-For", "semantic segmentation:Task"], ["FCNs:Method", "Synonym-Of", "Fully Convolutional Networks:Method"]]}
{"doc_id": "210839545", "sentence": "It benefits from the informative representations of object categories and semantic information learned by Convolutional Neural Networks ( CNNs ) [ 1 0 , 1 3 ] .", "ner": [["Convolutional Neural Networks", "Method"], ["CNNs", "Method"]], "rel": [["CNNs", "Synonym-Of", "Convolutional Neural Networks"]], "rel_plus": [["CNNs:Method", "Synonym-Of", "Convolutional Neural Networks:Method"]]}
{"doc_id": "210839545", "sentence": "Objects in semantic segmentation are in a large range of scales , deformations and different viewpoints , and the fixed field of view in CNNs is insufficient to deal with geometric variations .", "ner": [["semantic segmentation", "Task"], ["CNNs", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210839545", "sentence": "To mitigate the problem , PSPNet [ 3 3 ] applies pyramid pooling module to aggregate information from different scales of feature maps .", "ner": [["PSPNet", "Method"], ["pyramid pooling module", "Method"]], "rel": [["pyramid pooling module", "Part-Of", "PSPNet"]], "rel_plus": [["pyramid pooling module:Method", "Part-Of", "PSPNet:Method"]]}
{"doc_id": "210839545", "sentence": "ASPP [ 3 ] and DenseASPP [ 2 8 ] are introduced to use a series of atrous convolution layers to learn features with multiple dilation rates .", "ner": [["ASPP", "Method"], ["DenseASPP", "Method"], ["atrous convolution layers", "Method"]], "rel": [["atrous convolution layers", "Part-Of", "ASPP"], ["atrous convolution layers", "Part-Of", "DenseASPP"]], "rel_plus": [["atrous convolution layers:Method", "Part-Of", "ASPP:Method"], ["atrous convolution layers:Method", "Part-Of", "DenseASPP:Method"]]}
{"doc_id": "210839545", "sentence": "To spotlight locally discriminative information , the recent work Deformable Convolutional Network ( DCN ) [ 8 ] shows that the adaptive sample positions can be acquired by predicting additional offsets .", "ner": [["Deformable Convolutional Network", "Method"], ["DCN", "Method"]], "rel": [["DCN", "Synonym-Of", "Deformable Convolutional Network"]], "rel_plus": [["DCN:Method", "Synonym-Of", "Deformable Convolutional Network:Method"]]}
{"doc_id": "210839545", "sentence": "In this work , we introduce Gated Path Selection Network ( GPSNet ) to enhance the capability of CNNs to adaptively learn free form receptive fields for semantic segmentation .", "ner": [["Gated Path Selection Network", "Method"], ["GPSNet", "Method"], ["CNNs", "Method"], ["semantic segmentation", "Task"]], "rel": [["GPSNet", "Synonym-Of", "Gated Path Selection Network"], ["Gated Path Selection Network", "Used-For", "semantic segmentation"]], "rel_plus": [["GPSNet:Method", "Synonym-Of", "Gated Path Selection Network:Method"], ["Gated Path Selection Network:Method", "Used-For", "semantic segmentation:Task"]]}
{"doc_id": "210839545", "sentence": "The GPS module is light - weight as it adds a small amount of parameters and computation for multi - scale feature extraction .", "ner": [["GPS", "Method"], ["multi - scale feature extraction", "Task"]], "rel": [["GPS", "Used-For", "multi - scale feature extraction"]], "rel_plus": [["GPS:Method", "Used-For", "multi - scale feature extraction:Task"]]}
{"doc_id": "210839545", "sentence": "We conduct experiments on two competitive semantic segmentation datasets , i.e. , Cityscapes [ 7 ] , and ADE 2 0 K [ 3 6 ] .", "ner": [["semantic segmentation", "Task"], ["Cityscapes", "Dataset"], ["ADE 2 0 K", "Dataset"]], "rel": [["Cityscapes", "Benchmark-For", "semantic segmentation"], ["ADE 2 0 K", "Benchmark-For", "semantic segmentation"]], "rel_plus": [["Cityscapes:Dataset", "Benchmark-For", "semantic segmentation:Task"], ["ADE 2 0 K:Dataset", "Benchmark-For", "semantic segmentation:Task"]]}
{"doc_id": "210839545", "sentence": "Semantic segmentation is a fundamental problem in computer vision , which involves assigning a semantic category to each pixel .", "ner": [["Semantic segmentation", "Task"], ["computer vision", "Task"]], "rel": [["Semantic segmentation", "SubTask-Of", "computer vision"]], "rel_plus": [["Semantic segmentation:Task", "SubTask-Of", "computer vision:Task"]]}
{"doc_id": "210839545", "sentence": "Recent progress in this problem has been largely driven by deep fully convolutional neural networks ( FCNs ) [ 1 7 , 3 5 , 2 9 , 3 , 3 3 ] .", "ner": [["fully convolutional neural networks", "Method"], ["FCNs", "Method"]], "rel": [["FCNs", "Synonym-Of", "fully convolutional neural networks"]], "rel_plus": [["FCNs:Method", "Synonym-Of", "fully convolutional neural networks:Method"]]}
{"doc_id": "210839545", "sentence": "The pioneering work FCN [ 1 7 ] proposed to remove the fully connected layers in classification CNN networks [ 1 5 , 2 3 ] , leading to a fully convolutional architecture for dense semantic segmentation .", "ner": [["FCN", "Method"], ["fully connected layers", "Method"], ["classification CNN", "Method"], ["fully convolutional architecture", "Method"], ["dense semantic segmentation", "Task"]], "rel": [["classification CNN", "Part-Of", "FCN"], ["fully connected layers", "Part-Of", "classification CNN"], ["fully convolutional architecture", "Used-For", "dense semantic segmentation"]], "rel_plus": [["classification CNN:Method", "Part-Of", "FCN:Method"], ["fully connected layers:Method", "Part-Of", "classification CNN:Method"], ["fully convolutional architecture:Method", "Used-For", "dense semantic segmentation:Task"]]}
{"doc_id": "210839545", "sentence": "UNet [ 2 1 ] adopted skip connections to combine shallow representations from the encoder and deep features from the encoder , which exploit low level feature for accurate semantic segmentation .", "ner": [["UNet", "Method"], ["skip connections", "Method"], ["semantic segmentation", "Task"]], "rel": [["skip connections", "Part-Of", "UNet"], ["UNet", "Used-For", "semantic segmentation"]], "rel_plus": [["skip connections:Method", "Part-Of", "UNet:Method"], ["UNet:Method", "Used-For", "semantic segmentation:Task"]]}
{"doc_id": "210839545", "sentence": "Deeplab [ 3 ] and Dilated Conv [ 2 9 ] proposed atrous convolutions to enlarge the network receptive field without sacrificing the resolution , which enables the network to harvest contextual information in a larger region for semantic segmentation .", "ner": [["Deeplab", "Method"], ["Dilated Conv", "Method"], ["atrous convolutions", "Method"], ["semantic segmentation", "Task"]], "rel": [["atrous convolutions", "Part-Of", "Deeplab"], ["atrous convolutions", "Part-Of", "Dilated Conv"], ["Dilated Conv", "Used-For", "semantic segmentation"], ["Deeplab", "Used-For", "semantic segmentation"]], "rel_plus": [["atrous convolutions:Method", "Part-Of", "Deeplab:Method"], ["atrous convolutions:Method", "Part-Of", "Dilated Conv:Method"], ["Dilated Conv:Method", "Used-For", "semantic segmentation:Task"], ["Deeplab:Method", "Used-For", "semantic segmentation:Task"]]}
{"doc_id": "210839545", "sentence": "Moreover , an atrous spatial pyramid pooling ( ASPP ) module was developed to incorporate contextual information from multiple scales .", "ner": [["atrous spatial pyramid pooling", "Method"], ["ASPP", "Method"]], "rel": [["ASPP", "Synonym-Of", "atrous spatial pyramid pooling"]], "rel_plus": [["ASPP:Method", "Synonym-Of", "atrous spatial pyramid pooling:Method"]]}
{"doc_id": "210839545", "sentence": "ParseNet [ 1 6 ] proposed global average pooling layer which introduces global contextual information for semantic segmentation .", "ner": [["ParseNet", "Method"], ["global average pooling", "Method"], ["semantic segmentation", "Task"]], "rel": [["global average pooling", "Part-Of", "ParseNet"], ["ParseNet", "Used-For", "semantic segmentation"]], "rel_plus": [["global average pooling:Method", "Part-Of", "ParseNet:Method"], ["ParseNet:Method", "Used-For", "semantic segmentation:Task"]]}
{"doc_id": "210839545", "sentence": "Deformable convolution or attention mechanism in semantic segmentation .", "ner": [["Deformable convolution", "Method"], ["attention mechanism", "Method"], ["semantic segmentation", "Task"]], "rel": [["attention mechanism", "Used-For", "semantic segmentation"], ["Deformable convolution", "Used-For", "semantic segmentation"]], "rel_plus": [["attention mechanism:Method", "Used-For", "semantic segmentation:Task"], ["Deformable convolution:Method", "Used-For", "semantic segmentation:Task"]]}
{"doc_id": "210839545", "sentence": "Deformable convolution layer is introduced in [ 3 7 ] , which makes the convolution kernel adaptive to geometric variations of the object , extracting dynamic contextual information for image recognition .", "ner": [["Deformable convolution layer", "Method"], ["image recognition", "Task"]], "rel": [["Deformable convolution layer", "Used-For", "image recognition"]], "rel_plus": [["Deformable convolution layer:Method", "Used-For", "image recognition:Task"]]}
{"doc_id": "210839545", "sentence": "In this section , we present Gated Path Selection Network ( GPSNet ) for semantic segmentation in detail .", "ner": [["Gated Path Selection Network", "Method"], ["GPSNet", "Method"], ["semantic segmentation", "Task"]], "rel": [["GPSNet", "Synonym-Of", "Gated Path Selection Network"], ["Gated Path Selection Network", "Used-For", "semantic segmentation"]], "rel_plus": [["GPSNet:Method", "Synonym-Of", "Gated Path Selection Network:Method"], ["Gated Path Selection Network:Method", "Used-For", "semantic segmentation:Task"]]}
{"doc_id": "210839545", "sentence": "Specifically , in GPSNet we first carefully design a SuperNet with atrous convolution layers where the dilation rates varying in large range .", "ner": [["GPSNet", "Method"], ["SuperNet", "Method"], ["atrous convolution layers", "Method"]], "rel": [["SuperNet", "Part-Of", "GPSNet"], ["atrous convolution layers", "Part-Of", "SuperNet"]], "rel_plus": [["SuperNet:Method", "Part-Of", "GPSNet:Method"], ["atrous convolution layers:Method", "Part-Of", "SuperNet:Method"]]}
{"doc_id": "210839545", "sentence": "ASPP is proposed to concatenate feature maps in multiple parallel atrous convolution layers with different dilation rates .", "ner": [["ASPP", "Method"], ["atrous convolution layers", "Method"]], "rel": [["atrous convolution layers", "Part-Of", "ASPP"]], "rel_plus": [["atrous convolution layers:Method", "Part-Of", "ASPP:Method"]]}
{"doc_id": "210839545", "sentence": "To further enhance the capability to learn effective feature representations , we propose an improved ASPP - like network structure with multiple entrances and exits to propagate information among atrous convolution layers .", "ner": [["ASPP", "Method"], ["atrous convolution layers", "Method"]], "rel": [["atrous convolution layers", "Part-Of", "ASPP"]], "rel_plus": [["atrous convolution layers:Method", "Part-Of", "ASPP:Method"]]}
{"doc_id": "210839545", "sentence": "We first extend the ASPP atrous convolution layers to grid form .", "ner": [["ASPP atrous convolution layers", "Method"], ["grid form", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210839545", "sentence": "All the convolutions are followed by InplaceABNsync [ 2 2 ] .", "ner": [["convolutions", "Method"], ["InplaceABNsync", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210839545", "sentence": "To facilitate information flow between different atrous convolution layers , we use dense connectivity to bridge parallel bottlenecked branches in Super - Net .", "ner": [["atrous convolution layers", "Method"], ["Super - Net", "Method"]], "rel": [["atrous convolution layers", "Part-Of", "Super - Net"]], "rel_plus": [["atrous convolution layers:Method", "Part-Of", "Super - Net:Method"]]}
{"doc_id": "210839545", "sentence": "In comparison with ASPP and DenseASPP , because of the dense connectivity pattern , we can acquire abundant features with more diverse and denser context .", "ner": [["ASPP", "Method"], ["DenseASPP", "Method"]], "rel": [["ASPP", "Compare-With", "DenseASPP"]], "rel_plus": [["ASPP:Method", "Compare-With", "DenseASPP:Method"]]}
{"doc_id": "210839545", "sentence": "The soft gate masks M v , M h with the size of H \u00d7 W \u00d7 1 are predicted via a projection transformation F , where The transformation layer is defined with three consecutive operations : a 1 \u00d7 1 convolution , followed by a batch normalization ( BN ) and a rectified linear unit ( ReLU ) .", "ner": [["batch normalization", "Method"], ["BN", "Method"], ["rectified linear unit", "Method"], ["ReLU", "Method"]], "rel": [["BN", "Synonym-Of", "batch normalization"], ["ReLU", "Synonym-Of", "rectified linear unit"]], "rel_plus": [["BN:Method", "Synonym-Of", "batch normalization:Method"], ["ReLU:Method", "Synonym-Of", "rectified linear unit:Method"]]}
{"doc_id": "210839545", "sentence": "We first get the concatenation mask M c = [ M v , M h ] and a comparison function C a is applied to get the soft mask M \u2208 R H \u00d7 W \u00d7 2 : The comparison function C a in our experiments consists of a 1 \u00d7 1 convolution layer , followed by a BN layer and a ReLU layer .", "ner": [["1 \u00d7 1 convolution layer", "Method"], ["BN", "Method"], ["ReLU", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210839545", "sentence": "Weighted Sum .   In this section , we compare our GPSNet with the most relevant approaches including ASPP , Dense Atrous Spatial Pyramid Pooling ( DenseASPP ) , Deformable Convolution Network ( DCN ) .", "ner": [["GPSNet", "Method"], ["ASPP", "Method"], ["Dense Atrous Spatial Pyramid Pooling", "Method"], ["DenseASPP", "Method"], ["Deformable Convolution Network", "Method"], ["DCN", "Method"]], "rel": [["ASPP", "Part-Of", "GPSNet"], ["Dense Atrous Spatial Pyramid Pooling", "Part-Of", "GPSNet"], ["Deformable Convolution Network", "Part-Of", "GPSNet"], ["DenseASPP", "Synonym-Of", "Dense Atrous Spatial Pyramid Pooling"], ["DCN", "Synonym-Of", "Deformable Convolution Network"]], "rel_plus": [["ASPP:Method", "Part-Of", "GPSNet:Method"], ["Dense Atrous Spatial Pyramid Pooling:Method", "Part-Of", "GPSNet:Method"], ["Deformable Convolution Network:Method", "Part-Of", "GPSNet:Method"], ["DenseASPP:Method", "Synonym-Of", "Dense Atrous Spatial Pyramid Pooling:Method"], ["DCN:Method", "Synonym-Of", "Deformable Convolution Network:Method"]]}
{"doc_id": "210839545", "sentence": "ASPP [ 3 ] adopts atrous convolution layers to segment both small and large objects .", "ner": [["ASPP", "Method"], ["atrous convolution layers", "Method"]], "rel": [["atrous convolution layers", "Part-Of", "ASPP"]], "rel_plus": [["atrous convolution layers:Method", "Part-Of", "ASPP:Method"]]}
{"doc_id": "210839545", "sentence": "Moreover , by extending the parallel atrous convolution layers to grid form , GPSNet expands the receptive fields to large variations .", "ner": [["atrous convolution layers", "Method"], ["grid form", "Method"], ["GPSNet", "Method"]], "rel": [["atrous convolution layers", "Part-Of", "GPSNet"]], "rel_plus": [["atrous convolution layers:Method", "Part-Of", "GPSNet:Method"]]}
{"doc_id": "210839545", "sentence": "In order to achieve large enough receptive field size , DenseASPP introduces a base cascade network which consists of atrous convolution layers .", "ner": [["DenseASPP", "Method"], ["atrous convolution layers", "Method"]], "rel": [["atrous convolution layers", "Part-Of", "DenseASPP"]], "rel_plus": [["atrous convolution layers:Method", "Part-Of", "DenseASPP:Method"]]}
{"doc_id": "210839545", "sentence": "To demonstrate the effectiveness of our approach , we conducted extensive experiments on two representative semantic segmentation datasets , i.e. , Cityscapes [ 7 ] , ADE 2 0 K [ 3 6 ] .", "ner": [["semantic segmentation", "Task"], ["Cityscapes", "Dataset"], ["ADE 2 0 K", "Dataset"]], "rel": [["ADE 2 0 K", "Benchmark-For", "semantic segmentation"], ["Cityscapes", "Benchmark-For", "semantic segmentation"]], "rel_plus": [["ADE 2 0 K:Dataset", "Benchmark-For", "semantic segmentation:Task"], ["Cityscapes:Dataset", "Benchmark-For", "semantic segmentation:Task"]]}
{"doc_id": "210839545", "sentence": "To investigate the effectiveness of the individual components of the proposed approach , i.e. , Super - Net , Gate Module , Tuned Dilation and online hard example mining(OHEM ) [ 2 7 ] .", "ner": [["Super - Net", "Method"], ["Gate Module", "Method"], ["Tuned Dilation", "Method"], ["online hard example mining(OHEM )", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210839545", "sentence": "We integrate the components into two representative approaches , i.e. , ASPP , OCNet .", "ner": [["ASPP", "Method"], ["OCNet", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210839545", "sentence": "To further evaluate the effectiveness of Su - perNet , we compare the different OCNet trained with SuperNet and with the baseline OCNet .", "ner": [["Su - perNet", "Method"], ["OCNet", "Method"], ["SuperNet", "Method"], ["OCNet", "Method"]], "rel": [["SuperNet", "Part-Of", "OCNet"], ["OCNet", "Compare-With", "OCNet"]], "rel_plus": [["SuperNet:Method", "Part-Of", "OCNet:Method"], ["OCNet:Method", "Compare-With", "OCNet:Method"]]}
{"doc_id": "210839545", "sentence": "On Cityscapes , we compare GPSNet with several competitive baselines including the dilation - based methods , i.e. , DeepLabv 3 [ 4 ] , DUC - HDC [ 2 5 ] , DenseA - SPP [ 2 8 ] , region - based method i.e. , PSPNet [ 3 3 ] , and attention - based method i.e. , PSANet [ 3 4 ] , OCNet [ 3 0 ] .", "ner": [["Cityscapes", "Dataset"], ["GPSNet", "Method"], ["dilation - based methods", "Method"], ["DeepLabv 3", "Method"], ["DUC - HDC", "Method"], ["DenseA - SPP", "Method"], ["region - based method", "Method"], ["PSPNet", "Method"], ["attention - based method", "Method"], ["PSANet", "Method"], ["OCNet", "Method"]], "rel": [["GPSNet", "Evaluated-With", "Cityscapes"], ["GPSNet", "Compare-With", "dilation - based methods"], ["DeepLabv 3", "SubClass-Of", "dilation - based methods"], ["DUC - HDC", "SubClass-Of", "dilation - based methods"], ["DenseA - SPP", "SubClass-Of", "dilation - based methods"], ["GPSNet", "Compare-With", "DeepLabv 3"], ["GPSNet", "Compare-With", "DUC - HDC"], ["GPSNet", "Compare-With", "DenseA - SPP"], ["PSPNet", "SubClass-Of", "region - based method"], ["GPSNet", "Compare-With", "region - based method"], ["GPSNet", "Compare-With", "PSPNet"], ["PSANet", "SubClass-Of", "attention - based method"], ["OCNet", "SubClass-Of", "attention - based method"], ["GPSNet", "Compare-With", "attention - based method"], ["GPSNet", "Compare-With", "PSANet"], ["GPSNet", "Compare-With", "OCNet"]], "rel_plus": [["GPSNet:Method", "Evaluated-With", "Cityscapes:Dataset"], ["GPSNet:Method", "Compare-With", "dilation - based methods:Method"], ["DeepLabv 3:Method", "SubClass-Of", "dilation - based methods:Method"], ["DUC - HDC:Method", "SubClass-Of", "dilation - based methods:Method"], ["DenseA - SPP:Method", "SubClass-Of", "dilation - based methods:Method"], ["GPSNet:Method", "Compare-With", "DeepLabv 3:Method"], ["GPSNet:Method", "Compare-With", "DUC - HDC:Method"], ["GPSNet:Method", "Compare-With", "DenseA - SPP:Method"], ["PSPNet:Method", "SubClass-Of", "region - based method:Method"], ["GPSNet:Method", "Compare-With", "region - based method:Method"], ["GPSNet:Method", "Compare-With", "PSPNet:Method"], ["PSANet:Method", "SubClass-Of", "attention - based method:Method"], ["OCNet:Method", "SubClass-Of", "attention - based method:Method"], ["GPSNet:Method", "Compare-With", "attention - based method:Method"], ["GPSNet:Method", "Compare-With", "PSANet:Method"], ["GPSNet:Method", "Compare-With", "OCNet:Method"]]}
{"doc_id": "210839545", "sentence": "The prediction of GPSNet is substantially more accurate than the methods conducted with ResNet - 1 0 1 .", "ner": [["GPSNet", "Method"], ["ResNet - 1 0 1", "Method"]], "rel": [["GPSNet", "Compare-With", "ResNet - 1 0 1"]], "rel_plus": [["GPSNet:Method", "Compare-With", "ResNet - 1 0 1:Method"]]}
{"doc_id": "210839545", "sentence": "In addition , our result also outperforms DenseASPP which takes DenseNet - 1 6 1 as backbone .", "ner": [["DenseASPP", "Method"], ["DenseNet - 1 6 1", "Method"]], "rel": [["DenseNet - 1 6 1", "Part-Of", "DenseASPP"]], "rel_plus": [["DenseNet - 1 6 1:Method", "Part-Of", "DenseASPP:Method"]]}
{"doc_id": "210839545", "sentence": "The scene parsing dataset ADE 2 0 K contains 1 5 0 classes and diverse complex scenes with 1, 0 3 8 imagelevel categories .", "ner": [["scene parsing", "Task"], ["ADE 2 0 K", "Dataset"]], "rel": [["ADE 2 0 K", "Benchmark-For", "scene parsing"]], "rel_plus": [["ADE 2 0 K:Dataset", "Benchmark-For", "scene parsing:Task"]]}
{"doc_id": "210839545", "sentence": "On the ADE 2 0 K dataset , the base learning rate is set as 0.0 2 and with a weight decay 0.0 0 0 1 .", "ner": [["ADE 2 0 K", "Dataset"], ["weight decay", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210839545", "sentence": "The learning rate policy and data augmentation are the same as the Cityscapes dataset .", "ner": [["data augmentation", "Method"], ["Cityscapes", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "210839545", "sentence": "On ADE 2 0 K , we compare our evaluated GPSNet with three attention - based method , i.e. , PSANet [ 3 4 ] , EncNet [ 3 1 ] , OCNet [ 3 0 ] , and region - based method , i.e. , PSPNet .", "ner": [["ADE 2 0 K", "Dataset"], ["GPSNet", "Method"], ["attention - based method", "Method"], ["PSANet", "Method"], ["EncNet", "Method"], ["OCNet", "Method"], ["region - based method", "Method"], ["PSPNet", "Method"]], "rel": [["GPSNet", "Evaluated-With", "ADE 2 0 K"], ["GPSNet", "Compare-With", "attention - based method"], ["PSANet", "SubClass-Of", "attention - based method"], ["EncNet", "SubClass-Of", "attention - based method"], ["OCNet", "SubClass-Of", "attention - based method"], ["GPSNet", "Compare-With", "PSANet"], ["GPSNet", "Compare-With", "EncNet"], ["GPSNet", "Compare-With", "OCNet"], ["PSPNet", "SubClass-Of", "region - based method"], ["GPSNet", "Compare-With", "region - based method"], ["GPSNet", "Compare-With", "PSPNet"]], "rel_plus": [["GPSNet:Method", "Evaluated-With", "ADE 2 0 K:Dataset"], ["GPSNet:Method", "Compare-With", "attention - based method:Method"], ["PSANet:Method", "SubClass-Of", "attention - based method:Method"], ["EncNet:Method", "SubClass-Of", "attention - based method:Method"], ["OCNet:Method", "SubClass-Of", "attention - based method:Method"], ["GPSNet:Method", "Compare-With", "PSANet:Method"], ["GPSNet:Method", "Compare-With", "EncNet:Method"], ["GPSNet:Method", "Compare-With", "OCNet:Method"], ["PSPNet:Method", "SubClass-Of", "region - based method:Method"], ["GPSNet:Method", "Compare-With", "region - based method:Method"], ["GPSNet:Method", "Compare-With", "PSPNet:Method"]]}
{"doc_id": "210839545", "sentence": "Notable , GPSNet surpasses a 2 6 9 - layer PSPNet .", "ner": [["GPSNet", "Method"], ["PSPNet", "Method"]], "rel": [["GPSNet", "Compare-With", "PSPNet"]], "rel_plus": [["GPSNet:Method", "Compare-With", "PSPNet:Method"]]}
{"doc_id": "210839545", "sentence": "BaseNet Mean IoU ( % ) PSPNet [ 3 3 ] ResNet 2 6 9 The statistics of the size of receptive field , sample rate and the number of parameters of different methods are summarized in Table 4 .", "ner": [["PSPNet", "Method"], ["ResNet 2 6 9", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210839545", "sentence": "The result in the table shows that with the same dilation settings , the size of receptive fields of our method is substantially improved 9 9 % and 4 9 % over ASPP and DenseASPP respectively .", "ner": [["ASPP", "Method"], ["DenseASPP", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210839545", "sentence": "Comparing GPS module with other methods with the same dilation rates setting , GSP module provides larger RF , higher SR and introduces less parameters . higher SR .", "ner": [["GPS", "Method"], ["GSP module", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210839545", "sentence": "The results in Table 4 indicate that SuperNet utilizes parameters more effectively than alternative methods , i.e. , ASPP and DenseASPP .", "ner": [["SuperNet", "Method"], ["ASPP", "Method"], ["DenseASPP", "Method"]], "rel": [["SuperNet", "Compare-With", "ASPP"], ["SuperNet", "Compare-With", "DenseASPP"]], "rel_plus": [["SuperNet:Method", "Compare-With", "ASPP:Method"], ["SuperNet:Method", "Compare-With", "DenseASPP:Method"]]}
{"doc_id": "210839545", "sentence": "In this paper , we have presented Gated Path Selection Network ( GPSNet ) to learn adaptive receptive fields and increase sample rates in semantic segmentation .", "ner": [["Gated Path Selection Network", "Method"], ["GPSNet", "Method"], ["semantic segmentation", "Task"]], "rel": [["GPSNet", "Synonym-Of", "Gated Path Selection Network"], ["Gated Path Selection Network", "Used-For", "semantic segmentation"]], "rel_plus": [["GPSNet:Method", "Synonym-Of", "Gated Path Selection Network:Method"], ["Gated Path Selection Network:Method", "Used-For", "semantic segmentation:Task"]]}
{"doc_id": "210839545", "sentence": "The proposed method has shown its effectiveness on two competitive semantic segmentation datasets , i.e. , Cityscapes , ADE 2 0 K , and achieves new state - of - the - art results .", "ner": [["semantic segmentation", "Task"], ["Cityscapes", "Dataset"], ["ADE 2 0 K", "Dataset"]], "rel": [["Cityscapes", "Benchmark-For", "semantic segmentation"], ["ADE 2 0 K", "Benchmark-For", "semantic segmentation"]], "rel_plus": [["Cityscapes:Dataset", "Benchmark-For", "semantic segmentation:Task"], ["ADE 2 0 K:Dataset", "Benchmark-For", "semantic segmentation:Task"]]}
{"doc_id": "210839545", "sentence": "Future research may focus on extending our results to other types of computer vision tasks , such as object detection and image generation .", "ner": [["computer vision", "Task"], ["object detection", "Task"], ["image generation", "Task"]], "rel": [["object detection", "SubTask-Of", "computer vision"], ["image generation", "SubTask-Of", "computer vision"]], "rel_plus": [["object detection:Task", "SubTask-Of", "computer vision:Task"], ["image generation:Task", "SubTask-Of", "computer vision:Task"]]}
{"doc_id": "35249701", "sentence": "In particular , we are able to add three fine - grained classification tasks to a single ImageNet - trained VGG - 1 6 network and achieve accuracies close to those of separately trained networks for each task .", "ner": [["fine - grained classification", "Task"], ["ImageNet", "Dataset"], ["VGG - 1 6", "Method"]], "rel": [["ImageNet", "Benchmark-For", "fine - grained classification"], ["VGG - 1 6", "Used-For", "fine - grained classification"], ["VGG - 1 6", "Trained-With", "ImageNet"]], "rel_plus": [["ImageNet:Dataset", "Benchmark-For", "fine - grained classification:Task"], ["VGG - 1 6:Method", "Used-For", "fine - grained classification:Task"], ["VGG - 1 6:Method", "Trained-With", "ImageNet:Dataset"]]}
{"doc_id": "35249701", "sentence": "Current approaches to overcoming catastrophic forgetting , such as Learning without Forgetting ( LwF ) [ 1 8 ] and Elastic Weight Consolidation ( EWC ) [ 1 4 ] , try to preserve knowledge important to prior tasks through the use of proxy losses .", "ner": [["Learning without Forgetting", "Method"], ["LwF", "Method"], ["Elastic Weight Consolidation", "Method"], ["EWC", "Method"]], "rel": [["LwF", "Synonym-Of", "Learning without Forgetting"], ["EWC", "Synonym-Of", "Elastic Weight Consolidation"]], "rel_plus": [["LwF:Method", "Synonym-Of", "Learning without Forgetting:Method"], ["EWC:Method", "Synonym-Of", "Elastic Weight Consolidation:Method"]]}
{"doc_id": "35249701", "sentence": "In particular , we take a single ImageNet - trained VGG - 1 6 network [ 2 8 ] and add to it three fine - grained classification tasks -CUBS birds [ 2 9 ] , Stanford Cars [ 1 5 ] , and Oxford Flowers [ 2 1 ] -while achieving accuracies very close to those of separately trained networks for each individual task .", "ner": [["ImageNet", "Dataset"], ["VGG - 1 6", "Method"], ["fine - grained classification", "Task"], ["-CUBS birds", "Dataset"], ["Stanford Cars", "Dataset"], ["Oxford Flowers", "Dataset"]], "rel": [["VGG - 1 6", "Trained-With", "ImageNet"], ["-CUBS birds", "Benchmark-For", "fine - grained classification"], ["Stanford Cars", "Benchmark-For", "fine - grained classification"], ["Oxford Flowers", "Benchmark-For", "fine - grained classification"], ["VGG - 1 6", "Used-For", "fine - grained classification"], ["VGG - 1 6", "Evaluated-With", "-CUBS birds"], ["VGG - 1 6", "Evaluated-With", "Stanford Cars"], ["VGG - 1 6", "Evaluated-With", "Oxford Flowers"]], "rel_plus": [["VGG - 1 6:Method", "Trained-With", "ImageNet:Dataset"], ["-CUBS birds:Dataset", "Benchmark-For", "fine - grained classification:Task"], ["Stanford Cars:Dataset", "Benchmark-For", "fine - grained classification:Task"], ["Oxford Flowers:Dataset", "Benchmark-For", "fine - grained classification:Task"], ["VGG - 1 6:Method", "Used-For", "fine - grained classification:Task"], ["VGG - 1 6:Method", "Evaluated-With", "-CUBS birds:Dataset"], ["VGG - 1 6:Method", "Evaluated-With", "Stanford Cars:Dataset"], ["VGG - 1 6:Method", "Evaluated-With", "Oxford Flowers:Dataset"]]}
{"doc_id": "35249701", "sentence": "We also show that our method is superior to joint training when adding the large - scale Places 3 6 5 [ 3 0 ] dataset to an ImageNet - trained network , and obtain competitive performance on a broad range of architectures , including VGG - 1 6 with batch normalization [ 1 3 ] , ResNets [ 9 ] , and DenseNets [ 1 1 ] .", "ner": [["Places 3 6 5", "Dataset"], ["ImageNet", "Dataset"], ["VGG - 1 6", "Method"], ["batch normalization", "Method"], ["ResNets", "Method"], ["DenseNets", "Method"]], "rel": [["VGG - 1 6", "Trained-With", "Places 3 6 5"], ["ResNets", "Trained-With", "Places 3 6 5"], ["DenseNets", "Trained-With", "Places 3 6 5"], ["VGG - 1 6", "Trained-With", "ImageNet"], ["ResNets", "Trained-With", "ImageNet"], ["DenseNets", "Trained-With", "ImageNet"], ["batch normalization", "Part-Of", "VGG - 1 6"]], "rel_plus": [["VGG - 1 6:Method", "Trained-With", "Places 3 6 5:Dataset"], ["ResNets:Method", "Trained-With", "Places 3 6 5:Dataset"], ["DenseNets:Method", "Trained-With", "Places 3 6 5:Dataset"], ["VGG - 1 6:Method", "Trained-With", "ImageNet:Dataset"], ["ResNets:Method", "Trained-With", "ImageNet:Dataset"], ["DenseNets:Method", "Trained-With", "ImageNet:Dataset"], ["batch normalization:Method", "Part-Of", "VGG - 1 6:Method"]]}
{"doc_id": "35249701", "sentence": "Similar to LwF and EWC , we do not require the storage of older data .", "ner": [["LwF", "Method"], ["EWC", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "35249701", "sentence": "Most existing work on transfer and multi - task learning , like [ 3 , 1 4 , 1 6 , 2 6 ] , performed validation on small - image datasets ( MNIST , CIFAR - 1 0 ) or synthetic reinforcement learning environments ( Atari , 3D maze games ) .", "ner": [["transfer and multi - task learning", "Task"], ["MNIST", "Dataset"], ["CIFAR - 1 0", "Dataset"], ["reinforcement learning", "Task"], ["Atari", "Dataset"], ["3D maze games", "Dataset"]], "rel": [["MNIST", "Benchmark-For", "transfer and multi - task learning"], ["CIFAR - 1 0", "Benchmark-For", "transfer and multi - task learning"], ["Atari", "Benchmark-For", "reinforcement learning"], ["3D maze games", "Benchmark-For", "reinforcement learning"]], "rel_plus": [["MNIST:Dataset", "Benchmark-For", "transfer and multi - task learning:Task"], ["CIFAR - 1 0:Dataset", "Benchmark-For", "transfer and multi - task learning:Task"], ["Atari:Dataset", "Benchmark-For", "reinforcement learning:Task"], ["3D maze games:Dataset", "Benchmark-For", "reinforcement learning:Task"]]}
{"doc_id": "35249701", "sentence": "Experiments with EWC and LwF have demonstrated the addition of just one task , or subsets of the same dataset [ 1 6 , 1 8 ] .", "ner": [["EWC", "Method"], ["LwF", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "35249701", "sentence": "By contrast , we demonstrate the successful combination of up to four tasks in a single network : starting with an ImageNet - trained VGG - 1 6 network , we sequentially add three fine - grained classification tasks on CUBS birds [ 2 9 ] , Stanford Cars [ 1 5 ] , and Oxford Flowers [ 2 1 ] datasets .", "ner": [["ImageNet", "Dataset"], ["VGG - 1 6", "Method"], ["fine - grained classification", "Task"], ["CUBS birds", "Dataset"], ["Stanford Cars", "Dataset"], ["Oxford Flowers", "Dataset"]], "rel": [["VGG - 1 6", "Trained-With", "ImageNet"], ["CUBS birds", "Evaluated-With", "VGG - 1 6"], ["Stanford Cars", "Evaluated-With", "VGG - 1 6"], ["Oxford Flowers", "Evaluated-With", "VGG - 1 6"], ["CUBS birds", "Benchmark-For", "fine - grained classification"], ["Stanford Cars", "Benchmark-For", "fine - grained classification"], ["Oxford Flowers", "Benchmark-For", "fine - grained classification"], ["VGG - 1 6", "Used-For", "fine - grained classification"]], "rel_plus": [["VGG - 1 6:Method", "Trained-With", "ImageNet:Dataset"], ["CUBS birds:Dataset", "Evaluated-With", "VGG - 1 6:Method"], ["Stanford Cars:Dataset", "Evaluated-With", "VGG - 1 6:Method"], ["Oxford Flowers:Dataset", "Evaluated-With", "VGG - 1 6:Method"], ["CUBS birds:Dataset", "Benchmark-For", "fine - grained classification:Task"], ["Stanford Cars:Dataset", "Benchmark-For", "fine - grained classification:Task"], ["Oxford Flowers:Dataset", "Benchmark-For", "fine - grained classification:Task"], ["VGG - 1 6:Method", "Used-For", "fine - grained classification:Task"]]}
{"doc_id": "35249701", "sentence": "We also combine ImageNet classification with scene classification on the Places 3 6 5 [ 3 0 ] dataset that has 1. 8 M training examples .", "ner": [["ImageNet classification", "Task"], ["scene classification", "Task"], ["Places 3 6 5", "Dataset"]], "rel": [["Places 3 6 5", "Benchmark-For", "ImageNet classification"], ["Places 3 6 5", "Benchmark-For", "scene classification"]], "rel_plus": [["Places 3 6 5:Dataset", "Benchmark-For", "ImageNet classification:Task"], ["Places 3 6 5:Dataset", "Benchmark-For", "scene classification:Task"]]}
{"doc_id": "35249701", "sentence": "Further , we show that our pruning - based scheme generalizes to architectures with batch normalization [ 1 3 ] , residual connections [ 9 ] , and dense connections [ 1 1 ] .", "ner": [["batch normalization", "Method"], ["residual connections", "Method"], ["dense connections", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "35249701", "sentence": "We begin with a standard network learned for an initial task , such as the VGG - 1 6 [ 2 8 ] trained on ImageNet [ 2 5 ] classification , referred to as Task I. The initial weights of a filter are depicted in gray in Figure 1 ( a ) .", "ner": [["VGG - 1 6", "Method"], ["ImageNet", "Dataset"]], "rel": [["VGG - 1 6", "Trained-With", "ImageNet"]], "rel_plus": [["VGG - 1 6:Method", "Trained-With", "ImageNet:Dataset"]]}
{"doc_id": "35249701", "sentence": "In each round of pruning , we remove a fixed percentage of eligible weights from every convolutional and fully connected layer .", "ner": [["convolutional", "Method"], ["fully connected layer", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "35249701", "sentence": "The overhead for adding one and three tasks to the initial ImageNet - trained VGG - 1 6 network ( conv 1 1 to fc 7 ) of size 5 3 7 MB is only \u223c 1 7 MB and \u223c 3 4 MB , respectively .", "ner": [["ImageNet", "Dataset"], ["VGG - 1 6", "Method"]], "rel": [["VGG - 1 6", "Trained-With", "ImageNet"]], "rel_plus": [["VGG - 1 6:Method", "Trained-With", "ImageNet:Dataset"]]}
{"doc_id": "35249701", "sentence": "In the case of the Stanford Cars and CUBS datasets , we crop object bounding boxes out of the input images and resize them to 2 2 4 \u00d7 2 2 4 .", "ner": [["Stanford Cars", "Dataset"], ["CUBS", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "35249701", "sentence": "After pruning the initial ImageNet - trained network , we fine - tune it on the ImageNet dataset for 1 0 epochs with a learning rate of 1e - 3 decayed by a factor of 1 0 after 5 epochs .", "ner": [["ImageNet", "Dataset"], ["ImageNet", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "35249701", "sentence": "However , joint fine - tuning is rather tricky when dataset sizes are different ( e.g. ImageNet and CUBS ) , so we do not attempt it for our experiments with fine - grained datasets , especially since individually trained networks provide higher reference accuracies in any case .", "ner": [["ImageNet", "Dataset"], ["CUBS", "Dataset"]], "rel": [["ImageNet", "Compare-With", "CUBS"]], "rel_plus": [["ImageNet:Dataset", "Compare-With", "CUBS:Dataset"]]}
{"doc_id": "35249701", "sentence": "Joint training works better for similarlysized datasets , thus , when combining ImageNet and Places , we compare with the jointly trained network provided by the authors of [ 3 0 ] .", "ner": [["ImageNet", "Dataset"], ["Places", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "35249701", "sentence": "Table 2 summarizes the experiments in which we add the three fine - grained tasks of CUBS , Cars , and Flowers classification in varying orders to the VGG - 1 6 network .", "ner": [["CUBS", "Dataset"], ["Cars", "Dataset"], ["Flowers", "Dataset"], ["VGG - 1 6", "Method"]], "rel": [["CUBS", "Evaluated-With", "VGG - 1 6"], ["Cars", "Evaluated-With", "VGG - 1 6"], ["Flowers", "Evaluated-With", "VGG - 1 6"]], "rel_plus": [["CUBS:Dataset", "Evaluated-With", "VGG - 1 6:Method"], ["Cars:Dataset", "Evaluated-With", "VGG - 1 6:Method"], ["Flowers:Dataset", "Evaluated-With", "VGG - 1 6:Method"]]}
{"doc_id": "35249701", "sentence": "By comparing the Classifier Only and Individual Networks columns , we can clearly see that the fine - grained tasks benefit a lot by allowing the lower convolutional layers to change , with the top - 1 error on cars and birds classification dropping from 5 6 . 4 2 % to 1 3 . 9 7 % , and from 3 6 . 7 6 % to 2 2 . 5 7 % respectively .", "ner": [["convolutional layers", "Method"], ["cars and birds classification", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "35249701", "sentence": "By pruning and re - training the ImageNet - trained VGG - 1 6 network by 5 0 % and 7 5 % , the top - 1 error slightly increases from the initial 2 8 . 4 2 % to 2 9 . 3 3 % and 3 0 . 8 7 % , respectively , and the top - 5 error slightly increases from 9. 6 1 % to 9. 9 9 % and 1 0 . 9 3 % .", "ner": [["ImageNet", "Dataset"], ["VGG - 1 6", "Method"]], "rel": [["VGG - 1 6", "Trained-With", "ImageNet"]], "rel_plus": [["VGG - 1 6:Method", "Trained-With", "ImageNet:Dataset"]]}
{"doc_id": "35249701", "sentence": "When three tasks are added to the 7 5 % pruned initial network , we achieve errors CUBS , Stanford Cars , and Flowers that are only 2. 3 8 % , 1. 7 8 % , and 1. 1 0 % worse than the Individual Networks best case .", "ner": [["CUBS", "Dataset"], ["Stanford Cars", "Dataset"], ["Flowers", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "35249701", "sentence": "Table 3 shows the results of adding the large - scale Places 3 6 5 classification task to a pruned ImageNet network .", "ner": [["Places 3 6 5", "Dataset"], ["ImageNet", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "35249701", "sentence": "By adding Places 3 6 5 , which is larger than ImageNet ( 1. 8 M images v/s 1. 3 M images ) , to a 7 5 % pruned ImageNet - trained network , we achieve top - 1 error within 0. 6 4 % and top - 5 error within 0. 1 0 % of an individually trained network .", "ner": [["Places 3 6 5", "Dataset"], ["ImageNet", "Dataset"], ["ImageNet", "Dataset"]], "rel": [["Places 3 6 5", "Compare-With", "ImageNet"]], "rel_plus": [["Places 3 6 5:Dataset", "Compare-With", "ImageNet:Dataset"]]}
{"doc_id": "35249701", "sentence": "Newer architectures such as ResNets [ 9 ] and DenseNets [ 1 1 ] are much more compact , deeper , and better - performing .", "ner": [["ResNets", "Method"], ["DenseNets", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "35249701", "sentence": "For comparison , the Classifier Only models of VGG - 1 6 , ResNet - 5 0 , and DenseNet - 1 2 1 have 1 4 0 M , 2 7 M , and 8. 6 M parameters respectively .", "ner": [["VGG - 1 6", "Method"], ["ResNet - 5 0", "Method"], ["DenseNet - 1 2 1", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "35249701", "sentence": "Table 4 shows that our method can indeed be applied to all these architectures , which include residual connections , skip connections , and batch normalization .", "ner": [["residual connections", "Method"], ["skip connections", "Method"], ["batch normalization", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "35249701", "sentence": "As described in Section 3 , the batch normalization parameters ( gain , bias , running means , and variances ) are frozen after the network is pruned and retrained for ImageNet .", "ner": [["batch normalization parameters", "Method"], ["ImageNet", "Dataset"]], "rel": [["batch normalization parameters", "Trained-With", "ImageNet"]], "rel_plus": [["batch normalization parameters:Method", "Trained-With", "ImageNet:Dataset"]]}
{"doc_id": "35249701", "sentence": "The deeper ResNet and DenseNet networks with 5 0 and 1 2 1 layers , respectively , are very robust to pruning , losing just 0. 4 5 % and 0.0 4 % top - 1 accuracy on ImageNet , respectively .", "ner": [["ResNet", "Method"], ["DenseNet", "Method"], ["ImageNet", "Dataset"]], "rel": [["ResNet", "Evaluated-With", "ImageNet"], ["DenseNet", "Evaluated-With", "ImageNet"]], "rel_plus": [["ResNet:Method", "Evaluated-With", "ImageNet:Dataset"], ["DenseNet:Method", "Evaluated-With", "ImageNet:Dataset"]]}
{"doc_id": "35249701", "sentence": "Top - 5 error increases by 0.0 5 % for ResNet , and decreases by 0. 1 3 % for DenseNet .", "ner": [["ResNet", "Method"], ["DenseNet", "Method"]], "rel": [["ResNet", "Compare-With", "DenseNet"]], "rel_plus": [["ResNet:Method", "Compare-With", "DenseNet:Method"]]}
{"doc_id": "35249701", "sentence": "In the case of Flowers classification , we perform better than the individual network , probably because training the full network causes it to overfit to the Flowers dataset , which is the smallest .", "ner": [["Flowers", "Dataset"], ["Flowers", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "35249701", "sentence": "After the initial round of 5 0 % pruning for Task I ( ImageNet classification ) , we have \u223c 6 7 M free parameters .", "ner": [["ImageNet", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "35249701", "sentence": "We use an initial pruning ratio of 5 0 % for the ImageNet - trained VGG - 1 6 and a pruning ratio of 7 5 % after each dataset is added . 0. 5 0 , 0. 7 5 , 0. 7 5 pruning column of Table 2 reports the average over orderings . 1 6 . 7 5 M parameters are used by Task II , and 5 0 . 2 5 M free parameters available for subsequent tasks .", "ner": [["ImageNet", "Dataset"], ["VGG - 1 6", "Method"]], "rel": [["VGG - 1 6", "Trained-With", "ImageNet"]], "rel_plus": [["VGG - 1 6:Method", "Trained-With", "ImageNet:Dataset"]]}
{"doc_id": "35249701", "sentence": "In Figure 4 , we measure the effect of pruning and retraining for a task , when it is first added to a 5 0 % pruned VGG - 1 6 network ( except for the initial ImageNet task ) .", "ner": [["VGG - 1 6", "Method"], ["ImageNet", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "35249701", "sentence": "The values above correspond to the case when the respective dataset is added as the first task , to an ImageNet - trained VGG - 1 6 that is 5 0 % pruned , except for the values corresponding to the ImageNet dataset which correspond to initial pruning .", "ner": [["ImageNet", "Dataset"], ["VGG - 1 6", "Method"], ["ImageNet", "Dataset"]], "rel": [["VGG - 1 6", "Trained-With", "ImageNet"]], "rel_plus": [["VGG - 1 6:Method", "Trained-With", "ImageNet:Dataset"]]}
{"doc_id": "35249701", "sentence": "More surprisingly , even a very aggressive single - shot pruning ratio of 9 0 % followed by re - training results in a small error increase compared to the unpruned errors ( top - 1 error increases from 1 5 . 7 5 % to 1 7 . 8 4 % for Stanford Cars , 2 4 . 1 3 % to 2 4 . 7 2 % for CUBS , and 8. 9 6 % to 9. 4 8 % for Flowers ) .", "ner": [["Stanford Cars", "Dataset"], ["CUBS", "Dataset"], ["Flowers", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "35249701", "sentence": "This indicates effective transfer learning as very few parameter modifications ( 1 0 % of the available 5 0 % of total parameters after pruning , or 5% of the total VGG - 1 6 parameters ) are enough to obtain good accuracies .", "ner": [["transfer learning", "Task"], ["VGG - 1 6", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "35249701", "sentence": "Sharing biases reduces the storage overhead of our proposed method , as each convolutional , fully - connected , or batch - normalization layer can contain an associated bias term .", "ner": [["convolutional", "Method"], ["fully - connected", "Method"], ["batch - normalization", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "35249701", "sentence": "For this experiment , we start with the 5 0 % pruned ImageNet - trained vanilla VGG - 1 6 network , and add one new task .", "ner": [["ImageNet", "Dataset"], ["VGG - 1 6", "Method"]], "rel": [["VGG - 1 6", "Trained-With", "ImageNet"]], "rel_plus": [["VGG - 1 6:Method", "Trained-With", "ImageNet:Dataset"]]}
{"doc_id": "35249701", "sentence": "We could Table 6 : Comparison of filter - based and weight - based pruning for ImageNet - trained VGG - 1 6 .", "ner": [["ImageNet", "Dataset"], ["VGG - 1 6", "Method"]], "rel": [["VGG - 1 6", "Trained-With", "ImageNet"]], "rel_plus": [["VGG - 1 6:Method", "Trained-With", "ImageNet:Dataset"]]}
{"doc_id": "35249701", "sentence": "A further unfavorable observation is that most of the pruned filters ( 3, 7 3 0 out of 4, 0 0 0 ) were chosen from the fully connected layers ( Liu et al. [ 1 9 ] proposed a different filter - based pruning method and found similar behavior for VGG - 1 6 ) .", "ner": [["fully connected layers", "Method"], ["filter - based pruning", "Method"], ["VGG - 1 6", "Method"]], "rel": [["filter - based pruning", "Part-Of", "VGG - 1 6"]], "rel_plus": [["filter - based pruning:Method", "Part-Of", "VGG - 1 6:Method"]]}
{"doc_id": "35249701", "sentence": "As a result , filter - based pruning only allowed us to add one extra task to the ImageNet - trained VGG - 1 6 network , as shown in Table 6 .", "ner": [["filter - based pruning", "Method"], ["ImageNet", "Dataset"], ["VGG - 1 6", "Method"]], "rel": [["VGG - 1 6", "Trained-With", "ImageNet"], ["filter - based pruning", "Part-Of", "VGG - 1 6"]], "rel_plus": [["VGG - 1 6:Method", "Trained-With", "ImageNet:Dataset"], ["filter - based pruning:Method", "Part-Of", "VGG - 1 6:Method"]]}
{"doc_id": "35249701", "sentence": "It works not only for the relatively \" roomy \" VGG - 1 6 architecture , but also for more compact parameterefficient networks such as ResNets and DenseNets .", "ner": [["VGG - 1 6", "Method"], ["ResNets", "Method"], ["DenseNets", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "207880647", "sentence": "To this end , we propose an easy to use model consisting of the conjunction of the Transformer decoder GPT - 2 model with Transformer encoder BERT for the downstream task for question answering .", "ner": [["Transformer decoder GPT - 2", "Method"], ["Transformer encoder BERT", "Method"], ["question answering", "Task"]], "rel": [["Transformer decoder GPT - 2", "Used-For", "question answering"], ["Transformer encoder BERT", "Used-For", "question answering"]], "rel_plus": [["Transformer decoder GPT - 2:Method", "Used-For", "question answering:Task"], ["Transformer encoder BERT:Method", "Used-For", "question answering:Task"]]}
{"doc_id": "207880647", "sentence": "Our result of neural question generation from text on the SQuAD 1. 1 dataset suggests that our method can produce semantically correct and diverse questions .", "ner": [["neural question generation", "Task"], ["SQuAD 1. 1", "Dataset"]], "rel": [["SQuAD 1. 1", "Benchmark-For", "neural question generation"]], "rel_plus": [["SQuAD 1. 1:Dataset", "Benchmark-For", "neural question generation:Task"]]}
{"doc_id": "207880647", "sentence": "Recently , natural language processing ( NLP ) has enjoyed unprecedented progress largely due to developments in deep learning .", "ner": [["natural language processing", "Task"], ["NLP", "Task"], ["deep learning", "Method"]], "rel": [["NLP", "Synonym-Of", "natural language processing"], ["deep learning", "Used-For", "natural language processing"]], "rel_plus": [["NLP:Task", "Synonym-Of", "natural language processing:Task"], ["deep learning:Method", "Used-For", "natural language processing:Task"]]}
{"doc_id": "207880647", "sentence": "In this regard considerable attention in the NLP community is devoted to topics related to automatic question answering ( QA ) .", "ner": [["NLP", "Task"], ["question answering", "Task"], ["QA", "Task"]], "rel": [["QA", "Synonym-Of", "question answering"]], "rel_plus": [["QA:Task", "Synonym-Of", "question answering:Task"]]}
{"doc_id": "207880647", "sentence": "Although , question generation ( QG ) ( Du et al. , 2 0 1 7 ; Serban et al. , 2 0 1 6 ; Pan et al. , 2 0 1 9 ; Kim et al. , 2 0 1 9 ) enjoys a bit of niche existence , it has a plethora of potential applications such as improving the training of QA systems , and help in the creation of material in the educational domain ( Chen et al. , 2 0 1 8) .", "ner": [["question generation", "Task"], ["QG", "Task"], ["QA", "Task"]], "rel": [["QG", "Synonym-Of", "question generation"], ["question generation", "Used-For", "QA"]], "rel_plus": [["QG:Task", "Synonym-Of", "question generation:Task"], ["question generation:Task", "Used-For", "QA:Task"]]}
{"doc_id": "207880647", "sentence": "In this paper , we consider collaborative learning of QA and QG .", "ner": [["QA", "Task"], ["QG", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "207880647", "sentence": "The key idea of this work is that as question answering and generation are naturally related tasks , so leveraging their connection should be mutually beneficial in terms of performance as well as reducing the amount of labeled data , e.g. for training a QA system .", "ner": [["question answering", "Task"], ["QA", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "207880647", "sentence": "The proposed solution builds upon two recent variants of self - attention Transformer network architectures ( Vaswani et al. , 2 0 1 7 ) , namely GPT - 2 ( Radford et al. , 2 0 1 9 ) and BERT ( Devlin et al. , 2 0 1 8) .", "ner": [["self - attention Transformer", "Method"], ["GPT - 2", "Method"], ["BERT", "Method"]], "rel": [["GPT - 2", "SubClass-Of", "self - attention Transformer"], ["BERT", "SubClass-Of", "self - attention Transformer"]], "rel_plus": [["GPT - 2:Method", "SubClass-Of", "self - attention Transformer:Method"], ["BERT:Method", "SubClass-Of", "self - attention Transformer:Method"]]}
{"doc_id": "207880647", "sentence": "Compared to the original Transformer architecture , GPT - 2 discards the encoder blocks reducing it to a decoder stack .", "ner": [["Transformer", "Method"], ["GPT - 2", "Method"]], "rel": [["GPT - 2", "Compare-With", "Transformer"]], "rel_plus": [["GPT - 2:Method", "Compare-With", "Transformer:Method"]]}
{"doc_id": "207880647", "sentence": "It provides traditional language model functionality , allowing to BERT Encoder Encoder Encoder Encoder GPT - 2 Decoder Decoder Decoder Decoder What ?", "ner": [["BERT", "Method"], ["GPT - 2", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "207880647", "sentence": "For example , in OpenAI GPT , the authors use a left - to - right architecture , where every token can only attended to previous tokens in the self - attention Figure 1 : Illustration of the pre - training sketch : Each network , i.e. GPT - 2 and BERT , is individually trained to answer questions using a QA head assigning probabilities to each token to be beginning and/or end of the answer span .", "ner": [["GPT", "Method"], ["GPT - 2", "Method"], ["BERT", "Method"], ["QA head", "Method"]], "rel": [["QA head", "Part-Of", "GPT - 2"], ["QA head", "Part-Of", "BERT"]], "rel_plus": [["QA head:Method", "Part-Of", "GPT - 2:Method"], ["QA head:Method", "Part-Of", "BERT:Method"]]}
{"doc_id": "207880647", "sentence": "What is more , BERT is trained for discriminative QA with applying a specific regression head .", "ner": [["BERT", "Method"], ["QA", "Task"]], "rel": [["BERT", "Used-For", "QA"]], "rel_plus": [["BERT:Method", "Used-For", "QA:Task"]]}
{"doc_id": "207880647", "sentence": "However , even beyond QA BERT has shown extreme versatility in terms of applicability for numerous downstream tasks .", "ner": [["QA BERT", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "207880647", "sentence": "In comparison to the conventional Transformer network and in contrast to GPT - 2 , it discards the decoder blocks , reducing it to a pure encoder stack .", "ner": [["Transformer", "Method"], ["GPT - 2", "Method"]], "rel": [["Transformer", "Compare-With", "GPT - 2"]], "rel_plus": [["Transformer:Method", "Compare-With", "GPT - 2:Method"]]}
{"doc_id": "207880647", "sentence": "This work relates to recent studies which attempt to improve the performance of a discriminative QA model with generative QG models ( Lewis and Fan , 2 0 1 9 ; Wang et al. , 2 0 1 7 ; Duan et al. , 2 0 1 7 ; Dong et al. , 2 0 1 7 ; Yang et al. , 2 0 1 7 ; Harrison and Walker , 2 0 1 8 ; Tang et al. , 2 0 1 8) .", "ner": [["discriminative QA model", "Method"], ["generative QG models", "Method"]], "rel": [["generative QG models", "Part-Of", "discriminative QA model"]], "rel_plus": [["generative QG models:Method", "Part-Of", "discriminative QA model:Method"]]}
{"doc_id": "207880647", "sentence": "These works regard QA as the primary task and use auxiliary tasks , such as question generation and question paraphrasing , to improve the primary task .", "ner": [["QA", "Task"], ["question generation", "Task"], ["question paraphrasing", "Task"]], "rel": [["question generation", "Used-For", "QA"], ["question paraphrasing", "Used-For", "QA"]], "rel_plus": [["question generation:Task", "Used-For", "QA:Task"], ["question paraphrasing:Task", "Used-For", "QA:Task"]]}
{"doc_id": "207880647", "sentence": "Whereas this is one part of our goal , our other goal is to improve the QG model with the QA system and to further improve both tasks in a loop .", "ner": [["QG", "Task"], ["QA", "Task"]], "rel": [["QA", "Part-Of", "QG"]], "rel_plus": [["QA:Task", "Part-Of", "QG:Task"]]}
{"doc_id": "207880647", "sentence": "Specifically , a Transformer network model is proposed which consisting of the conjunction of the Transformer decoder GPT - 2 model with Transformer encoder BERT .", "ner": [["Transformer network model", "Method"], ["Transformer decoder GPT - 2", "Method"], ["Transformer encoder BERT", "Method"]], "rel": [["Transformer decoder GPT - 2", "Part-Of", "Transformer network model"], ["Transformer encoder BERT", "Part-Of", "Transformer network model"]], "rel_plus": [["Transformer decoder GPT - 2:Method", "Part-Of", "Transformer network model:Method"], ["Transformer encoder BERT:Method", "Part-Of", "Transformer network model:Method"]]}
{"doc_id": "207880647", "sentence": "In a similar work , ( Tang et al. , 2 0 1 7 a ) consider learning question generation and question answering as a dual task , but learning them jointly using the entire dataset .", "ner": [["question generation", "Task"], ["question answering", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "207880647", "sentence": "The generated questions are then combined with the human questions through a domain adaptation pipeline for training QA models .", "ner": [["domain adaptation", "Method"], ["QA", "Task"]], "rel": [["domain adaptation", "Used-For", "QA"]], "rel_plus": [["domain adaptation:Method", "Used-For", "QA:Task"]]}
{"doc_id": "207880647", "sentence": "Therefore , as another contribution of this paper , we propose to assess the quality of QG systems using the performance of a QA network trained on generated questions by QG as surrogate measure .", "ner": [["QG", "Task"], ["QA", "Task"], ["QG", "Task"]], "rel": [["QA", "Used-For", "QG"], ["QG", "Used-For", "QA"]], "rel_plus": [["QA:Task", "Used-For", "QG:Task"], ["QG:Task", "Used-For", "QA:Task"]]}
{"doc_id": "207880647", "sentence": "The proposed approach is evaluated on the SQuAD dataset ( Rajpurkar et al. , 2 0 1 6 ) for both \" question generation \" and \" question generation \" tasks .", "ner": [["SQuAD", "Dataset"], ["question generation", "Task"], ["question generation", "Task"]], "rel": [["SQuAD", "Benchmark-For", "question generation"], ["SQuAD", "Benchmark-For", "question generation"]], "rel_plus": [["SQuAD:Dataset", "Benchmark-For", "question generation:Task"], ["SQuAD:Dataset", "Benchmark-For", "question generation:Task"]]}
{"doc_id": "207880647", "sentence": "This study opens up avenues for exploiting inexpensive QG solutions similar to ours to achieve performance gain in QA task .", "ner": [["QG", "Task"], ["QA", "Task"]], "rel": [["QG", "Compare-With", "QA"]], "rel_plus": [["QG:Task", "Compare-With", "QA:Task"]]}
{"doc_id": "207880647", "sentence": "The contributions are two - fold : First , we leverage question generation by tying together GPT - 2 and BERT in end - to - end trainable fashion facilitating semi - supervised learning .", "ner": [["question generation", "Task"], ["GPT - 2", "Method"], ["BERT", "Method"], ["semi - supervised learning", "Method"]], "rel": [["BERT", "Used-For", "question generation"], ["GPT - 2", "Used-For", "question generation"], ["semi - supervised learning", "Used-For", "question generation"]], "rel_plus": [["BERT:Method", "Used-For", "question generation:Task"], ["GPT - 2:Method", "Used-For", "question generation:Task"], ["semi - supervised learning:Method", "Used-For", "question generation:Task"]]}
{"doc_id": "207880647", "sentence": "Second , we propose to use QA as a surrogate measure for assessing question generation quality .", "ner": [["QA", "Task"], ["question generation", "Task"]], "rel": [["QA", "Used-For", "question generation"]], "rel_plus": [["QA:Task", "Used-For", "question generation:Task"]]}
{"doc_id": "207880647", "sentence": "Question answering and question generation are intrinsically linked .", "ner": [["Question answering", "Task"], ["question generation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "207880647", "sentence": "Hence , the core of the proposed method consists of learning question generation network by making use of the feedback of a question answering network .", "ner": [["question generation", "Task"], ["question answering", "Task"]], "rel": [["question answering", "Used-For", "question generation"]], "rel_plus": [["question answering:Task", "Used-For", "question generation:Task"]]}
{"doc_id": "207880647", "sentence": "Here we propose to employ GPT - 2 's language model for question generation and BERT for question answering .", "ner": [["GPT - 2", "Method"], ["question generation", "Task"], ["BERT", "Method"], ["question answering", "Task"]], "rel": [["GPT - 2", "Used-For", "question generation"], ["BERT", "Used-For", "question answering"]], "rel_plus": [["GPT - 2:Method", "Used-For", "question generation:Task"], ["BERT:Method", "Used-For", "question answering:Task"]]}
{"doc_id": "207880647", "sentence": "We first briefly discuss the intrinsics of GPT - 2 and BERT .", "ner": [["GPT - 2", "Method"], ["BERT", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "207880647", "sentence": "This is followed by elaborating on how we adapt GPT - 2 's language model , for question generation .", "ner": [["GPT - 2", "Method"], ["question generation", "Task"]], "rel": [["GPT - 2", "Used-For", "question generation"]], "rel_plus": [["GPT - 2:Method", "Used-For", "question generation:Task"]]}
{"doc_id": "207880647", "sentence": "Subsequently , we explain the details on how to merge the process of question generation & question answering in a collaborative framework through mixing GPT - 2 and BERT .", "ner": [["question generation", "Task"], ["question answering", "Task"], ["GPT - 2", "Method"], ["BERT", "Method"]], "rel": [["GPT - 2", "Used-For", "question generation"], ["BERT", "Used-For", "question answering"]], "rel_plus": [["GPT - 2:Method", "Used-For", "question generation:Task"], ["BERT:Method", "Used-For", "question answering:Task"]]}
{"doc_id": "207880647", "sentence": "In this section we briefly review GPT - 2 ( Radford et al. , 2 0 1 9 ) and BERT ( Devlin et al. , 2 0 1 8) models .", "ner": [["GPT - 2", "Method"], ["BERT", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "207880647", "sentence": "This can largely be attributed to the self - attention mechanism in the Transformer that allows BERT facilitates generic applicability .", "ner": [["self - attention mechanism", "Method"], ["Transformer", "Method"], ["BERT", "Method"]], "rel": [["self - attention mechanism", "Part-Of", "Transformer"], ["Transformer", "Part-Of", "BERT"]], "rel_plus": [["self - attention mechanism:Method", "Part-Of", "Transformer:Method"], ["Transformer:Method", "Part-Of", "BERT:Method"]]}
{"doc_id": "207880647", "sentence": "Wang and Cho ( Wang and Cho , 2 0 1 9 ) also showed BERT is a combination of a Markov random field language model with pseudo loglikelihood training .", "ner": [["BERT", "Method"], ["Markov random field language model", "Method"]], "rel": [["BERT", "SubClass-Of", "Markov random field language model"]], "rel_plus": [["BERT:Method", "SubClass-Of", "Markov random field language model:Method"]]}
{"doc_id": "207880647", "sentence": "Technically , GPT - 2 and BERT are opposite slices of the Transformer stack .", "ner": [["GPT - 2", "Method"], ["BERT", "Method"], ["Transformer stack", "Method"]], "rel": [["Transformer stack", "Part-Of", "GPT - 2"], ["GPT - 2", "Compare-With", "BERT"], ["Transformer stack", "Part-Of", "BERT"]], "rel_plus": [["Transformer stack:Method", "Part-Of", "GPT - 2:Method"], ["GPT - 2:Method", "Compare-With", "BERT:Method"], ["Transformer stack:Method", "Part-Of", "BERT:Method"]]}
{"doc_id": "207880647", "sentence": "That is , GPT - 2 incorporates the Decoder stack of the Transformer architecture , whereas BERT consists of the Transformer Encoder stack .", "ner": [["GPT - 2", "Method"], ["Decoder stack", "Method"], ["Transformer", "Method"], ["BERT", "Method"], ["Transformer Encoder stack", "Method"]], "rel": [["Decoder stack", "Part-Of", "GPT - 2"], ["Transformer", "Part-Of", "GPT - 2"], ["Decoder stack", "Part-Of", "Transformer"], ["Transformer Encoder stack", "Part-Of", "BERT"]], "rel_plus": [["Decoder stack:Method", "Part-Of", "GPT - 2:Method"], ["Transformer:Method", "Part-Of", "GPT - 2:Method"], ["Decoder stack:Method", "Part-Of", "Transformer:Method"], ["Transformer Encoder stack:Method", "Part-Of", "BERT:Method"]]}
{"doc_id": "207880647", "sentence": "For question generation with GPT - 2 , we follow the standard strategy for text generation as proposed in the original paper ( Radford et al. , 2 0 1 9 ) .", "ner": [["question generation", "Task"], ["GPT - 2", "Method"], ["text generation", "Task"]], "rel": [["GPT - 2", "Used-For", "question generation"]], "rel_plus": [["GPT - 2:Method", "Used-For", "question generation:Task"]]}
{"doc_id": "207880647", "sentence": "However , in order to be tailored to the specifics of questions a number of extensions have to be GPT - 2 BERT ? ? ? ?", "ner": [["GPT - 2", "Method"], ["BERT", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "207880647", "sentence": "For example , in OpenAI GPT , the authors use a left - to - right architecture , where every token can only attended to previous tokens in the self - attention Figure 2 : Overview of the fine - tuning of the approach : Given a SQuAD context and an annotated answer ( blue box ) , a question is generated using GPT - 2 .", "ner": [["GPT", "Method"], ["SQuAD", "Dataset"], ["GPT - 2", "Method"]], "rel": [["GPT - 2", "Used-For", "SQuAD"]], "rel_plus": [["GPT - 2:Method", "Used-For", "SQuAD:Dataset"]]}
{"doc_id": "207880647", "sentence": "The SQuAD context endowed with the generated question is given to the pre - trained BERT network .", "ner": [["SQuAD", "Dataset"], ["BERT", "Method"]], "rel": [["BERT", "Used-For", "SQuAD"]], "rel_plus": [["BERT:Method", "Used-For", "SQuAD:Dataset"]]}
{"doc_id": "207880647", "sentence": "If BERT is unable to provide the correct answer , the language model 's loss is backpropagated to GPT - 2 w.r.t . the annotated context . ( Best viewed in color ) ( Radford et al. , 2 0 1 9 ) 2 4 made .", "ner": [["BERT", "Method"], ["GPT - 2", "Method"]], "rel": [["BERT", "Compare-With", "GPT - 2"]], "rel_plus": [["BERT:Method", "Compare-With", "GPT - 2:Method"]]}
{"doc_id": "207880647", "sentence": "Specifically , as GPT - 2 is trained for general text generation a fine - tuning stage has to be included , which allows for the conditional generation of questions given an annotated possible answer .", "ner": [["GPT - 2", "Method"], ["text generation", "Task"]], "rel": [["GPT - 2", "Used-For", "text generation"]], "rel_plus": [["GPT - 2:Method", "Used-For", "text generation:Task"]]}
{"doc_id": "207880647", "sentence": "However , in order to boost the performance with increased diversity in generation output , we have a subsequent downstream optimization step that ties together question generation with a QA module .", "ner": [["question generation", "Task"], ["QA module", "Method"]], "rel": [["QA module", "Used-For", "question generation"]], "rel_plus": [["QA module:Method", "Used-For", "question generation:Task"]]}
{"doc_id": "207880647", "sentence": "The models trained to this point consist of a rudimentary GPT - 2 network question generation and a BERT network for question answering , respectively .", "ner": [["GPT - 2", "Method"], ["question generation", "Task"], ["BERT", "Method"], ["question answering", "Task"]], "rel": [["GPT - 2", "Used-For", "question generation"], ["BERT", "Used-For", "question answering"]], "rel_plus": [["GPT - 2:Method", "Used-For", "question generation:Task"], ["BERT:Method", "Used-For", "question answering:Task"]]}
{"doc_id": "207880647", "sentence": "The underlying idea is to exploit the duality of the tasks in order to increase the diversity of the answer generation , specifically capitalizing on the QA power of BERT .", "ner": [["QA", "Task"], ["BERT", "Method"]], "rel": [["BERT", "Used-For", "QA"]], "rel_plus": [["BERT:Method", "Used-For", "QA:Task"]]}
{"doc_id": "207880647", "sentence": "Thereby , the QA module is employed statically for the task of question answering , whereas GPT - 2 's task is to generate questions which are improved over time .", "ner": [["QA module", "Method"], ["question answering", "Task"], ["GPT - 2", "Method"], ["generate questions", "Task"]], "rel": [["QA module", "Used-For", "question answering"], ["GPT - 2", "Used-For", "generate questions"]], "rel_plus": [["QA module:Method", "Used-For", "question answering:Task"], ["GPT - 2:Method", "Used-For", "generate questions:Task"]]}
{"doc_id": "207880647", "sentence": "That is , backpropagation is only performed w.r.t . weights of the QG module , namely the GPT - 2 language model weights , whereas the weights of QA remain unchanged .", "ner": [["QG module", "Method"], ["GPT - 2", "Method"], ["QA", "Task"]], "rel": [["GPT - 2", "Used-For", "QG module"]], "rel_plus": [["GPT - 2:Method", "Used-For", "QG module:Method"]]}
{"doc_id": "207880647", "sentence": "The BERT QA network then in turn generates an answer span , which is compared with the groundtruth .", "ner": [["BERT QA", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "207880647", "sentence": "The first part ( SP 2 ) is used for supervised pre - training of the QG and QA models .", "ner": [["QG", "Task"], ["QA", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "207880647", "sentence": "Initialization of the model entails pre - training of both BERT and GPT - 2 .", "ner": [["BERT", "Method"], ["GPT - 2", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "207880647", "sentence": "Whereas BERT is finetuned for the task of question answering , GPT - 2 is fine - tune for question generation .", "ner": [["BERT", "Method"], ["question answering", "Task"], ["GPT - 2", "Method"], ["question generation", "Task"]], "rel": [["BERT", "Used-For", "question answering"], ["GPT - 2", "Used-For", "question generation"]], "rel_plus": [["BERT:Method", "Used-For", "question answering:Task"], ["GPT - 2:Method", "Used-For", "question generation:Task"]]}
{"doc_id": "207880647", "sentence": "Questions generated also have higher quality than using the vanilla GPT - 2 language model , suggesting that learning from BERT in the feedback loop provides further language cues , which may be attributed to the strength of the context - specific Gen + GAN ( Ganin and Lempitsky , 2 0 1 5 ) 0. 6 1 1 0 0. 5 5 9 0 0. 4 0 4 4 0. 5 Gen + dual ( He et al. , 2 0 1 6 ) 0. 6 3 6 8 0. 5 7 4 6 0. 4 1 6 3 0. 5 Gen + domain ( Yang et al. , 2 0 1 7 ) 0. 6 3 7 8 0. 5 8 2 6 0. 4 2 6 1 0. 5 Gen + domain + adv ( Yang et al. , 2 0 1 7 ) 0. 6 3 7 5 0. 5 8 3 1 0. 4 2 6 7 0. 5 Our Proposed Method 0. 8 1 8 5 0. 7 5 6 4 0. 6 0 5 6 0. 9", "ner": [["GPT - 2", "Method"], ["BERT", "Method"], ["Gen + GAN", "Method"], ["Gen + dual", "Method"], ["Gen + domain", "Method"], ["Gen + domain + adv", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "207880647", "sentence": "Gen + GAN ( Ganin and Lempitsky , 2 0 1 5 ) 0. 6 3 9 6 0. 5 8 7 4 0. 4 3 1 7 0. 9 Gen + dual ( He et al. , 2 0 1 6 ) 0. 6 5 1 1 0. 5 8 9 2 0. 4 3 4 0 0. 9 Gen + domain ( Yang et al. , 2 0 1 7 ) 0. 6 6 1 1 0. 6 1 0 2 0. 4 5 7 3 0. 9 Gen + domain + adv ( Yang et al. , 2 0 1 7 ) 0. 6 5 8 5 0. 6 0 4 3 0. 4 4 9 7 0. 9 Our Proposed Method 0. 8 4 0 9 0. 7 7 5 5 0. 6 2 8 2 Table 2 : Performance with various labeling rates and methods with unlabeled dataset comprising 5 0 k samples . \" Dev \" denotes the development set , and \" Test \" denotes the test set , with exact measure ( EM ) and F 1 metric .", "ner": [["Gen + GAN", "Method"], ["Gen + dual", "Method"], ["Gen + domain", "Method"], ["Gen + domain + adv", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "207880647", "sentence": "Results from all approaches apart from the proposed one are taken from ( Yang et al. , 2 0 1 7 ) . embeddings of BERT that allows for establishing complex relationships in sentences as well as rich semantic representation that can be exploited by QA .", "ner": [["BERT", "Method"], ["QA", "Task"]], "rel": [["BERT", "Used-For", "QA"]], "rel_plus": [["BERT:Method", "Used-For", "QA:Task"]]}
{"doc_id": "207880647", "sentence": "Method EM F 1 Supervised ( Upper - bound ) 7 9 . 6 0 8 7 . 3 0 LM - init ( Radford et al. , 2 0 1 9 ) 6 7 . 5 1 7 7 . 1 5 Our Method ( GPT - 2 ) 7 0 . 6 1 7 9 . 7 3 Our Method ( BERT ) 7 5 . 3 7 8 4 . 4 2 Table 3 : Question answering performance on SQuAD 1. 1 ( SP 1 ) , with exact measure ( EM ) and F 1 metric .", "ner": [["GPT - 2", "Method"], ["BERT", "Method"], ["Question answering", "Task"], ["SQuAD 1. 1", "Dataset"]], "rel": [["SQuAD 1. 1", "Benchmark-For", "Question answering"]], "rel_plus": [["SQuAD 1. 1:Dataset", "Benchmark-For", "Question answering:Task"]]}
{"doc_id": "207880647", "sentence": "Our Method ( BERT ) denotes the proposed approach using GPT - 2 for question generation as well as BERT as question answering .", "ner": [["BERT", "Method"], ["GPT - 2", "Method"], ["question generation", "Task"], ["BERT", "Method"], ["question answering", "Task"]], "rel": [["GPT - 2", "Used-For", "question generation"], ["BERT", "Used-For", "question answering"]], "rel_plus": [["GPT - 2:Method", "Used-For", "question generation:Task"], ["BERT:Method", "Used-For", "question answering:Task"]]}
{"doc_id": "207880647", "sentence": "Our Method ( GPT - 2 ) denotes the approach employing GPT - 2 for question generation as well as the modified GPT - 2 QA module as discussed in the section dealing with the ablation study of the QA component .", "ner": [["GPT - 2", "Method"], ["GPT - 2", "Method"], ["question generation", "Task"], ["GPT - 2 QA", "Method"], ["QA", "Task"]], "rel": [["GPT - 2", "Used-For", "question generation"], ["GPT - 2 QA", "Used-For", "QA"]], "rel_plus": [["GPT - 2:Method", "Used-For", "question generation:Task"], ["GPT - 2 QA:Method", "Used-For", "QA:Task"]]}
{"doc_id": "207880647", "sentence": "Scores such as BLEU and Table 4 : Question answering performance on SQuAD 1. 1 ( all ) , with exact measure ( EM ) and F 1 .", "ner": [["Question answering", "Task"], ["SQuAD 1. 1", "Dataset"]], "rel": [["SQuAD 1. 1", "Benchmark-For", "Question answering"]], "rel_plus": [["SQuAD 1. 1:Dataset", "Benchmark-For", "Question answering:Task"]]}
{"doc_id": "207880647", "sentence": "Our Method denotes to the BERT QA model trained on entire training set of SQuAD , but half the training data is fully supervised ( SP 2 ) , but the other half ( SP 1 ) is generated by our proposed method , as discussed in the section dealing with quantitative evaluation using surrogate measure .", "ner": [["BERT QA", "Method"], ["SQuAD", "Dataset"]], "rel": [["BERT QA", "Trained-With", "SQuAD"]], "rel_plus": [["BERT QA:Method", "Trained-With", "SQuAD:Dataset"]]}
{"doc_id": "207880647", "sentence": "Colored text denotes question generated with different language models . \" GPT - 2 LM \" corresponds to the GPT - 2 LM fine - tuned for question generation without optimization . \" BERT - Feedback \" corresponds to the approach using BERT as QA feedback module during training . \" GPT - 2 Feedback \" corresponds to the approach employing GPT - 2 as feedback mechanism . \" GT \" stands for groundtruth .", "ner": [["GPT - 2 LM", "Method"], ["GPT - 2 LM", "Method"], ["question generation", "Task"], ["BERT - Feedback", "Method"], ["BERT", "Method"], ["QA", "Task"], ["GPT - 2 Feedback", "Method"], ["GPT - 2", "Method"]], "rel": [["GPT - 2 LM", "Used-For", "question generation"], ["BERT", "Part-Of", "BERT - Feedback"], ["BERT", "Used-For", "QA"], ["GPT - 2", "Part-Of", "GPT - 2 Feedback"]], "rel_plus": [["GPT - 2 LM:Method", "Used-For", "question generation:Task"], ["BERT:Method", "Part-Of", "BERT - Feedback:Method"], ["BERT:Method", "Used-For", "QA:Task"], ["GPT - 2:Method", "Part-Of", "GPT - 2 Feedback:Method"]]}
{"doc_id": "207880647", "sentence": "Motivated by this , we introduced to train a QA system on generated questions by QG system , and utilize the performance of the former model as a surrogate measure for the latter one .", "ner": [["QA system", "Method"], ["QG system", "Method"]], "rel": [["QG system", "Used-For", "QA system"]], "rel_plus": [["QG system:Method", "Used-For", "QA system:Method"]]}
{"doc_id": "207880647", "sentence": "For this purpose , we train a BERT QA model on generated questions , as well as combination of generated questions and groundtruth questions .", "ner": [["BERT QA", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "207880647", "sentence": "Specifically , in terms of BERT , this implies a widening of the semantic spectrum of the contextspecific embedding driving QA model .", "ner": [["BERT", "Method"], ["QA", "Task"]], "rel": [["BERT", "Used-For", "QA"]], "rel_plus": [["BERT:Method", "Used-For", "QA:Task"]]}
{"doc_id": "207880647", "sentence": "As can be seen in Tab . 3 , the performance in question answering using generated questions using BERT in the feedback loop almost reaches groundtruth benchmark performance .", "ner": [["question answering", "Task"], ["BERT", "Method"]], "rel": [["BERT", "Used-For", "question answering"]], "rel_plus": [["BERT:Method", "Used-For", "question answering:Task"]]}
{"doc_id": "207880647", "sentence": "This is followed by using GPT - 2 in the feedback loop and by a large margin GPT - 2 language model directly for question generation .", "ner": [["GPT - 2", "Method"], ["GPT - 2", "Method"], ["question generation", "Task"]], "rel": [["GPT - 2", "Used-For", "question generation"]], "rel_plus": [["GPT - 2:Method", "Used-For", "question generation:Task"]]}
{"doc_id": "207880647", "sentence": "To better analyze the power of our question generation in terms of semantic diversity , we provide additional groundtruth data for learning the QA model .", "ner": [["question generation", "Task"], ["QA", "Task"]], "rel": [["question generation", "Used-For", "QA"]], "rel_plus": [["question generation:Task", "Used-For", "QA:Task"]]}
{"doc_id": "207880647", "sentence": "For that , a BERT QA model is trained on entire training set of SQuAD , but half the training data is fully supervised ( i.e. , contain question / answer pairs ) , but the other half is not annotated with the questions , and we use our proposed method to generate question for the latter part .", "ner": [["BERT QA", "Method"], ["SQuAD", "Dataset"]], "rel": [["BERT QA", "Trained-With", "SQuAD"]], "rel_plus": [["BERT QA:Method", "Trained-With", "SQuAD:Dataset"]]}
{"doc_id": "207880647", "sentence": "The proposed approach employs BERT as QA component for co - training the QG component .", "ner": [["BERT", "Method"], ["QA", "Task"], ["QG", "Task"]], "rel": [["BERT", "Used-For", "QA"], ["QA", "Used-For", "QG"]], "rel_plus": [["BERT:Method", "Used-For", "QA:Task"], ["QA:Task", "Used-For", "QG:Task"]]}
{"doc_id": "207880647", "sentence": "In this section we analyze the effect of the type of QA system in collaboration with QG system .", "ner": [["QA", "Task"], ["QG", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "207880647", "sentence": "Specifically , we replace BERT with a variant of GPT - 2 that emulates BERT .", "ner": [["BERT", "Method"], ["GPT - 2", "Method"], ["BERT", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "207880647", "sentence": "In order achieve question answering emulating span regression in BERT - like fashion , the GPT - 2 architecture has to be altered .", "ner": [["question answering", "Task"], ["BERT", "Method"], ["GPT - 2", "Method"]], "rel": [["BERT", "Used-For", "question answering"], ["GPT - 2", "Used-For", "question answering"]], "rel_plus": [["BERT:Method", "Used-For", "question answering:Task"], ["GPT - 2:Method", "Used-For", "question answering:Task"]]}
{"doc_id": "207880647", "sentence": "Table 3 shows the results of using the proposed approach that employs BERT as QA module during training vs. using the modified GPT - 2 QA variant .", "ner": [["BERT", "Method"], ["QA", "Task"], ["GPT - 2 QA", "Method"]], "rel": [["BERT", "Used-For", "QA"], ["BERT", "Compare-With", "GPT - 2 QA"]], "rel_plus": [["BERT:Method", "Used-For", "QA:Task"], ["BERT:Method", "Compare-With", "GPT - 2 QA:Method"]]}
{"doc_id": "207880647", "sentence": "In both cases ( a separate ) BERT QA module was used as surrogate evaluation metric as introduced in the previous subsection .", "ner": [["BERT QA", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "207880647", "sentence": "The results suggest that using BERT leads to a much better performance than GPT - 2 QA .", "ner": [["BERT", "Method"], ["GPT - 2 QA", "Method"]], "rel": [["BERT", "Compare-With", "GPT - 2 QA"]], "rel_plus": [["BERT:Method", "Compare-With", "GPT - 2 QA:Method"]]}
{"doc_id": "207880647", "sentence": "In contrast to that BERT , with its context - specific embeddings allows for robust and reliable QA .", "ner": [["BERT", "Method"], ["QA", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "207880647", "sentence": "For that , we leverage question generation by tying together GPT - 2 and BERT in end - to - end trainable fashion facilitating semi - supervised learning .", "ner": [["question generation", "Task"], ["GPT - 2", "Method"], ["BERT", "Method"], ["semi - supervised learning", "Method"]], "rel": [["semi - supervised learning", "Used-For", "GPT - 2"], ["semi - supervised learning", "Used-For", "BERT"]], "rel_plus": [["semi - supervised learning:Method", "Used-For", "GPT - 2:Method"], ["semi - supervised learning:Method", "Used-For", "BERT:Method"]]}
{"doc_id": "207880647", "sentence": "BERT as QA module in the feedback loop model shows best performance , which may be attributed to the bi - directional context specific em - beddings leveraging a powerful feedback mechanism .", "ner": [["BERT", "Method"], ["QA", "Task"]], "rel": [["BERT", "Used-For", "QA"]], "rel_plus": [["BERT:Method", "Used-For", "QA:Task"]]}
{"doc_id": "207880647", "sentence": "Additionally , as the BLEU and similar scores are weak metric for assessing generative power , we proposed to use BERT QA as a surrogate measure for assessing question generation quality .", "ner": [["BERT QA", "Method"], ["question generation", "Task"]], "rel": [["BERT QA", "Used-For", "question generation"]], "rel_plus": [["BERT QA:Method", "Used-For", "question generation:Task"]]}
{"doc_id": "199668978", "sentence": "In real - world question - answering ( QA ) systems , ill - formed questions , such as wrong words , ill word order , and noisy expressions , are common and may prevent the QA systems from understanding and answering them accurately .", "ner": [["question - answering", "Task"], ["QA", "Task"], ["QA", "Task"]], "rel": [["QA", "Synonym-Of", "question - answering"]], "rel_plus": [["QA:Task", "Synonym-Of", "question - answering:Task"]]}
{"doc_id": "199668978", "sentence": "To improve the quality and retrieval performance of the generated questions , we make two major improvements : 1 ) To better encode the semantics of ill - formed questions , we enrich the representation of questions with character embedding and the recent proposed contextual word embedding such as BERT , besides the traditional context - free word embeddings ; 2 ) To make it capable to generate desired questions , we train the model with deep reinforcement learning techniques that considers an appropriate wording of the generation as an immediate reward and the correlation between generated question and answer as time - delayed long - term rewards .", "ner": [["character embedding", "Method"], ["contextual word embedding", "Method"], ["BERT", "Method"], ["context - free word embeddings", "Method"], ["deep reinforcement learning", "Method"]], "rel": [["BERT", "SubClass-Of", "contextual word embedding"]], "rel_plus": [["BERT:Method", "SubClass-Of", "contextual word embedding:Method"]]}
{"doc_id": "199668978", "sentence": "However , due to factors such as thoughtless questions from users , misoperations of keyboard input and ASR ( automatic speech recognizer ) error , the ill - formed questions asked by users are usually expressed with vagueness , ambiguity , noises and errors .", "ner": [["ASR", "Method"], ["automatic speech recognizer", "Method"]], "rel": [["ASR", "Synonym-Of", "automatic speech recognizer"]], "rel_plus": [["ASR:Method", "Synonym-Of", "automatic speech recognizer:Method"]]}
{"doc_id": "199668978", "sentence": "Directly using ill - formed questions to search for answers in a retrieval based QA systems [ 9 ] will hurt the downstream steps , e.g. , answer selection [ 3 3 ] and hence compromise QA systems ' e ectiveness .", "ner": [["retrieval based QA systems", "Method"], ["answer selection", "Task"], ["QA systems", "Method"]], "rel": [["retrieval based QA systems", "Used-For", "answer selection"]], "rel_plus": [["retrieval based QA systems:Method", "Used-For", "answer selection:Task"]]}
{"doc_id": "199668978", "sentence": "We can see that the task is complex since it contains the following subtasks : 1 ) word correction , e.g. , correct \" defenition \" to \" de nition \" ; 2 ) word reorder , e.g. , ill words order example in Table 1 ; 3 ) sentence simpli cation , e.g. , remove the redundant expression like \" based on tiresias prediction \" in noisy background example in Table 1 .", "ner": [["word correction", "Task"], ["word reorder", "Task"], ["sentence simpli cation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "199668978", "sentence": "Inspired by the successful usage of sequence - to - sequence ( Seq 2 Seq ) model [ 3 1 ] on related tasks such as machine translation [ 5 ] , text summarization [ 2 1 ] , and sentence simpli cation [ 3 9 ] , it is promising to use it in the question re nement task .", "ner": [["sequence - to - sequence", "Method"], ["Seq 2 Seq", "Method"], ["machine translation", "Task"], ["text summarization", "Task"], ["sentence simpli cation", "Task"]], "rel": [["Seq 2 Seq", "Synonym-Of", "sequence - to - sequence"], ["sequence - to - sequence", "Used-For", "machine translation"], ["sequence - to - sequence", "Used-For", "text summarization"], ["sequence - to - sequence", "Used-For", "sentence simpli cation"]], "rel_plus": [["Seq 2 Seq:Method", "Synonym-Of", "sequence - to - sequence:Method"], ["sequence - to - sequence:Method", "Used-For", "machine translation:Task"], ["sequence - to - sequence:Method", "Used-For", "text summarization:Task"], ["sequence - to - sequence:Method", "Used-For", "sentence simpli cation:Task"]]}
{"doc_id": "199668978", "sentence": "Seq 2 Seq model is exible enough to encode patterns for sequence transformation such as word correction , word recorder , and sentence simpli cation , if there are appropriate training datasets .", "ner": [["Seq 2 Seq", "Method"], ["word correction", "Task"], ["word recorder", "Task"], ["sentence simpli cation", "Task"]], "rel": [["Seq 2 Seq", "Used-For", "word correction"], ["Seq 2 Seq", "Used-For", "word recorder"], ["Seq 2 Seq", "Used-For", "sentence simpli cation"]], "rel_plus": [["Seq 2 Seq:Method", "Used-For", "word correction:Task"], ["Seq 2 Seq:Method", "Used-For", "word recorder:Task"], ["Seq 2 Seq:Method", "Used-For", "sentence simpli cation:Task"]]}
{"doc_id": "199668978", "sentence": "To overcome these problems , we develop a Seq 2 Seq model for question re nement called QREFINE .", "ner": [["Seq 2 Seq", "Method"], ["QREFINE", "Method"]], "rel": [["QREFINE", "SubClass-Of", "Seq 2 Seq"]], "rel_plus": [["QREFINE:Method", "SubClass-Of", "Seq 2 Seq:Method"]]}
{"doc_id": "199668978", "sentence": "In order to solve the low data e ciency and unrobust policy problems on the traditional policy gradient method , we use advanced policy gradient method proximal policy optimization ( PPO ) [ 3 4 ] for well - formed question generation [ 2 9 , 3 4 ] .", "ner": [["policy gradient method", "Method"], ["policy gradient method proximal policy optimization", "Method"], ["PPO", "Method"], ["question generation", "Task"]], "rel": [["PPO", "Synonym-Of", "policy gradient method proximal policy optimization"], ["policy gradient method proximal policy optimization", "Used-For", "question generation"]], "rel_plus": [["PPO:Method", "Synonym-Of", "policy gradient method proximal policy optimization:Method"], ["policy gradient method proximal policy optimization:Method", "Used-For", "question generation:Task"]]}
{"doc_id": "199668978", "sentence": "Re nement The Seq 2 Seq model adopts an encoder - decoder framework that learns to encode an ill - formed question x into a xed - length vector representation and to decode the xed - length vector back into a variable - length well - formed question y. From a probabilistic perspective , Seq 2 Seq model is a general method that learns the conditional distribution over a variable - length sequence conditioned on another variable - length sequence , namely , p lm ( 1 , ... , M |x 1 , ... , x N ) .", "ner": [["Seq 2 Seq", "Method"], ["encoder - decoder", "Method"], ["Seq 2 Seq", "Method"]], "rel": [["encoder - decoder", "Part-Of", "Seq 2 Seq"]], "rel_plus": [["encoder - decoder:Method", "Part-Of", "Seq 2 Seq:Method"]]}
{"doc_id": "199668978", "sentence": "Since LSTM [ 1 3 ] is good at learning longterm dependencies in the data [ 1 1 ] , we adopt LSTM to sequentially encode each word of ill - formed question x. As the LSTM reads each word , the hidden state of the LSTM is updated h n = LSTM encoder ( h n\u2212 1 , x n ) .", "ner": [["LSTM", "Method"], ["LSTM", "Method"], ["LSTM", "Method"], ["LSTM", "Method"], ["LSTM encoder", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "199668978", "sentence": "The decoder can be another LSTM which is trained to generate the current hidden state k m based on the current word m and the previous hidden state k m\u2212 1 :   where c m is a weighted sum of the hidden states of the ill - formed question x : The attention score \u03b1 nm between the n - th ill - formed question hidden unit h n and m - th well - formed question hidden unit k m is calculated as follows : Formally , the Seq 2 Seq model is formed as below : |V | is the output vocabulary size and d is the hidden unit size .", "ner": [["LSTM", "Method"], ["Seq 2 Seq", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "199668978", "sentence": "Two types of rewards , wording reward , and answer correlation reward , are received and the advanced policy gradient method PPO is used to update the agent .", "ner": [["policy gradient method", "Method"], ["PPO", "Method"]], "rel": [["PPO", "SubClass-Of", "policy gradient method"]], "rel_plus": [["PPO:Method", "SubClass-Of", "policy gradient method:Method"]]}
{"doc_id": "199668978", "sentence": "After that , we present the details of the processes for generating accurate and consistent questions by leveraging the REINFORCE and PPO method .", "ner": [["generating accurate and consistent questions", "Task"], ["REINFORCE", "Method"], ["PPO", "Method"]], "rel": [["REINFORCE", "Used-For", "generating accurate and consistent questions"], ["PPO", "Used-For", "generating accurate and consistent questions"]], "rel_plus": [["REINFORCE:Method", "Used-For", "generating accurate and consistent questions:Task"], ["PPO:Method", "Used-For", "generating accurate and consistent questions:Task"]]}
{"doc_id": "199668978", "sentence": "The widely - used context - free models such as Skip - gram [ 1 9 ] or GloVe [ 2 6 ] can not consider the correlation between words .", "ner": [["context - free models", "Method"], ["Skip - gram", "Method"], ["GloVe", "Method"]], "rel": [["Skip - gram", "SubClass-Of", "context - free models"], ["GloVe", "SubClass-Of", "context - free models"]], "rel_plus": [["Skip - gram:Method", "SubClass-Of", "context - free models:Method"], ["GloVe:Method", "SubClass-Of", "context - free models:Method"]]}
{"doc_id": "199668978", "sentence": "Characters are embedded into vectors , which can be considered as 1D inputs to the Bidirectional Long Shortterm Memory Network ( BI - LSTM ) [ 1 3 ] , and the hidden layer of the last LSTM unit is the xed - size vector of each word .", "ner": [["Bidirectional Long Shortterm Memory Network", "Method"], ["BI - LSTM", "Method"], ["LSTM", "Method"]], "rel": [["BI - LSTM", "Synonym-Of", "Bidirectional Long Shortterm Memory Network"]], "rel_plus": [["BI - LSTM:Method", "Synonym-Of", "Bidirectional Long Shortterm Memory Network:Method"]]}
{"doc_id": "199668978", "sentence": "The reward for system output y is the weighted sum of two types of rewards aimed at achieving well - formed readable and answer correlation question : wording rewards on the word - level from the Reward RNN and BERT , which aims to measure how well each generated word is in line with the language model ( LM ) rule , and the question - level answer correlation reward that has the ability to infer the correlation of the re ned question to its answer , even if it is not generating until the end of the well - formed question .", "ner": [["RNN", "Method"], ["BERT", "Method"], ["language model", "Method"], ["LM", "Method"]], "rel": [["LM", "Synonym-Of", "language model"]], "rel_plus": [["LM:Method", "Synonym-Of", "language model:Method"]]}
{"doc_id": "199668978", "sentence": "BERT pre - trained on the large dataset like Wikipedia could give the contextual wording reward r B ( t ) = p B ( t | 1 , ... , M ) , which is the probability of word t given by BERT model .", "ner": [["BERT", "Method"], ["Wikipedia", "Dataset"], ["BERT", "Method"]], "rel": [["BERT", "Trained-With", "Wikipedia"]], "rel_plus": [["BERT:Method", "Trained-With", "Wikipedia:Dataset"]]}
{"doc_id": "199668978", "sentence": "Moreover , for the domain - speci c , we also use the decoder of the pre - trained Seq 2 Seq module as a trained LM Reward RNN which is able to score the probability of the next word given the words generated so far .", "ner": [["decoder", "Method"], ["Seq 2 Seq", "Method"], ["LM Reward RNN", "Method"]], "rel": [["decoder", "Part-Of", "Seq 2 Seq"], ["decoder", "SubClass-Of", "LM Reward RNN"]], "rel_plus": [["decoder:Method", "Part-Of", "Seq 2 Seq:Method"], ["decoder:Method", "SubClass-Of", "LM Reward RNN:Method"]]}
{"doc_id": "199668978", "sentence": "Following the similar ranking loss in [ 1 0 , 3 3 ] , the answer correlation module de nes the training objective as a hinge loss :   LSTM a ( a ) ) + sim(LSTM q ( y ) , LSTM a ( a ) ) } , where we use two separate LSTMs , LSTM q and LSTM a , to encode the question and answer to the vector representation .", "ner": [["hinge loss", "Method"], ["LSTM a ( a ) )", "Method"], ["sim(LSTM q ( y )", "Method"], ["LSTM a ( a )", "Method"], ["LSTMs", "Method"], ["LSTM q", "Method"], ["LSTM a", "Method"]], "rel": [["LSTM a", "Part-Of", "LSTMs"], ["LSTM q", "Part-Of", "LSTMs"]], "rel_plus": [["LSTM a:Method", "Part-Of", "LSTMs:Method"], ["LSTM q:Method", "Part-Of", "LSTMs:Method"]]}
{"doc_id": "199668978", "sentence": "Furthermore , sim(LSTM q ( x ) , LSTM a ( a ) ) = LSTM q (x)W sim LSTM a ( a ) T computes a bi - linear term between the question and its correct answer .", "ner": [["sim(LSTM q ( x ) , LSTM a ( a ) )", "Method"], ["LSTM q (x)W", "Method"], ["LSTM a ( a )", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "199668978", "sentence": "We train the model by maximizing answer correlation reward r ac using ground - truth well - formed and ill - formed questions to learn the weight of LSTM q , LSTM a network and W sim .", "ner": [["LSTM q", "Method"], ["LSTM", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "199668978", "sentence": "Due to the high dimensional action space for question re nement and high diversity of the required generation result , policy gradient method , like REINFORCE [ 4 ] are more appropriate in the question generation than value - based methods like Q - learning [ 2 0 ] .", "ner": [["policy gradient method", "Method"], ["REINFORCE", "Method"], ["question generation", "Task"], ["Q - learning", "Method"]], "rel": [["REINFORCE", "SubClass-Of", "policy gradient method"], ["REINFORCE", "Used-For", "question generation"], ["policy gradient method", "Used-For", "question generation"], ["policy gradient method", "Compare-With", "Q - learning"], ["REINFORCE", "Compare-With", "Q - learning"]], "rel_plus": [["REINFORCE:Method", "SubClass-Of", "policy gradient method:Method"], ["REINFORCE:Method", "Used-For", "question generation:Task"], ["policy gradient method:Method", "Used-For", "question generation:Task"], ["policy gradient method:Method", "Compare-With", "Q - learning:Method"], ["REINFORCE:Method", "Compare-With", "Q - learning:Method"]]}
{"doc_id": "199668978", "sentence": "The question y \u223c \u03c0 \u03b8 ( \u00b7 |x ) is generated according to \u03c0 \u03b8 where \u03b8 is the policy 's parameter and the goal is to maximize the expected reward of the reformulated question under the policy , Given reward r t at each time step t , the parameter \u03c0 of policy \u03c0 ( a seq 2 seq model ) is updated by policy gradient as follows : To compute gradients for training we use REINFORCE [ 3 6 ] , This estimator is often found to have high variance , leading to unstable training [ 1 2 ] .", "ner": [["seq 2 seq", "Method"], ["policy gradient", "Method"], ["compute gradients", "Task"], ["REINFORCE", "Method"], ["estimator", "Method"]], "rel": [["policy gradient", "Part-Of", "seq 2 seq"], ["REINFORCE", "Used-For", "compute gradients"]], "rel_plus": [["policy gradient:Method", "Part-Of", "seq 2 seq:Method"], ["REINFORCE:Method", "Used-For", "compute gradients:Task"]]}
{"doc_id": "199668978", "sentence": "We use the advanced deep reinforce method proximal policy optimization ( PPO ) [ 2 9 ] to learn a more stable policy .", "ner": [["proximal policy optimization", "Method"], ["PPO", "Method"]], "rel": [["PPO", "Synonym-Of", "proximal policy optimization"]], "rel_plus": [["PPO:Method", "Synonym-Of", "proximal policy optimization:Method"]]}
{"doc_id": "199668978", "sentence": "Proximal policy optimization ( PPO ) [ 3 4 ] is an approximation method of trust region policy optimization ( TRPO ) [ 2 8 ] .", "ner": [["Proximal policy optimization", "Method"], ["PPO", "Method"], ["trust region policy optimization", "Method"], ["TRPO", "Method"]], "rel": [["PPO", "Synonym-Of", "Proximal policy optimization"], ["TRPO", "Synonym-Of", "trust region policy optimization"], ["Proximal policy optimization", "SubClass-Of", "trust region policy optimization"]], "rel_plus": [["PPO:Method", "Synonym-Of", "Proximal policy optimization:Method"], ["TRPO:Method", "Synonym-Of", "trust region policy optimization:Method"], ["Proximal policy optimization:Method", "SubClass-Of", "trust region policy optimization:Method"]]}
{"doc_id": "199668978", "sentence": "Di erent from TRPO which uses a second - order Taylor expansion , PPO uses only a rst - order approximation , which makes PPO very e ective in RNN networks and in a wide distribution space : where \u03c0 old is the old parameters before update .", "ner": [["TRPO", "Method"], ["second - order Taylor expansion", "Method"], ["PPO", "Method"], ["rst - order approximation", "Method"], ["PPO", "Method"], ["RNN", "Method"]], "rel": [["second - order Taylor expansion", "Part-Of", "TRPO"], ["rst - order approximation", "Part-Of", "PPO"], ["PPO", "Part-Of", "RNN"]], "rel_plus": [["second - order Taylor expansion:Method", "Part-Of", "TRPO:Method"], ["rst - order approximation:Method", "Part-Of", "PPO:Method"], ["PPO:Method", "Part-Of", "RNN:Method"]]}
{"doc_id": "199668978", "sentence": "Input : Ill - formed question X , Well - formed question Y , rating data R , the number of episodes K , \u03b5 - greedy parameter \u03b5 , ratio of language model reward and answer correlation reward c 1 , the discounted factor of RL \u03bb , the threshold \u03f5 and the entropy regularization c 2 Output : the learned policy p \u03b8 1 : Initialize policy p \u03b8 and old policy p \u03b8 ol d with supervised pretrained policy p \u03b8 2 : for episode = 1 , ... , K do 3 : Uniformly pick a batch of ill - formed question u \u2208 U t r ain as the environment 4 : Start to generate the word according to p \u03b8 ol d ( i |X ) until the < EOS > token is generated , the generated sentence as Y", "ner": [["entropy regularization", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "199668978", "sentence": "Send X and Y to the BERT mechine and pretrained word embedding model to calculate the word - level reward 6 : Send X , Y and Y to the qa - lstm model to calculate the sentence - level reward , based on Eq. 7 7 : Calculate the advantage function of each time step according to Eq. 1 5 8 : repeat 9 : Update the policy p \u03b8 using Eq. 1 6 1 0 : until convergence 1 1 : Set old policy p \u03b8 ol d to policy p \u03b8 1 2 : end for A t is the expected advantage function ( the expected rewards minus a baseline like value function V ( k t ) of time k t ) which can be calculated as : To improve the exploration of our model for generating diverse yet coherent words that could constitute a better well - formed question , we use entropy regularization .", "ner": [["BERT", "Method"], ["pretrained word embedding model", "Method"], ["calculate the word - level reward", "Task"], ["entropy regularization", "Method"]], "rel": [["BERT", "Used-For", "calculate the word - level reward"], ["pretrained word embedding model", "Used-For", "calculate the word - level reward"]], "rel_plus": [["BERT:Method", "Used-For", "calculate the word - level reward:Task"], ["pretrained word embedding model:Method", "Used-For", "calculate the word - level reward:Task"]]}
{"doc_id": "199668978", "sentence": "The integrated PPO method is shown as below : The algorithm of QREFINE with PPO optimization shows in Alg . 1 Learning The training stage of traditional models su er from the exposure bias [ 2 7 ] since in testing time the ground - truth is missing and previously generated words from the trained model distribution are used to predict the next word .", "ner": [["PPO", "Method"], ["QREFINE", "Method"], ["PPO", "Method"]], "rel": [["PPO", "Part-Of", "QREFINE"]], "rel_plus": [["PPO:Method", "Part-Of", "QREFINE:Method"]]}
{"doc_id": "199668978", "sentence": "Yahoo : The Yahoo non - factoid question dataset 2 is collected from Yahoo Webscope that the questions would contain non - factoid answers in English .", "ner": [["Yahoo", "Dataset"], ["Yahoo non - factoid question", "Dataset"]], "rel": [["Yahoo non - factoid question", "Synonym-Of", "Yahoo"]], "rel_plus": [["Yahoo non - factoid question:Dataset", "Synonym-Of", "Yahoo:Dataset"]]}
{"doc_id": "199668978", "sentence": "Customer Service Userlog ( CSU ): This anonymized dataset contains online question - answering Userlog from a commercial customer service platform containing 1 million instances in chinese language .", "ner": [["Customer Service Userlog", "Dataset"], ["CSU", "Dataset"], ["online question - answering", "Task"]], "rel": [["CSU", "Synonym-Of", "Customer Service Userlog"], ["Customer Service Userlog", "Benchmark-For", "online question - answering"]], "rel_plus": [["CSU:Dataset", "Synonym-Of", "Customer Service Userlog:Dataset"], ["Customer Service Userlog:Dataset", "Benchmark-For", "online question - answering:Task"]]}
{"doc_id": "199668978", "sentence": "The compared methods are summarized as follows : Seq 2 Seq is a basic encoder - decoder sequence learning system with Luong attention [ 1 8 ] and Bi - direction LSTM on encoder model . [ 8 ] is a NMT - based question paraphrasing method which assigns higher weights to those linguistic expressions likely to yield correct answers .", "ner": [["Seq 2 Seq", "Method"], ["encoder - decoder sequence learning system", "Method"], ["Luong attention", "Method"], ["Bi - direction LSTM", "Method"], ["NMT - based question paraphrasing method", "Method"]], "rel": [["Luong attention", "Part-Of", "Seq 2 Seq"], ["Seq 2 Seq", "SubClass-Of", "encoder - decoder sequence learning system"], ["Luong attention", "Part-Of", "encoder - decoder sequence learning system"], ["Bi - direction LSTM", "Part-Of", "encoder - decoder sequence learning system"]], "rel_plus": [["Luong attention:Method", "Part-Of", "Seq 2 Seq:Method"], ["Seq 2 Seq:Method", "SubClass-Of", "encoder - decoder sequence learning system:Method"], ["Luong attention:Method", "Part-Of", "encoder - decoder sequence learning system:Method"], ["Bi - direction LSTM:Method", "Part-Of", "encoder - decoder sequence learning system:Method"]]}
{"doc_id": "199668978", "sentence": "AQA [ 6 ] is the reinforce method seeking to reformulate questions such that the QA system has the best chance of returning the correct answer in the reading comprehension task .", "ner": [["AQA", "Method"], ["QA system", "Method"], ["reading comprehension", "Task"]], "rel": [["QA system", "Used-For", "reading comprehension"], ["AQA", "Used-For", "reading comprehension"]], "rel_plus": [["QA system:Method", "Used-For", "reading comprehension:Task"], ["AQA:Method", "Used-For", "reading comprehension:Task"]]}
{"doc_id": "199668978", "sentence": "Following [ 5 ] , we use a 2 https://ciir.cs.umass.edu/downloads/nfL 6 / bidirectional LSTM as the encoder and a 4 - layer stacked LSTM with attention as the decoder .", "ner": [["bidirectional LSTM", "Method"], ["LSTM", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "199668978", "sentence": "Q -RF combines both word - level wording reward and question - level answer correlation reward and uses RE - INFORCE policy gradient based to optimize . -PPO is the proposed model using PPO and combining both word - level wording reward and question - level reward .", "ner": [["-PPO", "Method"], ["PPO", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "199668978", "sentence": "The code is available on Github . 3 On Yahoo dataset , we use the released skip - gram model word embedding [ 1 9 ] with 3 0 0 dimensions 4 .", "ner": [["Yahoo", "Dataset"], ["word embedding", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "199668978", "sentence": "And we set the word out of the vocabulary as < UNK>. We choose word embedding of 2 0 0 dimensions for CSU dataset and use the Glove model [ 2 6 ] to get the pre - trained word embedding .", "ner": [["word embedding", "Task"], ["CSU", "Dataset"], ["Glove", "Method"], ["word embedding", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "199668978", "sentence": "For BERT word embedding 5 , it gives us 7 6 8 dimensions of the word embedding on both datasets .", "ner": [["BERT word embedding", "Method"], ["word embedding", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "199668978", "sentence": "We combine contextual - free word embedding , BERT embedding and character embedding for each of word on both dataset .", "ner": [["contextual - free word embedding", "Method"], ["BERT embedding", "Method"], ["character embedding", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "199668978", "sentence": "We set the LSTM hidden state to 3 0 0 on CSU dataset and 5 0 0 on WikiAnswer dataset .", "ner": [["LSTM", "Method"], ["CSU", "Dataset"], ["WikiAnswer", "Dataset"]], "rel": [["LSTM", "Evaluated-With", "CSU"], ["LSTM", "Evaluated-With", "WikiAnswer"]], "rel_plus": [["LSTM:Method", "Evaluated-With", "CSU:Dataset"], ["LSTM:Method", "Evaluated-With", "WikiAnswer:Dataset"]]}
{"doc_id": "199668978", "sentence": "Table 4 : Question Generation Evaluation on Yahoo and CSU dataset . use the precision - based automatic metrics BLEU - 1 , BLEU - 2 , BLEU - 3 , BLEU - 4 [ 2 5 ] which measures the average n - gram precision on a set of reference sentences , with a penalty for overly short sentences , and ROUGE [ 1 7 ] based on recall and METEOR [ 3 ] that is based on both precision and recall to measure the generation results .", "ner": [["Question Generation", "Task"], ["Yahoo", "Dataset"], ["CSU", "Dataset"], ["precision - based automatic metrics", "Method"], ["BLEU - 1", "Method"], ["BLEU - 2", "Method"], ["BLEU - 3", "Method"], ["BLEU - 4", "Method"], ["ROUGE", "Method"], ["METEOR", "Method"]], "rel": [["Yahoo", "Benchmark-For", "Question Generation"], ["CSU", "Benchmark-For", "Question Generation"], ["BLEU - 1", "SubClass-Of", "precision - based automatic metrics"], ["BLEU - 2", "SubClass-Of", "precision - based automatic metrics"], ["BLEU - 3", "SubClass-Of", "precision - based automatic metrics"], ["BLEU - 4", "SubClass-Of", "precision - based automatic metrics"]], "rel_plus": [["Yahoo:Dataset", "Benchmark-For", "Question Generation:Task"], ["CSU:Dataset", "Benchmark-For", "Question Generation:Task"], ["BLEU - 1:Method", "SubClass-Of", "precision - based automatic metrics:Method"], ["BLEU - 2:Method", "SubClass-Of", "precision - based automatic metrics:Method"], ["BLEU - 3:Method", "SubClass-Of", "precision - based automatic metrics:Method"], ["BLEU - 4:Method", "SubClass-Of", "precision - based automatic metrics:Method"]]}
{"doc_id": "199668978", "sentence": "The original AQA re nes the question for the reading comprehension .", "ner": [["AQA", "Method"], ["reading comprehension", "Task"]], "rel": [["AQA", "Used-For", "reading comprehension"]], "rel_plus": [["AQA:Method", "Used-For", "reading comprehension:Task"]]}
{"doc_id": "199668978", "sentence": "We use the QA - lstm as reward , which has similar framework as PARA - NMT but using REINFORCE optimization .", "ner": [["QA - lstm", "Method"], ["PARA - NMT", "Method"], ["REINFORCE", "Method"]], "rel": [["REINFORCE", "Part-Of", "QA - lstm"], ["QA - lstm", "Compare-With", "PARA - NMT"]], "rel_plus": [["REINFORCE:Method", "Part-Of", "QA - lstm:Method"], ["QA - lstm:Method", "Compare-With", "PARA - NMT:Method"]]}
{"doc_id": "199668978", "sentence": "Therefore AQA has the same problem as PARA - NMT .", "ner": [["AQA", "Method"], ["PARA - NMT", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "199668978", "sentence": "Compared to QR - ans , QR - word has a better performance , which shows that QR - word contributes more to the readability .", "ner": [["QR - ans", "Method"], ["QR - word", "Method"], ["QR - word", "Method"]], "rel": [["QR - word", "Compare-With", "QR - ans"]], "rel_plus": [["QR - word:Method", "Compare-With", "QR - ans:Method"]]}
{"doc_id": "199668978", "sentence": "Q -PPO is higher than Q -RF , which may because PPO method can improve the high variance problem using REINFORCE method on question re nement task .", "ner": [["Q -PPO", "Method"], ["Q -RF", "Method"], ["PPO", "Method"], ["REINFORCE", "Method"], ["question re nement task", "Task"]], "rel": [["Q -PPO", "Compare-With", "Q -RF"], ["REINFORCE", "Part-Of", "PPO"], ["REINFORCE", "Used-For", "question re nement task"]], "rel_plus": [["Q -PPO:Method", "Compare-With", "Q -RF:Method"], ["REINFORCE:Method", "Part-Of", "PPO:Method"], ["REINFORCE:Method", "Used-For", "question re nement task:Task"]]}
{"doc_id": "199668978", "sentence": "To demonstrate the readability and e ectiveness of the proposed method , Table 5 shows examples of generated outputs on Yahoo and CSU dataset .", "ner": [["Yahoo", "Dataset"], ["CSU", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "199668978", "sentence": "But the question generated by other baselines can not well express the original meaning in the ill - formed question or be misleading and also have problems like repeatedly the useless words ( e.g. , Seq 2 Seq in Yahoo/Case 1 ) , no natural language sentence ( e.g. , TOQR in Yahoo/Case 1 ) and express the di erent meaning with the ill - formed question ( e.g. , AQA in Yahoo/Case 1 ) .", "ner": [["Seq 2 Seq", "Method"], ["Yahoo/Case 1", "Dataset"], ["TOQR", "Method"], ["Yahoo/Case 1", "Dataset"], ["AQA", "Method"], ["Yahoo/Case 1", "Dataset"]], "rel": [["Seq 2 Seq", "Evaluated-With", "Yahoo/Case 1"], ["TOQR", "Evaluated-With", "Yahoo/Case 1"], ["AQA", "Evaluated-With", "Yahoo/Case 1"]], "rel_plus": [["Seq 2 Seq:Method", "Evaluated-With", "Yahoo/Case 1:Dataset"], ["TOQR:Method", "Evaluated-With", "Yahoo/Case 1:Dataset"], ["AQA:Method", "Evaluated-With", "Yahoo/Case 1:Dataset"]]}
{"doc_id": "199668978", "sentence": "S 2 S+W is seq 2 seq model using word - level embedding .", "ner": [["S 2 S+W", "Method"], ["seq 2 seq", "Method"], ["word - level embedding", "Method"]], "rel": [["word - level embedding", "Part-Of", "S 2 S+W"], ["S 2 S+W", "SubClass-Of", "seq 2 seq"]], "rel_plus": [["word - level embedding:Method", "Part-Of", "S 2 S+W:Method"], ["S 2 S+W:Method", "SubClass-Of", "seq 2 seq:Method"]]}
{"doc_id": "199668978", "sentence": "S 2 S+W&C is seq 2 seq model using word - level and char - level embedding .", "ner": [["S 2 S+W&C", "Method"], ["seq 2 seq", "Method"], ["word - level and char - level embedding", "Method"]], "rel": [["word - level and char - level embedding", "Part-Of", "S 2 S+W&C"], ["S 2 S+W&C", "SubClass-Of", "seq 2 seq"]], "rel_plus": [["word - level and char - level embedding:Method", "Part-Of", "S 2 S+W&C:Method"], ["S 2 S+W&C:Method", "SubClass-Of", "seq 2 seq:Method"]]}
{"doc_id": "199668978", "sentence": "S 2 S+W&C&B is seq 2 seq model using word - level , char - level , and BERT embedding .", "ner": [["S 2 S+W&C&B", "Method"], ["seq 2 seq", "Method"], ["word - level , char - level , and BERT embedding", "Method"]], "rel": [["word - level , char - level , and BERT embedding", "Part-Of", "S 2 S+W&C&B"], ["S 2 S+W&C&B", "SubClass-Of", "seq 2 seq"]], "rel_plus": [["word - level , char - level , and BERT embedding:Method", "Part-Of", "S 2 S+W&C&B:Method"], ["S 2 S+W&C&B:Method", "SubClass-Of", "seq 2 seq:Method"]]}
{"doc_id": "199668978", "sentence": "QR - word is seq 2 seq model considering three embeddings and using word reward to train the RL model .", "ner": [["QR - word", "Method"], ["seq 2 seq", "Method"], ["RL", "Method"]], "rel": [["QR - word", "SubClass-Of", "seq 2 seq"]], "rel_plus": [["QR - word:Method", "SubClass-Of", "seq 2 seq:Method"]]}
{"doc_id": "199668978", "sentence": "QR - ans is seq 2 seq model considering three embeddings and using answer coherence reward to train the model .", "ner": [["QR - ans", "Method"], ["seq 2 seq", "Method"]], "rel": [["QR - ans", "SubClass-Of", "seq 2 seq"]], "rel_plus": [["QR - ans:Method", "SubClass-Of", "seq 2 seq:Method"]]}
{"doc_id": "199668978", "sentence": "QR - RF considers multi - grain word embedding and both word reward and sentence reward but uses REINFORCE method .", "ner": [["QR - RF", "Method"], ["multi - grain word embedding", "Method"], ["REINFORCE", "Method"]], "rel": [["REINFORCE", "Part-Of", "QR - RF"], ["multi - grain word embedding", "Part-Of", "QR - RF"]], "rel_plus": [["REINFORCE:Method", "Part-Of", "QR - RF:Method"], ["multi - grain word embedding:Method", "Part-Of", "QR - RF:Method"]]}
{"doc_id": "199668978", "sentence": "From Fig. 2 ( a ) and 2(b ) , we can see that using multi - grain word embedding can help the model better to correct the ill - formed question than just using single word embedding .", "ner": [["multi - grain word embedding", "Method"], ["single word embedding", "Method"]], "rel": [["single word embedding", "Part-Of", "multi - grain word embedding"]], "rel_plus": [["single word embedding:Method", "Part-Of", "multi - grain word embedding:Method"]]}
{"doc_id": "199668978", "sentence": "From the testing results in Table 4 , we can see that the two optimization methods have comparable performance , but PPO achieves a slightly higher BLEU - 2 score than REINFORCE .", "ner": [["PPO", "Method"], ["BLEU - 2", "Method"], ["REINFORCE", "Method"]], "rel": [["PPO", "Compare-With", "REINFORCE"]], "rel_plus": [["PPO:Method", "Compare-With", "REINFORCE:Method"]]}
{"doc_id": "199668978", "sentence": "This shows that PPO methods can improve the high variance problem of using REINFORCE , and can help the learning converge quickly .", "ner": [["PPO", "Method"], ["REINFORCE", "Method"]], "rel": [["PPO", "Compare-With", "REINFORCE"]], "rel_plus": [["PPO:Method", "Compare-With", "REINFORCE:Method"]]}
{"doc_id": "199668978", "sentence": "These methods typically accomplish the reformulation by training a recurrent neural network such as an LSTM network to predict the next word in a sentence sequence . [ 2 3 ] uses the reinforcement learning to reformulate the query to maximize the number of relevant documents retrieved .", "ner": [["recurrent neural network", "Method"], ["LSTM", "Method"], ["predict the next word", "Task"], ["reinforcement learning", "Method"]], "rel": [["LSTM", "SubClass-Of", "recurrent neural network"], ["LSTM", "Used-For", "predict the next word"], ["recurrent neural network", "Used-For", "predict the next word"]], "rel_plus": [["LSTM:Method", "SubClass-Of", "recurrent neural network:Method"], ["LSTM:Method", "Used-For", "predict the next word:Task"], ["recurrent neural network:Method", "Used-For", "predict the next word:Task"]]}
{"doc_id": "199668978", "sentence": "Active QA ( AQA ) [ 6 ] uses an agent as a mediator between the user and a black box QA system , e.g. BiDAF , to generate questions that elicit the best possible answer .", "ner": [["Active QA", "Method"], ["AQA", "Method"], ["QA system", "Method"], ["BiDAF", "Method"]], "rel": [["AQA", "Synonym-Of", "Active QA"], ["BiDAF", "SubClass-Of", "QA system"]], "rel_plus": [["AQA:Method", "Synonym-Of", "Active QA:Method"], ["BiDAF:Method", "SubClass-Of", "QA system:Method"]]}
{"doc_id": "199668978", "sentence": "Moreover , BiDAF works on the reading comprehension , the answer is the paraphrase in the context , therefore there is a great change that this model always generates the same answer , which brings another challenge for training .", "ner": [["BiDAF", "Method"], ["reading comprehension", "Task"]], "rel": [["BiDAF", "Used-For", "reading comprehension"]], "rel_plus": [["BiDAF:Method", "Used-For", "reading comprehension:Task"]]}
{"doc_id": "199668978", "sentence": "Due to the high dimensional action space for text generation and high diversity of the required generation result , policy gradient methods are more appropriate in the text generation than valuebased methods like Q - learning [ 2 0 ] .", "ner": [["text generation", "Task"], ["policy gradient methods", "Method"], ["text generation", "Task"], ["Q - learning", "Method"]], "rel": [["policy gradient methods", "Used-For", "text generation"], ["Q - learning", "Used-For", "text generation"], ["policy gradient methods", "Compare-With", "Q - learning"]], "rel_plus": [["policy gradient methods:Method", "Used-For", "text generation:Task"], ["Q - learning:Method", "Used-For", "text generation:Task"], ["policy gradient methods:Method", "Compare-With", "Q - learning:Method"]]}
{"doc_id": "199668978", "sentence": "Uses of policy gradient for QA include [ 1 6 ] , who train a semantic parser to query a knowledge base , and [ 3 0 ] who propose query reduction networks that transform a query to answer questions that involve multi - hop common sense reasoning .", "ner": [["policy gradient", "Method"], ["QA", "Method"], ["multi - hop common sense reasoning", "Task"]], "rel": [["policy gradient", "Part-Of", "QA"]], "rel_plus": [["policy gradient:Method", "Part-Of", "QA:Method"]]}
{"doc_id": "199668978", "sentence": "The o -policy method like TRPO and PPO [ 2 8 , 2 9 ] recently applied on the game like Atari .", "ner": [["o -policy method", "Method"], ["TRPO", "Method"], ["PPO", "Method"]], "rel": [["TRPO", "SubClass-Of", "o -policy method"], ["PPO", "SubClass-Of", "o -policy method"]], "rel_plus": [["TRPO:Method", "SubClass-Of", "o -policy method:Method"], ["PPO:Method", "SubClass-Of", "o -policy method:Method"]]}
{"doc_id": "199668978", "sentence": "Tuan et al [ 3 4 ] apply the o -policy gradient method to the sequence generation task and shows that PPO surpass policy gradient on stability and performance .", "ner": [["o -policy gradient method", "Method"], ["sequence generation", "Task"], ["PPO", "Method"], ["policy gradient", "Method"]], "rel": [["o -policy gradient method", "Used-For", "sequence generation"], ["PPO", "Compare-With", "policy gradient"]], "rel_plus": [["o -policy gradient method:Method", "Used-For", "sequence generation:Task"], ["PPO:Method", "Compare-With", "policy gradient:Method"]]}
{"doc_id": "199668978", "sentence": "Session : Long -Reinforcement Learning CIKM ' 1 9 , November 3 - 7 , 2 0 1 9 , Beijing , China Question re nement aims to re ne ill - formed questions , which typically includes various types of subtasks such as spelling error correction , background removal and word order re nement .", "ner": [["spelling error correction", "Task"], ["background removal", "Task"], ["word order re nement", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "202540251", "sentence": "We propose to harness the potential of simulation for the semantic segmentation of real - world self - driving scenes in a domain generalization fashion .", "ner": [["semantic segmentation", "Task"], ["domain generalization", "Method"]], "rel": [["domain generalization", "Used-For", "semantic segmentation"]], "rel_plus": [["domain generalization:Method", "Used-For", "semantic segmentation:Task"]]}
{"doc_id": "202540251", "sentence": "Extensive experiments are conducted on the generalization from GTA and SYNTHIA to Cityscapes , BDDS and Mapillary ; and our method achieves superior results over the state - of - the - art techniques .", "ner": [["GTA", "Dataset"], ["SYNTHIA", "Dataset"], ["Cityscapes", "Dataset"], ["BDDS", "Dataset"], ["Mapillary", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "202540251", "sentence": "Remarkably , our generalization results are on par with or even better than those obtained by state - of - the - art simulation - to - real domain adaptation methods , which access the target domain data at training time .   1   Simulation has spurred growing interests for training deep neural nets ( DNNs ) for computer vision tasks [ 5 3 , 1 0 , 2 3 , 5 5 ] .", "ner": [["simulation - to - real domain adaptation", "Method"], ["deep neural nets", "Method"], ["DNNs", "Method"], ["computer vision", "Task"]], "rel": [["DNNs", "Synonym-Of", "deep neural nets"], ["deep neural nets", "Used-For", "computer vision"]], "rel_plus": [["DNNs:Method", "Synonym-Of", "deep neural nets:Method"], ["deep neural nets:Method", "Used-For", "computer vision:Task"]]}
{"doc_id": "202540251", "sentence": "In the second step of our approach , we train a deep CNN for semantic segmentation with a pyramid consistency loss .", "ner": [["CNN", "Method"], ["semantic segmentation", "Task"]], "rel": [["CNN", "Used-For", "semantic segmentation"]], "rel_plus": [["CNN:Method", "Used-For", "semantic segmentation:Task"]]}
{"doc_id": "202540251", "sentence": "We now discuss some related work on semantic segmentation , domain adaptation , domain generalization , domain randomization , and data augmentation .", "ner": [["semantic segmentation", "Task"], ["domain adaptation", "Method"], ["domain generalization", "Method"], ["domain randomization", "Method"], ["data augmentation", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202540251", "sentence": "Domain Adaptation for Semantic Segmentation .", "ner": [["Domain Adaptation", "Method"], ["Semantic Segmentation", "Task"]], "rel": [["Domain Adaptation", "Used-For", "Semantic Segmentation"]], "rel_plus": [["Domain Adaptation:Method", "Used-For", "Semantic Segmentation:Task"]]}
{"doc_id": "202540251", "sentence": "Until [ 1 9 , 5 7 ] first studied the domain shift problem in semantic segmentation , most works in domain adaptation had focused on the task of image classification .", "ner": [["semantic segmentation", "Task"], ["domain adaptation", "Method"], ["image classification", "Task"]], "rel": [["domain adaptation", "Used-For", "image classification"]], "rel_plus": [["domain adaptation:Method", "Used-For", "image classification:Task"]]}
{"doc_id": "202540251", "sentence": "After that , the problem subsequently became one of the tracks in the Visual Domain Adaptation Challenge ( VisDA ) 2 0 1 7 [ 3 5 ] and started receiving increasing attention .", "ner": [["Visual Domain Adaptation Challenge", "Task"], ["VisDA", "Task"]], "rel": [["VisDA", "Synonym-Of", "Visual Domain Adaptation Challenge"]], "rel_plus": [["VisDA:Task", "Synonym-Of", "Visual Domain Adaptation Challenge:Task"]]}
{"doc_id": "202540251", "sentence": "Most of these works were inspired by the unsupervised adversarial domain adaptation approach in [ 1 3 ] which shares similar idea with generative adversarial networks .", "ner": [["unsupervised adversarial domain adaptation", "Method"], ["generative adversarial networks", "Method"]], "rel": [["unsupervised adversarial domain adaptation", "Compare-With", "generative adversarial networks"]], "rel_plus": [["unsupervised adversarial domain adaptation:Method", "Compare-With", "generative adversarial networks:Method"]]}
{"doc_id": "202540251", "sentence": "Zhang et al. [ 5 7 ] perform segmentation adaptation by aligning label distributions both globally and across superpixels in an image .", "ner": [["segmentation adaptation", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202540251", "sentence": "Recently , an unsupervised domain adaptation method has been proposed for semantic segmentation via classbalanced self - training [ 6 3 ] .", "ner": [["unsupervised domain adaptation", "Method"], ["semantic segmentation", "Task"], ["classbalanced self - training", "Method"]], "rel": [["classbalanced self - training", "Used-For", "semantic segmentation"], ["unsupervised domain adaptation", "Used-For", "semantic segmentation"]], "rel_plus": [["classbalanced self - training:Method", "Used-For", "semantic segmentation:Task"], ["unsupervised domain adaptation:Method", "Used-For", "semantic segmentation:Task"]]}
{"doc_id": "202540251", "sentence": "Domain Generalization In contrast to Domain Adaptation , where the network is tested on a known target domain , and the images in the target domain , although without labels , are accessible during the training process , Domain Generalization is tested on unseen domains [ 3 1 , 1 2 ] .", "ner": [["Domain Generalization", "Method"], ["Domain Adaptation", "Method"], ["Domain Generalization", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202540251", "sentence": "Current domain generalization researches mostly focus on the image classification problem .", "ner": [["domain generalization", "Method"], ["image classification", "Task"]], "rel": [["domain generalization", "Used-For", "image classification"]], "rel_plus": [["domain generalization:Method", "Used-For", "image classification:Task"]]}
{"doc_id": "202540251", "sentence": "DR has also been utilized to do object detection and 6D pose estimation [ 4 9 , 3 6 , 4 5 ] .", "ner": [["DR", "Method"], ["object detection", "Task"], ["6D pose estimation", "Task"]], "rel": [["DR", "Used-For", "object detection"], ["DR", "Used-For", "6D pose estimation"]], "rel_plus": [["DR:Method", "Used-For", "object detection:Task"], ["DR:Method", "Used-For", "6D pose estimation:Task"]]}
{"doc_id": "202540251", "sentence": "A straightforward method is to train a CNN segmentation model using the augmented training set .", "ner": [["CNN segmentation", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202540251", "sentence": "In our experiments , we use GTA [ 3 7 ] and SYN - THIA [ 3 8 ] as the source domains and a small subset of ImageNet [ 8 ] as well as datasets used in CycleGAN [ 6 0 ] as the auxiliary domains for \" stylizing \" the source domain images .", "ner": [["GTA", "Dataset"], ["SYN - THIA", "Dataset"], ["ImageNet", "Dataset"], ["CycleGAN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202540251", "sentence": "We consider three target domains of realworld images , whose official validation sets are used as our test sets : Cityscapes [ 5 ] , Berkeley Deep Drive Segmentation ( BDDS ) [ 5 4 ] , and Mapillary [ 3 3 ] .", "ner": [["Cityscapes", "Dataset"], ["Berkeley Deep Drive Segmentation", "Dataset"], ["BDDS", "Dataset"], ["Mapillary", "Dataset"]], "rel": [["BDDS", "Synonym-Of", "Berkeley Deep Drive Segmentation"]], "rel_plus": [["BDDS:Dataset", "Synonym-Of", "Berkeley Deep Drive Segmentation:Dataset"]]}
{"doc_id": "202540251", "sentence": "There are 1 9 classes which are compatible with other semantic segmentation datasets of outdoor scenes e.g. Cityscapes .", "ner": [["semantic segmentation", "Task"], ["Cityscapes", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "202540251", "sentence": "A subset , SYNTHIA - RAND - CITYSCAPES , is used in our experiments which contains 9, 4 0 0 images with annotations compatible with Cityscapes .", "ner": [["SYNTHIA - RAND - CITYSCAPES", "Dataset"], ["Cityscapes", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "202540251", "sentence": "The annotations contain 6 6 object classes , but only the 1 9 classes that overlap with Cityscapes and GTA are used in our experiments .", "ner": [["Cityscapes", "Dataset"], ["GTA", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "202540251", "sentence": "To select a model for a particular real - world dataset D R ( e.g. Cityscapes ) , we randomly pick up 5 0 0 images from the training set of another real - world dataset D R ( e.g. BDDS ) as the validation set .", "ner": [["Cityscapes", "Dataset"], ["BDDS", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "202540251", "sentence": "To make it easier to compare with most of other methods , we use VGG - 1 6 [ 4 4 ] , ResNet - 5 0 , and ResNet - 1 0 1 [ 1 7 ] as FCN backbones .", "ner": [["VGG - 1 6", "Method"], ["ResNet - 5 0", "Method"], ["ResNet - 1 0 1", "Method"], ["FCN", "Method"]], "rel": [["VGG - 1 6", "Part-Of", "FCN"], ["ResNet - 5 0", "Part-Of", "FCN"], ["ResNet - 1 0 1", "Part-Of", "FCN"]], "rel_plus": [["VGG - 1 6:Method", "Part-Of", "FCN:Method"], ["ResNet - 5 0:Method", "Part-Of", "FCN:Method"], ["ResNet - 1 0 1:Method", "Part-Of", "FCN:Method"]]}
{"doc_id": "202540251", "sentence": "In total , we use two sets of 1 5 auxiliary domains : A ) 1 0 from ImageNet [ 8 ] and 5 from CycleGAN [ 6 0 ] , and B ) 1 5 from ImageNet with each domain corresponding to one semantic class in Cityscapes .", "ner": [["ImageNet", "Dataset"], ["CycleGAN", "Method"], ["ImageNet", "Dataset"], ["Cityscapes", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "202540251", "sentence": "To evaluate our domain randomization method , we conduct experiments generalizing from GTA to Cityscapes , BDDS , and Mapillary with FCN 8 s - VGG 1 6 .", "ner": [["domain randomization", "Method"], ["GTA", "Dataset"], ["Cityscapes", "Dataset"], ["BDDS", "Dataset"], ["Mapillary", "Dataset"], ["FCN 8 s - VGG 1 6", "Method"]], "rel": [["FCN 8 s - VGG 1 6", "Used-For", "domain randomization"], ["FCN 8 s - VGG 1 6", "Evaluated-With", "GTA"], ["domain randomization", "Evaluated-With", "GTA"], ["FCN 8 s - VGG 1 6", "Evaluated-With", "Cityscapes"], ["domain randomization", "Evaluated-With", "Cityscapes"], ["FCN 8 s - VGG 1 6", "Evaluated-With", "BDDS"], ["domain randomization", "Evaluated-With", "BDDS"], ["FCN 8 s - VGG 1 6", "Evaluated-With", "Mapillary"], ["domain randomization", "Evaluated-With", "Mapillary"]], "rel_plus": [["FCN 8 s - VGG 1 6:Method", "Used-For", "domain randomization:Method"], ["FCN 8 s - VGG 1 6:Method", "Evaluated-With", "GTA:Dataset"], ["domain randomization:Method", "Evaluated-With", "GTA:Dataset"], ["FCN 8 s - VGG 1 6:Method", "Evaluated-With", "Cityscapes:Dataset"], ["domain randomization:Method", "Evaluated-With", "Cityscapes:Dataset"], ["FCN 8 s - VGG 1 6:Method", "Evaluated-With", "BDDS:Dataset"], ["domain randomization:Method", "Evaluated-With", "BDDS:Dataset"], ["FCN 8 s - VGG 1 6:Method", "Evaluated-With", "Mapillary:Dataset"], ["domain randomization:Method", "Evaluated-With", "Mapillary:Dataset"]]}
{"doc_id": "202540251", "sentence": "The experiments are still adapting from GTA to the 3 tests with FCN 8 s - VGG 1 6 .", "ner": [["GTA", "Dataset"], ["FCN 8 s - VGG 1 6", "Method"]], "rel": [["FCN 8 s - VGG 1 6", "Evaluated-With", "GTA"]], "rel_plus": [["FCN 8 s - VGG 1 6:Method", "Evaluated-With", "GTA:Dataset"]]}
{"doc_id": "202540251", "sentence": "Table 1 details the mIoU improvement on Cityscapes , BDDS and Mapillary by considering one more factor each time : Domain Randomization ( DR ) , Pyramid Consistency across Domains ( PCD ) and within an Image ( PCI ) .", "ner": [["Cityscapes", "Dataset"], ["BDDS", "Dataset"], ["Mapillary", "Dataset"], ["Domain Randomization", "Method"], ["DR", "Method"], ["Pyramid Consistency across Domains", "Method"], ["PCD", "Method"], ["PCI", "Method"]], "rel": [["Domain Randomization", "Evaluated-With", "Cityscapes"], ["Pyramid Consistency across Domains", "Evaluated-With", "Cityscapes"], ["Domain Randomization", "Evaluated-With", "BDDS"], ["Pyramid Consistency across Domains", "Evaluated-With", "BDDS"], ["Domain Randomization", "Evaluated-With", "Mapillary"], ["Pyramid Consistency across Domains", "Evaluated-With", "Mapillary"], ["DR", "Synonym-Of", "Domain Randomization"], ["PCD", "Synonym-Of", "Pyramid Consistency across Domains"]], "rel_plus": [["Domain Randomization:Method", "Evaluated-With", "Cityscapes:Dataset"], ["Pyramid Consistency across Domains:Method", "Evaluated-With", "Cityscapes:Dataset"], ["Domain Randomization:Method", "Evaluated-With", "BDDS:Dataset"], ["Pyramid Consistency across Domains:Method", "Evaluated-With", "BDDS:Dataset"], ["Domain Randomization:Method", "Evaluated-With", "Mapillary:Dataset"], ["Pyramid Consistency across Domains:Method", "Evaluated-With", "Mapillary:Dataset"], ["DR:Method", "Synonym-Of", "Domain Randomization:Method"], ["PCD:Method", "Synonym-Of", "Pyramid Consistency across Domains:Method"]]}
{"doc_id": "202540251", "sentence": "Cityscapes to select the model which will be evaluated on BDDS/Mapillary .", "ner": [["Cityscapes", "Dataset"], ["BDDS/Mapillary", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "202540251", "sentence": "Then we compare our method with the only known existing state - of - the - art domain generalization method for semantic segmentation IBN - Net [ 3 4 ] under the generalization setting from GTA to Cityscapes .", "ner": [["domain generalization", "Method"], ["semantic segmentation", "Task"], ["IBN - Net", "Method"], ["GTA", "Dataset"], ["Cityscapes", "Dataset"]], "rel": [["domain generalization", "Used-For", "semantic segmentation"], ["IBN - Net", "Used-For", "semantic segmentation"], ["IBN - Net", "Evaluated-With", "GTA"], ["IBN - Net", "Evaluated-With", "Cityscapes"]], "rel_plus": [["domain generalization:Method", "Used-For", "semantic segmentation:Task"], ["IBN - Net:Method", "Used-For", "semantic segmentation:Task"], ["IBN - Net:Method", "Evaluated-With", "GTA:Dataset"], ["IBN - Net:Method", "Evaluated-With", "Cityscapes:Dataset"]]}
{"doc_id": "202540251", "sentence": "IBN - Net improves domain generalization by fine - tuning the ResNet building blocks .", "ner": [["IBN - Net", "Method"], ["domain generalization", "Method"], ["ResNet", "Method"]], "rel": [["ResNet", "Part-Of", "IBN - Net"], ["IBN - Net", "Used-For", "domain generalization"]], "rel_plus": [["ResNet:Method", "Part-Of", "IBN - Net:Method"], ["IBN - Net:Method", "Used-For", "domain generalization:Method"]]}
{"doc_id": "202540251", "sentence": "Since most of the previous works conducted adaptation to Cityscapes with VGG backbone networks , we present the adaptation mIoU comparison on GTA \u2192 Cityscapes and SYNTHIA \u2192 Cityscapes in Table 4 and Table 5 , leaving class - wise comparison details in the supplementary material .", "ner": [["Cityscapes", "Dataset"], ["VGG", "Method"], ["GTA", "Dataset"], ["Cityscapes", "Dataset"], ["SYNTHIA", "Dataset"], ["Cityscapes", "Dataset"]], "rel": [["VGG", "Evaluated-With", "Cityscapes"], ["VGG", "Evaluated-With", "GTA"], ["VGG", "Evaluated-With", "SYNTHIA"]], "rel_plus": [["VGG:Method", "Evaluated-With", "Cityscapes:Dataset"], ["VGG:Method", "Evaluated-With", "GTA:Dataset"], ["VGG:Method", "Evaluated-With", "SYNTHIA:Dataset"]]}
{"doc_id": "202540251", "sentence": "In this paper , we present a domain generalization approach for generalizing semantic segmentation networks from simulation to the real world without accessing any target domain data .", "ner": [["domain generalization", "Method"], ["semantic segmentation", "Task"]], "rel": [["domain generalization", "Used-For", "semantic segmentation"]], "rel_plus": [["domain generalization:Method", "Used-For", "semantic segmentation:Task"]]}
{"doc_id": "202540251", "sentence": "Finally , we experimentally validate our method on a variety of experimental settings , and show superior performance over state - ofthe - art methods in both domain generalization and domain adaptation , which clearly demonstrates the effectiveness of our proposed method .", "ner": [["domain generalization", "Method"], ["domain adaptation", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "150374036", "sentence": "Second , we propose a novel Region Attention Network ( RAN ) , to adaptively capture the importance of facial regions for occlusion and pose variant FER .", "ner": [["Region Attention Network", "Method"], ["RAN", "Method"], ["FER", "Task"]], "rel": [["RAN", "Synonym-Of", "Region Attention Network"], ["Region Attention Network", "Used-For", "FER"]], "rel_plus": [["RAN:Method", "Synonym-Of", "Region Attention Network:Method"], ["Region Attention Network:Method", "Used-For", "FER:Task"]]}
{"doc_id": "150374036", "sentence": "The RAN aggregates and embeds varied number of region features produced by a backbone convolutional neural network into a compact fixed - length representation .", "ner": [["RAN", "Method"], ["convolutional neural network", "Method"]], "rel": [["convolutional neural network", "Part-Of", "RAN"]], "rel_plus": [["convolutional neural network:Method", "Part-Of", "RAN:Method"]]}
{"doc_id": "150374036", "sentence": "We validate our RAN and region biased loss on both our built test datasets and four popular datasets : FERPlus , AffectNet , RAF - DB , and SFEW .", "ner": [["RAN", "Method"], ["FERPlus", "Dataset"], ["AffectNet", "Dataset"], ["RAF - DB", "Dataset"], ["SFEW", "Dataset"]], "rel": [["RAN", "Evaluated-With", "FERPlus"], ["RAN", "Evaluated-With", "AffectNet"], ["RAN", "Evaluated-With", "RAF - DB"], ["RAN", "Evaluated-With", "SFEW"]], "rel_plus": [["RAN:Method", "Evaluated-With", "FERPlus:Dataset"], ["RAN:Method", "Evaluated-With", "AffectNet:Dataset"], ["RAN:Method", "Evaluated-With", "RAF - DB:Dataset"], ["RAN:Method", "Evaluated-With", "SFEW:Dataset"]]}
{"doc_id": "150374036", "sentence": "Extensive experiments show that our RAN and region biased loss largely improve the performance of FER with occlusion and variant pose .", "ner": [["RAN", "Method"], ["FER", "Task"]], "rel": [["RAN", "Used-For", "FER"]], "rel_plus": [["RAN:Method", "Used-For", "FER:Task"]]}
{"doc_id": "150374036", "sentence": "Our method also achieves state - of - the - art results on FERPlus , AffectNet , RAF - DB , and SFEW .", "ner": [["FERPlus", "Dataset"], ["AffectNet", "Dataset"], ["RAF - DB", "Dataset"], ["SFEW", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "150374036", "sentence": "Due to its potential applications in various fields , such as intelligent tutoring systems , service robots , driver fatigue monitoring , Facial Expression Recognition ( FER ) has attracted increasing attention in the computer vision community recently [ 5 ] , [ 1 7 ] , [ 2 1 ] , [ 2 8 ] , [ 3 6 ] , [ 4 8 ] .", "ner": [["Facial Expression Recognition", "Task"], ["FER", "Task"], ["computer vision", "Task"]], "rel": [["FER", "Synonym-Of", "Facial Expression Recognition"], ["Facial Expression Recognition", "SubTask-Of", "computer vision"]], "rel_plus": [["FER:Task", "Synonym-Of", "Facial Expression Recognition:Task"], ["Facial Expression Recognition:Task", "SubTask-Of", "computer vision:Task"]]}
{"doc_id": "150374036", "sentence": "Kotsia et al. [ 3 2 ] present a comprehensive analysis on occluded FER based on Gabor features and human observers , and find that an occluded mouth degrades FER more than occluded eyes on JAFFE [ 4 4 ] and CK [ 3 0 ] .", "ner": [["FER", "Task"], ["FER", "Task"], ["JAFFE", "Dataset"], ["CK", "Dataset"]], "rel": [["JAFFE", "Benchmark-For", "FER"], ["CK", "Benchmark-For", "FER"]], "rel_plus": [["JAFFE:Dataset", "Benchmark-For", "FER:Task"], ["CK:Dataset", "Benchmark-For", "FER:Task"]]}
{"doc_id": "150374036", "sentence": "Sparse representation classifier ( SRC ) is widely used for artificially - occluded FER in 2 0 1 0 s [ 1 2 ] , [ 1 3 ] , [ 7 1 ] .", "ner": [["Sparse representation classifier", "Method"], ["SRC", "Method"], ["FER", "Task"]], "rel": [["SRC", "Synonym-Of", "Sparse representation classifier"], ["Sparse representation classifier", "Used-For", "FER"]], "rel_plus": [["SRC:Method", "Synonym-Of", "Sparse representation classifier:Method"], ["Sparse representation classifier:Method", "Used-For", "FER:Task"]]}
{"doc_id": "150374036", "sentence": "With the popularity of data - driven deep learning techniques , several recent efforts on FER have been made on the collection of large - scale datasets [ 2 1 ] , [ 3 6 ] , [ 4 8 ] , and many works [ 1 ] , [ 6 ] , [ 2 9 ] , [ 3 5 ] , [ 4 0 ] , [ 5 7 ] exploit deep convolutional neural networks ( CNN ) to improve the performance of FER .", "ner": [["data - driven deep learning techniques", "Method"], ["FER", "Task"], ["deep convolutional neural networks", "Method"], ["CNN", "Method"], ["FER", "Task"]], "rel": [["data - driven deep learning techniques", "Used-For", "FER"], ["CNN", "Synonym-Of", "deep convolutional neural networks"], ["deep convolutional neural networks", "Used-For", "FER"]], "rel_plus": [["data - driven deep learning techniques:Method", "Used-For", "FER:Task"], ["CNN:Method", "Synonym-Of", "deep convolutional neural networks:Method"], ["deep convolutional neural networks:Method", "Used-For", "FER:Task"]]}
{"doc_id": "150374036", "sentence": "In practices , occlusion and pose variations can lead to unseen regions of input faces , which bring difficulties for face alignment and harm the feature extraction process .", "ner": [["face alignment", "Task"], ["feature extraction", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "150374036", "sentence": "Inspired by these facts , this paper proposes a region based deep attention architecture for pose and occlusion robust FER , which adaptively integrates visual clues from regions and whole faces .", "ner": [["region based deep attention architecture", "Method"], ["FER", "Task"]], "rel": [["region based deep attention architecture", "Used-For", "FER"]], "rel_plus": [["region based deep attention architecture:Method", "Used-For", "FER:Task"]]}
{"doc_id": "150374036", "sentence": "First , to investigate the occlusion and pose variant FER problem , we build six real - world test datasets from FERPlus and AffectNet , namely Occlusion - FERPlus , Pose - FERPlus , Occlusion - AffectNet , and Pose - AffectNet , Occlusion - RAF - DB , and Pose - RAF - DB .", "ner": [["FER", "Task"], ["FERPlus", "Dataset"], ["AffectNet", "Dataset"], ["Occlusion - FERPlus", "Dataset"], ["Pose - FERPlus", "Dataset"], ["Occlusion - AffectNet", "Dataset"], ["Pose - AffectNet", "Dataset"], ["Occlusion - RAF - DB", "Dataset"], ["Pose - RAF - DB", "Dataset"]], "rel": [["Occlusion - FERPlus", "Benchmark-For", "FER"], ["Pose - FERPlus", "Benchmark-For", "FER"], ["Occlusion - AffectNet", "Benchmark-For", "FER"], ["Pose - AffectNet", "Benchmark-For", "FER"], ["Occlusion - RAF - DB", "Benchmark-For", "FER"], ["Pose - RAF - DB", "Benchmark-For", "FER"], ["Occlusion - FERPlus", "SubClass-Of", "FERPlus"], ["Pose - FERPlus", "SubClass-Of", "FERPlus"], ["Occlusion - AffectNet", "SubClass-Of", "AffectNet"], ["Pose - AffectNet", "SubClass-Of", "AffectNet"]], "rel_plus": [["Occlusion - FERPlus:Dataset", "Benchmark-For", "FER:Task"], ["Pose - FERPlus:Dataset", "Benchmark-For", "FER:Task"], ["Occlusion - AffectNet:Dataset", "Benchmark-For", "FER:Task"], ["Pose - AffectNet:Dataset", "Benchmark-For", "FER:Task"], ["Occlusion - RAF - DB:Dataset", "Benchmark-For", "FER:Task"], ["Pose - RAF - DB:Dataset", "Benchmark-For", "FER:Task"], ["Occlusion - FERPlus:Dataset", "SubClass-Of", "FERPlus:Dataset"], ["Pose - FERPlus:Dataset", "SubClass-Of", "FERPlus:Dataset"], ["Occlusion - AffectNet:Dataset", "SubClass-Of", "AffectNet:Dataset"], ["Pose - AffectNet:Dataset", "SubClass-Of", "AffectNet:Dataset"]]}
{"doc_id": "150374036", "sentence": "Second , we propose the Region Attention Network ( RAN ) , to capture the importance of facial regions for occlusion and pose robust FER .", "ner": [["Region Attention Network", "Method"], ["RAN", "Method"], ["FER", "Task"]], "rel": [["RAN", "Synonym-Of", "Region Attention Network"], ["Region Attention Network", "Used-For", "FER"]], "rel_plus": [["RAN:Method", "Synonym-Of", "Region Attention Network:Method"], ["Region Attention Network:Method", "Used-For", "FER:Task"]]}
{"doc_id": "150374036", "sentence": "Given a number of facial regions , our RAN learns attention weights for each regions in an end - to - end manner , and aggregates their CNN - based features into a compact fixed - length representation .", "ner": [["RAN", "Method"], ["CNN - based features", "Method"]], "rel": [["CNN - based features", "Part-Of", "RAN"]], "rel_plus": [["CNN - based features:Method", "Part-Of", "RAN:Method"]]}
{"doc_id": "150374036", "sentence": "Extensive experiments indicate that our RAN significantly improves the performance of FER in occlusion and pose variant conditions .", "ner": [["RAN", "Method"], ["FER", "Task"]], "rel": [["RAN", "Used-For", "FER"]], "rel_plus": [["RAN:Method", "Used-For", "FER:Task"]]}
{"doc_id": "150374036", "sentence": "Our FER solution achieves state - of - the - art results on FERPlus , AffectNet , RAF - DB , and SFEW with accuracies of 8 9 . 1 6 % , 5 9 . 5 % , 8 6 . 9 % , and 5 6 . 4 % , respectively .", "ner": [["FER", "Task"], ["FERPlus", "Dataset"], ["AffectNet", "Dataset"], ["RAF - DB", "Dataset"], ["SFEW", "Dataset"]], "rel": [["FERPlus", "Benchmark-For", "FER"], ["AffectNet", "Benchmark-For", "FER"], ["RAF - DB", "Benchmark-For", "FER"], ["SFEW", "Benchmark-For", "FER"]], "rel_plus": [["FERPlus:Dataset", "Benchmark-For", "FER:Task"], ["AffectNet:Dataset", "Benchmark-For", "FER:Task"], ["RAF - DB:Dataset", "Benchmark-For", "FER:Task"], ["SFEW:Dataset", "Benchmark-For", "FER:Task"]]}
{"doc_id": "150374036", "sentence": "In this section , we mainly present related works on normal FER problem , the occlusion and pose variant FER problem , and attention mechanism .", "ner": [["FER", "Task"], ["FER", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "150374036", "sentence": "Generally , a FER system mainly consists of three stages , namely face detection , feature extraction , and expression recognition .", "ner": [["FER", "Task"], ["face detection", "Task"], ["feature extraction", "Task"], ["expression recognition", "Task"]], "rel": [["face detection", "SubTask-Of", "FER"], ["feature extraction", "SubTask-Of", "FER"], ["expression recognition", "SubTask-Of", "FER"]], "rel_plus": [["face detection:Task", "SubTask-Of", "FER:Task"], ["feature extraction:Task", "SubTask-Of", "FER:Task"], ["expression recognition:Task", "SubTask-Of", "FER:Task"]]}
{"doc_id": "150374036", "sentence": "In face detection , several face detectors like MTCNN [ 6 7 ] and Dlib [ 3 ] ) are used to locate faces in complex scenes .", "ner": [["face detection", "Task"], ["MTCNN", "Method"], ["Dlib", "Method"]], "rel": [["MTCNN", "Used-For", "face detection"], ["Dlib", "Used-For", "face detection"]], "rel_plus": [["MTCNN:Method", "Used-For", "face detection:Task"], ["Dlib:Method", "Used-For", "face detection:Task"]]}
{"doc_id": "150374036", "sentence": "The texture - based features mainly include SIFT [ 4 9 ] , HOG [ 1 4 ] , Histograms of LBP [ 5 2 ] , Gabor wavelet coefficients [ 3 9 ] , etc .", "ner": [["texture - based features", "Method"], ["SIFT", "Method"], ["HOG", "Method"], ["Histograms of LBP", "Method"], ["Gabor wavelet coefficients", "Method"]], "rel": [["SIFT", "SubClass-Of", "texture - based features"], ["HOG", "SubClass-Of", "texture - based features"], ["Histograms of LBP", "SubClass-Of", "texture - based features"], ["Gabor wavelet coefficients", "SubClass-Of", "texture - based features"]], "rel_plus": [["SIFT:Method", "SubClass-Of", "texture - based features:Method"], ["HOG:Method", "SubClass-Of", "texture - based features:Method"], ["Histograms of LBP:Method", "SubClass-Of", "texture - based features:Method"], ["Gabor wavelet coefficients:Method", "SubClass-Of", "texture - based features:Method"]]}
{"doc_id": "150374036", "sentence": "Tang [ 5 7 ] and Kahou et al. [ 2 9 ] utilize deep CNNs for feature extraction , and win the FER 2 0 1 3 and Emotiw 2 0 1 3 challenge , respectively .", "ner": [["deep CNNs", "Method"], ["feature extraction", "Task"], ["FER 2 0 1 3", "Dataset"], ["Emotiw 2 0 1 3 challenge", "Dataset"]], "rel": [["deep CNNs", "Used-For", "feature extraction"], ["deep CNNs", "Evaluated-With", "FER 2 0 1 3"], ["deep CNNs", "Evaluated-With", "Emotiw 2 0 1 3 challenge"]], "rel_plus": [["deep CNNs:Method", "Used-For", "feature extraction:Task"], ["deep CNNs:Method", "Evaluated-With", "FER 2 0 1 3:Dataset"], ["deep CNNs:Method", "Evaluated-With", "Emotiw 2 0 1 3 challenge:Dataset"]]}
{"doc_id": "150374036", "sentence": "Liu et al. [ 4 0 ] propose a Facial Action Units based CNN architecture for expression recognition .", "ner": [["Facial Action Units based CNN", "Method"], ["expression recognition", "Task"]], "rel": [["Facial Action Units based CNN", "Used-For", "expression recognition"]], "rel_plus": [["Facial Action Units based CNN:Method", "Used-For", "expression recognition:Task"]]}
{"doc_id": "150374036", "sentence": "After feature extraction , the next stage is to feed the features into a supervised classifier such as Support Vector Machines ( SVMs ) , softmax layer , and logistic regression to assign expression categories .", "ner": [["feature extraction", "Task"], ["supervised classifier", "Method"], ["Support Vector Machines", "Method"], ["SVMs", "Method"], ["softmax layer", "Method"], ["logistic regression", "Method"]], "rel": [["Support Vector Machines", "SubClass-Of", "supervised classifier"], ["softmax layer", "SubClass-Of", "supervised classifier"], ["logistic regression", "SubClass-Of", "supervised classifier"], ["SVMs", "Synonym-Of", "Support Vector Machines"]], "rel_plus": [["Support Vector Machines:Method", "SubClass-Of", "supervised classifier:Method"], ["softmax layer:Method", "SubClass-Of", "supervised classifier:Method"], ["logistic regression:Method", "SubClass-Of", "supervised classifier:Method"], ["SVMs:Method", "Synonym-Of", "Support Vector Machines:Method"]]}
{"doc_id": "150374036", "sentence": "Levi and Hassner [ 3 3 ] leverage the CASIA - WebFace [ 6 3 ] face recognition dataset to pretrain four different VGGNet [ 5 3 ] and GoogleNet [ 5 5 ] .", "ner": [["CASIA - WebFace", "Dataset"], ["face recognition", "Task"], ["VGGNet", "Method"], ["GoogleNet", "Method"]], "rel": [["VGGNet", "Trained-With", "CASIA - WebFace"], ["GoogleNet", "Trained-With", "CASIA - WebFace"]], "rel_plus": [["VGGNet:Method", "Trained-With", "CASIA - WebFace:Dataset"], ["GoogleNet:Method", "Trained-With", "CASIA - WebFace:Dataset"]]}
{"doc_id": "150374036", "sentence": "Zhao et al. [ 7 2 ] propose a Peak Gradient Suppression ( PGS ) scheme for training and also pretrain their models on CASIA - WebFace .", "ner": [["Peak Gradient Suppression", "Method"], ["PGS", "Method"], ["CASIA - WebFace", "Dataset"]], "rel": [["PGS", "Synonym-Of", "Peak Gradient Suppression"], ["Peak Gradient Suppression", "Trained-With", "CASIA - WebFace"]], "rel_plus": [["PGS:Method", "Synonym-Of", "Peak Gradient Suppression:Method"], ["Peak Gradient Suppression:Method", "Trained-With", "CASIA - WebFace:Dataset"]]}
{"doc_id": "150374036", "sentence": "Ding et al. [ 2 0 ] propose a FaceNet 2 ExpNet framework which jointly trains FER task and face recognition task .   Occlusion and variant pose usually occur in real - world scenarios as facial regions can be easily occluded by sunglasses , a hat , a scarf , etc .", "ner": [["FaceNet 2 ExpNet", "Method"], ["FER", "Task"], ["face recognition", "Task"]], "rel": [["FaceNet 2 ExpNet", "Used-For", "FER"], ["FaceNet 2 ExpNet", "Used-For", "face recognition"]], "rel_plus": [["FaceNet 2 ExpNet:Method", "Used-For", "FER:Task"], ["FaceNet 2 ExpNet:Method", "Used-For", "face recognition:Task"]]}
{"doc_id": "150374036", "sentence": "Liu et al. [ 4 1 ] propose a novel FER method to address partial occlusion problem based on Gabor multi - orientation features fusion and local Gabor binary pattern histogram sequence ( LGBPHS ) .", "ner": [["FER", "Task"], ["Gabor multi - orientation features fusion", "Method"], ["local Gabor binary pattern histogram sequence", "Method"], ["LGBPHS", "Method"]], "rel": [["Gabor multi - orientation features fusion", "Used-For", "FER"], ["local Gabor binary pattern histogram sequence", "Used-For", "FER"], ["LGBPHS", "Synonym-Of", "local Gabor binary pattern histogram sequence"]], "rel_plus": [["Gabor multi - orientation features fusion:Method", "Used-For", "FER:Task"], ["local Gabor binary pattern histogram sequence:Method", "Used-For", "FER:Task"], ["LGBPHS:Method", "Synonym-Of", "local Gabor binary pattern histogram sequence:Method"]]}
{"doc_id": "150374036", "sentence": "The latest related work [ 3 7 ] designs a patch - based attention network for occlusion aware FER .", "ner": [["patch - based attention network", "Method"], ["FER", "Task"]], "rel": [["patch - based attention network", "Used-For", "FER"]], "rel_plus": [["patch - based attention network:Method", "Used-For", "FER:Task"]]}
{"doc_id": "150374036", "sentence": "As for the pose variant FER problem , Rudovic et al. [ 5 1 ] propose the Coupled Scaled Gaussian Process Regression ( CSGPR ) model for head - pose normalization .", "ner": [["FER", "Task"], ["Coupled Scaled Gaussian Process Regression", "Method"], ["CSGPR", "Method"]], "rel": [["Coupled Scaled Gaussian Process Regression", "Used-For", "FER"], ["CSGPR", "Synonym-Of", "Coupled Scaled Gaussian Process Regression"]], "rel_plus": [["Coupled Scaled Gaussian Process Regression:Method", "Used-For", "FER:Task"], ["CSGPR:Method", "Synonym-Of", "Coupled Scaled Gaussian Process Regression:Method"]]}
{"doc_id": "150374036", "sentence": "Mnih et al. [ 4 7 ] use the attention on the RNN model for image classification , and then it is successfully utilized for machine translation tasks .", "ner": [["RNN", "Method"], ["image classification", "Task"], ["machine translation", "Task"]], "rel": [["RNN", "Used-For", "image classification"], ["RNN", "Used-For", "machine translation"]], "rel_plus": [["RNN:Method", "Used-For", "image classification:Task"], ["RNN:Method", "Used-For", "machine translation:Task"]]}
{"doc_id": "150374036", "sentence": "Afterwards , many self - attention models are proposed for different tasks , such as LSTM for machine reading [ 1 1 ] , multi - head attention for machine translation [ 5 8 ] and attention clusters for video classification [ 4 2 ] .", "ner": [["self - attention models", "Method"], ["LSTM", "Method"], ["machine reading", "Task"], ["multi - head attention", "Method"], ["machine translation", "Task"], ["attention clusters", "Method"], ["video classification", "Task"]], "rel": [["attention clusters", "SubClass-Of", "self - attention models"], ["multi - head attention", "SubClass-Of", "self - attention models"], ["LSTM", "SubClass-Of", "self - attention models"], ["LSTM", "Used-For", "machine reading"], ["multi - head attention", "Used-For", "machine translation"], ["attention clusters", "Used-For", "video classification"]], "rel_plus": [["attention clusters:Method", "SubClass-Of", "self - attention models:Method"], ["multi - head attention:Method", "SubClass-Of", "self - attention models:Method"], ["LSTM:Method", "SubClass-Of", "self - attention models:Method"], ["LSTM:Method", "Used-For", "machine reading:Task"], ["multi - head attention:Method", "Used-For", "machine translation:Task"], ["attention clusters:Method", "Used-For", "video classification:Task"]]}
{"doc_id": "150374036", "sentence": "Perhaps the most similar work to ours is the Neural Aggregation Network ( NAN ) proposed by Yang et al. [ 6 2 ] .", "ner": [["Neural Aggregation Network", "Method"], ["NAN", "Method"]], "rel": [["NAN", "Synonym-Of", "Neural Aggregation Network"]], "rel_plus": [["NAN:Method", "Synonym-Of", "Neural Aggregation Network:Method"]]}
{"doc_id": "150374036", "sentence": "Our work differs from NAN by that self - attention and relation - attention module is used in RAN to aggregate facial region features for FER in static images , and a region biased loss is introduced to enhance region weights .", "ner": [["NAN", "Method"], ["self - attention", "Method"], ["relation - attention module", "Method"], ["RAN", "Method"], ["FER", "Task"]], "rel": [["relation - attention module", "Part-Of", "RAN"], ["self - attention", "Part-Of", "RAN"], ["RAN", "Used-For", "FER"]], "rel_plus": [["relation - attention module:Method", "Part-Of", "RAN:Method"], ["self - attention:Method", "Part-Of", "RAN:Method"], ["RAN:Method", "Used-For", "FER:Task"]]}
{"doc_id": "150374036", "sentence": "In this section , we first give an overview of our proposed region attention networks ( RAN ) , and then detail each module and the region biased loss in RAN .", "ner": [["region attention networks", "Method"], ["RAN", "Method"], ["region biased loss", "Method"], ["RAN", "Method"]], "rel": [["RAN", "Synonym-Of", "region attention networks"], ["region biased loss", "Part-Of", "RAN"]], "rel_plus": [["RAN:Method", "Synonym-Of", "region attention networks:Method"], ["region biased loss:Method", "Part-Of", "RAN:Method"]]}
{"doc_id": "150374036", "sentence": "Considering both large pose and occlusion issues in facial expression recognition , we propose a Region Attention Network ( RAN ) to alleviate the degradation of naive face based CNN models .", "ner": [["facial expression recognition", "Task"], ["Region Attention Network", "Method"], ["RAN", "Method"], ["naive face based CNN models", "Method"]], "rel": [["RAN", "Synonym-Of", "Region Attention Network"]], "rel_plus": [["RAN:Method", "Synonym-Of", "Region Attention Network:Method"]]}
{"doc_id": "150374036", "sentence": "Given a face image ( after face detection ) , we first crop it into a number of regions with fixed position cropping or random cropping .", "ner": [["face detection", "Task"], ["fixed position cropping", "Method"], ["random cropping", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "150374036", "sentence": "Subsequently , the self - attention module assigns an attention weight for each region using a fully - connected ( FC ) layer and the sigmoid function .", "ner": [["self - attention module", "Method"], ["fully - connected", "Method"], ["FC", "Method"], ["sigmoid", "Method"]], "rel": [["fully - connected", "Part-Of", "self - attention module"], ["sigmoid", "Part-Of", "self - attention module"], ["FC", "Synonym-Of", "fully - connected"]], "rel_plus": [["fully - connected:Method", "Part-Of", "self - attention module:Method"], ["sigmoid:Method", "Part-Of", "self - attention module:Method"], ["FC:Method", "Synonym-Of", "fully - connected:Method"]]}
{"doc_id": "150374036", "sentence": "The feature set X of I is defined by : where \u03b8 is the parameter of backbone CNN . 1 ) Self - Attention Module : With these region features , the self - attention module applies a FC layer and a sigmoid function to estimate coarse attention weights .", "ner": [["CNN", "Method"], ["Self - Attention Module", "Method"], ["self - attention module", "Method"], ["FC layer", "Method"], ["sigmoid", "Method"]], "rel": [["FC layer", "Part-Of", "self - attention module"], ["sigmoid", "Part-Of", "self - attention module"]], "rel_plus": [["FC layer:Method", "Part-Of", "self - attention module:Method"], ["sigmoid:Method", "Part-Of", "self - attention module:Method"]]}
{"doc_id": "150374036", "sentence": "Inspired by the global attention in neural machine translation [ 4 3 ] and the relation - Net in low - shot learning [ 6 1 ] , we use the sample concatenation and another FC layer to estimate new attention weights for region features .", "ner": [["global attention", "Method"], ["neural machine translation", "Task"], ["relation - Net", "Method"], ["low - shot learning", "Task"], ["sample concatenation", "Method"], ["FC layer", "Method"]], "rel": [["global attention", "Used-For", "neural machine translation"], ["relation - Net", "Used-For", "low - shot learning"]], "rel_plus": [["global attention:Method", "Used-For", "neural machine translation:Task"], ["relation - Net:Method", "Used-For", "low - shot learning:Task"]]}
{"doc_id": "150374036", "sentence": "In this paper , we evaluate three kinds of region generation schemes for our region attention networks , namely fixed position cropping , random cropping , and landmark - based cropping which are depicted in Figure 2 . 1 ) Fixed Position Cropping : Since the face image can be well aligned by the recent advanced face alignment methods , a simple region generation scheme is to crop regions in fixed positions with fixed scales .", "ner": [["region attention networks", "Method"], ["fixed position cropping", "Method"], ["random cropping", "Method"], ["landmark - based cropping", "Method"], ["Fixed Position Cropping", "Method"], ["face alignment", "Task"]], "rel": [["fixed position cropping", "Part-Of", "region attention networks"], ["random cropping", "Part-Of", "region attention networks"], ["landmark - based cropping", "Part-Of", "region attention networks"]], "rel_plus": [["fixed position cropping:Method", "Part-Of", "region attention networks:Method"], ["random cropping:Method", "Part-Of", "region attention networks:Method"], ["landmark - based cropping:Method", "Part-Of", "region attention networks:Method"]]}
{"doc_id": "150374036", "sentence": "Though our proposed RAN can be used for FER in any conditions , we focus on the real - world occlusion and pose variantion problems .", "ner": [["RAN", "Method"], ["FER", "Task"]], "rel": [["RAN", "Used-For", "FER"]], "rel_plus": [["RAN:Method", "Used-For", "FER:Task"]]}
{"doc_id": "150374036", "sentence": "From the test set of FERPlus [ 5 ] , the validation set of AffectNet [ 4 8 ] , and the test set of RAF - DB [ 3 4 ] , we collect the Occlusion - FERPlus , Pose - FERPlus , Occlusion - AffectNet , Pose - AffectNet , Occlusion - RAF - DB , and Pose - RAF - DB for testing .", "ner": [["FERPlus", "Dataset"], ["AffectNet", "Dataset"], ["RAF - DB", "Dataset"], ["Occlusion - FERPlus", "Dataset"], ["Pose - FERPlus", "Dataset"], ["Occlusion - AffectNet", "Dataset"], ["Pose - AffectNet", "Dataset"], ["Occlusion - RAF - DB", "Dataset"], ["Pose - RAF - DB", "Dataset"]], "rel": [["Occlusion - FERPlus", "SubClass-Of", "FERPlus"], ["Pose - FERPlus", "SubClass-Of", "FERPlus"], ["Occlusion - AffectNet", "SubClass-Of", "AffectNet"], ["Pose - AffectNet", "SubClass-Of", "AffectNet"], ["Occlusion - RAF - DB", "SubClass-Of", "RAF - DB"], ["Pose - RAF - DB", "SubClass-Of", "RAF - DB"]], "rel_plus": [["Occlusion - FERPlus:Dataset", "SubClass-Of", "FERPlus:Dataset"], ["Pose - FERPlus:Dataset", "SubClass-Of", "FERPlus:Dataset"], ["Occlusion - AffectNet:Dataset", "SubClass-Of", "AffectNet:Dataset"], ["Pose - AffectNet:Dataset", "SubClass-Of", "AffectNet:Dataset"], ["Occlusion - RAF - DB:Dataset", "SubClass-Of", "RAF - DB:Dataset"], ["Pose - RAF - DB:Dataset", "SubClass-Of", "RAF - DB:Dataset"]]}
{"doc_id": "150374036", "sentence": "Then we manually assign these categories to the test sets of FERPlus , AffectNet , and RAF - DB .", "ner": [["FERPlus", "Dataset"], ["AffectNet", "Dataset"], ["RAF - DB", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "150374036", "sentence": "Among all the occlusion types on FERPluse , AffectNet , and RAF - DB , the upper occlusion has the smallest samples .", "ner": [["FERPluse", "Dataset"], ["AffectNet", "Dataset"], ["RAF - DB", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "150374036", "sentence": "The total numbers of occlusion samples in FERPlus ( test ) , Affect - Net ( validation ) , and RAF - DB ( test ) are respectively 6 0 5 , 6 8 2 , and 7 3 5 , which are 1 6 . 8 6 % , 1 7 . 0 5 % , and 2 3 . 9 % of their original sets .", "ner": [["FERPlus ( test )", "Dataset"], ["Affect - Net ( validation )", "Dataset"], ["RAF - DB ( test )", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "150374036", "sentence": "For the variant pose issue , about one - third of FERPlus ( test ) , about two - fifths of RAF - DB , and about half of AffectNet ( validation ) have poses larger than 3 0 degrees ( in pitch or yaw ) .", "ner": [["FERPlus ( test )", "Dataset"], ["RAF - DB", "Dataset"], ["AffectNet ( validation )", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "150374036", "sentence": "We further explore each components of RAN on FERPlus [ 5 ] , AffectNet [ 4 8 ] , and SFEW [ 1 5 ] .", "ner": [["RAN", "Method"], ["FERPlus", "Dataset"], ["AffectNet", "Dataset"], ["SFEW", "Dataset"]], "rel": [["RAN", "Evaluated-With", "FERPlus"], ["RAN", "Evaluated-With", "AffectNet"], ["RAN", "Evaluated-With", "SFEW"]], "rel_plus": [["RAN:Method", "Evaluated-With", "FERPlus:Dataset"], ["RAN:Method", "Evaluated-With", "AffectNet:Dataset"], ["RAN:Method", "Evaluated-With", "SFEW:Dataset"]]}
{"doc_id": "150374036", "sentence": "To evaluate our method , we use four popular in - thewild facial expression datasets , namely FERPlus [ 5 ] , Affect - Net [ 4 8 ] , RAF - DB [ 3 4 ] , and SFEW [ 1 5 ] .", "ner": [["in - thewild facial expression", "Task"], ["FERPlus", "Dataset"], ["Affect - Net", "Dataset"], ["RAF - DB", "Dataset"], ["SFEW", "Dataset"]], "rel": [["FERPlus", "Benchmark-For", "in - thewild facial expression"], ["Affect - Net", "Benchmark-For", "in - thewild facial expression"], ["RAF - DB", "Benchmark-For", "in - thewild facial expression"], ["SFEW", "Benchmark-For", "in - thewild facial expression"]], "rel_plus": [["FERPlus:Dataset", "Benchmark-For", "in - thewild facial expression:Task"], ["Affect - Net:Dataset", "Benchmark-For", "in - thewild facial expression:Task"], ["RAF - DB:Dataset", "Benchmark-For", "in - thewild facial expression:Task"], ["SFEW:Dataset", "Benchmark-For", "in - thewild facial expression:Task"]]}
{"doc_id": "150374036", "sentence": "Besides , we also build occlusion and pose variant test datasets from FERPlus , AffectNet , and RAF - DB . 1 ) FERPlus : [ 5 ] .", "ner": [["FERPlus", "Dataset"], ["AffectNet", "Dataset"], ["RAF - DB", "Dataset"], ["FERPlus", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "150374036", "sentence": "The FERPlus is extent from FER 2 0 1 3 [ 2 3 ] introduced during the ICML 2 0 1 3 Challenges in Representation Learning .", "ner": [["FERPlus", "Dataset"], ["FER 2 0 1 3", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "150374036", "sentence": "The main difference between FER 2 0 1 3 and FERPlus is the annotation .", "ner": [["FER 2 0 1 3", "Dataset"], ["FERPlus", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "150374036", "sentence": "FER 2 0 1 3 is annotated with seven expression labels ( neutral , happiness , surprise , sadness , anger , disgust , fear ) by one tagger , while FERPlus adds a contempt label and is annotated by 1 0 labels .", "ner": [["FER 2 0 1 3", "Dataset"], ["FERPlus", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "150374036", "sentence": "The Static Facial Expressions in the Wild ( SFEW ) dataset is built by selecting frames from AFEW [ 1 6 ] , which covers unconstrained facial expressions , varied head poses , large age range , occlusions , varied focus , different resolution of the face and real - world illumination .", "ner": [["Static Facial Expressions in the Wild", "Dataset"], ["SFEW", "Dataset"], ["AFEW", "Dataset"]], "rel": [["SFEW", "Synonym-Of", "Static Facial Expressions in the Wild"], ["Static Facial Expressions in the Wild", "SubClass-Of", "AFEW"]], "rel_plus": [["SFEW:Dataset", "Synonym-Of", "Static Facial Expressions in the Wild:Dataset"], ["Static Facial Expressions in the Wild:Dataset", "SubClass-Of", "AFEW:Dataset"]]}
{"doc_id": "150374036", "sentence": "In all the following experiments , we use the CNN detector and the ERT [ 3 1 ] based face alignment method in Dlib toolbox 1 to crop and align faces , and then resize them to the size of 2 2 4 \u00d7 2 2 4 .", "ner": [["CNN detector", "Method"], ["ERT", "Method"], ["face alignment", "Task"], ["Dlib", "Method"]], "rel": [["ERT", "Used-For", "face alignment"], ["CNN detector", "Part-Of", "Dlib"], ["ERT", "Part-Of", "Dlib"]], "rel_plus": [["ERT:Method", "Used-For", "face alignment:Task"], ["CNN detector:Method", "Part-Of", "Dlib:Method"], ["ERT:Method", "Part-Of", "Dlib:Method"]]}
{"doc_id": "150374036", "sentence": "For the backbone CNN , we mainly use the ResNet - 1 8 [ 2 5 ] and VGG 1 6 [ 5 0 ] .", "ner": [["CNN", "Method"], ["ResNet - 1 8", "Method"], ["VGG 1 6", "Method"]], "rel": [["ResNet - 1 8", "SubClass-Of", "CNN"], ["VGG 1 6", "SubClass-Of", "CNN"]], "rel_plus": [["ResNet - 1 8:Method", "SubClass-Of", "CNN:Method"], ["VGG 1 6:Method", "SubClass-Of", "CNN:Method"]]}
{"doc_id": "150374036", "sentence": "The ResNet - 1 8 is pre - trained on MS - Celeb - 1 M face recognition dataset and VGG 1 6 is downloaded from website 3 .", "ner": [["ResNet - 1 8", "Method"], ["MS - Celeb - 1 M", "Dataset"], ["face recognition", "Task"], ["VGG 1 6", "Method"]], "rel": [["ResNet - 1 8", "Trained-With", "MS - Celeb - 1 M"], ["ResNet - 1 8", "Used-For", "face recognition"], ["MS - Celeb - 1 M", "Benchmark-For", "face recognition"]], "rel_plus": [["ResNet - 1 8:Method", "Trained-With", "MS - Celeb - 1 M:Dataset"], ["ResNet - 1 8:Method", "Used-For", "face recognition:Task"], ["MS - Celeb - 1 M:Dataset", "Benchmark-For", "face recognition:Task"]]}
{"doc_id": "150374036", "sentence": "The last pooling layer of ResNet - 1 8 , and the first FC feature of VGG 1 6 is used for facial representation .", "ner": [["pooling layer", "Method"], ["ResNet - 1 8", "Method"], ["FC", "Method"], ["VGG 1 6", "Method"]], "rel": [["pooling layer", "Part-Of", "ResNet - 1 8"], ["FC", "Part-Of", "VGG 1 6"]], "rel_plus": [["pooling layer:Method", "Part-Of", "ResNet - 1 8:Method"], ["FC:Method", "Part-Of", "VGG 1 6:Method"]]}
{"doc_id": "150374036", "sentence": "The margin in RB - Loss is default as 0.0 2 .   To address the occlusion and pose variant issues , we construct several test subsets with occlusion and pose annotations , i.e. Occlusion - FERPlus , Pose - FERPlus , Occlusion - AffectNet , Pose - AffectNet , Occlusion - RAF - DB , and Pose - RAF - DB .", "ner": [["RB - Loss", "Method"], ["Occlusion - FERPlus", "Dataset"], ["Pose - FERPlus", "Dataset"], ["Occlusion - AffectNet", "Dataset"], ["Pose - AffectNet", "Dataset"], ["Occlusion - RAF - DB", "Dataset"], ["Pose - RAF - DB", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "150374036", "sentence": "We evaluate our RAN on the collected datasets with the default setting ( i.e. ResNet 1 8 with alignment , RB - Loss and fixed cropping ) .", "ner": [["RAN", "Method"], ["ResNet 1 8", "Method"], ["alignment", "Method"], ["RB - Loss", "Method"], ["fixed cropping", "Method"]], "rel": [["alignment", "Part-Of", "ResNet 1 8"], ["RB - Loss", "Part-Of", "ResNet 1 8"], ["fixed cropping", "Part-Of", "ResNet 1 8"]], "rel_plus": [["alignment:Method", "Part-Of", "ResNet 1 8:Method"], ["RB - Loss:Method", "Part-Of", "ResNet 1 8:Method"], ["fixed cropping:Method", "Part-Of", "ResNet 1 8:Method"]]}
{"doc_id": "150374036", "sentence": "Our RAN improves the baseline method significantly , with gains 1 0 . 3 % , 1 0 . 0 2 % and 2, 5 3 % on Occlusion - FERPlus , Occlusion - AffectNet and Occlusion - RAF - DB , respectively .", "ner": [["RAN", "Method"], ["Occlusion - FERPlus", "Dataset"], ["Occlusion - AffectNet", "Dataset"], ["Occlusion - RAF - DB", "Dataset"]], "rel": [["RAN", "Evaluated-With", "Occlusion - FERPlus"], ["RAN", "Evaluated-With", "Occlusion - AffectNet"], ["RAN", "Evaluated-With", "Occlusion - RAF - DB"]], "rel_plus": [["RAN:Method", "Evaluated-With", "Occlusion - FERPlus:Dataset"], ["RAN:Method", "Evaluated-With", "Occlusion - AffectNet:Dataset"], ["RAN:Method", "Evaluated-With", "Occlusion - RAF - DB:Dataset"]]}
{"doc_id": "150374036", "sentence": "On Pose - FERPlus , Pose - AffectNet and Pose - RAF - DB , the RAN also outperforms the baseline with a large margin .", "ner": [["Pose - FERPlus", "Dataset"], ["Pose - AffectNet", "Dataset"], ["Pose - RAF - DB", "Dataset"], ["RAN", "Method"]], "rel": [["RAN", "Evaluated-With", "Pose - FERPlus"], ["RAN", "Evaluated-With", "Pose - AffectNet"], ["RAN", "Evaluated-With", "Pose - RAF - DB"]], "rel_plus": [["RAN:Method", "Evaluated-With", "Pose - FERPlus:Dataset"], ["RAN:Method", "Evaluated-With", "Pose - AffectNet:Dataset"], ["RAN:Method", "Evaluated-With", "Pose - RAF - DB:Dataset"]]}
{"doc_id": "150374036", "sentence": "Specifically , with pose larger than 3 0 degrees , the gains are 4. 1 2 % , 3. 0 9 % and 2. 7 0 % on Pose - FERPlus , Pose - AffectNet and Pose - RAF - DB , respectively .", "ner": [["Pose - FERPlus", "Dataset"], ["Pose - AffectNet", "Dataset"], ["Pose - RAF - DB", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "150374036", "sentence": "Overall , these results demonstrate the effectiveness of our proposed RAN on occlusion and variant pose FER data .", "ner": [["RAN", "Method"], ["FER", "Task"]], "rel": [["RAN", "Used-For", "FER"]], "rel_plus": [["RAN:Method", "Used-For", "FER:Task"]]}
{"doc_id": "150374036", "sentence": "We finally achieve 6 7 . 9 8 % which is clearly better than 6 6 . 5 % of [ 3 7 ] . 1 ) Individual Regions and Their Combination : Since our RAN integrates several regions in a single network , we present the performace of individual regions and their score fusion on Occlusion - and Pose - FERPlus in Table III .", "ner": [["RAN", "Method"], ["Occlusion - and Pose - FERPlus", "Dataset"]], "rel": [["RAN", "Evaluated-With", "Occlusion - and Pose - FERPlus"]], "rel_plus": [["RAN:Method", "Evaluated-With", "Occlusion - and Pose - FERPlus:Dataset"]]}
{"doc_id": "150374036", "sentence": "To investigating if the improvement of our RAN only comes from augmented data , we also train a traditional model by mixing all the regions and the original images for data augmentation , which is called aug . training .", "ner": [["RAN", "Method"], ["data augmentation", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "150374036", "sentence": "We conclude that i ) the performance of individual regions are comparable to each other except for the region I 1 and I 2 , ii ) a naive score fusion ( i.e. average ) and mixing all the regions improve individual performance Figure 6   To validate the generality of our method , we conduct an ablation study on the full test set of FERPlus and the full validation set of AffectNet with default setting .", "ner": [["FERPlus", "Dataset"], ["AffectNet", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "150374036", "sentence": "Face alignment is a standard pre - processing method for face analysis , while a few works do not utilize [ 5 ] , [ 4 8 ] for FER task .", "ner": [["Face alignment", "Task"], ["face analysis", "Task"], ["FER", "Task"]], "rel": [["Face alignment", "SubClass-Of", "face analysis"]], "rel_plus": [["Face alignment:Task", "SubClass-Of", "face analysis:Task"]]}
{"doc_id": "150374036", "sentence": "The evaluation results on FERPlus and AffectNet are presented on Table IV and   Table V , respectively .", "ner": [["FERPlus", "Dataset"], ["AffectNet", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "150374036", "sentence": "On FERPlus without face alignment , the self - attention ( F m in Eq. ( 3 ) ) improves the baseline by 0. 4 % .", "ner": [["FERPlus", "Dataset"], ["face alignment", "Task"], ["self - attention", "Method"]], "rel": [["self - attention", "Evaluated-With", "FERPlus"]], "rel_plus": [["self - attention:Method", "Evaluated-With", "FERPlus:Dataset"]]}
{"doc_id": "150374036", "sentence": "Adding the relation - attention module , our method outperforms the baseline by 1. 1 3 % and 3. 0 5 % on FERPlus and AffectNet without face alignment .", "ner": [["relation - attention module", "Method"], ["FERPlus", "Dataset"], ["AffectNet", "Dataset"], ["face alignment", "Task"]], "rel": [["relation - attention module", "Evaluated-With", "FERPlus"], ["relation - attention module", "Evaluated-With", "AffectNet"]], "rel_plus": [["relation - attention module:Method", "Evaluated-With", "FERPlus:Dataset"], ["relation - attention module:Method", "Evaluated-With", "AffectNet:Dataset"]]}
{"doc_id": "150374036", "sentence": "Face alignment is found to significantly boost the baseline method on both datasets , while its effect is limited when using our proposed RAN .", "ner": [["Face alignment", "Task"], ["RAN", "Method"]], "rel": [["Face alignment", "Used-For", "RAN"]], "rel_plus": [["Face alignment:Task", "Used-For", "RAN:Method"]]}
{"doc_id": "150374036", "sentence": "With face alignment , our attention modules improve the baselines by 0. 8 3 % and 1. 8 5 % on FERPlus and AffectNet , respectively .", "ner": [["face alignment", "Task"], ["attention modules", "Method"], ["FERPlus", "Dataset"], ["AffectNet", "Dataset"]], "rel": [["face alignment", "Used-For", "attention modules"], ["attention modules", "Evaluated-With", "FERPlus"], ["attention modules", "Evaluated-With", "AffectNet"]], "rel_plus": [["face alignment:Task", "Used-For", "attention modules:Method"], ["attention modules:Method", "Evaluated-With", "FERPlus:Dataset"], ["attention modules:Method", "Evaluated-With", "AffectNet:Dataset"]]}
{"doc_id": "150374036", "sentence": "From Table IV and Table V , we can see that the designed RB - Loss further improves performance on both FERPlus and Affect - Net consistently .", "ner": [["RB - Loss", "Method"], ["FERPlus", "Dataset"], ["Affect - Net", "Dataset"]], "rel": [["RB - Loss", "Evaluated-With", "FERPlus"], ["RB - Loss", "Evaluated-With", "Affect - Net"]], "rel_plus": [["RB - Loss:Method", "Evaluated-With", "FERPlus:Dataset"], ["RB - Loss:Method", "Evaluated-With", "Affect - Net:Dataset"]]}
{"doc_id": "150374036", "sentence": "Specifically , the improvement on AffectNet without face alignment is 0. 9 2 % .", "ner": [["AffectNet", "Dataset"], ["face alignment", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "150374036", "sentence": "With oversampling , our RAN with RB - Loss achieves 5 9 . 5 % on the validation set of AffectNet .", "ner": [["RAN", "Method"], ["RB - Loss", "Method"], ["AffectNet", "Dataset"]], "rel": [["RB - Loss", "Part-Of", "RAN"], ["RAN", "Evaluated-With", "AffectNet"]], "rel_plus": [["RB - Loss:Method", "Part-Of", "RAN:Method"], ["RAN:Method", "Evaluated-With", "AffectNet:Dataset"]]}
{"doc_id": "150374036", "sentence": "As the mater of fact , the result of this experiment is part of our motivation to keep the original face image for our method . 3 We conduct an evaluation of individual regions and different fusion schemes on the full FERPlus test datasets without face alignment .", "ner": [["FERPlus", "Dataset"], ["face alignment", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "150374036", "sentence": "We evaluate the fixed cropping , landmark - based cropping , and random cropping methods on FERPlus with the default setting for other parameters , the results are shown in Figure 9 .", "ner": [["fixed cropping", "Method"], ["landmark - based cropping", "Method"], ["random cropping", "Method"], ["FERPlus", "Dataset"]], "rel": [["fixed cropping", "Evaluated-With", "FERPlus"], ["landmark - based cropping", "Evaluated-With", "FERPlus"], ["random cropping", "Evaluated-With", "FERPlus"]], "rel_plus": [["fixed cropping:Method", "Evaluated-With", "FERPlus:Dataset"], ["landmark - based cropping:Method", "Evaluated-With", "FERPlus:Dataset"], ["random cropping:Method", "Evaluated-With", "FERPlus:Dataset"]]}
{"doc_id": "150374036", "sentence": "To explore the impact of region sizes for our RAN , we evaluate the region size of fixed cropping scheme on FERPlus with other parameter as default .", "ner": [["RAN", "Method"], ["FERPlus", "Dataset"]], "rel": [["RAN", "Evaluated-With", "FERPlus"]], "rel_plus": [["RAN:Method", "Evaluated-With", "FERPlus:Dataset"]]}
{"doc_id": "150374036", "sentence": "It may be explained that the regions of I 4 and I 5 almost degrade to the original image , and the information gain from enlarging regions disappeared if too large regions are used . 6 ) Evaluation of Inference Time : Since our RAN has five times feedforward operations than the baseline , we investigate the inference time on FERPlus test set .", "ner": [["RAN", "Method"], ["FERPlus", "Dataset"]], "rel": [["RAN", "Evaluated-With", "FERPlus"]], "rel_plus": [["RAN:Method", "Evaluated-With", "FERPlus:Dataset"]]}
{"doc_id": "150374036", "sentence": "In this section , we compare our best results to several state - of - the - art methods on FERPlus , AffectNet , SFEW , and RAF - DB .", "ner": [["FERPlus", "Dataset"], ["AffectNet", "Dataset"], ["SFEW", "Dataset"], ["RAF - DB", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "150374036", "sentence": "We compare our RAN to several state - of - the - art methods on the FERPlus dataset in Table VI .", "ner": [["RAN", "Method"], ["FERPlus", "Dataset"]], "rel": [["RAN", "Evaluated-With", "FERPlus"]], "rel_plus": [["RAN:Method", "Evaluated-With", "FERPlus:Dataset"]]}
{"doc_id": "150374036", "sentence": "Both [ 5 ] and [ 1 ] leverage the label distribution for each face as supervision . [ 1 ] pretrains a SeNet 5 0 [ 2 6 ] on VGGFace 2 . 0 [ 1 0 ] which includes amount of large - pose faces .", "ner": [["SeNet 5 0", "Method"], ["VGGFace 2 . 0", "Dataset"]], "rel": [["SeNet 5 0", "Trained-With", "VGGFace 2 . 0"]], "rel_plus": [["SeNet 5 0:Method", "Trained-With", "VGGFace 2 . 0:Dataset"]]}
{"doc_id": "150374036", "sentence": "With the KLDiv loss and label distribution supervision , we fine - tune the public VGGFace model ( VGG 1 6 pretrained Table VII presents the comparison on AffectNet .", "ner": [["KLDiv loss", "Method"], ["VGGFace", "Dataset"], ["VGG 1 6", "Method"], ["AffectNet", "Dataset"]], "rel": [["VGG 1 6", "Trained-With", "VGGFace"]], "rel_plus": [["VGG 1 6:Method", "Trained-With", "VGGFace:Dataset"]]}
{"doc_id": "150374036", "sentence": "It is worth noting that [ 4 8 ] only achieves 4 7 % with upsampling and [ 6 6 ] uses one more large - scale FER dataset and 8 0 layers ResNet for training with elaborated loss weights on them . 3 ) Comparison on SFEW : Table VIII presents the comparison on SFEW .", "ner": [["FER", "Task"], ["8 0 layers ResNet", "Method"], ["SFEW", "Dataset"], ["SFEW", "Dataset"]], "rel": [["8 0 layers ResNet", "Used-For", "FER"]], "rel_plus": [["8 0 layers ResNet:Method", "Used-For", "FER:Task"]]}
{"doc_id": "150374036", "sentence": "Reference [ 6 5 ] ensembles multiple CNNs with each CNN model initialized randomly or pretrained on FER 2 0 1 3 .", "ner": [["CNNs", "Method"], ["CNN", "Method"], ["FER 2 0 1 3", "Dataset"]], "rel": [["CNN", "Trained-With", "FER 2 0 1 3"]], "rel_plus": [["CNN:Method", "Trained-With", "FER 2 0 1 3:Dataset"]]}
{"doc_id": "150374036", "sentence": "Since model ensemble is popular on SFEW , we also conduct a naive model fusion by averaging the scores of ResNet 1 8 and VGG 1 6 which obtains 5 6 . 4 % .", "ner": [["SFEW", "Dataset"], ["ResNet 1 8", "Method"], ["VGG 1 6", "Method"]], "rel": [["ResNet 1 8", "Evaluated-With", "SFEW"], ["VGG 1 6", "Evaluated-With", "SFEW"]], "rel_plus": [["ResNet 1 8:Method", "Evaluated-With", "SFEW:Dataset"], ["VGG 1 6:Method", "Evaluated-With", "SFEW:Dataset"]]}
{"doc_id": "150374036", "sentence": "Reference [ 3 7 ] leverages patch - based attention [ 3 4 ] and [ 3 7 ] , respectively .   In this paper , we address the facial expression recognition in the real - world occlusion and pose - variant conditions .", "ner": [["patch - based attention", "Method"], ["facial expression recognition", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "150374036", "sentence": "We build several new FER test datasets on these conditions , and propose the Region Attention Network ( RAN ) which adaptively adjusts the importance of facial parts .", "ner": [["FER", "Task"], ["Region Attention Network", "Method"], ["RAN", "Method"]], "rel": [["Region Attention Network", "Used-For", "FER"], ["RAN", "Synonym-Of", "Region Attention Network"]], "rel_plus": [["Region Attention Network:Method", "Used-For", "FER:Task"], ["RAN:Method", "Synonym-Of", "Region Attention Network:Method"]]}
{"doc_id": "150374036", "sentence": "We evaluate our method on the collected datasets and make extensive studies on FER - Plus and AffectNet .", "ner": [["FER - Plus", "Dataset"], ["AffectNet", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "150374036", "sentence": "Our proposed method achieves state - ofthe - art results on FERPlus , SFEW , RAF - DB , and AffectNet .", "ner": [["FERPlus", "Dataset"], ["SFEW", "Dataset"], ["RAF - DB", "Dataset"], ["AffectNet", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "53719742", "sentence": "In this paper , we present a new Mask R - CNN based text detection approach which can robustly detect multi - oriented and curved text from natural scene images in a unified manner .", "ner": [["Mask R - CNN", "Method"], ["text detection", "Task"]], "rel": [["Mask R - CNN", "Used-For", "text detection"]], "rel_plus": [["Mask R - CNN:Method", "Used-For", "text detection:Task"]]}
{"doc_id": "53719742", "sentence": "To enhance the feature representation ability of Mask R - CNN for text detection tasks , we propose to use the Pyramid Attention Network ( PAN ) as a new backbone network of Mask R - CNN .", "ner": [["Mask R - CNN", "Method"], ["text detection", "Task"], ["Pyramid Attention Network", "Method"], ["PAN", "Method"], ["Mask R - CNN", "Method"]], "rel": [["Mask R - CNN", "Used-For", "text detection"], ["PAN", "Synonym-Of", "Pyramid Attention Network"], ["Pyramid Attention Network", "Part-Of", "Mask R - CNN"]], "rel_plus": [["Mask R - CNN:Method", "Used-For", "text detection:Task"], ["PAN:Method", "Synonym-Of", "Pyramid Attention Network:Method"], ["Pyramid Attention Network:Method", "Part-Of", "Mask R - CNN:Method"]]}
{"doc_id": "53719742", "sentence": "Our proposed approach has achieved superior performance on both multi - oriented ( ICDAR - 2 0 1 5 , ICDAR - 2 0 1 7 MLT ) and curved ( SCUT - CTW 1 5 0 0 ) text detection benchmark tasks by only using single - scale and single - model testing .", "ner": [["ICDAR - 2 0 1 5", "Dataset"], ["ICDAR - 2 0 1 7 MLT", "Dataset"], ["SCUT - CTW 1 5 0 0", "Dataset"], ["text detection", "Task"]], "rel": [["SCUT - CTW 1 5 0 0", "Benchmark-For", "text detection"], ["ICDAR - 2 0 1 7 MLT", "Benchmark-For", "text detection"], ["ICDAR - 2 0 1 5", "Benchmark-For", "text detection"]], "rel_plus": [["SCUT - CTW 1 5 0 0:Dataset", "Benchmark-For", "text detection:Task"], ["ICDAR - 2 0 1 7 MLT:Dataset", "Benchmark-For", "text detection:Task"], ["ICDAR - 2 0 1 5:Dataset", "Benchmark-For", "text detection:Task"]]}
{"doc_id": "53719742", "sentence": "Scene text detection has drawn increasing attentions from the computer vision community [ 1 3 , 2 0 , 2 5 , 3 3 , 4 1 ] since it has a wide range of applications in document analysis , robot navigation , OCR translation , image retrieval and augmented reality .", "ner": [["Scene text detection", "Task"], ["computer vision", "Task"], ["document analysis", "Task"], ["robot navigation", "Task"], ["OCR translation", "Task"], ["image retrieval", "Task"], ["augmented reality", "Task"]], "rel": [["Scene text detection", "Used-For", "document analysis"], ["Scene text detection", "Used-For", "robot navigation"], ["Scene text detection", "Used-For", "OCR translation"], ["Scene text detection", "Used-For", "image retrieval"], ["Scene text detection", "Used-For", "augmented reality"]], "rel_plus": [["Scene text detection:Task", "Used-For", "document analysis:Task"], ["Scene text detection:Task", "Used-For", "robot navigation:Task"], ["Scene text detection:Task", "Used-For", "OCR translation:Task"], ["Scene text detection:Task", "Used-For", "image retrieval:Task"], ["Scene text detection:Task", "Used-For", "augmented reality:Task"]]}
{"doc_id": "53719742", "sentence": "Lots of state - of - the - art convolutional neural network ( CNN ) based object detection and segmentation frameworks , such as Faster R - CNN [ 3 2 ] , SSD [ 2 3 ] and FCN [ 2 7 ] , have been borrowed to solve the text detection problem and substan - tially outperform traditional MSER [ 2 9 ] or SWT [ 7 ] based bottom - up text detection approaches .", "ner": [["convolutional neural network", "Method"], ["CNN", "Method"], ["object detection", "Task"], ["segmentation", "Task"], ["Faster R - CNN", "Method"], ["SSD", "Method"], ["FCN", "Method"], ["text detection", "Task"], ["MSER", "Method"], ["SWT", "Method"], ["text detection", "Task"]], "rel": [["CNN", "Synonym-Of", "convolutional neural network"], ["Faster R - CNN", "SubClass-Of", "convolutional neural network"], ["SSD", "SubClass-Of", "convolutional neural network"], ["FCN", "SubClass-Of", "convolutional neural network"], ["convolutional neural network", "Used-For", "object detection"], ["convolutional neural network", "Used-For", "segmentation"], ["Faster R - CNN", "Used-For", "text detection"], ["SSD", "Used-For", "text detection"], ["FCN", "Used-For", "text detection"], ["Faster R - CNN", "Compare-With", "MSER"], ["SSD", "Compare-With", "MSER"], ["FCN", "Compare-With", "MSER"], ["SWT", "Used-For", "text detection"], ["MSER", "Used-For", "text detection"]], "rel_plus": [["CNN:Method", "Synonym-Of", "convolutional neural network:Method"], ["Faster R - CNN:Method", "SubClass-Of", "convolutional neural network:Method"], ["SSD:Method", "SubClass-Of", "convolutional neural network:Method"], ["FCN:Method", "SubClass-Of", "convolutional neural network:Method"], ["convolutional neural network:Method", "Used-For", "object detection:Task"], ["convolutional neural network:Method", "Used-For", "segmentation:Task"], ["Faster R - CNN:Method", "Used-For", "text detection:Task"], ["SSD:Method", "Used-For", "text detection:Task"], ["FCN:Method", "Used-For", "text detection:Task"], ["Faster R - CNN:Method", "Compare-With", "MSER:Method"], ["SSD:Method", "Compare-With", "MSER:Method"], ["FCN:Method", "Compare-With", "MSER:Method"], ["SWT:Method", "Used-For", "text detection:Task"], ["MSER:Method", "Used-For", "text detection:Task"]]}
{"doc_id": "53719742", "sentence": "For example , some approaches [ 3 9 , 3 8 ] formulate text detection as a semantic segmentation problem and employ an FCN to make a pixel - level text/non - text prediction , based on which a text saliency map can be generated .", "ner": [["text detection", "Task"], ["semantic segmentation", "Task"], ["FCN", "Method"], ["pixel - level text/non - text prediction", "Task"]], "rel": [["FCN", "Used-For", "pixel - level text/non - text prediction"]], "rel_plus": [["FCN:Method", "Used-For", "pixel - level text/non - text prediction:Task"]]}
{"doc_id": "53719742", "sentence": "Unlike FCN - based methods , another category of methods treats text as a specific object and leverages effective object detection frameworks like R - CNN [ 9 ] , Faster R - CNN [ 3 2 ] , SSD [ 2 3 ] , YOLO [ 3 1 ] and DenseBox [ 1 4 ] to detect words or text - lines from images directly .", "ner": [["FCN - based methods", "Method"], ["object detection frameworks", "Method"], ["R - CNN", "Method"], ["Faster R - CNN", "Method"], ["SSD", "Method"], ["YOLO", "Method"], ["DenseBox", "Method"]], "rel": [["R - CNN", "SubClass-Of", "object detection frameworks"], ["Faster R - CNN", "SubClass-Of", "object detection frameworks"], ["SSD", "SubClass-Of", "object detection frameworks"], ["YOLO", "SubClass-Of", "object detection frameworks"], ["DenseBox", "SubClass-Of", "object detection frameworks"], ["FCN - based methods", "Compare-With", "object detection frameworks"]], "rel_plus": [["R - CNN:Method", "SubClass-Of", "object detection frameworks:Method"], ["Faster R - CNN:Method", "SubClass-Of", "object detection frameworks:Method"], ["SSD:Method", "SubClass-Of", "object detection frameworks:Method"], ["YOLO:Method", "SubClass-Of", "object detection frameworks:Method"], ["DenseBox:Method", "SubClass-Of", "object detection frameworks:Method"], ["FCN - based methods:Method", "Compare-With", "object detection frameworks:Method"]]}
{"doc_id": "53719742", "sentence": "To solve this problem , some recent approaches like PixelLink [ 6 ] , FTSN [ 5 ] , and IncepText [ 3 7 ] , propose to formulate text detection as an instance segmentation problem so that both straight text and curved text can be detected in a unified manner .", "ner": [["PixelLink", "Method"], ["FTSN", "Method"], ["IncepText", "Method"], ["text detection", "Task"], ["instance segmentation", "Task"]], "rel": [["IncepText", "Used-For", "text detection"], ["FTSN", "Used-For", "text detection"], ["PixelLink", "Used-For", "text detection"], ["PixelLink", "Used-For", "instance segmentation"], ["FTSN", "Used-For", "instance segmentation"], ["IncepText", "Used-For", "instance segmentation"]], "rel_plus": [["IncepText:Method", "Used-For", "text detection:Task"], ["FTSN:Method", "Used-For", "text detection:Task"], ["PixelLink:Method", "Used-For", "text detection:Task"], ["PixelLink:Method", "Used-For", "instance segmentation:Task"], ["FTSN:Method", "Used-For", "instance segmentation:Task"], ["IncepText:Method", "Used-For", "instance segmentation:Task"]]}
{"doc_id": "53719742", "sentence": "Specifically , PixelLink proposes to detect text by linking pixels within the same text instances together , while FTSN and IncepText borrow the FCIS framework [ 1 9 ] to solve the text detection problem .", "ner": [["PixelLink", "Method"], ["FTSN", "Method"], ["IncepText", "Method"], ["FCIS framework", "Method"], ["text detection", "Task"]], "rel": [["PixelLink", "Compare-With", "FTSN"], ["PixelLink", "Compare-With", "IncepText"], ["FTSN", "Used-For", "text detection"]], "rel_plus": [["PixelLink:Method", "Compare-With", "FTSN:Method"], ["PixelLink:Method", "Compare-With", "IncepText:Method"], ["FTSN:Method", "Used-For", "text detection:Task"]]}
{"doc_id": "53719742", "sentence": "Although promising results have been achieved , the used instance segmentation approaches have now been surpassed by the latest state - of - the - art Mask R - CNN approach on general instance segmentation tasks [ 1 1 ] .", "ner": [["instance segmentation", "Task"], ["Mask R - CNN", "Method"], ["instance segmentation", "Task"]], "rel": [["Mask R - CNN", "Used-For", "instance segmentation"]], "rel_plus": [["Mask R - CNN:Method", "Used-For", "instance segmentation:Task"]]}
{"doc_id": "53719742", "sentence": "Therefore , it is straightforward to use Mask R - CNN to further improve the text detection performance .", "ner": [["Mask R - CNN", "Method"], ["text detection", "Task"]], "rel": [["Mask R - CNN", "Used-For", "text detection"]], "rel_plus": [["Mask R - CNN:Method", "Used-For", "text detection:Task"]]}
{"doc_id": "53719742", "sentence": "In this paper , we present an effective Mask R - CNN based text detection approach which can detect multi - oriented and curved text from natural scene images in a unified manner .", "ner": [["Mask R - CNN", "Method"], ["text detection", "Task"]], "rel": [["Mask R - CNN", "Used-For", "text detection"]], "rel_plus": [["Mask R - CNN:Method", "Used-For", "text detection:Task"]]}
{"doc_id": "53719742", "sentence": "To enhance the feature representation ability of Mask R - CNN , we propose to use the Pyramid Attention Network ( PAN ) [ 1 8 ] as a new backbone network of Mask R - CNN .", "ner": [["Mask R - CNN", "Method"], ["Pyramid Attention Network", "Method"], ["PAN", "Method"], ["Mask R - CNN", "Method"]], "rel": [["PAN", "Synonym-Of", "Pyramid Attention Network"], ["Pyramid Attention Network", "Part-Of", "Mask R - CNN"]], "rel_plus": [["PAN:Method", "Synonym-Of", "Pyramid Attention Network:Method"], ["Pyramid Attention Network:Method", "Part-Of", "Mask R - CNN:Method"]]}
{"doc_id": "53719742", "sentence": "Our proposed approach has achieved superior performance on both multi - oriented ( ICDAR - 2 0 1 5 [ 1 7 ] , ICDAR - 2 0 1 7 MLT [ 3 0 ] ) and curved ( SCUT - CTW 1 5 0 0 [ 2 6 ] ) text detection benchmark tasks .", "ner": [["ICDAR - 2 0 1 5", "Dataset"], ["ICDAR - 2 0 1 7 MLT", "Dataset"], ["SCUT - CTW 1 5 0 0", "Dataset"], ["text detection", "Task"]], "rel": [["SCUT - CTW 1 5 0 0", "Benchmark-For", "text detection"], ["ICDAR - 2 0 1 7 MLT", "Benchmark-For", "text detection"], ["ICDAR - 2 0 1 5", "Benchmark-For", "text detection"]], "rel_plus": [["SCUT - CTW 1 5 0 0:Dataset", "Benchmark-For", "text detection:Task"], ["ICDAR - 2 0 1 7 MLT:Dataset", "Benchmark-For", "text detection:Task"], ["ICDAR - 2 0 1 5:Dataset", "Benchmark-For", "text detection:Task"]]}
{"doc_id": "53719742", "sentence": "In this section , we focus on reviewing recently proposed CNN based text detection approaches and recent developments in instance segmentation tasks .", "ner": [["CNN", "Method"], ["text detection", "Task"], ["instance segmentation", "Task"]], "rel": [["CNN", "Used-For", "text detection"], ["CNN", "Used-For", "instance segmentation"]], "rel_plus": [["CNN:Method", "Used-For", "text detection:Task"], ["CNN:Method", "Used-For", "instance segmentation:Task"]]}
{"doc_id": "53719742", "sentence": "State - of - the - art CNN based object detection and segmentation frameworks have been widely used to solve the text detection problem recently .", "ner": [["CNN", "Method"], ["object detection", "Task"], ["segmentation", "Task"], ["text detection", "Task"]], "rel": [["CNN", "Used-For", "object detection"], ["CNN", "Used-For", "segmentation"], ["CNN", "Used-For", "text detection"]], "rel_plus": [["CNN:Method", "Used-For", "object detection:Task"], ["CNN:Method", "Used-For", "segmentation:Task"], ["CNN:Method", "Used-For", "text detection:Task"]]}
{"doc_id": "53719742", "sentence": "Some of these methods [ 3 9 , 3 8 ] borrow the idea of semantic segmentation and employ an FCN to make a pixel - level text/non - text prediction , which produces a text saliency map for text detection .", "ner": [["semantic segmentation", "Task"], ["FCN", "Method"], ["pixel - level text/non - text prediction", "Task"], ["text detection", "Task"]], "rel": [["FCN", "Used-For", "pixel - level text/non - text prediction"], ["FCN", "Used-For", "text detection"]], "rel_plus": [["FCN:Method", "Used-For", "pixel - level text/non - text prediction:Task"], ["FCN:Method", "Used-For", "text detection:Task"]]}
{"doc_id": "53719742", "sentence": "Jaderberg et al. [ 1 5 ] adapted R - CNN for text detection , while its performance was limited by the traditional region proposal generation methods .", "ner": [["R - CNN", "Method"], ["text detection", "Task"], ["region proposal", "Task"]], "rel": [["R - CNN", "Used-For", "text detection"]], "rel_plus": [["R - CNN:Method", "Used-For", "text detection:Task"]]}
{"doc_id": "53719742", "sentence": "Gupta et al. [ 1 0 ] borrowed the YOLO framework and employed a fullyconvolutional regression network to perform text detection and bounding box regression at all locations and multiple scales of an image .", "ner": [["YOLO", "Method"], ["fullyconvolutional regression network", "Method"], ["text detection", "Task"], ["bounding box regression", "Task"]], "rel": [["fullyconvolutional regression network", "Used-For", "text detection"], ["fullyconvolutional regression network", "Used-For", "bounding box regression"]], "rel_plus": [["fullyconvolutional regression network:Method", "Used-For", "text detection:Task"], ["fullyconvolutional regression network:Method", "Used-For", "bounding box regression:Task"]]}
{"doc_id": "53719742", "sentence": "Zhong et al. [ 4 0 ] and Liao et al. [ 2 0 ] employed the Faster R - CNN and SSD frameworks to solve the word - level horizontal text detection problem , respectively .", "ner": [["Faster R - CNN", "Method"], ["SSD", "Method"], ["word - level horizontal text detection problem", "Task"]], "rel": [["Faster R - CNN", "Used-For", "word - level horizontal text detection problem"], ["SSD", "Used-For", "word - level horizontal text detection problem"]], "rel_plus": [["Faster R - CNN:Method", "Used-For", "word - level horizontal text detection problem:Task"], ["SSD:Method", "Used-For", "word - level horizontal text detection problem:Task"]]}
{"doc_id": "53719742", "sentence": "In order to extend Faster R - CNN and SSD to multi - oriented text detection , Ma et al. [ 2 8 ] and Liu et al. [ 2 5 ] proposed quadrilateral anchors to hunt for inclined text proposals which could better fit the multi - oriented text instances .", "ner": [["Faster R - CNN", "Method"], ["SSD", "Method"], ["multi - oriented text detection", "Task"]], "rel": [["SSD", "Used-For", "multi - oriented text detection"], ["Faster R - CNN", "Used-For", "multi - oriented text detection"]], "rel_plus": [["SSD:Method", "Used-For", "multi - oriented text detection:Task"], ["Faster R - CNN:Method", "Used-For", "multi - oriented text detection:Task"]]}
{"doc_id": "53719742", "sentence": "Intuitively , these methods can be applied for curved text detection , but they make the total text detection pipeline more sophisticated .", "ner": [["curved text detection", "Task"], ["text detection", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "53719742", "sentence": "To overcome the above problems , some recent approaches propose to formulate text detection as an instance segmentation problem so that both straight text and curved text can be detected in a unified manner .", "ner": [["text detection", "Task"], ["instance segmentation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "53719742", "sentence": "In this paper , we borrowed Mask R - CNN , which is the latest state - of - the - art instance segmentation approach , to further enhance the text detection performance .", "ner": [["Mask R - CNN", "Method"], ["instance segmentation", "Task"], ["text detection", "Task"]], "rel": [["Mask R - CNN", "Used-For", "instance segmentation"], ["Mask R - CNN", "Used-For", "text detection"]], "rel_plus": [["Mask R - CNN:Method", "Used-For", "instance segmentation:Task"], ["Mask R - CNN:Method", "Used-For", "text detection:Task"]]}
{"doc_id": "53719742", "sentence": "Instance segmentation is a challenging task because it requires the correct detection of all objects in an image while also precisely segmenting each instance .", "ner": [["Instance segmentation", "Task"], ["detection", "Task"]], "rel": [["detection", "SubTask-Of", "Instance segmentation"]], "rel_plus": [["detection:Task", "SubTask-Of", "Instance segmentation:Task"]]}
{"doc_id": "53719742", "sentence": "More recently , Mask R - CNN [ 1 1 ] extended Faster R - CNN [ 3 2 ] by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition .", "ner": [["Mask R - CNN", "Method"], ["Faster R - CNN", "Method"]], "rel": [["Mask R - CNN", "SubClass-Of", "Faster R - CNN"]], "rel_plus": [["Mask R - CNN:Method", "SubClass-Of", "Faster R - CNN:Method"]]}
{"doc_id": "53719742", "sentence": "It introduced RoIAlign [ 1 1 ] to replace RoIPool [ 8 ] to fix the pixel misalignment and used ResneXt [ 3 6 ] as the base network .", "ner": [["RoIAlign", "Method"], ["RoIPool", "Method"], ["ResneXt", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "53719742", "sentence": "Moreover , it took advantage of Feature Pyramid Network ( FPN [ 2 1 ] ) to strengthen feature representation ability and partially eased the problem of small object detection .", "ner": [["Feature Pyramid Network", "Method"], ["FPN", "Method"], ["small object detection", "Task"]], "rel": [["FPN", "Synonym-Of", "Feature Pyramid Network"], ["Feature Pyramid Network", "Used-For", "small object detection"]], "rel_plus": [["FPN:Method", "Synonym-Of", "Feature Pyramid Network:Method"], ["Feature Pyramid Network:Method", "Used-For", "small object detection:Task"]]}
{"doc_id": "53719742", "sentence": "In this paper , to further enhance the feature representation ability of Mask R - CNN , we propose to incorporate the Pyramid Attention Network ( PAN ) [ 1 8 ] into the Mask R - CNN framework .", "ner": [["Mask R - CNN", "Method"], ["Pyramid Attention Network", "Method"], ["PAN", "Method"], ["Mask R - CNN", "Method"]], "rel": [["PAN", "Synonym-Of", "Pyramid Attention Network"], ["Pyramid Attention Network", "Part-Of", "Mask R - CNN"]], "rel_plus": [["PAN:Method", "Synonym-Of", "Pyramid Attention Network:Method"], ["Pyramid Attention Network:Method", "Part-Of", "Mask R - CNN:Method"]]}
{"doc_id": "53719742", "sentence": "Our Mask R - CNN based text detection network is composed of four modules : 1 ) A PAN backbone network that is responsible for computing a multi - scale convolutional feature pyramid over a full image ; 2 ) A region proposal network ( RPN ) that generates rectangular text proposals ; 3 ) A Fast R - CNN detector that classifies extracted proposals and outputs the corresponding quadrilateral bounding boxes ; 4 ) A mask prediction network that predicts text masks for input proposals .", "ner": [["Mask R - CNN", "Method"], ["text detection", "Task"], ["PAN", "Method"], ["multi - scale convolutional feature pyramid", "Task"], ["region proposal network", "Method"], ["RPN", "Method"], ["Fast R - CNN", "Method"], ["mask prediction network", "Method"]], "rel": [["Mask R - CNN", "Used-For", "text detection"], ["PAN", "Used-For", "multi - scale convolutional feature pyramid"], ["RPN", "Synonym-Of", "region proposal network"]], "rel_plus": [["Mask R - CNN:Method", "Used-For", "text detection:Task"], ["PAN:Method", "Used-For", "multi - scale convolutional feature pyramid:Task"], ["RPN:Method", "Synonym-Of", "region proposal network:Method"]]}
{"doc_id": "53719742", "sentence": "Recently , Li et al. [ 1 8 ] proposed a Pyramid Attention Network ( PAN ) that combines the attention mechanism and spatial pyramid to extract precise dense features for semantic segmentation tasks .", "ner": [["Pyramid Attention Network", "Method"], ["PAN", "Method"], ["attention mechanism", "Method"], ["spatial pyramid", "Method"], ["extract precise dense features", "Task"], ["semantic segmentation", "Task"]], "rel": [["PAN", "Synonym-Of", "Pyramid Attention Network"], ["attention mechanism", "Part-Of", "Pyramid Attention Network"], ["spatial pyramid", "Part-Of", "Pyramid Attention Network"], ["Pyramid Attention Network", "Used-For", "extract precise dense features"], ["extract precise dense features", "Used-For", "semantic segmentation"]], "rel_plus": [["PAN:Method", "Synonym-Of", "Pyramid Attention Network:Method"], ["attention mechanism:Method", "Part-Of", "Pyramid Attention Network:Method"], ["spatial pyramid:Method", "Part-Of", "Pyramid Attention Network:Method"], ["Pyramid Attention Network:Method", "Used-For", "extract precise dense features:Task"], ["extract precise dense features:Task", "Used-For", "semantic segmentation:Task"]]}
{"doc_id": "53719742", "sentence": "It mainly consists of two mod - ules , i.e. , a Feature Pyramid Attention ( FPA ) module and a Global Attention Up - sample ( GAU ) module .", "ner": [["Feature Pyramid Attention", "Method"], ["FPA", "Method"], ["Global Attention Up - sample", "Method"], ["GAU", "Method"]], "rel": [["FPA", "Synonym-Of", "Feature Pyramid Attention"], ["GAU", "Synonym-Of", "Global Attention Up - sample"]], "rel_plus": [["FPA:Method", "Synonym-Of", "Feature Pyramid Attention:Method"], ["GAU:Method", "Synonym-Of", "Global Attention Up - sample:Method"]]}
{"doc_id": "53719742", "sentence": "Owing to these tactful designs , PAN achieves stateof - the - art segmentation performance on the VOC 2 0 1 2 and Cityscapes benchmark tasks .", "ner": [["PAN", "Method"], ["segmentation", "Task"], ["VOC 2 0 1 2", "Dataset"], ["Cityscapes", "Dataset"]], "rel": [["PAN", "Used-For", "segmentation"], ["Cityscapes", "Benchmark-For", "segmentation"], ["VOC 2 0 1 2", "Benchmark-For", "segmentation"], ["PAN", "Evaluated-With", "VOC 2 0 1 2"], ["PAN", "Evaluated-With", "Cityscapes"]], "rel_plus": [["PAN:Method", "Used-For", "segmentation:Task"], ["Cityscapes:Dataset", "Benchmark-For", "segmentation:Task"], ["VOC 2 0 1 2:Dataset", "Benchmark-For", "segmentation:Task"], ["PAN:Method", "Evaluated-With", "VOC 2 0 1 2:Dataset"], ["PAN:Method", "Evaluated-With", "Cityscapes:Dataset"]]}
{"doc_id": "53719742", "sentence": "Inspired by this , we propose to use PAN as a new backbone network to improve the feature representation learning for our Mask R - CNN based text detection model .", "ner": [["PAN", "Method"], ["feature representation learning", "Task"], ["Mask R - CNN based text detection model", "Method"]], "rel": [["PAN", "Used-For", "feature representation learning"], ["PAN", "Part-Of", "Mask R - CNN based text detection model"], ["feature representation learning", "Used-For", "Mask R - CNN based text detection model"]], "rel_plus": [["PAN:Method", "Used-For", "feature representation learning:Task"], ["PAN:Method", "Part-Of", "Mask R - CNN based text detection model:Method"], ["feature representation learning:Task", "Used-For", "Mask R - CNN based text detection model:Method"]]}
{"doc_id": "53719742", "sentence": "We build PAN on top of ResNet 5 0 [ 1 2 ] and ResNeXt 5 0 [ 3 6 ] .", "ner": [["PAN", "Method"], ["ResNet 5 0", "Method"], ["ResNeXt 5 0", "Method"]], "rel": [["ResNet 5 0", "Part-Of", "PAN"], ["ResNeXt 5 0", "Part-Of", "PAN"]], "rel_plus": [["ResNet 5 0:Method", "Part-Of", "PAN:Method"], ["ResNeXt 5 0:Method", "Part-Of", "PAN:Method"]]}
{"doc_id": "53719742", "sentence": "As shown in Fig.   2 , our FPA module takes the output features of the Res - 4 layers in ResNet 5 0 or ResNeXt 5 0 as input , on which it performs 3 \u00d7 3 dilated convolution with sampling rates 3 , 6 , 1 2 respectively to better extract context information .", "ner": [["FPA", "Method"], ["Res - 4 layers", "Method"], ["ResNet 5 0", "Method"], ["ResNeXt 5 0", "Method"], ["3 \u00d7 3 dilated convolution", "Method"]], "rel": [["Res - 4 layers", "Part-Of", "ResNet 5 0"], ["3 \u00d7 3 dilated convolution", "Part-Of", "ResNet 5 0"], ["Res - 4 layers", "Part-Of", "ResNeXt 5 0"], ["3 \u00d7 3 dilated convolution", "Part-Of", "ResNeXt 5 0"]], "rel_plus": [["Res - 4 layers:Method", "Part-Of", "ResNet 5 0:Method"], ["3 \u00d7 3 dilated convolution:Method", "Part-Of", "ResNet 5 0:Method"], ["Res - 4 layers:Method", "Part-Of", "ResNeXt 5 0:Method"], ["3 \u00d7 3 dilated convolution:Method", "Part-Of", "ResNeXt 5 0:Method"]]}
{"doc_id": "53719742", "sentence": "After that , FPA performs a 1 \u00d7 1 convolution on the input Res - 4 features further , whose output is multiplied with the above context features in a pixel - wise manner .", "ner": [["FPA", "Method"], ["1 \u00d7 1 convolution", "Method"]], "rel": [["1 \u00d7 1 convolution", "Part-Of", "FPA"]], "rel_plus": [["1 \u00d7 1 convolution:Method", "Part-Of", "FPA:Method"]]}
{"doc_id": "53719742", "sentence": "The GAU module , as is shown in Fig. 3 , performs 3 \u00d7 3 convolution on the low - level features to reduce channels of feature maps from CNNs .", "ner": [["GAU", "Method"], ["3 \u00d7 3 convolution", "Method"], ["CNNs", "Method"]], "rel": [["3 \u00d7 3 convolution", "Part-Of", "GAU"]], "rel_plus": [["3 \u00d7 3 convolution:Method", "Part-Of", "GAU:Method"]]}
{"doc_id": "53719742", "sentence": "The global context generated from high - level features is through a 1 \u00d7 1 convolution with instance nor - malization [ 3 5 ] and ReLU nonlinearity , then multiplied by the low - level features .", "ner": [["1 \u00d7 1 convolution", "Method"], ["ReLU", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "53719742", "sentence": "With the above FPA and GAU modules , we construct a powerful feature pyramid with three levels , i.e. , P 2 , P 3 and P 4 , whose strides are 4 , 8 and 1 6 , respectively .", "ner": [["FPA", "Method"], ["GAU", "Method"], ["feature pyramid", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "53719742", "sentence": "Three RPNs are attached to P 2 , P 3 and P 4 respectively , each of which slides a small network densely on the corresponding pyramid level to perform text/non - text classification and bounding box regression .", "ner": [["RPNs", "Method"], ["text/non - text classification", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "53719742", "sentence": "Specifically , we design 6 anchors at each sliding position on each pyramid level in { P 2 , P 3 , P 4 } by using 6 aspect ratios { 0. 2 , 0. 5 , 1. 0 , 2. 0 , 4. 0 , 8. 0 } and one scale in { 3 2 , 6 4 , 1 2 8 }. The detection results of all three RPNs are aggregated together to construct a proposal set {D}. Then , we use the standard non - maximum suppression ( NMS ) algorithm with an IoU threshold of 0. 7 to remove redundant proposals in { D } , and select the top - N scoring proposals for the succeeding Fast R - CNN and mask prediction network .", "ner": [["detection", "Task"], ["RPNs", "Method"], ["non - maximum suppression", "Method"], ["NMS", "Method"], ["Fast R - CNN", "Method"], ["mask prediction network", "Method"]], "rel": [["RPNs", "Used-For", "detection"], ["NMS", "Synonym-Of", "non - maximum suppression"]], "rel_plus": [["RPNs:Method", "Used-For", "detection:Task"], ["NMS:Method", "Synonym-Of", "non - maximum suppression:Method"]]}
{"doc_id": "53719742", "sentence": "After the region proposal generation step , extracting effective features for each proposal is critical to the performance of the following Fast R - CNN and mask prediction network .", "ner": [["Fast R - CNN", "Method"], ["mask prediction network", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "53719742", "sentence": "In the original Faster R - CNN [ 3 2 ] , the features of all proposals are extracted from the last convolution layer of the backbone network , which would lead to insufficient features for small proposals .", "ner": [["Faster R - CNN", "Method"], ["convolution layer", "Method"]], "rel": [["convolution layer", "Part-Of", "Faster R - CNN"]], "rel_plus": [["convolution layer:Method", "Part-Of", "Faster R - CNN:Method"]]}
{"doc_id": "53719742", "sentence": "Although more effective , there still exists room for further improvement Then , for Fast R - CNN , the output features are globally average pooled before the final text/non - text classification and quadrilateral bounding box regression layers , while for the mask prediction network , the output features are followed by four consecutive 3 \u00d7 3 convolutional layers and then upsampled before the final mask prediction layers .", "ner": [["Fast R - CNN", "Method"], ["text/non - text classification", "Task"], ["quadrilateral bounding box regression", "Task"], ["mask prediction network", "Method"], ["3 \u00d7 3 convolutional layers", "Method"], ["mask prediction layers", "Method"]], "rel": [["Fast R - CNN", "Used-For", "text/non - text classification"], ["Fast R - CNN", "Used-For", "quadrilateral bounding box regression"], ["mask prediction layers", "Part-Of", "mask prediction network"], ["3 \u00d7 3 convolutional layers", "Part-Of", "mask prediction network"]], "rel_plus": [["Fast R - CNN:Method", "Used-For", "text/non - text classification:Task"], ["Fast R - CNN:Method", "Used-For", "quadrilateral bounding box regression:Task"], ["mask prediction layers:Method", "Part-Of", "mask prediction network:Method"], ["3 \u00d7 3 convolutional layers:Method", "Part-Of", "mask prediction network:Method"]]}
{"doc_id": "53719742", "sentence": "Concretely , for each proposal , we apply ROIAlign over P 2 , P 3 and P 4 pyramid levels respectively and extract three feature descriptors with a fixed spatial size of 7 \u00d7 7 , which are concatenated and dimension reduced with a 1 \u00d7 1 convolutional layer to obtain the final ROI features .", "ner": [["ROIAlign", "Method"], ["1 \u00d7 1 convolutional layer", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "53719742", "sentence": "These ROI features are then fed into the network head for text/non - text classification , quadrilateral bounding box regression and mask prediction .", "ner": [["ROI features", "Method"], ["text/non - text classification", "Task"], ["quadrilateral bounding box regression", "Task"], ["mask prediction", "Task"]], "rel": [["ROI features", "Used-For", "text/non - text classification"], ["ROI features", "Used-For", "quadrilateral bounding box regression"], ["ROI features", "Used-For", "mask prediction"]], "rel_plus": [["ROI features:Method", "Used-For", "text/non - text classification:Task"], ["ROI features:Method", "Used-For", "quadrilateral bounding box regression:Task"], ["ROI features:Method", "Used-For", "mask prediction:Task"]]}
{"doc_id": "53719742", "sentence": "The head includes the 5 - th stage of ResNet 5 0 or ResNeXt 5 0 , which is shared by the Fast R - CNN and mask prediction network .", "ner": [["ResNet 5 0", "Method"], ["ResNeXt 5 0", "Method"], ["Fast R - CNN", "Method"], ["mask prediction network", "Method"]], "rel": [["ResNet 5 0", "Part-Of", "Fast R - CNN"], ["ResNeXt 5 0", "Part-Of", "Fast R - CNN"], ["ResNet 5 0", "Part-Of", "mask prediction network"], ["ResNeXt 5 0", "Part-Of", "mask prediction network"]], "rel_plus": [["ResNet 5 0:Method", "Part-Of", "Fast R - CNN:Method"], ["ResNeXt 5 0:Method", "Part-Of", "Fast R - CNN:Method"], ["ResNet 5 0:Method", "Part-Of", "mask prediction network:Method"], ["ResNeXt 5 0:Method", "Part-Of", "mask prediction network:Method"]]}
{"doc_id": "53719742", "sentence": "There are two sibling output layers for each individual RPN , i.e. , a text/non - text classification layer and a rectangular bounding box regression layer .", "ner": [["RPN", "Method"], ["text/non - text classification layer", "Method"], ["rectangular bounding box regression layer", "Method"]], "rel": [["text/non - text classification layer", "Part-Of", "RPN"], ["rectangular bounding box regression layer", "Part-Of", "RPN"]], "rel_plus": [["text/non - text classification layer:Method", "Part-Of", "RPN:Method"], ["rectangular bounding box regression layer:Method", "Part-Of", "RPN:Method"]]}
{"doc_id": "53719742", "sentence": "The multi - task loss function can be denoted as follows : where c and c * are predicted and ground - truth labels respectively , L R cls ( c , c * ) is a softmax loss for classification tasks ; r and r * represent the predicted and ground - truth 4 - dimensional parameterized regression targets as stated in [ 3 2 ] , L R loc ( r , r * ) is a smooth - L 1 loss [ 8 ] for regression tasks . \u03bb loc is a loss - balancing parameter , and we set \u03bb loc = 3 .", "ner": [["multi - task loss function", "Method"], ["softmax loss", "Method"], ["classification", "Task"]], "rel": [["softmax loss", "Used-For", "classification"]], "rel_plus": [["softmax loss:Method", "Used-For", "classification:Task"]]}
{"doc_id": "53719742", "sentence": "The total loss of RPN L RP N is the sum of the losses of the three RPNs .", "ner": [["RPN", "Method"], ["RPNs", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "53719742", "sentence": "Fast R - CNN also has two sibling output layers : 1 ) A text/non - text classification layer , which is the same as the above - mentioned RPN ; 2 ) A quadrilateral bounding box regression layer .", "ner": [["Fast R - CNN", "Method"], ["text/non - text classification layer", "Method"], ["RPN", "Method"], ["quadrilateral bounding box regression layer", "Method"]], "rel": [["text/non - text classification layer", "Part-Of", "Fast R - CNN"], ["quadrilateral bounding box regression layer", "Part-Of", "Fast R - CNN"], ["text/non - text classification layer", "Part-Of", "RPN"]], "rel_plus": [["text/non - text classification layer:Method", "Part-Of", "Fast R - CNN:Method"], ["quadrilateral bounding box regression layer:Method", "Part-Of", "Fast R - CNN:Method"], ["text/non - text classification layer:Method", "Part-Of", "RPN:Method"]]}
{"doc_id": "53719742", "sentence": "Based on these definitions , the loss function can be defined as follows : The overall loss function for training the proposed Mask R - CNN based text detection model can be denoted as : where \u03bb mask is a loss - balancing parameter for L MASK , and we set \u03bb mask = 0.0 3 1 2 5 .", "ner": [["Mask R - CNN", "Method"], ["text detection", "Task"]], "rel": [["Mask R - CNN", "Used-For", "text detection"]], "rel_plus": [["Mask R - CNN:Method", "Used-For", "text detection:Task"]]}
{"doc_id": "53719742", "sentence": "In each training iteration of RPN , we sample a mini - batch of 1 2 8 positive and 1 2 8 negative anchors for each RPN .", "ner": [["RPN", "Method"], ["RPN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "53719742", "sentence": "We evaluate our proposed method on several standard benchmark tasks including ICDAR - 2 0 1 5 [ 1 7 ] and ICDAR - 2 0 1 7 MLT 2 0 1 7 [ 3 0 ] for multi - oriented text detection , and SCUT - CTW 1 5 0 0 [ 2 6 ] for curved text detection .", "ner": [["ICDAR - 2 0 1 5", "Dataset"], ["ICDAR - 2 0 1 7 MLT 2 0 1 7", "Dataset"], ["multi - oriented text detection", "Task"], ["SCUT - CTW 1 5 0 0", "Dataset"], ["curved text detection", "Task"]], "rel": [["ICDAR - 2 0 1 5", "Benchmark-For", "multi - oriented text detection"], ["ICDAR - 2 0 1 7 MLT 2 0 1 7", "Benchmark-For", "multi - oriented text detection"], ["SCUT - CTW 1 5 0 0", "Benchmark-For", "curved text detection"]], "rel_plus": [["ICDAR - 2 0 1 5:Dataset", "Benchmark-For", "multi - oriented text detection:Task"], ["ICDAR - 2 0 1 7 MLT 2 0 1 7:Dataset", "Benchmark-For", "multi - oriented text detection:Task"], ["SCUT - CTW 1 5 0 0:Dataset", "Benchmark-For", "curved text detection:Task"]]}
{"doc_id": "53719742", "sentence": "ICDAR - 2 0 1 7 MLT is built for the multi - lingual scene text detection and script identification challenge in the ICDAR - 2 0 1 7 Robust Reading Competition , which includes 9 languages : Chinese , Japanese , Korean , English , French , Arabic , Italian , German and Indian .", "ner": [["ICDAR - 2 0 1 7 MLT", "Dataset"], ["multi - lingual scene text detection", "Task"], ["script identification", "Task"], ["ICDAR - 2 0 1 7", "Dataset"]], "rel": [["ICDAR - 2 0 1 7 MLT", "Benchmark-For", "multi - lingual scene text detection"], ["ICDAR - 2 0 1 7 MLT", "Benchmark-For", "script identification"]], "rel_plus": [["ICDAR - 2 0 1 7 MLT:Dataset", "Benchmark-For", "multi - lingual scene text detection:Task"], ["ICDAR - 2 0 1 7 MLT:Dataset", "Benchmark-For", "script identification:Task"]]}
{"doc_id": "53719742", "sentence": "ICDAR - 2 0 1 5 is built for the Incidental Scene Text challenge in the ICDAR - 2 0 1 5 Robust Reading Competition , which contains 1, 0 0 0 and 5 0 0 images for training and testing .", "ner": [["ICDAR - 2 0 1 5", "Dataset"], ["ICDAR - 2 0 1 5", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "53719742", "sentence": "SCUT - CTW 1 5 0 0 is a curved text detection dataset , including 1, 0 0 0 training images and 5 0 0 testing images .", "ner": [["SCUT - CTW 1 5 0 0", "Dataset"], ["curved text detection", "Task"]], "rel": [["SCUT - CTW 1 5 0 0", "Benchmark-For", "curved text detection"]], "rel_plus": [["SCUT - CTW 1 5 0 0:Dataset", "Benchmark-For", "curved text detection:Task"]]}
{"doc_id": "53719742", "sentence": "To make our results comparable to others , we use the online official evaluation tools to evaluate the performance of our approach on ICDAR - 2 0 1 7 MLT and ICDAR - 2 0 1 5 , and use the evaluation tool provided by the authors of [ 2 6 ] on SCUT - CTW 1 5 0 0 .", "ner": [["ICDAR - 2 0 1 7 MLT", "Dataset"], ["ICDAR - 2 0 1 5", "Dataset"], ["SCUT - CTW 1 5 0 0", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "53719742", "sentence": "The weights of ResNet 5 0 or ResNeXt 5 0 related layers in the PAN backbone network are initialized by using the corresponding pre - trained models from the ImageNet classification task [ 1 2 , 3 6 ] .", "ner": [["ResNet 5 0", "Method"], ["ResNeXt 5 0", "Method"], ["PAN", "Method"], ["ImageNet", "Dataset"], ["classification", "Task"]], "rel": [["ResNeXt 5 0", "Part-Of", "PAN"], ["ResNet 5 0", "Part-Of", "PAN"], ["ResNeXt 5 0", "Trained-With", "ImageNet"], ["ResNet 5 0", "Trained-With", "ImageNet"], ["ImageNet", "Benchmark-For", "classification"]], "rel_plus": [["ResNeXt 5 0:Method", "Part-Of", "PAN:Method"], ["ResNet 5 0:Method", "Part-Of", "PAN:Method"], ["ResNeXt 5 0:Method", "Trained-With", "ImageNet:Dataset"], ["ResNet 5 0:Method", "Trained-With", "ImageNet:Dataset"], ["ImageNet:Dataset", "Benchmark-For", "classification:Task"]]}
{"doc_id": "53719742", "sentence": "The weights of the new layers for PAN , RPN , Fast R - CNN and mask prediction network are initialized by using random weights with a Gaussian distribution of mean 0 and standard deviation 0.0 1 .", "ner": [["PAN", "Method"], ["RPN", "Method"], ["Fast R - CNN", "Method"], ["mask prediction network", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "53719742", "sentence": "Our Mask R - CNN based text detection model is trained in an endto - end manner and optimized by the standard SGD algorithm with a momentum of 0. 9 and weight decay of 0.0 0 0 5 .", "ner": [["Mask R - CNN", "Method"], ["text detection", "Task"], ["SGD", "Method"], ["momentum", "Method"], ["weight decay", "Method"]], "rel": [["Mask R - CNN", "Used-For", "text detection"], ["momentum", "Part-Of", "SGD"], ["weight decay", "Part-Of", "SGD"]], "rel_plus": [["Mask R - CNN:Method", "Used-For", "text detection:Task"], ["momentum:Method", "Part-Of", "SGD:Method"], ["weight decay:Method", "Part-Of", "SGD:Method"]]}
{"doc_id": "53719742", "sentence": "For ICDAR - 2 0 1 7 MLT , we use the training and validation data , i.e. , a total of 9, 0 0 0 images for training , while for both ICDAR - 2 0 1 5 and SCUT - CTW 1 5 0 0 , we only use the provided training images for training .", "ner": [["ICDAR - 2 0 1 7 MLT", "Dataset"], ["ICDAR - 2 0 1 5", "Dataset"], ["SCUT - CTW 1 5 0 0", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "53719742", "sentence": "In each training iteration , a selected training image is individually rescaled by randomly sampling a scale S from the set { 4 8 0 , 5 7 6 , 7 2 0 , 9 2 8 , 1 0 8 8 } , { 4 8 0 , 5 7 6 , 6 8 8 , 7 2 0 , 9 2 8 } , and { 3 0 0 , 4 0 0 , 5 0 0 , 6 0 0 , 7 0 4 } for ICDAR - 2 0 1 7 MLT , ICDAR - 2 0 1 5 and SCUT - CTW 1 5 0 0 , respectively .", "ner": [["ICDAR - 2 0 1 7 MLT", "Dataset"], ["ICDAR - 2 0 1 5", "Dataset"], ["SCUT - CTW 1 5 0 0", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "53719742", "sentence": "In the testing phase , we keep the top - 2 0 0 0 scoring text proposals generated by RPN for the succeeding Fast R - CNN .", "ner": [["RPN", "Method"], ["Fast R - CNN", "Method"]], "rel": [["RPN", "Part-Of", "Fast R - CNN"]], "rel_plus": [["RPN:Method", "Part-Of", "Fast R - CNN:Method"]]}
{"doc_id": "53719742", "sentence": "After the Fast R - CNN step , quadrilateral bounding boxes of detected text instances are predicted and suppressed by the Skewed NMS [ 2 8 ]   In this section , we conduct a series of ablation experiments to evaluate the effectiveness of the base convolutional network and PAN on ICDAR - 2 0 1 7 MLT , ICDAR - 2 0 1 5 and SCUT - CTW 1 5 0 0 text detection benchmark datasets .", "ner": [["Fast R - CNN", "Method"], ["Skewed NMS", "Method"], ["convolutional network", "Method"], ["PAN", "Method"], ["ICDAR - 2 0 1 7 MLT", "Dataset"], ["ICDAR - 2 0 1 5", "Dataset"], ["SCUT - CTW 1 5 0 0 text detection", "Dataset"]], "rel": [["PAN", "Evaluated-With", "ICDAR - 2 0 1 7 MLT"], ["convolutional network", "Evaluated-With", "ICDAR - 2 0 1 7 MLT"], ["PAN", "Evaluated-With", "ICDAR - 2 0 1 5"], ["convolutional network", "Evaluated-With", "ICDAR - 2 0 1 5"], ["PAN", "Evaluated-With", "SCUT - CTW 1 5 0 0 text detection"], ["convolutional network", "Evaluated-With", "SCUT - CTW 1 5 0 0 text detection"]], "rel_plus": [["PAN:Method", "Evaluated-With", "ICDAR - 2 0 1 7 MLT:Dataset"], ["convolutional network:Method", "Evaluated-With", "ICDAR - 2 0 1 7 MLT:Dataset"], ["PAN:Method", "Evaluated-With", "ICDAR - 2 0 1 5:Dataset"], ["convolutional network:Method", "Evaluated-With", "ICDAR - 2 0 1 5:Dataset"], ["PAN:Method", "Evaluated-With", "SCUT - CTW 1 5 0 0 text detection:Dataset"], ["convolutional network:Method", "Evaluated-With", "SCUT - CTW 1 5 0 0 text detection:Dataset"]]}
{"doc_id": "53719742", "sentence": "The scales of testing images are set as 1 4 4 0 , 1 0 2 4 and 5 1 2 for ICDAR - 2 0 1 7 MLT , ICDAR - 2 0 1 5 and SCUT - CTW 1 5 0 0 , respectively .", "ner": [["ICDAR - 2 0 1 7 MLT", "Dataset"], ["ICDAR - 2 0 1 5", "Dataset"], ["SCUT - CTW 1 5 0 0", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "53719742", "sentence": "ResNeXt 5 0 is better than ResNet 5 0 .", "ner": [["ResNeXt 5 0", "Method"], ["ResNet 5 0", "Method"]], "rel": [["ResNeXt 5 0", "Compare-With", "ResNet 5 0"]], "rel_plus": [["ResNeXt 5 0:Method", "Compare-With", "ResNet 5 0:Method"]]}
{"doc_id": "53719742", "sentence": "As an important part of a backbone network ( e.g. , ResNet 5 0 - FPN ) , the base convolutional network ( e.g. , ResNet 5 0 ) affects the text detection performance a lot .", "ner": [["ResNet 5 0 - FPN", "Method"], ["convolutional network", "Method"], ["ResNet 5 0", "Method"], ["text detection", "Task"]], "rel": [["ResNet 5 0", "SubClass-Of", "convolutional network"], ["convolutional network", "Used-For", "text detection"]], "rel_plus": [["ResNet 5 0:Method", "SubClass-Of", "convolutional network:Method"], ["convolutional network:Method", "Used-For", "text detection:Task"]]}
{"doc_id": "53719742", "sentence": "Here we compare the performance of two different base convolutional networks , i.e. , Resnet 5 0 and ResneXt 5 0 , on ICDAR - 2 0 1 7 MLT and ICDAR - 2 0 1 5 .", "ner": [["convolutional networks", "Method"], ["Resnet 5 0", "Method"], ["ResneXt 5 0", "Method"], ["ICDAR - 2 0 1 7 MLT", "Dataset"], ["ICDAR - 2 0 1 5", "Dataset"]], "rel": [["ResneXt 5 0", "SubClass-Of", "convolutional networks"], ["Resnet 5 0", "SubClass-Of", "convolutional networks"], ["convolutional networks", "Evaluated-With", "ICDAR - 2 0 1 7 MLT"], ["Resnet 5 0", "Evaluated-With", "ICDAR - 2 0 1 7 MLT"], ["ResneXt 5 0", "Evaluated-With", "ICDAR - 2 0 1 7 MLT"], ["convolutional networks", "Evaluated-With", "ICDAR - 2 0 1 5"], ["Resnet 5 0", "Evaluated-With", "ICDAR - 2 0 1 5"], ["ResneXt 5 0", "Evaluated-With", "ICDAR - 2 0 1 5"]], "rel_plus": [["ResneXt 5 0:Method", "SubClass-Of", "convolutional networks:Method"], ["Resnet 5 0:Method", "SubClass-Of", "convolutional networks:Method"], ["convolutional networks:Method", "Evaluated-With", "ICDAR - 2 0 1 7 MLT:Dataset"], ["Resnet 5 0:Method", "Evaluated-With", "ICDAR - 2 0 1 7 MLT:Dataset"], ["ResneXt 5 0:Method", "Evaluated-With", "ICDAR - 2 0 1 7 MLT:Dataset"], ["convolutional networks:Method", "Evaluated-With", "ICDAR - 2 0 1 5:Dataset"], ["Resnet 5 0:Method", "Evaluated-With", "ICDAR - 2 0 1 5:Dataset"], ["ResneXt 5 0:Method", "Evaluated-With", "ICDAR - 2 0 1 5:Dataset"]]}
{"doc_id": "53719742", "sentence": "As shown in Table 1 and Table 2 , ResneXt 5 0 can consistently outperform ResNet 5 0 .", "ner": [["ResneXt 5 0", "Method"], ["ResNet 5 0", "Method"]], "rel": [["ResneXt 5 0", "Compare-With", "ResNet 5 0"]], "rel_plus": [["ResneXt 5 0:Method", "Compare-With", "ResNet 5 0:Method"]]}
{"doc_id": "53719742", "sentence": "In the following experiments , we will use ResneXt 5 0 as our base convolutional network .   We compare the performance of our approach with other most competitive results on the ICDAR - 2 0 1 7 MLT , ICDAR - 2 0 1 5 and SCUT - CTW 1 5 0 0 text detection benchmark datasets .", "ner": [["ResneXt 5 0", "Method"], ["convolutional network", "Method"], ["ICDAR - 2 0 1 7 MLT", "Dataset"], ["ICDAR - 2 0 1 5", "Dataset"], ["SCUT - CTW 1 5 0 0 text detection", "Dataset"]], "rel": [["ResneXt 5 0", "SubClass-Of", "convolutional network"]], "rel_plus": [["ResneXt 5 0:Method", "SubClass-Of", "convolutional network:Method"]]}
{"doc_id": "53719742", "sentence": "A new Mask R - CNN based text detection approach has been proposed in this paper .", "ner": [["Mask R - CNN", "Method"], ["text detection", "Task"]], "rel": [["Mask R - CNN", "Used-For", "text detection"]], "rel_plus": [["Mask R - CNN:Method", "Used-For", "text detection:Task"]]}
{"doc_id": "53719742", "sentence": "Moreover , we demonstrate that using the Pyramid Attention Network ( PAN ) as a new backbone network of Mask R - CNN enhances the feature representation ability of Mask R - CNN significantly , so that false alarms caused by text - like backgrounds are suppressed more effectively .", "ner": [["Pyramid Attention Network", "Method"], ["PAN", "Method"], ["Mask R - CNN", "Method"], ["Mask R - CNN", "Method"]], "rel": [["PAN", "Synonym-Of", "Pyramid Attention Network"], ["Pyramid Attention Network", "Part-Of", "Mask R - CNN"]], "rel_plus": [["PAN:Method", "Synonym-Of", "Pyramid Attention Network:Method"], ["Pyramid Attention Network:Method", "Part-Of", "Mask R - CNN:Method"]]}
{"doc_id": "53719742", "sentence": "Our proposed approach has achieved superior performance on both multi - oriented ( ICDAR - 2 0 1 5 , DMPNet [ 2 5 ] 0. 5 6 0 0. 6 9 9 0. 6 2 2 EAST [ 4 1 ] 0. 4 9 1 0. 7 8 7 0. 6 0 4 CTPN [ 3 4 ] 0. 5 3 8 0. 6 0 4 0. 5 6 9 Table 6 : Comparison with prior arts on SCUT - CTW 1 5 0 0 .", "ner": [["ICDAR - 2 0 1 5", "Dataset"], ["DMPNet", "Method"], ["EAST", "Method"], ["CTPN", "Method"], ["SCUT - CTW 1 5 0 0", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "53719742", "sentence": "ICDAR - 2 0 1 7 MLT ) and curved ( SCUT - CTW 1 5 0 0 ) text detection benchmark tasks by only using single - scale and single - model testing .", "ner": [["ICDAR - 2 0 1 7 MLT", "Dataset"], ["SCUT - CTW 1 5 0 0", "Dataset"], ["text detection", "Task"]], "rel": [["SCUT - CTW 1 5 0 0", "Benchmark-For", "text detection"], ["ICDAR - 2 0 1 7 MLT", "Benchmark-For", "text detection"]], "rel_plus": [["SCUT - CTW 1 5 0 0:Dataset", "Benchmark-For", "text detection:Task"], ["ICDAR - 2 0 1 7 MLT:Dataset", "Benchmark-For", "text detection:Task"]]}
{"doc_id": "53719742", "sentence": "First , the running speed of our approach is not fast enough due to the computation intensive PAN backbone network and Mask R - CNN framework .", "ner": [["PAN", "Method"], ["Mask R - CNN", "Method"]], "rel": [["PAN", "Part-Of", "Mask R - CNN"]], "rel_plus": [["PAN:Method", "Part-Of", "Mask R - CNN:Method"]]}
{"doc_id": "59599694", "sentence": "A fuzzy clustering method finds cluster of neighboring time series based on similarity of time series residuals ; as they can be meaningful short - term patterns for spatial time series .", "ner": [["fuzzy clustering", "Method"], ["spatial time series", "Task"]], "rel": [["fuzzy clustering", "Used-For", "spatial time series"]], "rel_plus": [["fuzzy clustering:Method", "Used-For", "spatial time series:Task"]]}
{"doc_id": "59599694", "sentence": "The output of convolution layer is concatenated by trends and followed by convolution - LSTM layer to capture long - term patterns in larger regional areas .", "ner": [["convolution", "Method"], ["convolution - LSTM", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "59599694", "sentence": "Recently , several new scalable time series analyses have been studied , such as forecasting [ 1 ] , anomaly detection [ 2 ] , classification [ 3 ] and clustering [ 4 ] .", "ner": [["time series analyses", "Task"], ["forecasting", "Task"], ["anomaly detection", "Task"], ["classification", "Task"], ["clustering", "Task"]], "rel": [["forecasting", "SubTask-Of", "time series analyses"], ["anomaly detection", "SubTask-Of", "time series analyses"], ["classification", "SubTask-Of", "time series analyses"], ["clustering", "SubTask-Of", "time series analyses"]], "rel_plus": [["forecasting:Task", "SubTask-Of", "time series analyses:Task"], ["anomaly detection:Task", "SubTask-Of", "time series analyses:Task"], ["classification:Task", "SubTask-Of", "time series analyses:Task"], ["clustering:Task", "SubTask-Of", "time series analyses:Task"]]}
{"doc_id": "59599694", "sentence": "Spatial - temporal data arise in diverse areas of power grids [ 5 ] , load demand forecasting [ 6 ] , weather forecasting [ 7 ] , smart city applications [ 8 ] , and transportation systems , such as traffic flow forecasting [ 9 ] , [ 1 0 ] .", "ner": [["power grids", "Task"], ["load demand forecasting", "Task"], ["weather forecasting", "Task"], ["smart city applications", "Task"], ["traffic flow forecasting", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "59599694", "sentence": "Starting in the 1 9 7 0 's with the original work of Gazis and Knapp [ 1 4 ] there have been many studies applying time series forecasting techniques to traffic flow prediction problem , including parametric techniques , such as auto - regressive integrated moving average ( ARIMA ) [ 1 5 ] and Seasonal - ARIMA [ 1 6 ] , and statistical techniques , such as Bayesian analysis [ 1 7 ] , Markov chain [ 1 8 ] and Bayesian networks [ 1 9 ] .", "ner": [["time series forecasting techniques", "Method"], ["traffic flow prediction", "Task"], ["parametric techniques", "Method"], ["auto - regressive integrated moving average", "Method"], ["ARIMA", "Method"], ["Seasonal - ARIMA", "Method"], ["statistical techniques", "Method"], ["Bayesian analysis", "Method"], ["Markov chain", "Method"], ["Bayesian networks", "Method"]], "rel": [["time series forecasting techniques", "Used-For", "traffic flow prediction"], ["auto - regressive integrated moving average", "SubClass-Of", "parametric techniques"], ["Seasonal - ARIMA", "SubClass-Of", "parametric techniques"], ["ARIMA", "Synonym-Of", "auto - regressive integrated moving average"], ["Bayesian analysis", "SubClass-Of", "statistical techniques"], ["Markov chain", "SubClass-Of", "statistical techniques"], ["Bayesian networks", "SubClass-Of", "statistical techniques"]], "rel_plus": [["time series forecasting techniques:Method", "Used-For", "traffic flow prediction:Task"], ["auto - regressive integrated moving average:Method", "SubClass-Of", "parametric techniques:Method"], ["Seasonal - ARIMA:Method", "SubClass-Of", "parametric techniques:Method"], ["ARIMA:Method", "Synonym-Of", "auto - regressive integrated moving average:Method"], ["Bayesian analysis:Method", "SubClass-Of", "statistical techniques:Method"], ["Markov chain:Method", "SubClass-Of", "statistical techniques:Method"], ["Bayesian networks:Method", "SubClass-Of", "statistical techniques:Method"]]}
{"doc_id": "59599694", "sentence": "The primary work related to ours proposes a stacked autoencoder ( SAE ) model to learn traffic flow features and illustrate the advantage of SAE model versus Multi - layer Perceptron [ 1 ] .", "ner": [["stacked autoencoder", "Method"], ["SAE", "Method"], ["SAE", "Method"], ["Multi - layer Perceptron", "Method"]], "rel": [["SAE", "Synonym-Of", "stacked autoencoder"], ["SAE", "Compare-With", "Multi - layer Perceptron"]], "rel_plus": [["SAE:Method", "Synonym-Of", "stacked autoencoder:Method"], ["SAE:Method", "Compare-With", "Multi - layer Perceptron:Method"]]}
{"doc_id": "59599694", "sentence": "In [ 2 0 ] , they propose stacked autoencoders with multi - task learning at the top layers of the neural network .", "ner": [["autoencoders with multi - task learning", "Method"], ["neural network", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "59599694", "sentence": "A Deep Belief Network(DBN ) composed by layers of restricted boltzman machine is proposed [ 2 1 ] .", "ner": [["Deep Belief Network(DBN )", "Method"], ["restricted boltzman machine", "Method"]], "rel": [["restricted boltzman machine", "Part-Of", "Deep Belief Network(DBN )"]], "rel_plus": [["restricted boltzman machine:Method", "Part-Of", "Deep Belief Network(DBN ):Method"]]}
{"doc_id": "59599694", "sentence": "In [ 2 3 ] , an ensemble of DBN with Support Vector Regression for aggregation of outputs is proposed for time series forecasting problem .", "ner": [["DBN", "Method"], ["Support Vector Regression", "Method"], ["time series forecasting", "Task"]], "rel": [["Support Vector Regression", "Part-Of", "DBN"], ["DBN", "Used-For", "time series forecasting"]], "rel_plus": [["Support Vector Regression:Method", "Part-Of", "DBN:Method"], ["DBN:Method", "Used-For", "time series forecasting:Task"]]}
{"doc_id": "59599694", "sentence": "Convolutional Neural Networks ( CNN ) extract features of various types of input data , such as images , videos , and audio .", "ner": [["Convolutional Neural Networks", "Method"], ["CNN", "Method"]], "rel": [["CNN", "Synonym-Of", "Convolutional Neural Networks"]], "rel_plus": [["CNN:Method", "Synonym-Of", "Convolutional Neural Networks:Method"]]}
{"doc_id": "59599694", "sentence": "The performance of deep CNNs in multi - variate time series forecasting is examined ; in [ 2 5 ] , a spatial - temporal relation of traffic flow data is represented as images .", "ner": [["CNNs", "Method"], ["multi - variate time series forecasting", "Task"]], "rel": [["CNNs", "Used-For", "multi - variate time series forecasting"]], "rel_plus": [["CNNs:Method", "Used-For", "multi - variate time series forecasting:Task"]]}
{"doc_id": "59599694", "sentence": "In [ 2 6 ] , they studied image - like representation of spatial time series data using convolution layers and ensemble learning .", "ner": [["spatial time series", "Task"], ["convolution", "Method"], ["ensemble learning", "Method"]], "rel": [["ensemble learning", "Used-For", "spatial time series"], ["convolution", "Used-For", "spatial time series"]], "rel_plus": [["ensemble learning:Method", "Used-For", "spatial time series:Task"], ["convolution:Method", "Used-For", "spatial time series:Task"]]}
{"doc_id": "59599694", "sentence": "Moreover , in the presence of temporal data , recurrent neural networks have shown great performance in time series forecasting [ 3 0 ] .", "ner": [["recurrent neural networks", "Method"], ["time series forecasting", "Task"]], "rel": [["recurrent neural networks", "Used-For", "time series forecasting"]], "rel_plus": [["recurrent neural networks:Method", "Used-For", "time series forecasting:Task"]]}
{"doc_id": "59599694", "sentence": "The vanishing gradient in deep Multilayer perceptron and recurrent neural network problem is solved by employing a Long - Short Term Model ( LSTM ) [ 3 1 ] , which significantly improves time series forecasting [ 3 2 ] , traffic speed prediction [ 3 3 ] and traffic flow estimation with missing data [ 3 4 ] .", "ner": [["deep Multilayer perceptron", "Method"], ["recurrent neural network", "Method"], ["Long - Short Term Model", "Method"], ["LSTM", "Method"], ["time series forecasting", "Task"], ["traffic speed prediction", "Task"], ["traffic flow estimation", "Task"]], "rel": [["LSTM", "Synonym-Of", "Long - Short Term Model"], ["Long - Short Term Model", "Used-For", "time series forecasting"], ["Long - Short Term Model", "Used-For", "traffic speed prediction"], ["Long - Short Term Model", "Used-For", "traffic flow estimation"]], "rel_plus": [["LSTM:Method", "Synonym-Of", "Long - Short Term Model:Method"], ["Long - Short Term Model:Method", "Used-For", "time series forecasting:Task"], ["Long - Short Term Model:Method", "Used-For", "traffic speed prediction:Task"], ["Long - Short Term Model:Method", "Used-For", "traffic flow estimation:Task"]]}
{"doc_id": "59599694", "sentence": "While convolutional neural networks can exhibit excellent performance on spatial data , and recurrent neural networks have advantages on problems with temporal data ; spatial - temporal problems combine both of these .", "ner": [["convolutional neural networks", "Method"], ["recurrent neural networks", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "59599694", "sentence": "In [ 3 5 ] , they propose convolutional - LSTM layer for weather forecasting problem , in which consider spatiotemporal sequences .", "ner": [["LSTM", "Method"], ["weather forecasting", "Task"]], "rel": [["LSTM", "Used-For", "weather forecasting"]], "rel_plus": [["LSTM:Method", "Used-For", "weather forecasting:Task"]]}
{"doc_id": "59599694", "sentence": "A convolutional layer is followed by an LSTM layer for downstream and upstream traffic flow data .", "ner": [["convolutional layer", "Method"], ["LSTM", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "59599694", "sentence": "In [ 3 8 ] , they illustrate a CNN and gated CNN followed by attention layers for spatialtemporal data .", "ner": [["CNN", "Method"], ["gated CNN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "59599694", "sentence": "In the aforementioned works , spatial time series forecasting has been studied with the objective of proposing various types of convolution and recurrent neural network layers .", "ner": [["spatial time series forecasting", "Task"], ["convolution", "Method"], ["recurrent neural network", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "59599694", "sentence": "In existence of long - term patterns , an LSTM layers shows great performance in forecasting problems because it can separately capture detrending data .", "ner": [["LSTM", "Method"], ["forecasting", "Task"]], "rel": [["LSTM", "Used-For", "forecasting"]], "rel_plus": [["LSTM:Method", "Used-For", "forecasting:Task"]]}
{"doc_id": "59599694", "sentence": "The contribution of the paper is described as follows : \u2022 We illustrate an approach for explicitly considering various types of patterns in a deep neural network architecture for a spatial multi - variate time series forecasting problem . \u2022 We describe a Dynamic Time Warping - based clustering method and time series decomposition with the objective of finding compact regions with similar time series residuals . \u2022 A multi - kernel convolution layer is designed for spatial time series data , to keep the spatial structure of time series data and extract short - term and spatial patterns .", "ner": [["multi - variate time series forecasting", "Task"], ["Dynamic Time Warping", "Method"], ["clustering", "Task"], ["multi - kernel convolution layer", "Method"], ["spatial time series", "Task"]], "rel": [["Dynamic Time Warping", "Used-For", "clustering"], ["multi - kernel convolution layer", "Used-For", "spatial time series"]], "rel_plus": [["Dynamic Time Warping:Method", "Used-For", "clustering:Task"], ["multi - kernel convolution layer:Method", "Used-For", "spatial time series:Task"]]}
{"doc_id": "59599694", "sentence": "It follows by a convolution - LSTM component to capture long - term patterns from trends , and a pretrained denoising autoencoder to have robust prediction to missing data . \u2022 The spatial and temporal patterns in traffic flow data is analyzed and the performance gains of the proposed model relative to baseline and state - of - art - the - art deep neural networks are illustrated for a traffic flow prediction , capturing meaningful time series residuals and a robust prediction to missing data .", "ner": [["convolution - LSTM", "Method"], ["denoising autoencoder", "Method"], ["deep neural networks", "Method"], ["traffic flow prediction", "Task"]], "rel": [["deep neural networks", "Used-For", "traffic flow prediction"]], "rel_plus": [["deep neural networks:Method", "Used-For", "traffic flow prediction:Task"]]}
{"doc_id": "59599694", "sentence": "Algorithm 1 Multi - dimensional Dynamic Time Warping Two input time series X , Y \u2190 Normalize(X , Y ) 6 : Initialization of distance and path matrix 7 : for i \u2190 2 to N do 9 : end for for j \u2190 2 to M do 1 2 : end for 1 4 : for i \u2190 2 to N do 1 5 : for j \u2190 2 to M do 1 6 : 1 7 : end for end for 2 0 : Return the nonlinear distance of two time series 2 1 : end procedure Here , we detail the core components of the proposed approach , including Fuzzy Hierarchical Agglomerative Clustering , Convolutional layers , Convolutional LSTM layers and Denoising Autoencoder .", "ner": [["Multi - dimensional Dynamic Time Warping", "Method"], ["Fuzzy Hierarchical Agglomerative Clustering", "Method"], ["Convolutional layers", "Method"], ["Convolutional LSTM", "Method"], ["Denoising Autoencoder", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "59599694", "sentence": "A Dynamic time warping ( DTW ) algorithm finds an optimal path between two time series .", "ner": [["Dynamic time warping", "Method"], ["DTW", "Method"]], "rel": [["DTW", "Synonym-Of", "Dynamic time warping"]], "rel_plus": [["DTW:Method", "Synonym-Of", "Dynamic time warping:Method"]]}
{"doc_id": "59599694", "sentence": "Algorithm ( 1 ) finds the minimum distance between two K - dimensional time series with size of N and M. Given data points X = { x 1 , . . . , x n } , a fuzzy hierarchical clustering method finds a membership matrix C \u2208 R n \u00d7 c , where c is the number of clusters and C ij \u2208 [ 0 , 1 ] illustrates the distance of data points i to cluster j. To apply a DTW - based clustering method , the main challenge is to compute the mean of a cluster addressed in [ 4 0 ] , [ 4 1 ] , [ 3 9 ] , because the initial values impacts on the final results of the algorithm .", "ner": [["fuzzy hierarchical clustering method", "Method"], ["DTW", "Method"], ["clustering", "Task"]], "rel": [["DTW", "Used-For", "clustering"]], "rel_plus": [["DTW:Method", "Used-For", "clustering:Task"]]}
{"doc_id": "59599694", "sentence": "Given an input matrix X \u2208 R n \u00d7 t \u00d7 k , a 2 - dimension convolution layer has a weight matrix W \u2208 R a \u00d7 b \u00d7 k , called as a kernel , where a \u2264 n and b \u2264 t. A convolution multiplication X * W with strides s 1 and s 2 is obtained by sliding a kernel all over input matrix .", "ner": [["2 - dimension convolution", "Method"], ["convolution", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "59599694", "sentence": "Given X l as the input of layer l , a layer l + 1 obtains by X l+ 1 = \u03c3(X l * W l + b ) for an activation function \u03c3 ( . ) and bias vector b. Pooling layers X l+ 1 = maxPool(X l ) among successive convolution layers reduces size of hidden layers , while extract features in locally connected layers , which selects the maximum value in a matrix of sizeW \u2208 R m \u00d7 n , and reduce the dimension of layers divided by m and n. A Long - Short Term Memory ( LSTM ) is a special recurrent neural network cell with powerful modelling of long - term dependencies [ 3 1 ] .", "ner": [["Pooling", "Method"], ["convolution", "Method"], ["Long - Short Term Memory", "Method"], ["LSTM", "Method"], ["recurrent neural network", "Method"]], "rel": [["LSTM", "Synonym-Of", "Long - Short Term Memory"], ["Long - Short Term Memory", "SubClass-Of", "recurrent neural network"]], "rel_plus": [["LSTM:Method", "Synonym-Of", "Long - Short Term Memory:Method"], ["Long - Short Term Memory:Method", "SubClass-Of", "recurrent neural network:Method"]]}
{"doc_id": "59599694", "sentence": "A memory cell c A convolution - LSTM layer have same structure of convolution layers , but having LSTM cells .", "ner": [["convolution - LSTM", "Method"], ["convolution", "Method"], ["LSTM", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "59599694", "sentence": "The convolution - LSTM layer has a input of X \u2208 R w \u00d7 a \u00d7 b \u00d7 k , where w is time windows and the matrix W \u2208 R a \u00d7 b \u00d7 k is the spatial information on a grid of size a and b and each element W ij has k features .", "ner": [["convolution - LSTM", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "59599694", "sentence": "Applying algorithm 2 with aforementioned modifications on spatial time series finds fuzzy clusters of time series based on DTW distance .   The details of the deep neural network is represented in Fig. 2 .", "ner": [["spatial time series", "Task"], ["DTW", "Method"]], "rel": [["DTW", "Used-For", "spatial time series"]], "rel_plus": [["DTW:Method", "Used-For", "spatial time series:Task"]]}
{"doc_id": "59599694", "sentence": "Several convolution - RELU - Pooling layers extracts shortterm and spatial patterns from the time series residuals in each neighborhood .", "ner": [["convolution - RELU - Pooling", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "59599694", "sentence": "The output of kernels are concatenated and connected to a fully - connected layer ) and represented with a hidden layer h l+ 1 \u2208 R w \u00d7 s \u00d7 v \u00d7 1 , where v is the number of represented features in convolution layers and s is the total number of sensors .", "ner": [["fully - connected layer", "Method"], ["convolution", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "59599694", "sentence": "The model follows by a 2 - dimension convolution LSTM layers .", "ner": [["2 - dimension convolution LSTM", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "59599694", "sentence": "A 2 - dimension convolution LSTM layer , described in section 3. 4 , receives an input h l+ 2 , and apply the convolution on the matrix of size ( a = s , b \u2264 v ) with two channels .", "ner": [["2 - dimension convolution LSTM", "Method"], ["convolution", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "59599694", "sentence": "This convolutional layer has different architecture with the first multi - kernel convolution layer , that is , each neural cell is an LSTM cell and is applied on all input sensors .", "ner": [["convolutional layer", "Method"], ["multi - kernel convolution layer", "Method"], ["LSTM", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "59599694", "sentence": "Some layers of convolution LSTM layers extract features from residuals and trends .", "ner": [["convolution LSTM", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "59599694", "sentence": "In the pretraining step , for a prediction horizon h and a cluster j , each denoising autoencoder decoder generates x = DA j ( x ) , where x \u2208 R s \u00d7 h \u00d7 k and drop out layer are between each successive layers .", "ner": [["denoising autoencoder", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "59599694", "sentence": "A denoising autoencoder component generates a predictions\u0233 d = DA(\u0233 ) .", "ner": [["denoising autoencoder", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "59599694", "sentence": "As the output of autoencoders is designed based on the clusters , there are some sensors x k \u2208 c i \u2229c j , i = j , where the fully - connected target layer FC t is connected to all common variables between denoising autoencoders with a linear activation function y output = FC t ( DA 1 ( \u0233 ) , . . . , DA |C| ( \u0233 ) ) .", "ner": [["autoencoders", "Method"], ["denoising autoencoders", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "59599694", "sentence": "We use traffic flow data from the Bay Area of California represented in Fig. 3 which is commonly used and available in PEMS [ 4 7 ] .", "ner": [["traffic flow data from the Bay Area of California", "Dataset"], ["PEMS", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "59599694", "sentence": "By contrast , presented in section 5. 2 . 1 , the average DTW distance of time series residuals for all pair of sensors is 4. 5 , while applying the fuzzy clustering method on time series reduces the average DTW of clusters to 0. 6 .", "ner": [["DTW", "Method"], ["fuzzy clustering", "Method"], ["DTW", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "59599694", "sentence": "For MLP , LSTM , CNN and the proposed multikernel CNN - LSTM models , the input dimension is reshaped to have a appropriate dimensions , described in model details section 5. 4 . 2 .", "ner": [["MLP", "Method"], ["LSTM", "Method"], ["CNN", "Method"], ["multikernel CNN - LSTM", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "59599694", "sentence": "For the models without time series decomposition component , including MLP , LSTM and CNN , we transform the data into stationary data by subtracting all input values from the value at time step t , while detrending of the models with time series decomposition components is as follows .", "ner": [["MLP", "Method"], ["LSTM", "Method"], ["CNN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "59599694", "sentence": "A Multi - layer perceptron ( MLP ) with three fully connected layers and Xavier initialization [ 5 0 ] , RELU activation function , and ( 5 0 0 , 3 0 0 , 2 0 0 ) hidden units is used .", "ner": [["Multi - layer perceptron", "Method"], ["MLP", "Method"], ["fully connected layers", "Method"], ["Xavier initialization", "Method"], ["RELU", "Method"]], "rel": [["MLP", "Synonym-Of", "Multi - layer perceptron"], ["fully connected layers", "Part-Of", "Multi - layer perceptron"], ["Xavier initialization", "Part-Of", "Multi - layer perceptron"], ["RELU", "Part-Of", "Multi - layer perceptron"]], "rel_plus": [["MLP:Method", "Synonym-Of", "Multi - layer perceptron:Method"], ["fully connected layers:Method", "Part-Of", "Multi - layer perceptron:Method"], ["Xavier initialization:Method", "Part-Of", "Multi - layer perceptron:Method"], ["RELU:Method", "Part-Of", "Multi - layer perceptron:Method"]]}
{"doc_id": "59599694", "sentence": "A deep belief network ( DBN ) with greedy layer wise pretraining of autoencoders finds a good initialization for a fully - connected neural network .", "ner": [["deep belief network", "Method"], ["DBN", "Method"], ["greedy layer", "Method"], ["autoencoders", "Method"], ["fully - connected neural network", "Method"]], "rel": [["DBN", "Synonym-Of", "deep belief network"], ["greedy layer", "Part-Of", "deep belief network"]], "rel_plus": [["DBN:Method", "Synonym-Of", "deep belief network:Method"], ["greedy layer:Method", "Part-Of", "deep belief network:Method"]]}
{"doc_id": "59599694", "sentence": "Fully connected Long - Short Term Memory neural network ( LSTM ) is capable of capturing long - term temporal patterns .", "ner": [["Long - Short Term Memory neural network", "Method"], ["LSTM", "Method"]], "rel": [["LSTM", "Synonym-Of", "Long - Short Term Memory neural network"]], "rel_plus": [["LSTM:Method", "Synonym-Of", "Long - Short Term Memory neural network:Method"]]}
{"doc_id": "59599694", "sentence": "To use a convolutional neural network ( CNN ) for time series forecasting , the input matrix is reshaped to three dimension ( w , s , k ) .", "ner": [["convolutional neural network", "Method"], ["CNN", "Method"], ["time series forecasting", "Task"]], "rel": [["CNN", "Part-Of", "convolutional neural network"], ["convolutional neural network", "Used-For", "time series forecasting"]], "rel_plus": [["CNN:Method", "Part-Of", "convolutional neural network:Method"], ["convolutional neural network:Method", "Used-For", "time series forecasting:Task"]]}
{"doc_id": "59599694", "sentence": "The optimum implemented deep CNN model has four layers with max - pooling and batch normalization layers .", "ner": [["CNN", "Method"], ["max - pooling", "Method"], ["batch normalization", "Method"]], "rel": [["max - pooling", "Part-Of", "CNN"], ["batch normalization", "Part-Of", "CNN"]], "rel_plus": [["max - pooling:Method", "Part-Of", "CNN:Method"], ["batch normalization:Method", "Part-Of", "CNN:Method"]]}
{"doc_id": "59599694", "sentence": "Two fully connected layer connect the convolution layers to output layer .", "ner": [["fully connected layer", "Method"], ["convolution", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "59599694", "sentence": "The ( CNN - LSTM ) model captures short - term and spatial patterns in CNN layers , and temporal patterns in LSTM layers .", "ner": [["CNN - LSTM", "Method"], ["CNN", "Method"], ["LSTM", "Method"]], "rel": [["CNN", "Part-Of", "CNN - LSTM"], ["LSTM", "Part-Of", "CNN - LSTM"]], "rel_plus": [["CNN:Method", "Part-Of", "CNN - LSTM:Method"], ["LSTM:Method", "Part-Of", "CNN - LSTM:Method"]]}
{"doc_id": "59599694", "sentence": "An LSTM layer of size ( 3 0 0 , 1 5 0 ) follows the output of CNN model , following by a fully connected layer .", "ner": [["LSTM", "Method"], ["CNN", "Method"], ["fully connected layer", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "59599694", "sentence": "The model ( C - CNN - LSTM ) is a clustering based CNN - LSTM , in which a multi - kernel convolution layer extract spatial , short - term patterns from time series residuals .", "ner": [["C - CNN - LSTM", "Method"], ["clustering", "Task"], ["CNN - LSTM", "Method"], ["multi - kernel convolution layer", "Method"]], "rel": [["multi - kernel convolution layer", "Part-Of", "C - CNN - LSTM"], ["C - CNN - LSTM", "Used-For", "clustering"], ["CNN - LSTM", "Used-For", "clustering"]], "rel_plus": [["multi - kernel convolution layer:Method", "Part-Of", "C - CNN - LSTM:Method"], ["C - CNN - LSTM:Method", "Used-For", "clustering:Task"], ["CNN - LSTM:Method", "Used-For", "clustering:Task"]]}
{"doc_id": "59599694", "sentence": "A pretraining denoising stacked auto encoder decoder is applied on each cluster of sensors to generate a robust output .", "ner": [["denoising", "Task"], ["auto encoder", "Method"]], "rel": [["auto encoder", "Used-For", "denoising"]], "rel_plus": [["auto encoder:Method", "Used-For", "denoising:Task"]]}
{"doc_id": "59599694", "sentence": "As the average size of clusters is nearly 1 0 and standard deviation of 4 , described in section 5. 3 , we used a same size of architecture for all of them with size of ( 4 0 , 2 0 , 1 0 , 2 0 , 4 0 ) units with fully connected layers , and RELU activation function .", "ner": [["fully connected layers", "Method"], ["RELU activation function", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "59599694", "sentence": "The Cluster - based CNN - LSTM with Denoising autoencoder ( C - CNN - LSTM - DA ) is the proposed model in section 4 which uses clustering of time series residuals , trends , and seasonal along with denoising autoencoders and time series decomposition components for each cluster .", "ner": [["Cluster - based CNN - LSTM", "Method"], ["Denoising autoencoder", "Method"], ["C - CNN - LSTM - DA", "Method"], ["clustering", "Task"], ["denoising autoencoders", "Method"]], "rel": [["Denoising autoencoder", "Part-Of", "Cluster - based CNN - LSTM"], ["C - CNN - LSTM - DA", "Synonym-Of", "Denoising autoencoder"], ["Cluster - based CNN - LSTM", "Used-For", "clustering"]], "rel_plus": [["Denoising autoencoder:Method", "Part-Of", "Cluster - based CNN - LSTM:Method"], ["C - CNN - LSTM - DA:Method", "Synonym-Of", "Denoising autoencoder:Method"], ["Cluster - based CNN - LSTM:Method", "Used-For", "clustering:Task"]]}
{"doc_id": "59599694", "sentence": "The proposed architecture , in section 4 , consists of 2 convolution layers with RELU and max - pooling layers with filters ( 3 2 , 6 4 ) .", "ner": [["convolution layers", "Method"], ["RELU", "Method"], ["max - pooling", "Method"]], "rel": [["RELU", "Part-Of", "convolution layers"], ["max - pooling", "Part-Of", "convolution layers"]], "rel_plus": [["RELU:Method", "Part-Of", "convolution layers:Method"], ["max - pooling:Method", "Part-Of", "convolution layers:Method"]]}
{"doc_id": "59599694", "sentence": "It follows by two fully connected layers , two 2 - dimension convolutional LSTM for capturing long - term patterns ( 1 6 , 3 2 ) .", "ner": [["fully connected layers", "Method"], ["2 - dimension convolutional LSTM", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "59599694", "sentence": "The LSTM model has a better performance than MLP , BN and CNN models , demonstrating its improved performance in time series forecasting .", "ner": [["LSTM", "Method"], ["MLP", "Method"], ["BN", "Method"], ["CNN", "Method"], ["time series forecasting", "Task"]], "rel": [["LSTM", "Compare-With", "MLP"], ["LSTM", "Compare-With", "BN"], ["LSTM", "Compare-With", "CNN"], ["LSTM", "Used-For", "time series forecasting"], ["MLP", "Used-For", "time series forecasting"], ["BN", "Used-For", "time series forecasting"], ["CNN", "Used-For", "time series forecasting"]], "rel_plus": [["LSTM:Method", "Compare-With", "MLP:Method"], ["LSTM:Method", "Compare-With", "BN:Method"], ["LSTM:Method", "Compare-With", "CNN:Method"], ["LSTM:Method", "Used-For", "time series forecasting:Task"], ["MLP:Method", "Used-For", "time series forecasting:Task"], ["BN:Method", "Used-For", "time series forecasting:Task"], ["CNN:Method", "Used-For", "time series forecasting:Task"]]}
{"doc_id": "59599694", "sentence": "CNN - LSTM models are more capable for capturing short - term and long - term patterns an are comparable with LSTM .", "ner": [["CNN - LSTM", "Method"], ["LSTM", "Method"]], "rel": [["CNN - LSTM", "Compare-With", "LSTM"]], "rel_plus": [["CNN - LSTM:Method", "Compare-With", "LSTM:Method"]]}
{"doc_id": "59599694", "sentence": "Two models , C - CNN - LSTM and C - CNN - LSTM - DA , have better performance due to explicitly separating spatial regions .", "ner": [["C - CNN - LSTM", "Method"], ["C - CNN - LSTM - DA", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "59599694", "sentence": "The performance of C - CNN - LSTM and C - CNN - LSTM - DA is almost quite close .", "ner": [["C - CNN - LSTM", "Method"], ["C - CNN - LSTM - DA", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "59599694", "sentence": "In Fig.   9 , the output of the C - CNN - LSTM - DA and MLP models are illustrated .", "ner": [["C - CNN - LSTM - DA", "Method"], ["MLP", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "59599694", "sentence": "Among neural network models , the MLP model has the worst traffic flow prediction performance , while C - CNN - LSTM - DA is the best in Table 1 .", "ner": [["MLP", "Method"], ["traffic flow prediction", "Task"], ["C - CNN - LSTM - DA", "Method"]], "rel": [["MLP", "Used-For", "traffic flow prediction"], ["MLP", "Compare-With", "C - CNN - LSTM - DA"]], "rel_plus": [["MLP:Method", "Used-For", "traffic flow prediction:Task"], ["MLP:Method", "Compare-With", "C - CNN - LSTM - DA:Method"]]}
{"doc_id": "59599694", "sentence": "We select the MLP and LSTM models which only capture temporal patterns , along with the proposed model which carefully captures spatial patterns .", "ner": [["MLP", "Method"], ["LSTM", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "59599694", "sentence": "However , the performance of C - CNN - LSTM - DA in peak hours is highly better than LSTM model .", "ner": [["C - CNN - LSTM - DA", "Method"], ["LSTM", "Method"]], "rel": [["C - CNN - LSTM - DA", "Compare-With", "LSTM"]], "rel_plus": [["C - CNN - LSTM - DA:Method", "Compare-With", "LSTM:Method"]]}
{"doc_id": "59599694", "sentence": "In Fig. 1 0 , we plot the comparison of LSTM and C - CNN - LSTM - DA .", "ner": [["LSTM", "Method"], ["C - CNN - LSTM - DA", "Method"]], "rel": [["LSTM", "Compare-With", "C - CNN - LSTM - DA"]], "rel_plus": [["LSTM:Method", "Compare-With", "C - CNN - LSTM - DA:Method"]]}
{"doc_id": "59599694", "sentence": "It is shown that C - CNN - LSTM - DA captures big residuals compared to LSTM model .", "ner": [["C - CNN - LSTM - DA", "Method"], ["LSTM", "Method"]], "rel": [["C - CNN - LSTM - DA", "Compare-With", "LSTM"]], "rel_plus": [["C - CNN - LSTM - DA:Method", "Compare-With", "LSTM:Method"]]}
{"doc_id": "59599694", "sentence": "The figure shows a reduced increase of error in C - CNN - LSTM - DA , as the time series decomposition and denoising autoencoder components generates a more robust prediction to missing values .", "ner": [["C - CNN - LSTM - DA", "Method"], ["denoising autoencoder", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "59599694", "sentence": "In the existence of missing data , the value of forecasting can be distracted far from real values in LSTM neural network , Fig. 1 1 b .   This paper illustrates a new framework for spatial time series forecasting problem and its application on traffic flow data .", "ner": [["forecasting", "Task"], ["LSTM neural network", "Method"], ["spatial time series forecasting", "Task"]], "rel": [["LSTM neural network", "Used-For", "forecasting"]], "rel_plus": [["LSTM neural network:Method", "Used-For", "forecasting:Task"]]}
{"doc_id": "59599694", "sentence": "Table 1 shows the comparison of the baseline , state - of - arts neural network , and CNN - LSTM models for traffic flow prediction .", "ner": [["neural network", "Method"], ["CNN - LSTM", "Method"], ["traffic flow prediction", "Task"]], "rel": [["neural network", "Used-For", "traffic flow prediction"], ["CNN - LSTM", "Used-For", "traffic flow prediction"]], "rel_plus": [["neural network:Method", "Used-For", "traffic flow prediction:Task"], ["CNN - LSTM:Method", "Used-For", "traffic flow prediction:Task"]]}
{"doc_id": "59599694", "sentence": "We illustrate the performance of using pre - trained denoising autoencoder decoder as the last component of C - CNN - LSTM - DA in Fig. 1 1 .", "ner": [["denoising autoencoder", "Method"], ["C - CNN - LSTM - DA", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "59599694", "sentence": "Related analyses can be constructed for other spatial - temporal problems , such as anomaly detection , missing data imputation , time series clustering and time series classification problems .", "ner": [["spatial - temporal problems", "Task"], ["anomaly detection", "Task"], ["missing data imputation", "Task"], ["time series clustering", "Task"], ["time series classification", "Task"]], "rel": [["anomaly detection", "SubTask-Of", "spatial - temporal problems"], ["missing data imputation", "SubTask-Of", "spatial - temporal problems"], ["time series clustering", "SubTask-Of", "spatial - temporal problems"], ["time series classification", "SubTask-Of", "spatial - temporal problems"]], "rel_plus": [["anomaly detection:Task", "SubTask-Of", "spatial - temporal problems:Task"], ["missing data imputation:Task", "SubTask-Of", "spatial - temporal problems:Task"], ["time series clustering:Task", "SubTask-Of", "spatial - temporal problems:Task"], ["time series classification:Task", "SubTask-Of", "spatial - temporal problems:Task"]]}
{"doc_id": "28984897", "sentence": "The evaluation is performed on two tasks , the MNIST handwritten digit recognition and the LFW face verification , using a LeNet - 5 and a VGG 1 6 network architecture .", "ner": [["MNIST", "Dataset"], ["handwritten digit recognition", "Task"], ["LFW", "Dataset"], ["face verification", "Task"], ["LeNet - 5", "Method"], ["VGG 1 6", "Method"]], "rel": [["VGG 1 6", "Evaluated-With", "MNIST"], ["MNIST", "Benchmark-For", "handwritten digit recognition"], ["VGG 1 6", "Used-For", "handwritten digit recognition"], ["LeNet - 5", "Evaluated-With", "LFW"], ["LFW", "Benchmark-For", "face verification"], ["LeNet - 5", "Used-For", "face verification"]], "rel_plus": [["VGG 1 6:Method", "Evaluated-With", "MNIST:Dataset"], ["MNIST:Dataset", "Benchmark-For", "handwritten digit recognition:Task"], ["VGG 1 6:Method", "Used-For", "handwritten digit recognition:Task"], ["LeNet - 5:Method", "Evaluated-With", "LFW:Dataset"], ["LFW:Dataset", "Benchmark-For", "face verification:Task"], ["LeNet - 5:Method", "Used-For", "face verification:Task"]]}
{"doc_id": "28984897", "sentence": "Furthermore , it will be shown that neuron pruning can be combined with subsequent weight pruning , reducing the size of the LeNet - 5 and VGG 1 6 up to $ 9 2 \\%$ and $ 8 0 \\%$ respectively .", "ner": [["weight pruning", "Method"], ["LeNet - 5", "Method"], ["VGG 1 6", "Method"]], "rel": [["weight pruning", "Used-For", "LeNet - 5"], ["weight pruning", "Used-For", "VGG 1 6"]], "rel_plus": [["weight pruning:Method", "Used-For", "LeNet - 5:Method"], ["weight pruning:Method", "Used-For", "VGG 1 6:Method"]]}
{"doc_id": "28984897", "sentence": "Having today available a big number of large - scale datasets and powerful GPUs , deep neural networks have become the state - of - the - art in many computer vision , and speech recognition tasks [ 1 , 6 , 1 0 ] .", "ner": [["deep neural networks", "Method"], ["computer vision", "Task"], ["speech recognition", "Task"]], "rel": [["deep neural networks", "Used-For", "computer vision"], ["deep neural networks", "Used-For", "speech recognition"]], "rel_plus": [["deep neural networks:Method", "Used-For", "computer vision:Task"], ["deep neural networks:Method", "Used-For", "speech recognition:Task"]]}
{"doc_id": "28984897", "sentence": "They achieve high performance in many applications , e.g. , scene and object recognition , object detection , scene parsing , face recognition , and medical imaging .", "ner": [["scene and object recognition", "Task"], ["object detection", "Task"], ["scene parsing", "Task"], ["face recognition", "Task"], ["medical imaging", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "28984897", "sentence": "For example , AlexNet and DeepFace have around 6 0 M and 1 2 0 M parameters , respectively [ 7 ] .", "ner": [["AlexNet", "Method"], ["DeepFace", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "28984897", "sentence": "The proposed approach for pruning neurons is based on the good performance of maxout units [ 5 ] , which were developed for boosting the impact of dropout in training , and on their capacity to combine neurons for approximating more complex functions .", "ner": [["maxout units", "Method"], ["dropout", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "28984897", "sentence": "Experiments on two datasets , the MNIST digits dataset and the labeled faces in the wild dataset , will be shown in section 5 .", "ner": [["MNIST digits", "Dataset"], ["labeled faces in the wild", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "28984897", "sentence": "For example , in [ 1 7 ] a very deep convolutional neural network ( CNN ) is trained by continuously adding convolutional layers to an initial CNN of 1 1 layers for obtaining a better performance .", "ner": [["convolutional neural network", "Method"], ["CNN", "Method"], ["convolutional layers", "Method"], ["CNN", "Method"]], "rel": [["CNN", "Synonym-Of", "convolutional neural network"], ["convolutional layers", "Part-Of", "CNN"]], "rel_plus": [["CNN:Method", "Synonym-Of", "convolutional neural network:Method"], ["convolutional layers:Method", "Part-Of", "CNN:Method"]]}
{"doc_id": "28984897", "sentence": "They used a combination of three steps : weight pruning , weight quantization and Huffman coding .", "ner": [["weight pruning", "Method"], ["weight quantization", "Method"], ["Huffman coding", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "28984897", "sentence": "To make this clear , we show in Fig. 1 the relation between the proportion of remaining neurons in the network versus the proportion of pruned weights using the LeNet - 5 [ 1 1 ] and the VGG 1 6 [ 1 6 ] as examples .", "ner": [["LeNet - 5", "Method"], ["VGG 1 6", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "28984897", "sentence": "Given an input layer X = [ x 0 , x 1 , x 2 , ... , x N ] with N neurons , a maxout layer computes : where k is the number of neurons that are combined into a single maxout unit .", "ner": [["maxout layer", "Method"], ["maxout unit", "Method"]], "rel": [["maxout unit", "Part-Of", "maxout layer"]], "rel_plus": [["maxout unit:Method", "Part-Of", "maxout layer:Method"]]}
{"doc_id": "28984897", "sentence": "The idea of the proposed approach is to use the maxout units and their model selection abilities for pruning entire neurons from an architecture without expensive processing .", "ner": [["maxout units", "Method"], ["model selection", "Task"]], "rel": [["maxout units", "Used-For", "model selection"]], "rel_plus": [["maxout units:Method", "Used-For", "model selection:Task"]]}
{"doc_id": "28984897", "sentence": "Following the assumption that redundancies exist in a deep neural network , it is assumed that if a network contains a maxout layer , redundancies will , also , exist in the maxout units .", "ner": [["deep neural network", "Method"], ["maxout layer", "Method"], ["maxout units", "Method"]], "rel": [["maxout layer", "Part-Of", "deep neural network"], ["maxout units", "Part-Of", "deep neural network"]], "rel_plus": [["maxout layer:Method", "Part-Of", "deep neural network:Method"], ["maxout units:Method", "Part-Of", "deep neural network:Method"]]}
{"doc_id": "28984897", "sentence": "For reducing the size of a CNN using maxout units , an iterative process is followed .", "ner": [["CNN", "Method"], ["maxout units", "Method"]], "rel": [["maxout units", "Part-Of", "CNN"]], "rel_plus": [["maxout units:Method", "Part-Of", "CNN:Method"]]}
{"doc_id": "28984897", "sentence": "First , a CNN with a maxout layer is trained .", "ner": [["CNN", "Method"], ["maxout layer", "Method"]], "rel": [["maxout layer", "Part-Of", "CNN"]], "rel_plus": [["maxout layer:Method", "Part-Of", "CNN:Method"]]}
{"doc_id": "28984897", "sentence": "This maxout layer performs a max function among k adjacent neurons , reducing the amount of weights connecting with the next layer by a factor of k. So , placing this maxout layer after the one with the highest number of weights would be advisable .", "ner": [["maxout layer", "Method"], ["maxout layer", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "28984897", "sentence": "Second , by counting the number of times neurons become the maximal value in each maxout unit when computing a forward pass over the training dataset , the least active neurons of each maxout unit are removed from the network .", "ner": [["maxout unit", "Method"], ["maxout unit", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "28984897", "sentence": "An evaluation of both neuron and weight pruning is carried out for two different tasks : handwritten digit recognition , MNIST dataset [ 1 1 ] , and face verification , LFW dataset [ 9 ] .", "ner": [["handwritten digit recognition", "Task"], ["MNIST", "Dataset"], ["face verification", "Task"], ["LFW", "Dataset"]], "rel": [["MNIST", "Benchmark-For", "handwritten digit recognition"], ["LFW", "Benchmark-For", "face verification"]], "rel_plus": [["MNIST:Dataset", "Benchmark-For", "handwritten digit recognition:Task"], ["LFW:Dataset", "Benchmark-For", "face verification:Task"]]}
{"doc_id": "28984897", "sentence": "In general , the performance of the networks is evaluated with a varying percentage of pruned weights : after applying maxout , when pruning several neurons from the maxout units , and finally after applying additional weight pruning .", "ner": [["maxout", "Method"], ["maxout units", "Method"], ["weight pruning", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "28984897", "sentence": "While in the first task a very small LeNet - 5 architecture is compressed , in the second task a large VGG 1 6 architecture is compressed .", "ner": [["LeNet - 5", "Method"], ["VGG 1 6", "Method"]], "rel": [["LeNet - 5", "Compare-With", "VGG 1 6"]], "rel_plus": [["LeNet - 5:Method", "Compare-With", "VGG 1 6:Method"]]}
{"doc_id": "28984897", "sentence": "For the experiments , we chose k = 4 for the size of the maxout units as it allows for a fairly good compression and does not reduce the descriptiveness of the network compared to a network without maxout units .", "ner": [["maxout units", "Method"], ["maxout units", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "28984897", "sentence": "For the digit - recognition task , two networks , using the LeNet - 5 architecture [ 1 1 ] with two convolutional layers , a fully connected layer and a softmax layer as a classificator , were trained .", "ner": [["digit - recognition", "Task"], ["LeNet - 5", "Method"], ["convolutional layers", "Method"], ["fully connected layer", "Method"], ["softmax layer", "Method"]], "rel": [["LeNet - 5", "Used-For", "digit - recognition"], ["convolutional layers", "Part-Of", "LeNet - 5"], ["fully connected layer", "Part-Of", "LeNet - 5"], ["softmax layer", "Part-Of", "LeNet - 5"]], "rel_plus": [["LeNet - 5:Method", "Used-For", "digit - recognition:Task"], ["convolutional layers:Method", "Part-Of", "LeNet - 5:Method"], ["fully connected layer:Method", "Part-Of", "LeNet - 5:Method"], ["softmax layer:Method", "Part-Of", "LeNet - 5:Method"]]}
{"doc_id": "28984897", "sentence": "One network contains a maxout layer after the fully connected layer ( LeNet - MFC ) , while the other has a maxout layer after the last convolutional layer ( LeNet - MC ) .", "ner": [["maxout layer", "Method"], ["fully connected layer", "Method"], ["LeNet - MFC", "Method"], ["maxout layer", "Method"], ["convolutional layer", "Method"], ["LeNet - MC", "Method"]], "rel": [["fully connected layer", "Part-Of", "LeNet - MFC"], ["maxout layer", "Part-Of", "LeNet - MFC"], ["convolutional layer", "Part-Of", "LeNet - MC"], ["maxout layer", "Part-Of", "LeNet - MC"]], "rel_plus": [["fully connected layer:Method", "Part-Of", "LeNet - MFC:Method"], ["maxout layer:Method", "Part-Of", "LeNet - MFC:Method"], ["convolutional layer:Method", "Part-Of", "LeNet - MC:Method"], ["maxout layer:Method", "Part-Of", "LeNet - MC:Method"]]}
{"doc_id": "28984897", "sentence": "We used stochastic gradient descent ( SGD ) with a momentum of 0. 9 , weight decay of 5 \u00d7 1 0 \u2212 4 with inverse decay , a base learning rate of 0.0 1 that is iteratively reduced and a batch size of 6 4 for training .", "ner": [["stochastic gradient descent", "Method"], ["SGD", "Method"], ["momentum", "Method"], ["weight decay", "Method"]], "rel": [["SGD", "Synonym-Of", "stochastic gradient descent"], ["momentum", "Part-Of", "stochastic gradient descent"], ["weight decay", "Part-Of", "stochastic gradient descent"]], "rel_plus": [["SGD:Method", "Synonym-Of", "stochastic gradient descent:Method"], ["momentum:Method", "Part-Of", "stochastic gradient descent:Method"], ["weight decay:Method", "Part-Of", "stochastic gradient descent:Method"]]}
{"doc_id": "28984897", "sentence": "Table 1 shows the classification accuracy for both networks with different fully connected layer sizes , with and without maxout ( after the fully connected layer or the last convolutional layer ) .", "ner": [["classification", "Task"], ["fully connected layer", "Method"], ["maxout", "Method"], ["fully connected layer", "Method"], ["convolutional layer", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "28984897", "sentence": "Moreover , the number of weights are considerably reduced with up to 7 0 % for LeNet - MFC and 7 4 % for LeNet - MC .", "ner": [["LeNet - MFC", "Method"], ["LeNet - MC", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "28984897", "sentence": "In LeNet - MFC , each neuron pruning step reduces the number of weights by 1 9 . 8 % , because the neurons are pruned from the fully connected layer , which has the largest number of weights in the network .", "ner": [["LeNet - MFC", "Method"], ["fully connected layer", "Method"]], "rel": [["fully connected layer", "Part-Of", "LeNet - MFC"]], "rel_plus": [["fully connected layer:Method", "Part-Of", "LeNet - MFC:Method"]]}
{"doc_id": "28984897", "sentence": "Besides , the maxout layer does not provide a considerable reduction , since it reduces the size of the softmax layer that has less number of weights compared with the other layers .", "ner": [["maxout layer", "Method"], ["softmax layer", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "28984897", "sentence": "In contrast , the weight reduction in LeNet - MC due to neuron pruning is just 1. 4 % per step , and it comes mostly from the maxout layer .", "ner": [["LeNet - MC", "Method"], ["maxout layer", "Method"]], "rel": [["maxout layer", "Part-Of", "LeNet - MC"]], "rel_plus": [["maxout layer:Method", "Part-Of", "LeNet - MC:Method"]]}
{"doc_id": "28984897", "sentence": "In this case , the maxout layer reduces the fully connected layer instead , and the neurons are pruned from the last convolutional layer .", "ner": [["maxout", "Method"], ["fully connected layer", "Method"], ["convolutional layer", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "28984897", "sentence": "So , a total compression rate of 9 1 % for LeNet - MFC and 9 2 % for LeNet - MC , of pruned and zeroed weights , can be reached .", "ner": [["LeNet - MFC", "Method"], ["LeNet - MC", "Method"]], "rel": [["LeNet - MFC", "Compare-With", "LeNet - MC"]], "rel_plus": [["LeNet - MFC:Method", "Compare-With", "LeNet - MC:Method"]]}
{"doc_id": "28984897", "sentence": "For that purpose , the VGG 1 6 network [ 1 6 ] was utilized , using The Visual Geometry Group Face Dataset ( VGG face - dataset ) as a training - dataset .", "ner": [["VGG 1 6", "Method"], ["Visual Geometry Group Face", "Dataset"], ["VGG face", "Dataset"]], "rel": [["VGG face", "Synonym-Of", "Visual Geometry Group Face"]], "rel_plus": [["VGG face:Dataset", "Synonym-Of", "Visual Geometry Group Face:Dataset"]]}
{"doc_id": "28984897", "sentence": "It does not contain overlapping identities with standard benchmark datasets ( LFW , YFT ) , so it is suitable for training .", "ner": [["LFW", "Dataset"], ["YFT", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "28984897", "sentence": "The VGG 1 6 network , configuration D in [ 1 6 ] , is a deep CNN with 1 6 layers : 1 3 convolutional layer , two fully connected layers , and a softmax layer .", "ner": [["VGG 1 6", "Method"], ["CNN", "Method"], ["convolutional layer", "Method"], ["fully connected layers", "Method"], ["softmax layer", "Method"]], "rel": [["convolutional layer", "Part-Of", "VGG 1 6"], ["fully connected layers", "Part-Of", "VGG 1 6"], ["softmax layer", "Part-Of", "VGG 1 6"], ["VGG 1 6", "SubClass-Of", "CNN"]], "rel_plus": [["convolutional layer:Method", "Part-Of", "VGG 1 6:Method"], ["fully connected layers:Method", "Part-Of", "VGG 1 6:Method"], ["softmax layer:Method", "Part-Of", "VGG 1 6:Method"], ["VGG 1 6:Method", "SubClass-Of", "CNN:Method"]]}
{"doc_id": "28984897", "sentence": "Analogous to the previous LeNet configurations , two configuration of VGG 1 6 are used , in which a maxout network with k = 4 is added after the first fully connected layer ( f c 6 ) , called VGG 1 6 - MFC , and after the last convolutional layer ( conv 5 ) , called VGG 1 6 - MC , see Fig. 3 .", "ner": [["LeNet", "Method"], ["VGG 1 6", "Method"], ["maxout network", "Method"], ["fully connected layer", "Method"], ["f c 6", "Method"], ["VGG 1 6 - MFC", "Method"], ["convolutional layer", "Method"], ["conv 5", "Method"], ["VGG 1 6 - MC", "Method"]], "rel": [["f c 6", "Synonym-Of", "fully connected layer"], ["maxout network", "Part-Of", "fully connected layer"], ["fully connected layer", "Part-Of", "VGG 1 6 - MFC"], ["VGG 1 6", "Part-Of", "VGG 1 6 - MFC"], ["conv 5", "Synonym-Of", "convolutional layer"], ["convolutional layer", "Part-Of", "VGG 1 6 - MC"], ["VGG 1 6", "Part-Of", "VGG 1 6 - MC"], ["fully connected layer", "Part-Of", "VGG 1 6 - MC"], ["maxout network", "Part-Of", "VGG 1 6 - MC"]], "rel_plus": [["f c 6:Method", "Synonym-Of", "fully connected layer:Method"], ["maxout network:Method", "Part-Of", "fully connected layer:Method"], ["fully connected layer:Method", "Part-Of", "VGG 1 6 - MFC:Method"], ["VGG 1 6:Method", "Part-Of", "VGG 1 6 - MFC:Method"], ["conv 5:Method", "Synonym-Of", "convolutional layer:Method"], ["convolutional layer:Method", "Part-Of", "VGG 1 6 - MC:Method"], ["VGG 1 6:Method", "Part-Of", "VGG 1 6 - MC:Method"], ["fully connected layer:Method", "Part-Of", "VGG 1 6 - MC:Method"], ["maxout network:Method", "Part-Of", "VGG 1 6 - MC:Method"]]}
{"doc_id": "28984897", "sentence": "We used SGD with a momentum of 0. 9 , weight decay of 5 \u00d7 1 0 \u2212 4 , three learning rates [ 1 0 \u2212 2 , 1 0 \u2212 3 , 1 0 \u2212 4 ] , as [ 1 6 ] , and a batch size of 1 2 8 .", "ner": [["SGD", "Method"], ["momentum", "Method"], ["weight decay", "Method"]], "rel": [["momentum", "Part-Of", "SGD"], ["weight decay", "Part-Of", "SGD"]], "rel_plus": [["momentum:Method", "Part-Of", "SGD:Method"], ["weight decay:Method", "Part-Of", "SGD:Method"]]}
{"doc_id": "28984897", "sentence": "The network was tested following the procedure in [ 1 6 ] , but using the restricted configuration of The Labeled Faces in the Wild ( LFW ) [ 9 ] .", "ner": [["Labeled Faces in the Wild", "Dataset"], ["LFW", "Dataset"]], "rel": [["LFW", "Synonym-Of", "Labeled Faces in the Wild"]], "rel_plus": [["LFW:Dataset", "Synonym-Of", "Labeled Faces in the Wild:Dataset"]]}
{"doc_id": "28984897", "sentence": "The LFW dataset is a standard benchmark dataset for face verification .", "ner": [["LFW", "Dataset"], ["face verification", "Task"]], "rel": [["LFW", "Benchmark-For", "face verification"]], "rel_plus": [["LFW:Dataset", "Benchmark-For", "face verification:Task"]]}
{"doc_id": "28984897", "sentence": "Faces in images Table 3 : EER in [ % ] and pruned weight 's proportions for the VGG 1 6 with maxout layer ( k = 4 ) after the first fully connected layer ( VGG 1 6 - MFC ) and after the last convolutional layer ( VGG 1 6 - MC ) . were detected using the Viola - Jones face detector [ 9 ] .", "ner": [["VGG 1 6", "Method"], ["maxout layer", "Method"], ["fully connected layer", "Method"], ["VGG 1 6 - MFC", "Method"], ["convolutional layer", "Method"], ["VGG 1 6 - MC", "Method"], ["Viola - Jones face detector", "Method"]], "rel": [["VGG 1 6", "Part-Of", "VGG 1 6 - MFC"], ["maxout layer", "Part-Of", "VGG 1 6 - MFC"], ["fully connected layer", "Part-Of", "VGG 1 6 - MFC"], ["convolutional layer", "Part-Of", "VGG 1 6 - MC"], ["VGG 1 6", "Part-Of", "VGG 1 6 - MC"], ["maxout layer", "Part-Of", "VGG 1 6 - MC"], ["fully connected layer", "Part-Of", "VGG 1 6 - MC"]], "rel_plus": [["VGG 1 6:Method", "Part-Of", "VGG 1 6 - MFC:Method"], ["maxout layer:Method", "Part-Of", "VGG 1 6 - MFC:Method"], ["fully connected layer:Method", "Part-Of", "VGG 1 6 - MFC:Method"], ["convolutional layer:Method", "Part-Of", "VGG 1 6 - MC:Method"], ["VGG 1 6:Method", "Part-Of", "VGG 1 6 - MC:Method"], ["maxout layer:Method", "Part-Of", "VGG 1 6 - MC:Method"], ["fully connected layer:Method", "Part-Of", "VGG 1 6 - MC:Method"]]}
{"doc_id": "28984897", "sentence": "Table 3 shows the EER for networks without a maxout layer and with a maxout layer with k = 4 , as well as the results for pruning from one up to three neurons from each maxout unit .", "ner": [["maxout layer", "Method"], ["maxout layer", "Method"], ["maxout unit", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "28984897", "sentence": "In fact , the EER decreases by 1% and 0. 4 7 % for the VGG 1 6 - MFC and the VGG 1 6 - MC respectively .", "ner": [["VGG 1 6 - MFC", "Method"], ["VGG 1 6 - MC", "Method"]], "rel": [["VGG 1 6 - MFC", "Compare-With", "VGG 1 6 - MC"]], "rel_plus": [["VGG 1 6 - MFC:Method", "Compare-With", "VGG 1 6 - MC:Method"]]}
{"doc_id": "28984897", "sentence": "The network 's performance will be , deeply , affected if more than 5 0 % for the VGG 1 6 - MFC and 3 0 % for the VGG 1 6 - MC of the weights are pruned .", "ner": [["VGG 1 6 - MFC", "Method"], ["VGG 1 6 - MC", "Method"]], "rel": [["VGG 1 6 - MFC", "Compare-With", "VGG 1 6 - MC"]], "rel_plus": [["VGG 1 6 - MFC:Method", "Compare-With", "VGG 1 6 - MC:Method"]]}
{"doc_id": "28984897", "sentence": "Nevertheless , a total compression rate of 8 0 . 1 % for VGG 1 6 - MFC and 6 8 % for VGG 1 6 - MC without performance deterioration can be reached .   We have presented an efficient approach for reducing the size of deep neural networks .", "ner": [["VGG 1 6 - MFC", "Method"], ["VGG 1 6 - MC", "Method"], ["deep neural networks", "Method"]], "rel": [["VGG 1 6 - MFC", "Compare-With", "VGG 1 6 - MC"]], "rel_plus": [["VGG 1 6 - MFC:Method", "Compare-With", "VGG 1 6 - MC:Method"]]}
{"doc_id": "28984897", "sentence": "A maxout layer reduces the number of weights between two adjacent layers by k. By using these maxout units , the network 's performance is not negatively affected , since they boost the dropout benefits reducing redundancies in the network .", "ner": [["maxout layer", "Method"], ["maxout units", "Method"], ["dropout", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "28984897", "sentence": "As inputs from maxout units are the neurons to be pruned , it is advisable to place the maxout units after the largest layer in the network , because neurons in this layer have large numbers of weights compared with neurons in other layers .", "ner": [["maxout units", "Method"], ["maxout units", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "201124533", "sentence": "High - resolution representations are essential for position - sensitive vision problems , such as human pose estimation , semantic segmentation , and object detection .", "ner": [["human pose estimation", "Task"], ["semantic segmentation", "Task"], ["object detection", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "201124533", "sentence": "Existing state - of - the - art frameworks first encode the input image as a low - resolution representation through a subnetwork that is formed by connecting high - to - low resolution convolutions \\emph{in series } ( e.g. , ResNet , VGGNet ) , and then recover the high - resolution representation from the encoded low - resolution representation .", "ner": [["high - to - low resolution convolutions", "Method"], ["ResNet", "Method"], ["VGGNet", "Method"]], "rel": [["ResNet", "SubClass-Of", "high - to - low resolution convolutions"], ["VGGNet", "SubClass-Of", "high - to - low resolution convolutions"]], "rel_plus": [["ResNet:Method", "SubClass-Of", "high - to - low resolution convolutions:Method"], ["VGGNet:Method", "SubClass-Of", "high - to - low resolution convolutions:Method"]]}
{"doc_id": "201124533", "sentence": "Instead , our proposed network , named as High - Resolution Network ( HRNet ) , maintains high - resolution representations through the whole process .", "ner": [["High - Resolution Network", "Method"], ["HRNet", "Method"]], "rel": [["HRNet", "Synonym-Of", "High - Resolution Network"]], "rel_plus": [["HRNet:Method", "Synonym-Of", "High - Resolution Network:Method"]]}
{"doc_id": "201124533", "sentence": "We show the superiority of the proposed HRNet in a wide range of applications , including human pose estimation , semantic segmentation , and object detection , suggesting that the HRNet is a stronger backbone for computer vision problems .", "ner": [["HRNet", "Method"], ["human pose estimation", "Task"], ["semantic segmentation", "Task"], ["object detection", "Task"], ["HRNet", "Method"], ["computer vision", "Task"]], "rel": [["HRNet", "Used-For", "human pose estimation"], ["HRNet", "Used-For", "semantic segmentation"], ["HRNet", "Used-For", "object detection"], ["HRNet", "Used-For", "computer vision"]], "rel_plus": [["HRNet:Method", "Used-For", "human pose estimation:Task"], ["HRNet:Method", "Used-For", "semantic segmentation:Task"], ["HRNet:Method", "Used-For", "object detection:Task"], ["HRNet:Method", "Used-For", "computer vision:Task"]]}
{"doc_id": "201124533", "sentence": "The first one , named as HRNetV 1 , only outputs the high - resolution representation computed from the high - resolution convolution stream .", "ner": [["HRNetV 1", "Method"], ["high - resolution convolution stream", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "201124533", "sentence": "We apply it to human pose estimation by following the heatmap estimation framework .", "ner": [["human pose estimation", "Task"], ["heatmap estimation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "201124533", "sentence": "We empirically demonstrate the superior pose estimation performance on the COCO keypoint detection dataset [ 7 4 ] .", "ner": [["pose estimation", "Task"], ["COCO keypoint detection", "Dataset"]], "rel": [["COCO keypoint detection", "Benchmark-For", "pose estimation"]], "rel_plus": [["COCO keypoint detection:Dataset", "Benchmark-For", "pose estimation:Task"]]}
{"doc_id": "201124533", "sentence": "We apply it to semantic segmentation through estimating segmentation maps from the combined highresolution representation .", "ner": [["semantic segmentation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "201124533", "sentence": "The proposed approach achieves state - of - the - art results on PASCAL - Context , Cityscapes , and LIP with similar model sizes and lower computation com - [ 1 0 1 ] , ResNet [ 3 9 ] ) , which is formed by connecting high - to - low convolutions in series . ( b ) A high - resolution representation recovering subnetwork , which is formed by connecting low - to - high convolutions in series .", "ner": [["PASCAL - Context", "Dataset"], ["Cityscapes", "Dataset"], ["LIP", "Dataset"], ["ResNet", "Method"], ["high - to - low convolutions", "Method"], ["low - to - high convolutions", "Method"]], "rel": [["high - to - low convolutions", "Part-Of", "ResNet"]], "rel_plus": [["high - to - low convolutions:Method", "Part-Of", "ResNet:Method"]]}
{"doc_id": "201124533", "sentence": "Representative examples include SegNet [ 3 ] , DeconvNet [ 8 5 ] , UNet [ 9 5 ] and Hourglass [ 8 3 ] , encoder - decoder [ 9 0 ] , and SimpleBaseline [ 1 2 4 ] . plexity .", "ner": [["SegNet", "Method"], ["DeconvNet", "Method"], ["UNet", "Method"], ["Hourglass", "Method"], ["encoder - decoder", "Method"], ["SimpleBaseline", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "201124533", "sentence": "We observe similar performance for HRNetV 1 and HRNetV 2 over COCO pose estimation , and the superiority of HRNetV 2 to HRNet 1 in semantic segmentation .", "ner": [["HRNetV 1", "Method"], ["HRNetV 2", "Method"], ["COCO", "Dataset"], ["pose estimation", "Task"], ["HRNetV 2", "Method"], ["HRNet 1", "Method"], ["semantic segmentation", "Task"]], "rel": [["HRNetV 1", "Used-For", "pose estimation"], ["HRNetV 2", "Used-For", "pose estimation"], ["COCO", "Benchmark-For", "pose estimation"], ["HRNetV 2", "Used-For", "semantic segmentation"], ["HRNet 1", "Used-For", "semantic segmentation"]], "rel_plus": [["HRNetV 1:Method", "Used-For", "pose estimation:Task"], ["HRNetV 2:Method", "Used-For", "pose estimation:Task"], ["COCO:Dataset", "Benchmark-For", "pose estimation:Task"], ["HRNetV 2:Method", "Used-For", "semantic segmentation:Task"], ["HRNet 1:Method", "Used-For", "semantic segmentation:Task"]]}
{"doc_id": "201124533", "sentence": "In addition , we construct a multi - level representation , named as HRNetV 2 p , from the high - resolution representation output from HRNetV 2 , and apply it to state - of - the - art detection frameworks , including Faster R - CNN , Cascade R - CNN [ 9 ] , FCOS [ 1 1 1 ] , and CenterNet [ 2 7 ] , and state - of - theart joint detection and instance segmentation frameworks , including Mask R - CNN [ 3 8 ] , Cascade Mask R - CNN , and Hybrid Task Cascade [ 1 2 ] .", "ner": [["HRNetV 2 p", "Method"], ["HRNetV 2", "Method"], ["Faster R - CNN", "Method"], ["Cascade R - CNN", "Method"], ["FCOS", "Method"], ["CenterNet", "Method"], ["instance segmentation", "Task"], ["Mask R - CNN", "Method"], ["Cascade Mask R - CNN", "Method"], ["Hybrid Task Cascade", "Method"]], "rel": [["HRNetV 2 p", "SubClass-Of", "HRNetV 2"], ["HRNetV 2 p", "Part-Of", "Faster R - CNN"], ["HRNetV 2 p", "Part-Of", "Cascade R - CNN"], ["HRNetV 2 p", "Part-Of", "FCOS"], ["HRNetV 2 p", "Part-Of", "CenterNet"], ["Mask R - CNN", "Used-For", "instance segmentation"], ["Cascade Mask R - CNN", "Used-For", "instance segmentation"], ["Hybrid Task Cascade", "Used-For", "instance segmentation"], ["HRNetV 2 p", "Part-Of", "Mask R - CNN"], ["HRNetV 2 p", "Part-Of", "Cascade Mask R - CNN"], ["HRNetV 2 p", "Part-Of", "Hybrid Task Cascade"]], "rel_plus": [["HRNetV 2 p:Method", "SubClass-Of", "HRNetV 2:Method"], ["HRNetV 2 p:Method", "Part-Of", "Faster R - CNN:Method"], ["HRNetV 2 p:Method", "Part-Of", "Cascade R - CNN:Method"], ["HRNetV 2 p:Method", "Part-Of", "FCOS:Method"], ["HRNetV 2 p:Method", "Part-Of", "CenterNet:Method"], ["Mask R - CNN:Method", "Used-For", "instance segmentation:Task"], ["Cascade Mask R - CNN:Method", "Used-For", "instance segmentation:Task"], ["Hybrid Task Cascade:Method", "Used-For", "instance segmentation:Task"], ["HRNetV 2 p:Method", "Part-Of", "Mask R - CNN:Method"], ["HRNetV 2 p:Method", "Part-Of", "Cascade Mask R - CNN:Method"], ["HRNetV 2 p:Method", "Part-Of", "Hybrid Task Cascade:Method"]]}
{"doc_id": "201124533", "sentence": "We review closely - related representation learning techniques developed mainly for human pose estimation [ 4 2 ] , semantic segmentation and object detection , from three aspects : low - resolution representation learning , highresolution representation recovering , and high - resolution representation maintaining .", "ner": [["representation learning", "Task"], ["human pose estimation", "Task"], ["semantic segmentation", "Task"], ["object detection", "Task"], ["low - resolution representation learning", "Task"], ["highresolution representation recovering", "Task"], ["high - resolution representation maintaining", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "201124533", "sentence": "The fullyconvolutional network approaches [ 7 9 ] , [ 9 9 ] compute lowresolution representations by removing the fully - connected layers in a classification network , and estimate their coarse segmentation maps .", "ner": [["fullyconvolutional network", "Method"], ["fully - connected layers", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "201124533", "sentence": "The estimated segmentation maps are improved by combining the fine segmentation score maps estimated from intermediate low - level medium - resolution representations [ 7 9 ] , or iterating the processes [ 5 8 ] .", "ner": [], "rel": [], "rel_plus": []}
{"doc_id": "201124533", "sentence": "Similar techniques have also been applied to edge detection , e.g. , holistic edge detection [ 1 2 7 ] .", "ner": [["edge detection", "Task"], ["holistic edge detection", "Task"]], "rel": [["holistic edge detection", "SubTask-Of", "edge detection"]], "rel_plus": [["holistic edge detection:Task", "SubTask-Of", "edge detection:Task"]]}
{"doc_id": "201124533", "sentence": "The fully convolutional network is extended , by replacing a few ( typically two ) strided convolutions and the associated convolutions with dilated convolutions , to the dilation version , leading to medium - resolution representations [ 1 4 ] , [ 1 5 ] , [ 6 6 ] , [ 1 3 5 ] , [ 1 4 4 ] .", "ner": [["fully convolutional network", "Method"], ["strided convolutions", "Method"], ["convolutions", "Method"], ["dilated convolutions", "Method"]], "rel": [["dilated convolutions", "Part-Of", "fully convolutional network"]], "rel_plus": [["dilated convolutions:Method", "Part-Of", "fully convolutional network:Method"]]}
{"doc_id": "201124533", "sentence": "The upsample subnetwork could be a symmetric version of the downsample process ( e.g. , VGGNet ) , with skipping connection over some mirrored layers to transform the pooling indices , e.g. , SegNet [ 3 ] and DeconvNet [ 8 5 ] , or copying the feature maps , e.g. , U - Net [ 9 5 ] and Hourglass [ 6 ] , [ 7 ] , [ 2 1 ] , [ 2 4 ] , [ 5 1 ] , [ 8 3 ] , [ 1 0 9 ] , [ 1 3 1 ] , [ 1 3 2 ] , encoder - decoder [ 9 0 ] , and so on .", "ner": [["downsample process", "Method"], ["VGGNet", "Method"], ["skipping connection", "Method"], ["SegNet", "Method"], ["DeconvNet", "Method"], ["U - Net", "Method"], ["Hourglass", "Method"], ["encoder - decoder", "Method"]], "rel": [["VGGNet", "SubClass-Of", "downsample process"]], "rel_plus": [["VGGNet:Method", "SubClass-Of", "downsample process:Method"]]}
{"doc_id": "201124533", "sentence": "An extension of U - Net , full - resolution residual network [ 9 2 ] , introduces an extra full - resolution stream that carries information at the full image resolution , to replace the skip connections , and each unit in the downsample and upsample subnetworks receives information from and sends information to the full - resolution stream .", "ner": [["U - Net", "Method"], ["full - resolution residual network", "Method"], ["full - resolution stream", "Method"], ["skip connections", "Method"], ["full - resolution stream", "Method"]], "rel": [["full - resolution residual network", "SubClass-Of", "U - Net"], ["full - resolution stream", "Part-Of", "full - resolution residual network"]], "rel_plus": [["full - resolution residual network:Method", "SubClass-Of", "U - Net:Method"], ["full - resolution stream:Method", "Part-Of", "full - resolution residual network:Method"]]}
{"doc_id": "201124533", "sentence": "Other works include : light upsample process [ 5 ] , [ 1 9 ] , [ 7 2 ] , [ 1 2 4 ] , possibly with dilated convolutions used in the backbone [ 4 7 ] , [ 6 9 ] , [ 9 1 ] ; light downsample and heavy upsample processes [ 1 1 5 ] , recombinator networks [ 4 0 ] ; improving skip connections with more or complicated convolutional units [ 4 8 ] , [ 8 9 ] , [ 1 4 3 ] , as well as sending information from low - resolution skip connections to highresolution skip connections [ 1 5 1 ] or exchanging information between them [ 3 4 ] ; studying the details of the upsample process [ 1 2 0 ] ; combining multi - scale pyramid representations [ 1 8 ] , [ 1 2 5 ] ; stacking multiple DeconvNets/UNets/Hourglass [ 3 1 ] , [ 1 2 2 ] with dense connections [ 1 1 0 ] .", "ner": [["light upsample process", "Method"], ["dilated convolutions", "Method"], ["light downsample and heavy upsample processes", "Method"], ["recombinator networks", "Method"], ["skip connections", "Method"], ["convolutional units", "Method"], ["low - resolution skip connections", "Method"], ["highresolution skip connections", "Method"], ["upsample process", "Method"], ["multi - scale pyramid representations", "Method"], ["DeconvNets/UNets/Hourglass", "Method"], ["dense connections", "Method"]], "rel": [["dilated convolutions", "Part-Of", "light upsample process"]], "rel_plus": [["dilated convolutions:Method", "Part-Of", "light upsample process:Method"]]}
{"doc_id": "201124533", "sentence": "Our work is closely related to several works that can also generate highresolution representations , e.g. , convolutional neural fabrics [ 9 8 ] , interlinked CNNs [ 1 5 0 ] , GridNet [ 2 9 ] , and multiscale DenseNet [ 4 3 ] .", "ner": [["convolutional neural fabrics", "Method"], ["interlinked CNNs", "Method"], ["GridNet", "Method"], ["multiscale DenseNet", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "201124533", "sentence": "The two early works , convolutional neural fabrics [ 9 8 ] and interlinked CNNs [ 1 5 0 ] , lack careful design on when to start low - resolution parallel streams , and how and where to exchange information across parallel streams , and do not use batch normalization and residual connections , thus not showing satisfactory performance .", "ner": [["convolutional neural fabrics", "Method"], ["interlinked CNNs", "Method"], ["batch normalization", "Method"], ["residual connections", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "201124533", "sentence": "GridNet [ 2 9 ] is like a combination of multiple U - Nets and includes two symmetric information exchange stages : the first stage passes information only from high resolution to low resolution , and the second stage passes information only from low resolution to high resolution .", "ner": [["GridNet", "Method"], ["U - Nets", "Method"]], "rel": [["U - Nets", "Part-Of", "GridNet"]], "rel_plus": [["U - Nets:Method", "Part-Of", "GridNet:Method"]]}
{"doc_id": "201124533", "sentence": "Hourglass [ 8 3 ] , U - Net [ 9 5 ] , and SegNet [ 3 ] combine low - level features in the high - to - low downsample process into the same - resolution high - level features in the low - to - high upsample process progressively through skip connections .", "ner": [["Hourglass", "Method"], ["U - Net", "Method"], ["SegNet", "Method"], ["high - to - low downsample process", "Method"], ["low - to - high upsample process", "Method"], ["skip connections", "Method"]], "rel": [["skip connections", "Part-Of", "Hourglass"], ["skip connections", "Part-Of", "U - Net"], ["skip connections", "Part-Of", "SegNet"]], "rel_plus": [["skip connections:Method", "Part-Of", "Hourglass:Method"], ["skip connections:Method", "Part-Of", "U - Net:Method"], ["skip connections:Method", "Part-Of", "SegNet:Method"]]}
{"doc_id": "201124533", "sentence": "PSPNet [ 1 4 4 ] and DeepLabV 2 / 3 [ 1 5 ] and atrous spatial pyramid pooling .", "ner": [["PSPNet", "Method"], ["DeepLabV 2", "Method"], ["atrous spatial pyramid pooling", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "201124533", "sentence": "This paper represents a very substantial extension of our previous conference paper [ 1 0 5 ] with an additional material added from our unpublished technical report [ 1 0 6 ] as well as more object detection results under recentlydeveloped start - of - the - art object detection and instance segmentation frameworks .", "ner": [["object detection", "Task"], ["object detection", "Task"], ["instance segmentation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "201124533", "sentence": "The main technical novelties compared with [ 1 0 5 ] lie in threefold . ( 1 ) We extend the network ( named as HRNetV 1 ) proposed in [ 1 0 5 ] , to two versions : HRNetV 2 and HRNetV 2 p , which explore all the four - resolution representations . ( 2 ) We build the connection between multi - resolution fusion and regular convolution , which provides an evidence for the necessity of exploring all the four - resolution representations in HRNetV 2 and HRNetV 2 p. ( 3 ) We show the superiority of HRNetV 2 and HRNetV 2 p over HRNetV 1 and present the applications of HRNetV 2 and HRNetV 2 p in a broad range of vision problems , including semantic segmentation and object detection .", "ner": [["HRNetV 1", "Method"], ["HRNetV 2", "Method"], ["HRNetV 2 p", "Method"], ["multi - resolution fusion", "Method"], ["convolution", "Method"], ["HRNetV 2", "Method"], ["HRNetV 2 p.", "Method"], ["HRNetV 2", "Method"], ["HRNetV 2 p", "Method"], ["HRNetV 1", "Method"], ["HRNetV 2", "Method"], ["HRNetV 2 p", "Method"], ["semantic segmentation", "Task"], ["object detection", "Task"]], "rel": [["HRNetV 2", "SubClass-Of", "HRNetV 1"], ["HRNetV 2 p", "SubClass-Of", "HRNetV 1"], ["multi - resolution fusion", "Part-Of", "HRNetV 2"], ["convolution", "Part-Of", "HRNetV 2"], ["multi - resolution fusion", "Part-Of", "HRNetV 2 p."], ["convolution", "Part-Of", "HRNetV 2 p."], ["HRNetV 2 p", "Used-For", "semantic segmentation"], ["HRNetV 2", "Used-For", "object detection"]], "rel_plus": [["HRNetV 2:Method", "SubClass-Of", "HRNetV 1:Method"], ["HRNetV 2 p:Method", "SubClass-Of", "HRNetV 1:Method"], ["multi - resolution fusion:Method", "Part-Of", "HRNetV 2:Method"], ["convolution:Method", "Part-Of", "HRNetV 2:Method"], ["multi - resolution fusion:Method", "Part-Of", "HRNetV 2 p.:Method"], ["convolution:Method", "Part-Of", "HRNetV 2 p.:Method"], ["HRNetV 2 p:Method", "Used-For", "semantic segmentation:Task"], ["HRNetV 2:Method", "Used-For", "object detection:Task"]]}
{"doc_id": "201124533", "sentence": "For instance , one stride - 2 3 \u00d7 3 convolution for 2 \u00d7 downsampling , and two consecutive stride - 2 3 \u00d7 3 convolutions for 4 \u00d7 downsampling .", "ner": [["one stride - 2 3 \u00d7 3 convolution", "Method"], ["2 \u00d7 downsampling", "Task"], ["consecutive stride - 2 3 \u00d7 3 convolutions", "Method"], ["4 \u00d7 downsampling", "Task"]], "rel": [["one stride - 2 3 \u00d7 3 convolution", "Used-For", "2 \u00d7 downsampling"], ["consecutive stride - 2 3 \u00d7 3 convolutions", "Used-For", "4 \u00d7 downsampling"]], "rel_plus": [["one stride - 2 3 \u00d7 3 convolution:Method", "Used-For", "2 \u00d7 downsampling:Task"], ["consecutive stride - 2 3 \u00d7 3 convolutions:Method", "Used-For", "4 \u00d7 downsampling:Task"]]}
{"doc_id": "201124533", "sentence": "The functions are depicted in Figure 3 .   We have three kinds of representation heads that are illustrated in Figure 4 , and call them as HRNetV 1 , HRNetV 2 , and HRNetV 1 p , respectively .", "ner": [["HRNetV 1", "Method"], ["HRNetV 2", "Method"], ["HRNetV 1 p", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "201124533", "sentence": "In this paper , we will show the results of applying HRNetV 1 to human pose estimation , HRNetV 2 to semantic segmentation , and HRNetV 2 p to object detection .", "ner": [["HRNetV 1", "Method"], ["human pose estimation", "Task"], ["HRNetV 2", "Method"], ["semantic segmentation", "Task"], ["HRNetV 2 p", "Method"], ["object detection", "Task"]], "rel": [["HRNetV 1", "Used-For", "human pose estimation"], ["HRNetV 2", "Used-For", "semantic segmentation"], ["HRNetV 2 p", "Used-For", "object detection"]], "rel_plus": [["HRNetV 1:Method", "Used-For", "human pose estimation:Task"], ["HRNetV 2:Method", "Used-For", "semantic segmentation:Task"], ["HRNetV 2 p:Method", "Used-For", "object detection:Task"]]}
{"doc_id": "201124533", "sentence": "Each unit contains two 3 \u00d7 3 convolutions for each resolution , where each convolution is followed by batch normalization and the nonlinear activation ReLU .", "ner": [["3 \u00d7 3 convolutions", "Method"], ["convolution", "Method"], ["batch normalization", "Method"], ["ReLU", "Method"]], "rel": [["convolution", "Part-Of", "3 \u00d7 3 convolutions"], ["batch normalization", "Part-Of", "convolution"], ["ReLU", "Part-Of", "convolution"]], "rel_plus": [["convolution:Method", "Part-Of", "3 \u00d7 3 convolutions:Method"], ["batch normalization:Method", "Part-Of", "convolution:Method"], ["ReLU:Method", "Part-Of", "convolution:Method"]]}
{"doc_id": "201124533", "sentence": "The multi - resolution parallel convolution resembles the group convolution .", "ner": [["multi - resolution parallel convolution", "Method"], ["group convolution", "Method"]], "rel": [["group convolution", "Part-Of", "multi - resolution parallel convolution"]], "rel_plus": [["group convolution:Method", "Part-Of", "multi - resolution parallel convolution:Method"]]}
{"doc_id": "201124533", "sentence": "It divides the input channels into several subsets of channels and performs a regular convolution over each subset over different spatial resolutions separately , while in the group convolution , the resolutions are the same .", "ner": [["convolution", "Method"], ["group convolution", "Method"]], "rel": [["convolution", "Compare-With", "group convolution"]], "rel_plus": [["convolution:Method", "Compare-With", "group convolution:Method"]]}
{"doc_id": "201124533", "sentence": "This connection implies that the multi - resolution parallel convolution enjoys some benefit of the group convolution .", "ner": [["multi - resolution parallel convolution", "Method"], ["group convolution", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "201124533", "sentence": "A regular convolution can be divided as multiple small convolutions as explained in [ 1 4 1 ] .", "ner": [["convolution", "Method"], ["small convolutions", "Method"]], "rel": [["small convolutions", "Part-Of", "convolution"]], "rel_plus": [["small convolutions:Method", "Part-Of", "convolution:Method"]]}
{"doc_id": "201124533", "sentence": "The connection between multi - resolution fusion and regular convolution provides an evidence for exploring all the four - resolution representations done in HRNetV 2 and HRNetV 2 p. Human pose estimation , a.k.a . keypoint detection , aims to detect the locations of K keypoints or parts ( e.g. , elbow , wrist , etc ) from an image I of size W \u00d7 H \u00d7 3 .", "ner": [["multi - resolution fusion", "Method"], ["convolution", "Method"], ["HRNetV 2", "Method"], ["HRNetV 2 p.", "Method"], ["Human pose estimation", "Task"], ["keypoint detection", "Task"]], "rel": [["multi - resolution fusion", "Part-Of", "HRNetV 2"], ["convolution", "Part-Of", "HRNetV 2"], ["convolution", "Part-Of", "HRNetV 2 p."], ["multi - resolution fusion", "Part-Of", "HRNetV 2 p."], ["keypoint detection", "SubTask-Of", "Human pose estimation"]], "rel_plus": [["multi - resolution fusion:Method", "Part-Of", "HRNetV 2:Method"], ["convolution:Method", "Part-Of", "HRNetV 2:Method"], ["convolution:Method", "Part-Of", "HRNetV 2 p.:Method"], ["multi - resolution fusion:Method", "Part-Of", "HRNetV 2 p.:Method"], ["keypoint detection:Task", "SubTask-Of", "Human pose estimation:Task"]]}
{"doc_id": "201124533", "sentence": "We empirically observed that the performance is almost the same for HRNetV 1 and HRNetV 2 , and thus we choose HRNetV 1 as its computation complexity is a little lower .", "ner": [["HRNetV 1", "Method"], ["HRNetV 2", "Method"], ["HRNetV 1", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "201124533", "sentence": "The network -HRNetV 1 - W 3 2 , trained from scratch with the input size 2 5 6 \u00d7 1 9 2 , achieves an AP score 7 3 . 4 , outperforming other methods with the same input size . ( i ) Compared to Hourglass [ 8 3 ] , the network improves AP by 6. 5 points , and the GFLOP of our network is much lower and less than half , while the numbers of parameters are similar and ours is slightly larger . ( ii ) Compared to CPN [ 1 9 ] w/o and w/ OHKM , our network , with slightly larger model size and slightly higher complexity , achieves 4. 8 and 4. 0 points gain , respectively . ( iii ) Compared to the previous best - performed SimpleBaseline [ 1 2 4 ] , our HRNetV 1 - W 3 2 obtains significant improvements : 3. 0 points gain for the backbone ResNet - 5 0 with a similar model size and GFLOPs , and 1. 4 points gain for the backbone ResNet - 1 5 2 whose model size ( # Params ) and GFLOPs are twice as many as ours .", "ner": [["-HRNetV 1 - W 3 2", "Method"], ["Hourglass", "Method"], ["CPN", "Method"], ["OHKM", "Method"], ["SimpleBaseline", "Method"], ["HRNetV 1 - W 3 2", "Method"], ["ResNet - 5 0", "Method"], ["ResNet - 1 5 2", "Method"]], "rel": [["OHKM", "Part-Of", "CPN"], ["HRNetV 1 - W 3 2", "Compare-With", "SimpleBaseline"], ["ResNet - 5 0", "Part-Of", "HRNetV 1 - W 3 2"], ["ResNet - 1 5 2", "Part-Of", "HRNetV 1 - W 3 2"]], "rel_plus": [["OHKM:Method", "Part-Of", "CPN:Method"], ["HRNetV 1 - W 3 2:Method", "Compare-With", "SimpleBaseline:Method"], ["ResNet - 5 0:Method", "Part-Of", "HRNetV 1 - W 3 2:Method"], ["ResNet - 1 5 2:Method", "Part-Of", "HRNetV 1 - W 3 2:Method"]]}
{"doc_id": "201124533", "sentence": "Our nets can benefit from ( i ) training from the model pretrained on the ImageNet : The gain is 1. 0 points for HRNetV 1 - W 3 2 ; ( ii ) increasing the capacity by increasing the width : HRNetV 1 - W 4 8 gets 0. 7 and 0. 5 points gain for the input sizes 2 5 6 \u00d7 1 9 2 and 3 8 4 \u00d7 2 8 8 , respectively .", "ner": [["ImageNet", "Dataset"], ["HRNetV 1 - W 3 2", "Method"], ["HRNetV 1 - W 4 8", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "201124533", "sentence": "Considering the input size 3 8 4 \u00d7 2 8 8 , our HRNetV 1 - W 3 2 and HRNetV 1 - W 4 8 , get the 7 5 . 8 and 7 6 . 3 AP , which have 1. 4 and 1. 2 improvements compared to the input size 2 5 6 \u00d7 1 9 2 .", "ner": [["HRNetV 1 - W 3 2", "Method"], ["HRNetV 1 - W 4 8", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "201124533", "sentence": "In comparison to the SimpleBaseline [ 1 2 4 ] that uses ResNet - 1 5 2 as the backbone , our HRNetV 1 - W 3 2 and HRNetV 1 - W 4 8 attain 1. 5 and 2. 0 points gain in terms of AP at 4 5 % and 9 2 . 4 % computational cost , respectively .", "ner": [["SimpleBaseline", "Method"], ["ResNet - 1 5 2", "Method"], ["HRNetV 1 - W 3 2", "Method"], ["HRNetV 1 - W 4 8", "Method"]], "rel": [["ResNet - 1 5 2", "Part-Of", "SimpleBaseline"], ["HRNetV 1 - W 3 2", "Compare-With", "SimpleBaseline"], ["HRNetV 1 - W 4 8", "Compare-With", "SimpleBaseline"]], "rel_plus": [["ResNet - 1 5 2:Method", "Part-Of", "SimpleBaseline:Method"], ["HRNetV 1 - W 3 2:Method", "Compare-With", "SimpleBaseline:Method"], ["HRNetV 1 - W 4 8:Method", "Compare-With", "SimpleBaseline:Method"]]}
{"doc_id": "201124533", "sentence": "With the additional data from AI Challenger [ 1 2 1 ] for training , our single big network can obtain an AP of 7 7 . 0 .   Semantic segmentation is a problem of assigning a class label to each pixel .", "ner": [["AI Challenger", "Dataset"], ["Semantic segmentation", "Task"]], "rel": [["AI Challenger", "Benchmark-For", "Semantic segmentation"]], "rel_plus": [["AI Challenger:Dataset", "Benchmark-For", "Semantic segmentation:Task"]]}
{"doc_id": "201124533", "sentence": "We feed the input image to the HRNetV 2 ( Figure 4 ( b ) ) and then pass the resulting 1 5 C - dimensional representation at each position to a linear classifier with the softmax loss to predict the segmentation maps .", "ner": [["HRNetV 2", "Method"], ["linear classifier", "Method"], ["softmax loss", "Method"]], "rel": [["softmax loss", "Part-Of", "linear classifier"]], "rel_plus": [["softmax loss:Method", "Part-Of", "linear classifier:Method"]]}
{"doc_id": "201124533", "sentence": "We report the results over two scene parsing datasets , PASCALContext [ 8 1 ] and Cityscapes [ 2 2 ] , and a human parsing dataset , LIP [ 3 3 ] .", "ner": [["scene parsing", "Task"], ["PASCALContext", "Dataset"], ["Cityscapes", "Dataset"], ["human parsing", "Task"], ["LIP", "Dataset"]], "rel": [["PASCALContext", "Benchmark-For", "scene parsing"], ["Cityscapes", "Benchmark-For", "scene parsing"], ["LIP", "Benchmark-For", "human parsing"]], "rel_plus": [["PASCALContext:Dataset", "Benchmark-For", "scene parsing:Task"], ["Cityscapes:Dataset", "Benchmark-For", "scene parsing:Task"], ["LIP:Dataset", "Benchmark-For", "human parsing:Task"]]}
{"doc_id": "201124533", "sentence": "We use the SGD optimizer with the base learning rate of 0.0 1 , the momentum of 0. 9 and the weight decay of 0.0 0 0 5 .", "ner": [["SGD optimizer", "Method"], ["momentum", "Method"], ["weight decay", "Method"]], "rel": [["momentum", "Part-Of", "SGD optimizer"], ["weight decay", "Part-Of", "SGD optimizer"]], "rel_plus": [["momentum:Method", "Part-Of", "SGD optimizer:Method"], ["weight decay:Method", "Part-Of", "SGD optimizer:Method"]]}
{"doc_id": "201124533", "sentence": "Table 3 provides the comparison with several representative methods on the Cityscapes val set in terms of parameter and computation complexity and mIoU class . ( i ) HRNetV 2 - W 4 0 ( 4 0 indicates the width of the high - resolution convolution ) , with similar model size to DeepLabv 3 + and much lower computation complexity , gets better performance : 4. 7 points gain over UNet++ , 1. 7 points gain over DeepLabv 3 and about 0. 5 points gain over PSPNet , DeepLabv 3 + . ( ii ) HRNetV 2 - W 4 8 , with similar model size to PSPNet and much lower computation complexity , achieves much significant improvement : 5. 6 points gain over UNet++ , 2. 6 points gain over DeepLabv 3 and about 1. 4 points gain over PSPNet , DeepLabv 3 + .", "ner": [["Cityscapes", "Dataset"], ["HRNetV 2 - W 4 0", "Method"], ["high - resolution convolution", "Method"], ["DeepLabv 3 +", "Method"], ["UNet++", "Method"], ["DeepLabv 3", "Method"], ["PSPNet", "Method"], ["DeepLabv 3 +", "Method"], ["HRNetV 2 - W 4 8", "Method"], ["PSPNet", "Method"], ["UNet++", "Method"], ["DeepLabv 3", "Method"], ["PSPNet", "Method"], ["DeepLabv 3 +", "Method"]], "rel": [["high - resolution convolution", "Part-Of", "HRNetV 2 - W 4 0"], ["HRNetV 2 - W 4 0", "Compare-With", "UNet++"], ["HRNetV 2 - W 4 0", "Compare-With", "DeepLabv 3"], ["HRNetV 2 - W 4 0", "Compare-With", "PSPNet"], ["HRNetV 2 - W 4 0", "Compare-With", "DeepLabv 3 +"], ["HRNetV 2 - W 4 8", "Compare-With", "UNet++"], ["HRNetV 2 - W 4 8", "Compare-With", "DeepLabv 3"], ["HRNetV 2 - W 4 8", "Compare-With", "PSPNet"], ["HRNetV 2 - W 4 8", "Compare-With", "DeepLabv 3 +"]], "rel_plus": [["high - resolution convolution:Method", "Part-Of", "HRNetV 2 - W 4 0:Method"], ["HRNetV 2 - W 4 0:Method", "Compare-With", "UNet++:Method"], ["HRNetV 2 - W 4 0:Method", "Compare-With", "DeepLabv 3:Method"], ["HRNetV 2 - W 4 0:Method", "Compare-With", "PSPNet:Method"], ["HRNetV 2 - W 4 0:Method", "Compare-With", "DeepLabv 3 +:Method"], ["HRNetV 2 - W 4 8:Method", "Compare-With", "UNet++:Method"], ["HRNetV 2 - W 4 8:Method", "Compare-With", "DeepLabv 3:Method"], ["HRNetV 2 - W 4 8:Method", "Compare-With", "PSPNet:Method"], ["HRNetV 2 - W 4 8:Method", "Compare-With", "DeepLabv 3 +:Method"]]}
{"doc_id": "201124533", "sentence": "In the following comparisons , we adopt HRNetV 2 - W 4 8 that is pretrained on ImageNet and has similar model size as most DilatedResNet - 1 0 1 based methods .", "ner": [["HRNetV 2 - W 4 8", "Method"], ["ImageNet", "Dataset"], ["DilatedResNet - 1 0 1", "Method"]], "rel": [["HRNetV 2 - W 4 8", "Trained-With", "ImageNet"]], "rel_plus": [["HRNetV 2 - W 4 8:Method", "Trained-With", "ImageNet:Dataset"]]}
{"doc_id": "201124533", "sentence": "The data augmentation and learning rate policy are the same as Cityscapes .", "ner": [["data augmentation", "Method"], ["Cityscapes", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "201124533", "sentence": "The data augmentation and learning rate policy are the same as Cityscapes .", "ner": [["data augmentation", "Method"], ["learning rate policy", "Method"], ["Cityscapes", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "201124533", "sentence": "We apply our multi - level representations ( HRNetV 2 p ) 4 , shown in Figure 4 ( c ) , for object detection .", "ner": [["HRNetV 2 p", "Method"], ["object detection", "Task"]], "rel": [["HRNetV 2 p", "Used-For", "object detection"]], "rel_plus": [["HRNetV 2 p:Method", "Used-For", "object detection:Task"]]}
{"doc_id": "201124533", "sentence": "We compare our HRNet with the standard models : ResNet and ResNeXt .", "ner": [["HRNet", "Method"], ["ResNet", "Method"], ["ResNeXt", "Method"]], "rel": [["ResNet", "Part-Of", "HRNet"], ["ResNeXt", "Part-Of", "HRNet"]], "rel_plus": [["ResNet:Method", "Part-Of", "HRNet:Method"], ["ResNeXt:Method", "Part-Of", "HRNet:Method"]]}
{"doc_id": "201124533", "sentence": "We evaluate the detection performance on COCO val . under two anchor - based frameworks : Faster R - CNN [ 9 4 ] and Cascade R - CNN [ 9 ] , and two recently - developed anchor - free frameworks : FCOS [ 1 1 1 ] and CenterNet [ 2 7 ] .", "ner": [["COCO val .", "Dataset"], ["Faster R - CNN", "Method"], ["Cascade R - CNN", "Method"], ["FCOS", "Method"], ["CenterNet", "Method"]], "rel": [["Faster R - CNN", "Evaluated-With", "COCO val ."], ["Cascade R - CNN", "Evaluated-With", "COCO val ."], ["FCOS", "Evaluated-With", "COCO val ."], ["CenterNet", "Evaluated-With", "COCO val ."]], "rel_plus": [["Faster R - CNN:Method", "Evaluated-With", "COCO val .:Dataset"], ["Cascade R - CNN:Method", "Evaluated-With", "COCO val .:Dataset"], ["FCOS:Method", "Evaluated-With", "COCO val .:Dataset"], ["CenterNet:Method", "Evaluated-With", "COCO val .:Dataset"]]}
{"doc_id": "201124533", "sentence": "We train the Faster R - CNN and Cascade TABLE 1 1 Comparison with the state - of - the - art single - model object detectors on COCO test - dev with BN parameters fixed and without mutli - scale training and testing . * means that the result is from the original paper [ 9 ] .", "ner": [["Faster R - CNN", "Method"], ["COCO test - dev", "Dataset"], ["BN", "Method"]], "rel": [["Faster R - CNN", "Evaluated-With", "COCO test - dev"]], "rel_plus": [["Faster R - CNN:Method", "Evaluated-With", "COCO test - dev:Dataset"]]}
{"doc_id": "201124533", "sentence": "GFLOPs and # parameters of the models are given in R - CNN models for both our HRNetV 2 p and the ResNet on the public MMDetection platform [ 1 3 ] with the provided training setup , except that we use the learning rate schedule suggested in [ 3 7 ] for 2 \u00d7 , and FCOS [ 1 1 1 ] and CenterNet [ 2 7 ] from the implementations provided by the authors .", "ner": [["R - CNN", "Method"], ["HRNetV 2 p", "Method"], ["ResNet", "Method"], ["FCOS", "Method"], ["CenterNet", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "201124533", "sentence": "We also evaluate the performance of joint detection and instance segmentation , under three frameworks : Mask R - CNN [ 3 8 ] , Cascade Mask R - CNN [ 1 0 ] , and Hybrid Task Cascade [ 1 2 ] .", "ner": [["instance segmentation", "Task"], ["Mask R - CNN", "Method"], ["Cascade Mask R - CNN", "Method"], ["Hybrid Task Cascade", "Method"]], "rel": [["Mask R - CNN", "Used-For", "instance segmentation"], ["Cascade Mask R - CNN", "Used-For", "instance segmentation"], ["Hybrid Task Cascade", "Used-For", "instance segmentation"]], "rel_plus": [["Mask R - CNN:Method", "Used-For", "instance segmentation:Task"], ["Cascade Mask R - CNN:Method", "Used-For", "instance segmentation:Task"], ["Hybrid Task Cascade:Method", "Used-For", "instance segmentation:Task"]]}
{"doc_id": "201124533", "sentence": "On the one hand , as shown in Tables 8 and 9 , the overall object detection performance of HRNetV 2 is better than ResNet under similar model size and computation complexity .", "ner": [["object detection", "Task"], ["HRNetV 2", "Method"], ["ResNet", "Method"]], "rel": [["HRNetV 2", "Used-For", "object detection"], ["ResNet", "Used-For", "object detection"], ["HRNetV 2", "Compare-With", "ResNet"]], "rel_plus": [["HRNetV 2:Method", "Used-For", "object detection:Task"], ["ResNet:Method", "Used-For", "object detection:Task"], ["HRNetV 2:Method", "Compare-With", "ResNet:Method"]]}
{"doc_id": "201124533", "sentence": "In some cases , for 1 \u00d7 , HRNetV 2 p - W 1 8 performs worse than ResNet - 5 0 - FPN , which might come from insufficient optimization iterations .", "ner": [["HRNetV 2 p - W 1 8", "Method"], ["ResNet - 5 0 - FPN", "Method"]], "rel": [["HRNetV 2 p - W 1 8", "Compare-With", "ResNet - 5 0 - FPN"]], "rel_plus": [["HRNetV 2 p - W 1 8:Method", "Compare-With", "ResNet - 5 0 - FPN:Method"]]}
{"doc_id": "201124533", "sentence": "On the other hand , as shown in Table 1 0 , the overall object detection and instance segmentation performance is better than ResNet and ResNeXt .", "ner": [["object detection", "Task"], ["instance segmentation", "Task"], ["ResNet", "Method"], ["ResNeXt", "Method"]], "rel": [["ResNet", "Used-For", "object detection"], ["ResNeXt", "Used-For", "object detection"], ["ResNeXt", "Used-For", "instance segmentation"], ["ResNet", "Used-For", "instance segmentation"]], "rel_plus": [["ResNet:Method", "Used-For", "object detection:Task"], ["ResNeXt:Method", "Used-For", "object detection:Task"], ["ResNeXt:Method", "Used-For", "instance segmentation:Task"], ["ResNet:Method", "Used-For", "instance segmentation:Task"]]}
{"doc_id": "201124533", "sentence": "In particular , under the Hybrid Task Cascade framework , the HRNet performs slightly worse than ResNeXt - 1 0 1 - 6 4 \u00d7 4 d - FPN for 2 0 e , but better for 2 8 e. This implies that our HRNet benefits more from longer training .", "ner": [["Task Cascade", "Method"], ["HRNet", "Method"], ["ResNeXt - 1 0 1 - 6 4 \u00d7 4 d - FPN", "Method"], ["HRNet", "Method"]], "rel": [["HRNet", "Compare-With", "ResNeXt - 1 0 1 - 6 4 \u00d7 4 d - FPN"]], "rel_plus": [["HRNet:Method", "Compare-With", "ResNeXt - 1 0 1 - 6 4 \u00d7 4 d - FPN:Method"]]}
{"doc_id": "201124533", "sentence": "In the Faster R - CNN framework , our networks perform better than ResNets with similar parameter and computation complexity : HRNetV 2 p - W 3 2 vs. ResNet - 1 0 1 - FPN , HRNetV 2 p - W 4 0 vs. ResNet - 1 5 2 - FPN , HRNetV 2 p - W 4 8 vs. X - 1 0 1 - 6 4 \u00d7 4d - FPN .", "ner": [["Faster R - CNN", "Method"], ["ResNets", "Method"], ["HRNetV 2 p - W 3 2", "Method"], ["ResNet - 1 0 1 - FPN", "Method"], ["HRNetV 2 p - W 4 0", "Method"], ["ResNet - 1 5 2 - FPN", "Method"], ["HRNetV 2 p - W 4 8", "Method"], ["X - 1 0 1 - 6 4 \u00d7 4d - FPN", "Method"]], "rel": [["HRNetV 2 p - W 3 2", "Compare-With", "ResNet - 1 0 1 - FPN"], ["HRNetV 2 p - W 4 0", "Compare-With", "ResNet - 1 5 2 - FPN"], ["HRNetV 2 p - W 4 8", "Compare-With", "X - 1 0 1 - 6 4 \u00d7 4d - FPN"]], "rel_plus": [["HRNetV 2 p - W 3 2:Method", "Compare-With", "ResNet - 1 0 1 - FPN:Method"], ["HRNetV 2 p - W 4 0:Method", "Compare-With", "ResNet - 1 5 2 - FPN:Method"], ["HRNetV 2 p - W 4 8:Method", "Compare-With", "X - 1 0 1 - 6 4 \u00d7 4d - FPN:Method"]]}
{"doc_id": "201124533", "sentence": "In the Cascade R - CNN and CenterNet framework , our HRNetV 2 also performs better .", "ner": [["Cascade R - CNN", "Method"], ["CenterNet", "Method"], ["HRNetV 2", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "201124533", "sentence": "In the Cascade Mask R - CNN and Hybrid Task Cascade frameworks , the HRNet gets the overall better performance .", "ner": [["Cascade Mask R - CNN", "Method"], ["Hybrid Task Cascade", "Method"], ["HRNet", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "201124533", "sentence": "We perform the ablation study for the components in HRNet over two tasks : human pose estimation on COCO validation and semantic segmentation on Cityscapes validation .", "ner": [["HRNet", "Method"], ["human pose estimation", "Task"], ["COCO validation", "Dataset"], ["semantic segmentation", "Task"], ["Cityscapes validation", "Dataset"]], "rel": [["HRNet", "Used-For", "human pose estimation"], ["COCO validation", "Benchmark-For", "human pose estimation"], ["HRNet", "Evaluated-With", "COCO validation"], ["HRNet", "Used-For", "semantic segmentation"], ["Cityscapes validation", "Benchmark-For", "semantic segmentation"], ["HRNet", "Evaluated-With", "Cityscapes validation"]], "rel_plus": [["HRNet:Method", "Used-For", "human pose estimation:Task"], ["COCO validation:Dataset", "Benchmark-For", "human pose estimation:Task"], ["HRNet:Method", "Evaluated-With", "COCO validation:Dataset"], ["HRNet:Method", "Used-For", "semantic segmentation:Task"], ["Cityscapes validation:Dataset", "Benchmark-For", "semantic segmentation:Task"], ["HRNet:Method", "Evaluated-With", "Cityscapes validation:Dataset"]]}
{"doc_id": "201124533", "sentence": "We mainly use HRNetV 1 - W 3 2 for human pose estimation , and HRNetV 2 - W 4 8 for semantic segmentation .", "ner": [["HRNetV 1 - W 3 2", "Method"], ["human pose estimation", "Task"], ["HRNetV 2 - W 4 8", "Method"], ["semantic segmentation", "Task"]], "rel": [["HRNetV 1 - W 3 2", "Used-For", "human pose estimation"], ["HRNetV 2 - W 4 8", "Used-For", "semantic segmentation"]], "rel_plus": [["HRNetV 1 - W 3 2:Method", "Used-For", "human pose estimation:Task"], ["HRNetV 2 - W 4 8:Method", "Used-For", "semantic segmentation:Task"]]}
{"doc_id": "201124533", "sentence": "Then , we present the results for comparing HRNetV 1 and HRNetV 2 .", "ner": [["HRNetV 1", "Method"], ["HRNetV 2", "Method"]], "rel": [["HRNetV 1", "Compare-With", "HRNetV 2"]], "rel_plus": [["HRNetV 1:Method", "Compare-With", "HRNetV 2:Method"]]}
{"doc_id": "201124533", "sentence": "We study how the representation resolution affects the pose estimation performance by checking the quality of the heatmap estimated from the feature maps of each resolution from high to low .", "ner": [["pose estimation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "201124533", "sentence": "We train two HRNetV 1 networks initialized by the model pretrained for the ImageNet classification .", "ner": [["HRNetV 1", "Method"], ["ImageNet", "Dataset"]], "rel": [["HRNetV 1", "Trained-With", "ImageNet"]], "rel_plus": [["HRNetV 1:Method", "Trained-With", "ImageNet:Dataset"]]}
{"doc_id": "201124533", "sentence": "The human pose estimation performance ( AP ) on COCO val for the variant is 7 2 . 5 , which is lower than 7 3 . 4 for HRNetV 1 - W 3 2 .", "ner": [["human pose estimation", "Task"], ["COCO val", "Dataset"], ["HRNetV 1 - W 3 2", "Method"]], "rel": [["COCO val", "Benchmark-For", "human pose estimation"]], "rel_plus": [["COCO val:Dataset", "Benchmark-For", "human pose estimation:Task"]]}
{"doc_id": "201124533", "sentence": "The segmentation performance ( mIoU ) on Cityscapes val for the variant is 7 5 . 7 , which is lower than 7 6 . 4 for HRNetV 2 - W 4 8 .", "ner": [["segmentation", "Task"], ["Cityscapes", "Dataset"], ["HRNetV 2 - W 4 8", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "201124533", "sentence": "In addition , another simple variant , only the high - resolution stream of similar # parameters and GFLOPs without low - resolution parallel streams shows much lower performance on COCO and Cityscapes .", "ner": [["COCO", "Dataset"], ["Cityscapes", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "201124533", "sentence": "We compare HRNetV 2 and HRNetV 2 p , to HRNetV 1 on pose estimation , semantic segmentation and COCO object detection .", "ner": [["HRNetV 2", "Method"], ["HRNetV 2 p", "Method"], ["HRNetV 1", "Method"], ["pose estimation", "Task"], ["semantic segmentation", "Task"], ["COCO", "Dataset"], ["object detection", "Task"]], "rel": [["HRNetV 2", "Compare-With", "HRNetV 1"], ["HRNetV 2 p", "Compare-With", "HRNetV 1"], ["HRNetV 2", "Used-For", "pose estimation"], ["HRNetV 2 p", "Used-For", "pose estimation"], ["HRNetV 1", "Used-For", "pose estimation"], ["HRNetV 2 p", "Used-For", "semantic segmentation"], ["HRNetV 2", "Used-For", "semantic segmentation"], ["HRNetV 1", "Used-For", "semantic segmentation"], ["HRNetV 1", "Used-For", "object detection"], ["HRNetV 2 p", "Used-For", "object detection"], ["HRNetV 2", "Used-For", "object detection"]], "rel_plus": [["HRNetV 2:Method", "Compare-With", "HRNetV 1:Method"], ["HRNetV 2 p:Method", "Compare-With", "HRNetV 1:Method"], ["HRNetV 2:Method", "Used-For", "pose estimation:Task"], ["HRNetV 2 p:Method", "Used-For", "pose estimation:Task"], ["HRNetV 1:Method", "Used-For", "pose estimation:Task"], ["HRNetV 2 p:Method", "Used-For", "semantic segmentation:Task"], ["HRNetV 2:Method", "Used-For", "semantic segmentation:Task"], ["HRNetV 1:Method", "Used-For", "semantic segmentation:Task"], ["HRNetV 1:Method", "Used-For", "object detection:Task"], ["HRNetV 2 p:Method", "Used-For", "object detection:Task"], ["HRNetV 2:Method", "Used-For", "object detection:Task"]]}
{"doc_id": "201124533", "sentence": "For example , HRNetV 2 - W 3 2 ( w/o ImageNet pretraining ) achieves the AP score 7 3 . 6 , which is slightly higher than 7 3 . 4 HRNetV 1 - W 3 2 .", "ner": [["HRNetV 2 - W 3 2", "Method"], ["ImageNet", "Dataset"], ["HRNetV 1 - W 3 2", "Method"]], "rel": [["HRNetV 2 - W 3 2", "Compare-With", "HRNetV 1 - W 3 2"]], "rel_plus": [["HRNetV 2 - W 3 2:Method", "Compare-With", "HRNetV 1 - W 3 2:Method"]]}
{"doc_id": "201124533", "sentence": "The segmentation and object detection results , given in Figure 1 0 ( a ) and Figure 1 0 ( b ) , imply that HRNetV 2 outperforms HRNetV 1 significantly , except that the gain is minor in the large model case ( 1 \u00d7 ) in segmentation for Cityscapes .", "ner": [["segmentation", "Task"], ["object detection", "Task"], ["HRNetV 2", "Method"], ["HRNetV 1", "Method"], ["segmentation", "Task"], ["Cityscapes", "Dataset"]], "rel": [["HRNetV 2", "Used-For", "segmentation"], ["HRNetV 1", "Used-For", "segmentation"], ["HRNetV 2", "Used-For", "object detection"], ["HRNetV 1", "Used-For", "object detection"], ["HRNetV 2", "Compare-With", "HRNetV 1"], ["Cityscapes", "Benchmark-For", "segmentation"]], "rel_plus": [["HRNetV 2:Method", "Used-For", "segmentation:Task"], ["HRNetV 1:Method", "Used-For", "segmentation:Task"], ["HRNetV 2:Method", "Used-For", "object detection:Task"], ["HRNetV 1:Method", "Used-For", "object detection:Task"], ["HRNetV 2:Method", "Compare-With", "HRNetV 1:Method"], ["Cityscapes:Dataset", "Benchmark-For", "segmentation:Task"]]}
{"doc_id": "201124533", "sentence": "We also test a variant ( denoted by HRNetV 1 h ) , which is built by appending a 1 \u00d7 1 convolution to align the dimension of the output high - resolution representation with the dimension of HRNetV 2 .", "ner": [["HRNetV 1 h", "Method"], ["1 \u00d7 1 convolution", "Method"], ["HRNetV 2", "Method"]], "rel": [["1 \u00d7 1 convolution", "Part-Of", "HRNetV 1 h"]], "rel_plus": [["1 \u00d7 1 convolution:Method", "Part-Of", "HRNetV 1 h:Method"]]}
{"doc_id": "201124533", "sentence": "The results in Figure 1 0 ( a ) and Figure 1 0 ( b ) show that the variant achieves slight improvement to HRNetV 1 , implying that aggregating the representations from low - resolution parallel convolutions in our HRNetV 2 is essential for improving the capability .", "ner": [["HRNetV 1", "Method"], ["low - resolution parallel convolutions", "Method"], ["HRNetV 2", "Method"]], "rel": [["low - resolution parallel convolutions", "Part-Of", "HRNetV 2"]], "rel_plus": [["low - resolution parallel convolutions:Method", "Part-Of", "HRNetV 2:Method"]]}
{"doc_id": "201124533", "sentence": "There are three fundamental differences from existing low - resolution classification networks and high - resolution representation learning networks : ( i ) Connect high and low resolution convolutions in parallel other than in series ; ( ii ) Maintain high resolution through the whole process instead of recovering high resolution from low resolution ; and ( iii ) Fuse multi - resolution representations repeatedly , rendering strong high - resolution representations .", "ner": [["low - resolution classification", "Task"], ["high - resolution representation learning", "Task"], ["low resolution convolutions", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "201124533", "sentence": "The superior results on a wide range of visual recognition problems suggest that our proposed HRNet is a stronger backbone for computer vision problems .", "ner": [["HRNet", "Method"], ["computer vision", "Task"]], "rel": [["HRNet", "Used-For", "computer vision"]], "rel_plus": [["HRNet:Method", "Used-For", "computer vision:Task"]]}
{"doc_id": "201124533", "sentence": "We will study the combination of the HRNet with other techniques for semantic segmentation and instance segmentation .", "ner": [["HRNet", "Method"], ["semantic segmentation", "Task"], ["instance segmentation", "Task"]], "rel": [["HRNet", "Used-For", "semantic segmentation"], ["HRNet", "Used-For", "instance segmentation"]], "rel_plus": [["HRNet:Method", "Used-For", "semantic segmentation:Task"], ["HRNet:Method", "Used-For", "instance segmentation:Task"]]}
{"doc_id": "201124533", "sentence": "Currently , we have results ( mIoU ) by combining the HRNet with a contextual model , a variant of object context [ 4 4 ] , [ 1 3 6 ] : 8 2 . 3 % by using the additional coarse data on Cityscapes test 5 , 5 6 . 2 % on PASCAL - context , and 5 6 . 6 6 % on LIP .", "ner": [["HRNet", "Method"], ["Cityscapes", "Dataset"], ["PASCAL - context", "Dataset"], ["LIP", "Dataset"]], "rel": [["HRNet", "Evaluated-With", "Cityscapes"], ["HRNet", "Evaluated-With", "PASCAL - context"], ["HRNet", "Evaluated-With", "LIP"]], "rel_plus": [["HRNet:Method", "Evaluated-With", "Cityscapes:Dataset"], ["HRNet:Method", "Evaluated-With", "PASCAL - context:Dataset"], ["HRNet:Method", "Evaluated-With", "LIP:Dataset"]]}
{"doc_id": "201124533", "sentence": "The applications of the HRNet are not limited to the above that we have done , and are suitable to other positionsensitive vision applications , such as super - resolution , optical flow estimation , depth estimation , and so on .", "ner": [["HRNet", "Method"], ["super - resolution", "Task"], ["optical flow estimation", "Task"], ["depth estimation", "Task"]], "rel": [["HRNet", "Used-For", "super - resolution"], ["HRNet", "Used-For", "optical flow estimation"], ["HRNet", "Used-For", "depth estimation"]], "rel_plus": [["HRNet:Method", "Used-For", "super - resolution:Task"], ["HRNet:Method", "Used-For", "optical flow estimation:Task"], ["HRNet:Method", "Used-For", "depth estimation:Task"]]}
{"doc_id": "201124533", "sentence": "There are already followup works , e.g. , image stylization [ 6 3 ] , inpainting [ 3 5 ] , image enhancement [ 4 6 ] , image dehazing [ 1 ] , and temporal pose estimation [ 4 ] .", "ner": [["image stylization", "Task"], ["inpainting", "Task"], ["image enhancement", "Task"], ["image dehazing", "Task"], ["temporal pose estimation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "201124533", "sentence": "We pretrain our network , which is augmented by a representation head for classification shown in Figure 1 1 , on ImageNet [ 9 6 ] .", "ner": [["ImageNet", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "201124533", "sentence": "Last , we transform the 1 0 2 4 channels to 2 0 4 8 channels through a 1 \u00d7 1 convolution , followed by a global average pooling operation .", "ner": [["1 \u00d7 1 convolution", "Method"], ["global average pooling operation", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "201124533", "sentence": "We use SGD with a weight decay of 0.0 0 0 1 and a Nesterov momentum of 0. 9 .", "ner": [["SGD", "Method"], ["weight decay", "Method"], ["Nesterov momentum", "Method"]], "rel": [["weight decay", "Part-Of", "SGD"], ["Nesterov momentum", "Part-Of", "SGD"]], "rel_plus": [["weight decay:Method", "Part-Of", "SGD:Method"], ["Nesterov momentum:Method", "Part-Of", "SGD:Method"]]}
{"doc_id": "201124533", "sentence": "We follow the PyTorch ResNet implementation and replace the 7 \u00d7 7 convolution in the input stem with two stride - 2 3 \u00d7 3 convolutions decreasing the resolution to 1/ 4 as in our networks .", "ner": [["ResNet", "Method"], ["7 \u00d7 7 convolution", "Method"], ["stride - 2 3 \u00d7 3 convolutions", "Method"]], "rel": [["stride - 2 3 \u00d7 3 convolutions", "Part-Of", "ResNet"]], "rel_plus": [["stride - 2 3 \u00d7 3 convolutions:Method", "Part-Of", "ResNet:Method"]]}
{"doc_id": "201124533", "sentence": "In addition , we look at the results of two alternative schemes : ( i ) the feature maps on each resolution go through a global pooling separately and then are concatenated together to output a 1 5 C - dimensional representation vector , named HRNet - Wx - Ci ; ( ii ) the feature maps on each resolution are fed into several stride - 2 residual units ( bottleneck , each dimension is increased to the double ) to increase the dimension to 5 1 2 , and concatenate and average - pool them together to reach a 2 0 4 8 - dimensional representation vector , named HRNet - Wx - Cii .", "ner": [["HRNet - Wx - Ci", "Method"], ["stride - 2 residual units", "Method"], ["HRNet - Wx - Cii", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "201646309", "sentence": "BERT ( Devlin et al. , 2 0 1 8) and RoBERTa ( Liu et al. , 2 0 1 9 ) has set a new state - of - the - art performance on sentence - pair regression tasks like semantic textual similarity ( STS ) .", "ner": [["BERT", "Method"], ["RoBERTa", "Method"], ["sentence - pair regression", "Task"], ["semantic textual similarity", "Task"], ["STS", "Task"]], "rel": [["BERT", "Used-For", "sentence - pair regression"], ["RoBERTa", "Used-For", "sentence - pair regression"], ["BERT", "Used-For", "semantic textual similarity"], ["RoBERTa", "Used-For", "semantic textual similarity"], ["STS", "Synonym-Of", "semantic textual similarity"]], "rel_plus": [["BERT:Method", "Used-For", "sentence - pair regression:Task"], ["RoBERTa:Method", "Used-For", "sentence - pair regression:Task"], ["BERT:Method", "Used-For", "semantic textual similarity:Task"], ["RoBERTa:Method", "Used-For", "semantic textual similarity:Task"], ["STS:Task", "Synonym-Of", "semantic textual similarity:Task"]]}
{"doc_id": "201646309", "sentence": "The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering .", "ner": [["BERT", "Method"], ["semantic similarity search", "Task"]], "rel": [["BERT", "Used-For", "semantic similarity search"]], "rel_plus": [["BERT:Method", "Used-For", "semantic similarity search:Task"]]}
{"doc_id": "201646309", "sentence": "In this publication , we present Sentence - BERT ( SBERT ) , a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine - similarity .", "ner": [["Sentence - BERT", "Method"], ["SBERT", "Method"], ["BERT", "Method"], ["sentence embeddings", "Task"]], "rel": [["SBERT", "Synonym-Of", "Sentence - BERT"], ["Sentence - BERT", "SubClass-Of", "BERT"]], "rel_plus": [["SBERT:Method", "Synonym-Of", "Sentence - BERT:Method"], ["Sentence - BERT:Method", "SubClass-Of", "BERT:Method"]]}
{"doc_id": "201646309", "sentence": "This reduces the effort for finding the most similar pair from 6 5 hours with BERT / RoBERTa to about 5 seconds with SBERT , while maintaining the accuracy from BERT .", "ner": [["BERT", "Method"], ["RoBERTa", "Method"], ["SBERT", "Method"], ["BERT", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "201646309", "sentence": "We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks , where it outperforms other state - of - the - art sentence embeddings methods .", "ner": [["SBERT", "Method"], ["SRoBERTa", "Method"], ["STS", "Task"], ["transfer learning", "Task"], ["sentence embeddings", "Task"]], "rel": [["SBERT", "Used-For", "STS"], ["SRoBERTa", "Used-For", "transfer learning"]], "rel_plus": [["SBERT:Method", "Used-For", "STS:Task"], ["SRoBERTa:Method", "Used-For", "transfer learning:Task"]]}
{"doc_id": "201646309", "sentence": "In this publication , we present Sentence - BERT ( SBERT ) , a modification of the BERT network using siamese and triplet networks that is able to derive semantically meaningful sentence embeddings 2 .", "ner": [["Sentence - BERT", "Method"], ["SBERT", "Method"], ["BERT", "Method"], ["sentence embeddings", "Task"]], "rel": [["SBERT", "Synonym-Of", "Sentence - BERT"], ["Sentence - BERT", "SubClass-Of", "BERT"]], "rel_plus": [["SBERT:Method", "Synonym-Of", "Sentence - BERT:Method"], ["Sentence - BERT:Method", "SubClass-Of", "BERT:Method"]]}
{"doc_id": "201646309", "sentence": "This enables BERT to be used for certain new tasks , which up - to - now were not applicable for BERT .", "ner": [["BERT", "Method"], ["BERT", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "201646309", "sentence": "BERT set new state - of - the - art performance on various sentence classification and sentence - pair regression tasks .", "ner": [["BERT", "Method"], ["sentence classification", "Task"], ["sentence - pair regression", "Task"]], "rel": [["BERT", "Used-For", "sentence classification"], ["BERT", "Used-For", "sentence - pair regression"]], "rel_plus": [["BERT:Method", "Used-For", "sentence classification:Task"], ["BERT:Method", "Used-For", "sentence - pair regression:Task"]]}
{"doc_id": "201646309", "sentence": "BERT uses a cross - encoder : Two sentences are passed to the transformer network and the target value is predicted .", "ner": [["BERT", "Method"], ["cross - encoder", "Method"], ["transformer", "Method"]], "rel": [["cross - encoder", "Part-Of", "BERT"]], "rel_plus": [["cross - encoder:Method", "Part-Of", "BERT:Method"]]}
{"doc_id": "201646309", "sentence": "Researchers have started to input individual sentences into BERT and to derive fixedsize sentence embeddings .", "ner": [["BERT", "Method"], ["sentence embeddings", "Task"]], "rel": [["BERT", "Used-For", "sentence embeddings"]], "rel_plus": [["BERT:Method", "Used-For", "sentence embeddings:Task"]]}
{"doc_id": "201646309", "sentence": "The most commonly used approach is to average the BERT output layer ( known as BERT embeddings ) or by using the output of the first token ( the [ CLS ] token ) .", "ner": [["BERT", "Method"], ["BERT embeddings", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "201646309", "sentence": "As we will show , this common practice yields rather bad sentence embeddings , often worse than averaging GloVe embeddings ( Pennington et al. , 2 0 1 4 ) .", "ner": [["sentence embeddings", "Task"], ["averaging GloVe embeddings", "Task"]], "rel": [["sentence embeddings", "Compare-With", "averaging GloVe embeddings"]], "rel_plus": [["sentence embeddings:Task", "Compare-With", "averaging GloVe embeddings:Task"]]}
{"doc_id": "201646309", "sentence": "These similarity measures can be performed extremely efficient on modern hardware , allowing SBERT to be used for semantic similarity search as well as for clustering .", "ner": [["SBERT", "Method"], ["semantic similarity", "Task"]], "rel": [["SBERT", "Used-For", "semantic similarity"]], "rel_plus": [["SBERT:Method", "Used-For", "semantic similarity:Task"]]}
{"doc_id": "201646309", "sentence": "The complexity for finding the most similar sentence pair in a collection of 1 0 , 0 0 0 sentences is reduced from 6 5 hours with BERT to the computation of 1 0 , 0 0 0 sentence embeddings ( ~ 5 seconds with SBERT ) and computing cosinesimilarity ( ~ 0 . 0 1 seconds ) .", "ner": [["BERT", "Method"], ["sentence embeddings", "Task"], ["SBERT", "Method"], ["cosinesimilarity", "Task"]], "rel": [["BERT", "Used-For", "sentence embeddings"], ["SBERT", "Used-For", "sentence embeddings"], ["BERT", "Compare-With", "SBERT"], ["BERT", "Used-For", "cosinesimilarity"], ["SBERT", "Used-For", "cosinesimilarity"]], "rel_plus": [["BERT:Method", "Used-For", "sentence embeddings:Task"], ["SBERT:Method", "Used-For", "sentence embeddings:Task"], ["BERT:Method", "Compare-With", "SBERT:Method"], ["BERT:Method", "Used-For", "cosinesimilarity:Task"], ["SBERT:Method", "Used-For", "cosinesimilarity:Task"]]}
{"doc_id": "201646309", "sentence": "We fine - tune SBERT on NLI data , which creates sentence embeddings that significantly outperform other state - of - the - art sentence embedding methods like InferSent ( Conneau et al. , 2 0 1 7 ) and Universal Sentence Encoder .", "ner": [["SBERT", "Method"], ["NLI", "Task"], ["sentence embeddings", "Task"], ["sentence embedding", "Task"], ["InferSent", "Method"], ["Universal Sentence Encoder", "Method"]], "rel": [["SBERT", "Used-For", "NLI"], ["SBERT", "Used-For", "sentence embeddings"], ["InferSent", "Used-For", "sentence embedding"], ["Universal Sentence Encoder", "Used-For", "sentence embedding"], ["SBERT", "Compare-With", "InferSent"], ["SBERT", "Compare-With", "Universal Sentence Encoder"]], "rel_plus": [["SBERT:Method", "Used-For", "NLI:Task"], ["SBERT:Method", "Used-For", "sentence embeddings:Task"], ["InferSent:Method", "Used-For", "sentence embedding:Task"], ["Universal Sentence Encoder:Method", "Used-For", "sentence embedding:Task"], ["SBERT:Method", "Compare-With", "InferSent:Method"], ["SBERT:Method", "Compare-With", "Universal Sentence Encoder:Method"]]}
{"doc_id": "201646309", "sentence": "On seven Semantic Textual Similarity ( STS ) tasks , SBERT achieves an improvement of 1 1 . 7 points compared to InferSent and 5. 5 points compared to Universal Sentence Encoder .", "ner": [["Semantic Textual Similarity", "Task"], ["STS", "Task"], ["SBERT", "Method"], ["InferSent", "Method"], ["Universal Sentence Encoder .", "Method"]], "rel": [["STS", "Synonym-Of", "Semantic Textual Similarity"], ["SBERT", "Compare-With", "InferSent"], ["SBERT", "Compare-With", "Universal Sentence Encoder ."]], "rel_plus": [["STS:Task", "Synonym-Of", "Semantic Textual Similarity:Task"], ["SBERT:Method", "Compare-With", "InferSent:Method"], ["SBERT:Method", "Compare-With", "Universal Sentence Encoder .:Method"]]}
{"doc_id": "201646309", "sentence": "On SentEval ( Conneau and Kiela , 2 0 1 8) , an evaluation toolkit for sentence embeddings , we achieve an improvement of 2. 1 and 2. 6 points , respectively .", "ner": [["SentEval", "Dataset"], ["sentence embeddings", "Task"]], "rel": [["SentEval", "Benchmark-For", "sentence embeddings"]], "rel_plus": [["SentEval:Dataset", "Benchmark-For", "sentence embeddings:Task"]]}
{"doc_id": "201646309", "sentence": "The paper is structured in the following way : Section 3 presents SBERT , section 4 evaluates SBERT on common STS tasks and on the challenging Argument Facet Similarity ( AFS ) corpus ( Misra et al. , 2 0 1 6 ) .", "ner": [["SBERT", "Method"], ["SBERT", "Method"], ["STS", "Task"], ["Argument Facet Similarity", "Dataset"], ["AFS", "Dataset"]], "rel": [["SBERT", "Used-For", "STS"], ["AFS", "Synonym-Of", "Argument Facet Similarity"], ["SBERT", "Evaluated-With", "Argument Facet Similarity"]], "rel_plus": [["SBERT:Method", "Used-For", "STS:Task"], ["AFS:Dataset", "Synonym-Of", "Argument Facet Similarity:Dataset"], ["SBERT:Method", "Evaluated-With", "Argument Facet Similarity:Dataset"]]}
{"doc_id": "201646309", "sentence": "Section 5 evaluates SBERT on SentEval .", "ner": [["SBERT", "Method"], ["SentEval", "Dataset"]], "rel": [["SBERT", "Evaluated-With", "SentEval"]], "rel_plus": [["SBERT:Method", "Evaluated-With", "SentEval:Dataset"]]}
{"doc_id": "201646309", "sentence": "In section 7 , we compare the computational efficiency of SBERT sentence embeddings in contrast to other state - of - the - art sentence embedding methods .", "ner": [["SBERT sentence embeddings", "Method"], ["sentence embedding", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "201646309", "sentence": "We first introduce BERT , then , we discuss stateof - the - art sentence embedding methods .", "ner": [["BERT", "Method"], ["sentence embedding", "Task"]], "rel": [["BERT", "Used-For", "sentence embedding"]], "rel_plus": [["BERT:Method", "Used-For", "sentence embedding:Task"]]}
{"doc_id": "201646309", "sentence": "BERT ( Devlin et al. , 2 0 1 8 ) is a pre - trained transformer network ( Vaswani et al. , 2 0 1 7 ) , which set for various NLP tasks new state - of - the - art results , including question answering , sentence classification , and sentence - pair regression .", "ner": [["BERT", "Method"], ["transformer", "Method"], ["NLP", "Task"], ["question answering", "Task"], ["sentence classification", "Task"], ["sentence - pair regression", "Task"]], "rel": [["BERT", "SubClass-Of", "transformer"], ["question answering", "SubTask-Of", "NLP"], ["sentence classification", "SubTask-Of", "NLP"], ["sentence - pair regression", "SubTask-Of", "NLP"], ["BERT", "Used-For", "NLP"], ["BERT", "Used-For", "question answering"], ["BERT", "Used-For", "sentence classification"], ["BERT", "Used-For", "sentence - pair regression"]], "rel_plus": [["BERT:Method", "SubClass-Of", "transformer:Method"], ["question answering:Task", "SubTask-Of", "NLP:Task"], ["sentence classification:Task", "SubTask-Of", "NLP:Task"], ["sentence - pair regression:Task", "SubTask-Of", "NLP:Task"], ["BERT:Method", "Used-For", "NLP:Task"], ["BERT:Method", "Used-For", "question answering:Task"], ["BERT:Method", "Used-For", "sentence classification:Task"], ["BERT:Method", "Used-For", "sentence - pair regression:Task"]]}
{"doc_id": "201646309", "sentence": "The input for BERT for sentence - pair regression consists of the two sentences , separated by a special [ SEP ] token .", "ner": [["BERT", "Method"], ["sentence - pair regression", "Task"]], "rel": [["BERT", "Used-For", "sentence - pair regression"]], "rel_plus": [["BERT:Method", "Used-For", "sentence - pair regression:Task"]]}
{"doc_id": "201646309", "sentence": "Using this setup , BERT set a new state - of - the - art performance on the Semantic Textual Semilarity ( STS ) benchmark ( Cer et al. , 2 0 1 7 ) .", "ner": [["BERT", "Method"], ["Semantic Textual Semilarity", "Dataset"], ["STS", "Dataset"]], "rel": [["STS", "Synonym-Of", "Semantic Textual Semilarity"], ["BERT", "Evaluated-With", "Semantic Textual Semilarity"]], "rel_plus": [["STS:Dataset", "Synonym-Of", "Semantic Textual Semilarity:Dataset"], ["BERT:Method", "Evaluated-With", "Semantic Textual Semilarity:Dataset"]]}
{"doc_id": "201646309", "sentence": "RoBERTa showed , that the performance of BERT can further improved by small adaptations to the pre - training process .", "ner": [["RoBERTa", "Method"], ["BERT", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "201646309", "sentence": "We also tested XLNet ( Yang et al. , 2 0 1 9 ) , but it led in general to worse results than BERT .", "ner": [["XLNet", "Method"], ["BERT", "Method"]], "rel": [["XLNet", "Compare-With", "BERT"]], "rel_plus": [["XLNet:Method", "Compare-With", "BERT:Method"]]}
{"doc_id": "201646309", "sentence": "A large disadvantage of the BERT network structure is that no independent sentence embeddings are computed , which makes it difficult to derive sentence embeddings from BERT .", "ner": [["BERT", "Method"], ["sentence embeddings", "Task"], ["sentence embeddings", "Task"], ["BERT", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "201646309", "sentence": "To bypass this limitations , researchers passed single sentences through BERT and then derive a fixed sized vector by either averaging the outputs ( similar to average word embeddings ) or by using the output of the special CLS token ( for example : May et al. ( 2 0 1 9 ) ; Zhang et al. ( 2 0 1 9 ) ; Qiao et al. ( 2 0 1 9 ) ) .", "ner": [["BERT", "Method"], ["word embeddings", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "201646309", "sentence": "Skip - Thought ( Kiros et al. , 2 0 1 5 ) trains an encoder - decoder architecture to predict the surrounding sentences .", "ner": [["Skip - Thought", "Method"], ["encoder - decoder", "Method"]], "rel": [["Skip - Thought", "Used-For", "encoder - decoder"]], "rel_plus": [["Skip - Thought:Method", "Used-For", "encoder - decoder:Method"]]}
{"doc_id": "201646309", "sentence": "InferSent ( Conneau et al. , 2 0 1 7 ) uses labeled data of the Stanford Natural Language Inference dataset ( Bowman et al. , 2 0 1 5 ) and the MultiGenre NLI dataset ( Williams et al. , 2 0 1 8) to train a siamese BiLSTM network with max - pooling over the output .", "ner": [["InferSent", "Method"], ["Stanford Natural Language Inference", "Dataset"], ["MultiGenre NLI", "Dataset"], ["siamese BiLSTM", "Method"], ["max - pooling", "Method"]], "rel": [["siamese BiLSTM", "Trained-With", "Stanford Natural Language Inference"], ["siamese BiLSTM", "Trained-With", "MultiGenre NLI"], ["max - pooling", "Part-Of", "siamese BiLSTM"]], "rel_plus": [["siamese BiLSTM:Method", "Trained-With", "Stanford Natural Language Inference:Dataset"], ["siamese BiLSTM:Method", "Trained-With", "MultiGenre NLI:Dataset"], ["max - pooling:Method", "Part-Of", "siamese BiLSTM:Method"]]}
{"doc_id": "201646309", "sentence": "Conneau et al. showed , that InferSent consistently outperforms unsupervised methods like SkipThought .", "ner": [["InferSent", "Method"], ["SkipThought", "Method"]], "rel": [["InferSent", "Compare-With", "SkipThought"]], "rel_plus": [["InferSent:Method", "Compare-With", "SkipThought:Method"]]}
{"doc_id": "201646309", "sentence": "Universal Sentence Encoder trains a transformer network and augments unsupervised learning with training on SNLI .", "ner": [["Universal Sentence Encoder", "Method"], ["transformer", "Method"], ["unsupervised learning", "Task"], ["SNLI", "Dataset"]], "rel": [["unsupervised learning", "Used-For", "transformer"], ["Universal Sentence Encoder", "Trained-With", "SNLI"]], "rel_plus": [["unsupervised learning:Task", "Used-For", "transformer:Method"], ["Universal Sentence Encoder:Method", "Trained-With", "SNLI:Dataset"]]}
{"doc_id": "201646309", "sentence": "Previous work ( Conneau et al. , 2 0 1 7 ; found that the SNLI datasets are suitable for training sentence embeddings . presented a method to train on conversations from Reddit using siamese DAN and siamese transformer networks , which yielded good results on the STS benchmark dataset .", "ner": [["SNLI", "Dataset"], ["sentence embeddings", "Task"], ["siamese DAN", "Method"], ["siamese transformer networks", "Method"], ["STS benchmark dataset", "Dataset"]], "rel": [["SNLI", "Benchmark-For", "sentence embeddings"], ["siamese DAN", "Evaluated-With", "STS benchmark dataset"], ["siamese transformer networks", "Evaluated-With", "STS benchmark dataset"]], "rel_plus": [["SNLI:Dataset", "Benchmark-For", "sentence embeddings:Task"], ["siamese DAN:Method", "Evaluated-With", "STS benchmark dataset:Dataset"], ["siamese transformer networks:Method", "Evaluated-With", "STS benchmark dataset:Dataset"]]}
{"doc_id": "201646309", "sentence": "In this publication , we use the pre - trained BERT and RoBERTa network and only fine - tune it to yield useful sentence embeddings .", "ner": [["BERT", "Method"], ["RoBERTa", "Method"], ["sentence embeddings", "Task"]], "rel": [["BERT", "Used-For", "sentence embeddings"], ["RoBERTa", "Used-For", "sentence embeddings"]], "rel_plus": [["BERT:Method", "Used-For", "sentence embeddings:Task"], ["RoBERTa:Method", "Used-For", "sentence embeddings:Task"]]}
{"doc_id": "201646309", "sentence": "This reduces significantly the needed training time : SBERT can be tuned in less than 2 0 minutes , while yielding better results than comparable sentence embedding methods .", "ner": [["SBERT", "Method"], ["sentence embedding", "Task"]], "rel": [["SBERT", "Used-For", "sentence embedding"]], "rel_plus": [["SBERT:Method", "Used-For", "sentence embedding:Task"]]}
{"doc_id": "201646309", "sentence": "SBERT adds a pooling operation to the output of BERT / RoBERTa to derive a fixed sized sentence embedding .", "ner": [["SBERT", "Method"], ["BERT", "Method"], ["RoBERTa", "Method"], ["sentence embedding", "Task"]], "rel": [["SBERT", "Used-For", "sentence embedding"]], "rel_plus": [["SBERT:Method", "Used-For", "sentence embedding:Task"]]}
{"doc_id": "201646309", "sentence": "In order to fine - tune BERT / RoBERTa , we create siamese and triplet networks ( Schroff et al. , 2 0 1 5 ) to update the weights such that the produced sentence embeddings are semantically meaningful and can be compared with cosine - similarity .", "ner": [["BERT", "Method"], ["RoBERTa", "Method"], ["sentence embeddings", "Task"], ["cosine - similarity", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "201646309", "sentence": "We concatenate the sentence embeddings u and v with the element - wise difference |u \u2212 v| and multiply it with the trainable weight W t \u2208 R 3n \u00d7 k : where n is the dimension of the sentence embeddings and k the number of labels .", "ner": [["sentence embeddings", "Task"], ["sentence embeddings", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "201646309", "sentence": "Given an anchor sentence a , a positive sentence p , and a negative sentence n , triplet loss tunes the network such that the distance between a and p is smaller than the distance between a and n. Mathematically , we minimize the following loss function : with s x the sentence embedding for a/n/p , || \u00b7 || a distance metric and margin .", "ner": [["triplet loss", "Method"], ["sentence embedding", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "201646309", "sentence": "We train SBERT on the combination of the SNLI ( Bowman et al. , 2 0 1 5 ) and the Multi - Genre NLI   We evaluate the performance of SBERT for common Semantic Textual Similarity ( STS ) tasks .", "ner": [["SBERT", "Method"], ["SNLI", "Dataset"], ["Multi - Genre NLI", "Dataset"], ["SBERT", "Method"], ["Semantic Textual Similarity", "Task"], ["STS", "Task"]], "rel": [["SBERT", "Trained-With", "SNLI"], ["SBERT", "Trained-With", "Multi - Genre NLI"], ["STS", "Synonym-Of", "Semantic Textual Similarity"], ["SBERT", "Used-For", "Semantic Textual Similarity"]], "rel_plus": [["SBERT:Method", "Trained-With", "SNLI:Dataset"], ["SBERT:Method", "Trained-With", "Multi - Genre NLI:Dataset"], ["STS:Task", "Synonym-Of", "Semantic Textual Similarity:Task"], ["SBERT:Method", "Used-For", "Semantic Textual Similarity:Task"]]}
{"doc_id": "201646309", "sentence": "We evaluate the performance of SBERT for STS without using any STS specific training data .", "ner": [["SBERT", "Method"], ["STS", "Task"], ["STS", "Task"]], "rel": [["SBERT", "Used-For", "STS"]], "rel_plus": [["SBERT:Method", "Used-For", "STS:Task"]]}
{"doc_id": "201646309", "sentence": "We use the STS tasks 2 0 1 2 - 2 0 1 6 ( Agirre et al. , 2 0 1 2 ( Agirre et al. , , 2 0 1 3 ( Agirre et al. , , 2 0 1 4 ( Agirre et al. , , 2 0 1 5 ( Agirre et al. , , 2 0 1 6 , the STS benchmark ( Cer et al. , 2 0 1 7 ) , and the SICK - Relatedness dataset ( Marelli et al. , 2 0 1 4 ) .", "ner": [["STS tasks 2 0 1 2 - 2 0 1 6", "Dataset"], ["STS benchmark", "Dataset"], ["SICK - Relatedness", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "201646309", "sentence": "Using the described siamese network structure and fine - tuning mechanism substantially improves the correlation , outperforming both InferSent and Universal Sentence Encoder substantially .", "ner": [["siamese network", "Method"], ["InferSent", "Method"], ["Universal Sentence Encoder", "Method"]], "rel": [["siamese network", "Compare-With", "InferSent"], ["siamese network", "Compare-With", "Universal Sentence Encoder"]], "rel_plus": [["siamese network:Method", "Compare-With", "InferSent:Method"], ["siamese network:Method", "Compare-With", "Universal Sentence Encoder:Method"]]}
{"doc_id": "201646309", "sentence": "The only dataset where SBERT performs worse than Universal Sentence Encoder is SICK - R. Universal Sentence Encoder was trained on various datasets , including news , question - answer pages and discussion forums , which appears to be more suitable to the data of SICK - R. In contrast , SBERT was pre - trained only on Wikipedia ( via BERT ) and on NLI data .", "ner": [["SBERT", "Method"], ["Universal Sentence Encoder", "Method"], ["SICK - R.", "Dataset"], ["Universal Sentence Encoder", "Method"], ["SICK - R.", "Dataset"], ["SBERT", "Method"], ["Wikipedia", "Dataset"], ["BERT", "Method"], ["NLI data", "Dataset"]], "rel": [["SBERT", "Compare-With", "Universal Sentence Encoder"], ["SBERT", "Evaluated-With", "SICK - R."], ["Universal Sentence Encoder", "Evaluated-With", "SICK - R."], ["SBERT", "Trained-With", "Wikipedia"], ["SBERT", "Trained-With", "NLI data"]], "rel_plus": [["SBERT:Method", "Compare-With", "Universal Sentence Encoder:Method"], ["SBERT:Method", "Evaluated-With", "SICK - R.:Dataset"], ["Universal Sentence Encoder:Method", "Evaluated-With", "SICK - R.:Dataset"], ["SBERT:Method", "Trained-With", "Wikipedia:Dataset"], ["SBERT:Method", "Trained-With", "NLI data:Dataset"]]}
{"doc_id": "201646309", "sentence": "While RoBERTa was able to improve the performance for several supervised tasks , we only observe minor difference between SBERT and SRoBERTa for generating sentence embeddings .", "ner": [["RoBERTa", "Method"], ["SBERT", "Method"], ["SRoBERTa", "Method"], ["generating sentence embeddings", "Task"]], "rel": [["SBERT", "Compare-With", "SRoBERTa"], ["SRoBERTa", "Used-For", "generating sentence embeddings"], ["SBERT", "Used-For", "generating sentence embeddings"]], "rel_plus": [["SBERT:Method", "Compare-With", "SRoBERTa:Method"], ["SRoBERTa:Method", "Used-For", "generating sentence embeddings:Task"], ["SBERT:Method", "Used-For", "generating sentence embeddings:Task"]]}
{"doc_id": "201646309", "sentence": "The STS benchmark ( STSb ) ( Cer et al. , 2 0 1 7 ) provides is a popular dataset to evaluate supervised STS systems .", "ner": [["STS benchmark", "Dataset"], ["STSb", "Dataset"], ["STS", "Task"]], "rel": [["STSb", "Synonym-Of", "STS benchmark"], ["STS benchmark", "Benchmark-For", "STS"]], "rel_plus": [["STSb:Dataset", "Synonym-Of", "STS benchmark:Dataset"], ["STS benchmark:Dataset", "Benchmark-For", "STS:Task"]]}
{"doc_id": "201646309", "sentence": "SBERT was fine - tuned on the STSb dataset , SBERT - NLI was pretrained on the NLI datasets , then fine - tuned on the STSb dataset .", "ner": [["SBERT", "Method"], ["STSb", "Dataset"], ["SBERT - NLI", "Method"], ["NLI datasets", "Dataset"], ["STSb", "Dataset"]], "rel": [["SBERT", "Trained-With", "STSb"], ["SBERT - NLI", "Trained-With", "NLI datasets"], ["SBERT - NLI", "Trained-With", "STSb"]], "rel_plus": [["SBERT:Method", "Trained-With", "STSb:Dataset"], ["SBERT - NLI:Method", "Trained-With", "NLI datasets:Dataset"], ["SBERT - NLI:Method", "Trained-With", "STSb:Dataset"]]}
{"doc_id": "201646309", "sentence": "We experimented with two setups : Only training on STSb , and first training on NLI , then training on STSb .", "ner": [["STSb", "Dataset"], ["NLI", "Dataset"], ["STSb", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "201646309", "sentence": "We do not observe a significant difference between BERT and RoBERTa .", "ner": [["BERT", "Method"], ["RoBERTa", "Method"]], "rel": [["BERT", "Compare-With", "RoBERTa"]], "rel_plus": [["BERT:Method", "Compare-With", "RoBERTa:Method"]]}
{"doc_id": "201646309", "sentence": "We evaluate SBERT on the Argument Facet Similarity ( AFS ) corpus by Misra et al. ( 2 0 1 6 ) .", "ner": [["SBERT", "Method"], ["Argument Facet Similarity", "Dataset"], ["AFS", "Dataset"]], "rel": [["AFS", "Synonym-Of", "Argument Facet Similarity"], ["SBERT", "Evaluated-With", "Argument Facet Similarity"]], "rel_plus": [["AFS:Dataset", "Synonym-Of", "Argument Facet Similarity:Dataset"], ["SBERT:Method", "Evaluated-With", "Argument Facet Similarity:Dataset"]]}
{"doc_id": "201646309", "sentence": "The similarity notion in the AFS corpus is fairly different to the similarity notion in the STS datasets from SemEval .", "ner": [["AFS", "Dataset"], ["STS datasets from SemEval", "Dataset"]], "rel": [["AFS", "Compare-With", "STS datasets from SemEval"]], "rel_plus": [["AFS:Dataset", "Compare-With", "STS datasets from SemEval:Dataset"]]}
{"doc_id": "201646309", "sentence": "STS data is usually descriptive , while AFS data are argumentative excerpts from dialogs .", "ner": [["STS", "Dataset"], ["AFS", "Dataset"]], "rel": [["STS", "Compare-With", "AFS"]], "rel_plus": [["STS:Dataset", "Compare-With", "AFS:Dataset"]]}
{"doc_id": "201646309", "sentence": "We evaluate SBERT on this dataset in two scenarios : 1 ) As proposed by Misra et al. , we evaluate SBERT using 1 0 - fold cross - validation .", "ner": [["SBERT", "Method"], ["SBERT", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "201646309", "sentence": "Unsupervised methods like tf - idf , average GloVe embeddings or InferSent perform rather badly on this dataset with low scores .", "ner": [["tf - idf", "Method"], ["average GloVe embeddings", "Method"], ["InferSent", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "201646309", "sentence": "Training SBERT in the 1 0 - fold cross - validation setup gives a performance that is nearly on - par with BERT .", "ner": [["SBERT", "Method"], ["BERT", "Method"]], "rel": [["SBERT", "Compare-With", "BERT"]], "rel_plus": [["SBERT:Method", "Compare-With", "BERT:Method"]]}
{"doc_id": "201646309", "sentence": "BERT is able to use attention to compare directly both sentences ( e.g. word - by - word comparison ) , while SBERT must map individual sentences from an unseen topic to a vector space such that arguments with similar claims and reasons are close .", "ner": [["BERT", "Method"], ["SBERT", "Method"]], "rel": [["BERT", "Compare-With", "SBERT"]], "rel_plus": [["BERT:Method", "Compare-With", "SBERT:Method"]]}
{"doc_id": "201646309", "sentence": "Dor et al. finetuned a BiLSTM architecture with triplet loss to derive sentence embeddings for this dataset .", "ner": [["BiLSTM", "Method"], ["triplet loss", "Method"], ["sentence embeddings", "Task"]], "rel": [["triplet loss", "Part-Of", "BiLSTM"], ["BiLSTM", "Used-For", "sentence embeddings"]], "rel_plus": [["triplet loss:Method", "Part-Of", "BiLSTM:Method"], ["BiLSTM:Method", "Used-For", "sentence embeddings:Task"]]}
{"doc_id": "201646309", "sentence": "As the table shows , SBERT clearly outperforms the BiLSTM approach by Dor et al. SentEval ( Conneau and Kiela , 2 0 1 8 ) is a popular toolkit to evaluate the quality of sentence embeddings .", "ner": [["SBERT", "Method"], ["BiLSTM", "Method"], ["SentEval", "Dataset"], ["sentence embeddings", "Task"]], "rel": [["SBERT", "Compare-With", "BiLSTM"], ["SentEval", "Benchmark-For", "sentence embeddings"]], "rel_plus": [["SBERT:Method", "Compare-With", "BiLSTM:Method"], ["SentEval:Dataset", "Benchmark-For", "sentence embeddings:Task"]]}
{"doc_id": "201646309", "sentence": "Sentence embeddings are used as features for a logistic regression classifier .", "ner": [["Sentence embeddings", "Task"], ["logistic regression", "Method"]], "rel": [["Sentence embeddings", "Used-For", "logistic regression"]], "rel_plus": [["Sentence embeddings:Task", "Used-For", "logistic regression:Method"]]}
{"doc_id": "201646309", "sentence": "SBERT trained with triplet loss for one epoch .", "ner": [["SBERT", "Method"], ["triplet loss", "Method"]], "rel": [["triplet loss", "Part-Of", "SBERT"]], "rel_plus": [["triplet loss:Method", "Part-Of", "SBERT:Method"]]}
{"doc_id": "201646309", "sentence": "The purpose of SBERT sentence embeddings are not to be used for transfer learning for other tasks .", "ner": [["SBERT sentence embeddings", "Method"], ["transfer learning", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "201646309", "sentence": "Here , we think fine - tuning BERT as described by Devlin et al. ( 2 0 1 8) for new tasks is the more suitable method , as it updates all layers of the BERT network .", "ner": [["BERT", "Method"], ["BERT", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "201646309", "sentence": "However , SentEval can still give an impression on the quality of our sentence embeddings for various tasks .", "ner": [["SentEval", "Dataset"], ["sentence embeddings", "Task"]], "rel": [["SentEval", "Benchmark-For", "sentence embeddings"]], "rel_plus": [["SentEval:Dataset", "Benchmark-For", "sentence embeddings:Task"]]}
{"doc_id": "201646309", "sentence": "We compare the SBERT sentence embeddings to other sentence embeddings methods on the following seven SentEval transfer tasks : \u2022 MR : Sentiment prediction for movie reviews snippets on a five start scale ( Pang and Lee , 2 0 0 5 ) . \u2022 CR : Sentiment prediction of customer product reviews ( Hu and Liu , 2 0 0 4 ) . \u2022 SUBJ : Subjectivity prediction of sentences from movie reviews and plot summaries ( Pang and Lee , 2 0 0 4 ) . \u2022 MPQA : Phrase level opinion polarity classification from newswire ( Wiebe et al. , 2 0 0 5 ) . \u2022 SST : Stanford Sentiment Treebank with binary labels ( Socher et al. , 2 0 1 3 ) . \u2022 TREC : Fine grained question - type classification from TREC ( Li and Roth , 2 0 0 2 ) . \u2022 MRPC : Microsoft Research Paraphrase Corpus from parallel news sources ( Dolan et al. , 2 0 0 4 ) .", "ner": [["SBERT sentence embeddings", "Method"], ["sentence embeddings methods", "Method"], ["SentEval", "Dataset"], ["MR", "Dataset"], ["Sentiment prediction", "Task"], ["CR", "Dataset"], ["Sentiment prediction", "Task"], ["SUBJ", "Dataset"], ["Subjectivity prediction", "Task"], ["MPQA", "Dataset"], ["Phrase level opinion polarity classification", "Task"], ["SST", "Dataset"], ["Stanford Sentiment Treebank", "Dataset"], ["TREC", "Dataset"], ["question - type classification", "Task"], ["TREC", "Dataset"], ["MRPC", "Dataset"], ["Microsoft Research Paraphrase Corpus", "Dataset"]], "rel": [["SBERT sentence embeddings", "Compare-With", "sentence embeddings methods"], ["SBERT sentence embeddings", "Evaluated-With", "SentEval"], ["sentence embeddings methods", "Evaluated-With", "SentEval"]], "rel_plus": [["SBERT sentence embeddings:Method", "Compare-With", "sentence embeddings methods:Method"], ["SBERT sentence embeddings:Method", "Evaluated-With", "SentEval:Dataset"], ["sentence embeddings methods:Method", "Evaluated-With", "SentEval:Dataset"]]}
{"doc_id": "201646309", "sentence": "The average performance increases by about 2 percentage points compared to InferSent as well as the Universal Sentence Encoder .", "ner": [["InferSent", "Method"], ["Universal Sentence Encoder", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "201646309", "sentence": "Even though transfer learning is not the purpose of SBERT , it outperforms other state - of - the - art sentence embeddings methods on this task .", "ner": [["transfer learning", "Task"], ["SBERT", "Method"], ["sentence embeddings methods", "Method"]], "rel": [["SBERT", "Compare-With", "sentence embeddings methods"]], "rel_plus": [["SBERT:Method", "Compare-With", "sentence embeddings methods:Method"]]}
{"doc_id": "201646309", "sentence": "Table 5 : Evaluation of SBERT sentence embeddings using the SentEval toolkit .", "ner": [["SBERT sentence embeddings", "Method"], ["SentEval", "Dataset"]], "rel": [["SBERT sentence embeddings", "Evaluated-With", "SentEval"]], "rel_plus": [["SBERT sentence embeddings:Method", "Evaluated-With", "SentEval:Dataset"]]}
{"doc_id": "201646309", "sentence": "SentEval evaluates sentence embeddings on different sentence classification tasks by training a logistic regression classifier using the sentence embeddings as features .", "ner": [["SentEval", "Dataset"], ["sentence embeddings", "Task"], ["sentence classification", "Task"], ["logistic regression", "Method"], ["sentence embeddings", "Task"]], "rel": [["SentEval", "Benchmark-For", "sentence embeddings"], ["SentEval", "Benchmark-For", "sentence classification"]], "rel_plus": [["SentEval:Dataset", "Benchmark-For", "sentence embeddings:Task"], ["SentEval:Dataset", "Benchmark-For", "sentence classification:Task"]]}
{"doc_id": "201646309", "sentence": "It appears that the sentence embeddings from SBERT capture well sentiment information : We observe large improvements for all sentiment tasks ( MR , CR , and SST ) from SentEval in comparison to InferSent and Universal Sentence Encoder .", "ner": [["sentence embeddings", "Task"], ["SBERT", "Method"], ["MR", "Dataset"], ["CR", "Dataset"], ["SST", "Dataset"], ["SentEval", "Dataset"], ["InferSent", "Method"], ["Universal Sentence Encoder", "Method"]], "rel": [["SBERT", "Used-For", "sentence embeddings"], ["SBERT", "Evaluated-With", "SentEval"], ["InferSent", "Evaluated-With", "SentEval"], ["Universal Sentence Encoder", "Evaluated-With", "SentEval"], ["SBERT", "Compare-With", "InferSent"], ["SBERT", "Compare-With", "Universal Sentence Encoder"]], "rel_plus": [["SBERT:Method", "Used-For", "sentence embeddings:Task"], ["SBERT:Method", "Evaluated-With", "SentEval:Dataset"], ["InferSent:Method", "Evaluated-With", "SentEval:Dataset"], ["Universal Sentence Encoder:Method", "Evaluated-With", "SentEval:Dataset"], ["SBERT:Method", "Compare-With", "InferSent:Method"], ["SBERT:Method", "Compare-With", "Universal Sentence Encoder:Method"]]}
{"doc_id": "201646309", "sentence": "The only dataset where SBERT is significantly worse than Universal Sentence Encoder is the TREC dataset .", "ner": [["SBERT", "Method"], ["Universal Sentence Encoder", "Method"], ["TREC", "Dataset"]], "rel": [["SBERT", "Compare-With", "Universal Sentence Encoder"], ["SBERT", "Evaluated-With", "TREC"], ["Universal Sentence Encoder", "Evaluated-With", "TREC"]], "rel_plus": [["SBERT:Method", "Compare-With", "Universal Sentence Encoder:Method"], ["SBERT:Method", "Evaluated-With", "TREC:Dataset"], ["Universal Sentence Encoder:Method", "Evaluated-With", "TREC:Dataset"]]}
{"doc_id": "201646309", "sentence": "Universal Sentence Encoder was pre - trained on question - answering data , which appears to be beneficial for the question - type classification task of the TREC dataset .", "ner": [["Universal Sentence Encoder", "Method"], ["question - type classification", "Task"], ["TREC", "Dataset"]], "rel": [["TREC", "Benchmark-For", "question - type classification"], ["Universal Sentence Encoder", "Used-For", "question - type classification"], ["Universal Sentence Encoder", "Evaluated-With", "TREC"]], "rel_plus": [["TREC:Dataset", "Benchmark-For", "question - type classification:Task"], ["Universal Sentence Encoder:Method", "Used-For", "question - type classification:Task"], ["Universal Sentence Encoder:Method", "Evaluated-With", "TREC:Dataset"]]}
{"doc_id": "201646309", "sentence": "Average BERT embeddings or using the CLStoken output from a BERT network achieved bad results for various STS tasks ( Table 1 ) , worse than average GloVe embeddings .", "ner": [["Average BERT embeddings", "Method"], ["BERT", "Method"], ["STS", "Task"], ["average GloVe embeddings", "Method"]], "rel": [["Average BERT embeddings", "Used-For", "STS"], ["BERT", "Used-For", "STS"], ["average GloVe embeddings", "Used-For", "STS"], ["BERT", "Compare-With", "average GloVe embeddings"], ["Average BERT embeddings", "Compare-With", "average GloVe embeddings"]], "rel_plus": [["Average BERT embeddings:Method", "Used-For", "STS:Task"], ["BERT:Method", "Used-For", "STS:Task"], ["average GloVe embeddings:Method", "Used-For", "STS:Task"], ["BERT:Method", "Compare-With", "average GloVe embeddings:Method"], ["Average BERT embeddings:Method", "Compare-With", "average GloVe embeddings:Method"]]}
{"doc_id": "201646309", "sentence": "However , for SentEval , average BERT embeddings and the BERT CLS - token output achieves decent results ( Table 5 ) , outperforming average GloVe embeddings .", "ner": [["SentEval", "Dataset"], ["average BERT embeddings", "Method"], ["BERT CLS - token output", "Method"], ["average GloVe embeddings", "Method"]], "rel": [["average BERT embeddings", "Evaluated-With", "SentEval"], ["average GloVe embeddings", "Evaluated-With", "SentEval"], ["BERT CLS - token output", "Evaluated-With", "SentEval"], ["average BERT embeddings", "Compare-With", "average GloVe embeddings"], ["BERT CLS - token output", "Compare-With", "average GloVe embeddings"]], "rel_plus": [["average BERT embeddings:Method", "Evaluated-With", "SentEval:Dataset"], ["average GloVe embeddings:Method", "Evaluated-With", "SentEval:Dataset"], ["BERT CLS - token output:Method", "Evaluated-With", "SentEval:Dataset"], ["average BERT embeddings:Method", "Compare-With", "average GloVe embeddings:Method"], ["BERT CLS - token output:Method", "Compare-With", "average GloVe embeddings:Method"]]}
{"doc_id": "201646309", "sentence": "For the STS tasks , we used cosine - similarity to estimate the similarities between sentence embeddings .", "ner": [["STS", "Task"], ["cosine - similarity", "Method"], ["sentence embeddings", "Task"]], "rel": [["cosine - similarity", "Used-For", "sentence embeddings"]], "rel_plus": [["cosine - similarity:Method", "Used-For", "sentence embeddings:Task"]]}
{"doc_id": "201646309", "sentence": "In contrast , SentEval fits a logistic regression classifier to the sentence embeddings .", "ner": [["SentEval", "Dataset"], ["logistic regression", "Method"], ["sentence embeddings", "Task"]], "rel": [["logistic regression", "Trained-With", "SentEval"], ["logistic regression", "Used-For", "sentence embeddings"], ["SentEval", "Benchmark-For", "sentence embeddings"]], "rel_plus": [["logistic regression:Method", "Trained-With", "SentEval:Dataset"], ["logistic regression:Method", "Used-For", "sentence embeddings:Task"], ["SentEval:Dataset", "Benchmark-For", "sentence embeddings:Task"]]}
{"doc_id": "201646309", "sentence": "We conclude that average BERT embeddings / CLS - token output from BERT return sentence embeddings that are infeasible to be used with cosinesimilarity or with Manhatten / Euclidean distance .", "ner": [["average BERT embeddings", "Method"], ["CLS - token output from BERT", "Method"], ["sentence embeddings", "Task"], ["cosinesimilarity", "Method"], ["Manhatten", "Method"], ["Euclidean distance", "Method"]], "rel": [["CLS - token output from BERT", "Used-For", "sentence embeddings"], ["average BERT embeddings", "Used-For", "sentence embeddings"]], "rel_plus": [["CLS - token output from BERT:Method", "Used-For", "sentence embeddings:Task"], ["average BERT embeddings:Method", "Used-For", "sentence embeddings:Task"]]}
{"doc_id": "201646309", "sentence": "For transfer learning , they yield slightly worse results than InferSent or Universal Sentence Encoder .", "ner": [["transfer learning", "Task"], ["InferSent", "Method"], ["Universal Sentence Encoder", "Method"]], "rel": [["InferSent", "Used-For", "transfer learning"], ["Universal Sentence Encoder", "Used-For", "transfer learning"]], "rel_plus": [["InferSent:Method", "Used-For", "transfer learning:Task"], ["Universal Sentence Encoder:Method", "Used-For", "transfer learning:Task"]]}
{"doc_id": "201646309", "sentence": "However , using the described fine - tuning setup with a siamese network structure on NLI datasets yields sentence embeddings that achieve a new state - of - the - art for the SentEval toolkit .", "ner": [["siamese network", "Method"], ["NLI datasets", "Dataset"], ["sentence embeddings", "Task"], ["SentEval", "Dataset"]], "rel": [["siamese network", "Trained-With", "NLI datasets"], ["siamese network", "Used-For", "sentence embeddings"], ["siamese network", "Evaluated-With", "SentEval"]], "rel_plus": [["siamese network:Method", "Trained-With", "NLI datasets:Dataset"], ["siamese network:Method", "Used-For", "sentence embeddings:Task"], ["siamese network:Method", "Evaluated-With", "SentEval:Dataset"]]}
{"doc_id": "201646309", "sentence": "We have demonstrated strong empirical results for the quality of SBERT sentence embeddings .", "ner": [["SBERT sentence embeddings", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "201646309", "sentence": "The objective function ( classification vs. regression ) depends on the annotated dataset .", "ner": [], "rel": [], "rel_plus": []}
{"doc_id": "201646309", "sentence": "For the classification objective function , we train SBERTbase on the SNLI and the Multi - NLI dataset .", "ner": [["classification objective function", "Method"], ["SBERTbase", "Method"], ["SNLI", "Dataset"], ["Multi - NLI", "Dataset"]], "rel": [["classification objective function", "Part-Of", "SBERTbase"], ["SBERTbase", "Trained-With", "SNLI"], ["SBERTbase", "Trained-With", "Multi - NLI"]], "rel_plus": [["classification objective function:Method", "Part-Of", "SBERTbase:Method"], ["SBERTbase:Method", "Trained-With", "SNLI:Dataset"], ["SBERTbase:Method", "Trained-With", "Multi - NLI:Dataset"]]}
{"doc_id": "201646309", "sentence": "For the regression objective function , we train on the training set of the STS benchmark dataset .", "ner": [["regression objective function", "Method"], ["STS benchmark", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "201646309", "sentence": "Results are shown in Table 6 . ( u , v ) 6 6 . 0 4 Table 6 : SBERT trained on NLI data with the classification objective function , on the STS benchmark ( STSb ) with the regression objective function .", "ner": [["SBERT", "Method"], ["NLI data", "Dataset"], ["classification objective function", "Method"], ["STS benchmark", "Dataset"], ["STSb", "Dataset"], ["regression objective function", "Method"]], "rel": [["classification objective function", "Part-Of", "SBERT"], ["regression objective function", "Part-Of", "SBERT"], ["SBERT", "Trained-With", "NLI data"], ["STSb", "Synonym-Of", "STS benchmark"], ["SBERT", "Trained-With", "STS benchmark"]], "rel_plus": [["classification objective function:Method", "Part-Of", "SBERT:Method"], ["regression objective function:Method", "Part-Of", "SBERT:Method"], ["SBERT:Method", "Trained-With", "NLI data:Dataset"], ["STSb:Dataset", "Synonym-Of", "STS benchmark:Dataset"], ["SBERT:Method", "Trained-With", "STS benchmark:Dataset"]]}
{"doc_id": "201646309", "sentence": "When trained with the classification objective function on NLI data , the pooling strategy has a rather minor impact .", "ner": [["classification objective function", "Method"], ["NLI data", "Dataset"], ["pooling strategy", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "201646309", "sentence": "InferSent ( Conneau et al. , 2 0 1 7 ) and Universal Sentence Encoder ( Cer et al. , 2 0 1 8) both use ( u , v , |u \u2212 v| , u * v ) as input for a softmax classifier .", "ner": [["InferSent", "Method"], ["Universal Sentence Encoder", "Method"], ["softmax", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "201646309", "sentence": "At inference , when predicting similarities for the STS benchmark dataset , only the sentence embeddings u and v are used in combination with cosine - similarity .", "ner": [["STS benchmark", "Dataset"], ["sentence embeddings", "Task"], ["cosine - similarity", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "201646309", "sentence": "This is in contrast to ( Conneau et al. , 2 0 1 7 ) , who found it beneficial for the BiLSTM - layer of InferSent to use MAX instead of MEAN pooling .", "ner": [["BiLSTM - layer", "Method"], ["InferSent", "Method"], ["MAX", "Method"], ["MEAN pooling", "Method"]], "rel": [["BiLSTM - layer", "Part-Of", "InferSent"], ["MAX", "Part-Of", "InferSent"]], "rel_plus": [["BiLSTM - layer:Method", "Part-Of", "InferSent:Method"], ["MAX:Method", "Part-Of", "InferSent:Method"]]}
{"doc_id": "201646309", "sentence": "In this section , we compare SBERT to average GloVe embeddings , InferSent ( Conneau et al. , 2 0 1 7 ) , and Universal Sentence Encoder .", "ner": [["SBERT", "Method"], ["average GloVe embeddings", "Method"], ["InferSent", "Method"], ["Universal Sentence Encoder", "Method"]], "rel": [["SBERT", "Compare-With", "average GloVe embeddings"], ["SBERT", "Compare-With", "InferSent"], ["SBERT", "Compare-With", "Universal Sentence Encoder"]], "rel_plus": [["SBERT:Method", "Compare-With", "average GloVe embeddings:Method"], ["SBERT:Method", "Compare-With", "InferSent:Method"], ["SBERT:Method", "Compare-With", "Universal Sentence Encoder:Method"]]}
{"doc_id": "201646309", "sentence": "On CPU , InferSent is about 6 5 % faster than SBERT .", "ner": [["InferSent", "Method"], ["SBERT", "Method"]], "rel": [["InferSent", "Compare-With", "SBERT"]], "rel_plus": [["InferSent:Method", "Compare-With", "SBERT:Method"]]}
{"doc_id": "201646309", "sentence": "InferSent uses a single Bi - LSTM layer , while BERT uses 1 2 stacked transformer layers .", "ner": [["InferSent", "Method"], ["Bi - LSTM layer", "Method"], ["BERT", "Method"], ["transformer layers", "Method"]], "rel": [["Bi - LSTM layer", "Part-Of", "InferSent"], ["transformer layers", "Part-Of", "BERT"], ["InferSent", "Compare-With", "BERT"]], "rel_plus": [["Bi - LSTM layer:Method", "Part-Of", "InferSent:Method"], ["transformer layers:Method", "Part-Of", "BERT:Method"], ["InferSent:Method", "Compare-With", "BERT:Method"]]}
{"doc_id": "201646309", "sentence": "There , SBERT with smart batching is about 9% faster than InferSent and about 5 5 % faster than Universal Sentence Encoder .", "ner": [["SBERT", "Method"], ["InferSent", "Method"], ["Universal Sentence Encoder", "Method"]], "rel": [["SBERT", "Compare-With", "InferSent"], ["SBERT", "Compare-With", "Universal Sentence Encoder"]], "rel_plus": [["SBERT:Method", "Compare-With", "InferSent:Method"], ["SBERT:Method", "Compare-With", "Universal Sentence Encoder:Method"]]}
{"doc_id": "201646309", "sentence": "Average GloVe embeddings is obviously by a large margin the fastest method to compute sentence embeddings .", "ner": [["Average GloVe embeddings", "Method"], ["sentence embeddings", "Task"]], "rel": [["Average GloVe embeddings", "Used-For", "sentence embeddings"]], "rel_plus": [["Average GloVe embeddings:Method", "Used-For", "sentence embeddings:Task"]]}
{"doc_id": "201646309", "sentence": "The performance for seven STS tasks was below the performance of average GloVe embeddings .", "ner": [["STS", "Task"], ["average GloVe embeddings", "Method"]], "rel": [["average GloVe embeddings", "Used-For", "STS"]], "rel_plus": [["average GloVe embeddings:Method", "Used-For", "STS:Task"]]}
{"doc_id": "201646309", "sentence": "To overcome this shortcoming , we presented Sentence - BERT ( SBERT ) .", "ner": [["Sentence - BERT", "Method"], ["SBERT", "Method"]], "rel": [["SBERT", "Synonym-Of", "Sentence - BERT"]], "rel_plus": [["SBERT:Method", "Synonym-Of", "Sentence - BERT:Method"]]}
{"doc_id": "201646309", "sentence": "SBERT fine - tunes BERT in a siamese / triplet network architecture .", "ner": [["SBERT", "Method"], ["BERT", "Method"]], "rel": [["SBERT", "SubClass-Of", "BERT"]], "rel_plus": [["SBERT:Method", "SubClass-Of", "BERT:Method"]]}
{"doc_id": "201646309", "sentence": "Replacing BERT with RoBERTa did not yield a significant improvement in our experiments .", "ner": [["BERT", "Method"], ["RoBERTa", "Method"]], "rel": [["BERT", "Compare-With", "RoBERTa"]], "rel_plus": [["BERT:Method", "Compare-With", "RoBERTa:Method"]]}
{"doc_id": "201646309", "sentence": "On a GPU , it is about 9% faster than InferSent and about 5 5 % faster than Universal Sentence Encoder .", "ner": [["InferSent", "Method"], ["Universal Sentence Encoder", "Method"]], "rel": [["InferSent", "Compare-With", "Universal Sentence Encoder"]], "rel_plus": [["InferSent:Method", "Compare-With", "Universal Sentence Encoder:Method"]]}
{"doc_id": "201646309", "sentence": "SBERT can be used for tasks which are computationally not feasible to be modeled with BERT .", "ner": [["SBERT", "Method"], ["BERT", "Method"]], "rel": [["SBERT", "Compare-With", "BERT"]], "rel_plus": [["SBERT:Method", "Compare-With", "BERT:Method"]]}
{"doc_id": "198147921", "sentence": "For instance , robots , autonomous vehicles , and surveillance and security systems rely on accurate detection of 3D objects to enable efficient object recognition , grasping , manipulation , obstacle avoidance , scene understanding , and accurate navigation .", "ner": [["detection of 3D objects", "Task"], ["object recognition", "Task"], ["grasping", "Task"], ["manipulation", "Task"], ["obstacle avoidance", "Task"], ["scene understanding", "Task"], ["accurate navigation", "Task"]], "rel": [["detection of 3D objects", "Used-For", "object recognition"], ["detection of 3D objects", "Used-For", "grasping"], ["detection of 3D objects", "Used-For", "manipulation"], ["detection of 3D objects", "Used-For", "obstacle avoidance"], ["detection of 3D objects", "Used-For", "scene understanding"], ["detection of 3D objects", "Used-For", "accurate navigation"]], "rel_plus": [["detection of 3D objects:Task", "Used-For", "object recognition:Task"], ["detection of 3D objects:Task", "Used-For", "grasping:Task"], ["detection of 3D objects:Task", "Used-For", "manipulation:Task"], ["detection of 3D objects:Task", "Used-For", "obstacle avoidance:Task"], ["detection of 3D objects:Task", "Used-For", "scene understanding:Task"], ["detection of 3D objects:Task", "Used-For", "accurate navigation:Task"]]}
{"doc_id": "198147921", "sentence": "Many applications , e.g. autonomous driving , require realtime object detection .", "ner": [["realtime object detection", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "198147921", "sentence": "This is usually sufficient for applications such as object recognition and autonomous navigation .", "ner": [["object recognition", "Task"], ["autonomous navigation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "198147921", "sentence": "For instance , the region proposal module can use traditional techniques based on hand - crafted features , while the recognition and localization module can use deep learning techniques .", "ner": [["region proposal", "Task"], ["recognition", "Task"], ["localization", "Task"], ["deep learning", "Method"]], "rel": [["region proposal", "Compare-With", "recognition"], ["deep learning", "Used-For", "recognition"], ["region proposal", "Compare-With", "localization"], ["deep learning", "Used-For", "localization"]], "rel_plus": [["region proposal:Task", "Compare-With", "recognition:Task"], ["deep learning:Method", "Used-For", "recognition:Task"], ["region proposal:Task", "Compare-With", "localization:Task"], ["deep learning:Method", "Used-For", "localization:Task"]]}
{"doc_id": "198147921", "sentence": "These descriptors can be used to refine the candidate region selection , either by using some supervised recognition techniques , e.g. Support Vector Machines [ 8 0 ] , Adaboost [ 1 6 ] , and hierarchical cascaded forests [ 6 ] , or by using unsupervised procedures .", "ner": [["supervised recognition techniques", "Method"], ["Support Vector Machines", "Method"], ["Adaboost", "Method"], ["hierarchical cascaded forests", "Method"], ["unsupervised procedures", "Method"]], "rel": [["Support Vector Machines", "SubClass-Of", "supervised recognition techniques"], ["Adaboost", "SubClass-Of", "supervised recognition techniques"], ["hierarchical cascaded forests", "SubClass-Of", "supervised recognition techniques"]], "rel_plus": [["Support Vector Machines:Method", "SubClass-Of", "supervised recognition techniques:Method"], ["Adaboost:Method", "SubClass-Of", "supervised recognition techniques:Method"], ["hierarchical cascaded forests:Method", "SubClass-Of", "supervised recognition techniques:Method"]]}
{"doc_id": "198147921", "sentence": "Examples include colour statistics , Histogram of Oriented Gradients ( HOG ) descriptor [ 1 3 ] , Scale - Invariant Feature Transform ( SIFT ) [ 1 4 ] , the Chamfer distance [ 7 ] , and Local Binary Patterns ( LBPs ) [ 3 1 ] .", "ner": [["Histogram of Oriented Gradients", "Method"], ["HOG", "Method"], ["Scale - Invariant Feature Transform", "Method"], ["SIFT", "Method"], ["Chamfer distance", "Method"], ["Local Binary Patterns", "Method"], ["LBPs", "Method"]], "rel": [["HOG", "Synonym-Of", "Histogram of Oriented Gradients"], ["SIFT", "Synonym-Of", "Scale - Invariant Feature Transform"], ["LBPs", "Synonym-Of", "Local Binary Patterns"]], "rel_plus": [["HOG:Method", "Synonym-Of", "Histogram of Oriented Gradients:Method"], ["SIFT:Method", "Synonym-Of", "Scale - Invariant Feature Transform:Method"], ["LBPs:Method", "Synonym-Of", "Local Binary Patterns:Method"]]}
{"doc_id": "198147921", "sentence": "To compute 3D normals , one can pick n nearest neighbors for each point , and estimate the surface normal at that point using principal component analysis ( PCA ) .", "ner": [["principal component analysis", "Method"], ["PCA", "Method"]], "rel": [["PCA", "Synonym-Of", "principal component analysis"]], "rel_plus": [["PCA:Method", "Synonym-Of", "principal component analysis:Method"]]}
{"doc_id": "198147921", "sentence": "This can include the first and second order statistics as well as the histogram of depth . \u2022 Truncated Signed Distance Function ( TSDF ) [ 5 9 ] .", "ner": [["Truncated Signed Distance Function", "Method"], ["TSDF", "Method"]], "rel": [["TSDF", "Synonym-Of", "Truncated Signed Distance Function"]], "rel_plus": [["TSDF:Method", "Synonym-Of", "Truncated Signed Distance Function:Method"]]}
{"doc_id": "198147921", "sentence": "This descriptor measures the saliency of a superpixel by considering its depth contrast with respect to all other superpixels . \u2022 Local Background Enclosure ( LBE ) descriptor [ 2 2 ] .", "ner": [["Local Background Enclosure", "Method"], ["LBE", "Method"]], "rel": [["LBE", "Synonym-Of", "Local Background Enclosure"]], "rel_plus": [["LBE:Method", "Synonym-Of", "Local Background Enclosure:Method"]]}
{"doc_id": "198147921", "sentence": "Feng et al. [ 2 2 ] found that LBE features outperform depth - based features such as anisotropic Center - Surround Difference ( ACSD ) [ 3 6 ] , multi - scale depth contrast ( LMH - D ) [ 6 0 ] , and global depth contrast ( GP - D ) [ 6 7 ] , when evaluated on the RGBD 1 0 0 0 [ 6 0 ] and NJUDS 2 0 0 0 [ 3 6 ] RGB - D benchmarks . \u2022 Cloud of Oriented Gradients ( COG ) descriptor [ 7 0 ] .", "ner": [["LBE", "Method"], ["anisotropic Center - Surround Difference", "Method"], ["ACSD", "Method"], ["multi - scale depth contrast", "Method"], ["LMH - D", "Method"], ["global depth contrast", "Method"], ["GP - D", "Method"], ["RGBD 1 0 0 0", "Dataset"], ["NJUDS 2 0 0 0", "Dataset"], ["RGB - D", "Dataset"], ["Cloud of Oriented Gradients", "Method"], ["COG", "Method"]], "rel": [["ACSD", "Synonym-Of", "anisotropic Center - Surround Difference"], ["LBE", "Compare-With", "anisotropic Center - Surround Difference"], ["LMH - D", "Synonym-Of", "multi - scale depth contrast"], ["LBE", "Compare-With", "multi - scale depth contrast"], ["GP - D", "Synonym-Of", "global depth contrast"], ["LBE", "Compare-With", "global depth contrast"], ["LBE", "Evaluated-With", "RGBD 1 0 0 0"], ["anisotropic Center - Surround Difference", "Evaluated-With", "RGBD 1 0 0 0"], ["multi - scale depth contrast", "Evaluated-With", "RGBD 1 0 0 0"], ["global depth contrast", "Evaluated-With", "RGBD 1 0 0 0"], ["LBE", "Evaluated-With", "NJUDS 2 0 0 0"], ["anisotropic Center - Surround Difference", "Evaluated-With", "NJUDS 2 0 0 0"], ["multi - scale depth contrast", "Evaluated-With", "NJUDS 2 0 0 0"], ["global depth contrast", "Evaluated-With", "NJUDS 2 0 0 0"], ["LBE", "Evaluated-With", "RGB - D"], ["anisotropic Center - Surround Difference", "Evaluated-With", "RGB - D"], ["multi - scale depth contrast", "Evaluated-With", "RGB - D"], ["global depth contrast", "Evaluated-With", "RGB - D"], ["COG", "Synonym-Of", "Cloud of Oriented Gradients"]], "rel_plus": [["ACSD:Method", "Synonym-Of", "anisotropic Center - Surround Difference:Method"], ["LBE:Method", "Compare-With", "anisotropic Center - Surround Difference:Method"], ["LMH - D:Method", "Synonym-Of", "multi - scale depth contrast:Method"], ["LBE:Method", "Compare-With", "multi - scale depth contrast:Method"], ["GP - D:Method", "Synonym-Of", "global depth contrast:Method"], ["LBE:Method", "Compare-With", "global depth contrast:Method"], ["LBE:Method", "Evaluated-With", "RGBD 1 0 0 0:Dataset"], ["anisotropic Center - Surround Difference:Method", "Evaluated-With", "RGBD 1 0 0 0:Dataset"], ["multi - scale depth contrast:Method", "Evaluated-With", "RGBD 1 0 0 0:Dataset"], ["global depth contrast:Method", "Evaluated-With", "RGBD 1 0 0 0:Dataset"], ["LBE:Method", "Evaluated-With", "NJUDS 2 0 0 0:Dataset"], ["anisotropic Center - Surround Difference:Method", "Evaluated-With", "NJUDS 2 0 0 0:Dataset"], ["multi - scale depth contrast:Method", "Evaluated-With", "NJUDS 2 0 0 0:Dataset"], ["global depth contrast:Method", "Evaluated-With", "NJUDS 2 0 0 0:Dataset"], ["LBE:Method", "Evaluated-With", "RGB - D:Dataset"], ["anisotropic Center - Surround Difference:Method", "Evaluated-With", "RGB - D:Dataset"], ["multi - scale depth contrast:Method", "Evaluated-With", "RGB - D:Dataset"], ["global depth contrast:Method", "Evaluated-With", "RGB - D:Dataset"], ["COG:Method", "Synonym-Of", "Cloud of Oriented Gradients:Method"]]}
{"doc_id": "198147921", "sentence": "It extends the HOG descriptor , which was originally designed for 2D images [ 1 0 , 2 4 ] , to 3D data . \u2022 Histogram of Control Points ( HOCP ) descriptor [ 7 3 , 7 4 ] .", "ner": [["HOG", "Method"], ["Histogram of Control Points", "Method"], ["HOCP", "Method"]], "rel": [["HOCP", "Synonym-Of", "Histogram of Control Points"]], "rel_plus": [["HOCP:Method", "Synonym-Of", "Histogram of Control Points:Method"]]}
{"doc_id": "198147921", "sentence": "For the superpixel pairwise term of Equation ( 3. 3 ) , Khan et al. [ 3 9 ] defined a contrast - sensitive Potts model on spatially neighboring superpixels , which encouraged the smoothness of cluttered and non - cluttered regions .", "ner": [], "rel": [], "rel_plus": []}
{"doc_id": "198147921", "sentence": "It consists of two terms : a superpixel membership potential , and a superpixel - cuboid occlusion potential .", "ner": [], "rel": [], "rel_plus": []}
{"doc_id": "198147921", "sentence": "The latter ensures that a cuboid should not appear in front of a superpixel which is classified as clutter , i.e. a detected cuboid can not completely occlude a superpixel on the 2D plane which takes a clutter label .", "ner": [], "rel": [], "rel_plus": []}
{"doc_id": "198147921", "sentence": "Khan et al. [ 3 9 ] , on the other hand , transformed the minimization problem into a Mixed Integer Linear Program ( MILP ) with linear constraints , which can be solved using the branch - and - bound method .", "ner": [["Mixed Integer Linear Program", "Method"], ["MILP", "Method"]], "rel": [["MILP", "Synonym-Of", "Mixed Integer Linear Program"]], "rel_plus": [["MILP:Method", "Synonym-Of", "Mixed Integer Linear Program:Method"]]}
{"doc_id": "198147921", "sentence": "They generally operate following the same pipeline as the traditional techniques ( see Section 3. 3 ) , i.e. region proposals extraction , object recognition , and 3D bounding box location and pose estimation .", "ner": [["region proposals extraction", "Task"], ["object recognition", "Task"], ["3D bounding box location", "Task"], ["pose estimation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "198147921", "sentence": "Note that these techniques can be combined with traditional techniques ; e.g. one can use traditional techniques for region proposals and deep learning networks for object recognition , bounding box location and pose refinement .", "ner": [["deep learning networks", "Method"], ["object recognition", "Task"], ["bounding box location", "Task"], ["pose refinement", "Task"]], "rel": [["deep learning networks", "Used-For", "object recognition"], ["deep learning networks", "Used-For", "bounding box location"], ["deep learning networks", "Used-For", "pose refinement"]], "rel_plus": [["deep learning networks:Method", "Used-For", "object recognition:Task"], ["deep learning networks:Method", "Used-For", "bounding box location:Task"], ["deep learning networks:Method", "Used-For", "pose refinement:Task"]]}
{"doc_id": "198147921", "sentence": "Region Proposal Networks ( RPNs ) are central to this task since they reduce the search space considered by the remainder of the object detection pipeline .", "ner": [["Region Proposal Networks", "Method"], ["RPNs", "Method"], ["object detection", "Task"]], "rel": [["RPNs", "Synonym-Of", "Region Proposal Networks"]], "rel_plus": [["RPNs:Method", "Synonym-Of", "Region Proposal Networks:Method"]]}
{"doc_id": "198147921", "sentence": "In fact , the success of the recent object detection pipelines can be largely attributed to the automatic feature learning aspect of convolutional neural networks .", "ner": [["object detection", "Task"], ["convolutional neural networks", "Method"]], "rel": [["convolutional neural networks", "Used-For", "object detection"]], "rel_plus": [["convolutional neural networks:Method", "Used-For", "object detection:Task"]]}
{"doc_id": "198147921", "sentence": "For instance , Qi et al. [ 6 2 ] used the Feature Pyramid Networks ( FPN ) [ 5 2 ] , which operate on RGB images , to detect region proposals .", "ner": [["Feature Pyramid Networks", "Method"], ["FPN", "Method"]], "rel": [["FPN", "Synonym-Of", "Feature Pyramid Networks"]], "rel_plus": [["FPN:Method", "Synonym-Of", "Feature Pyramid Networks:Method"]]}
{"doc_id": "198147921", "sentence": "Lahoud and Ghanem [ 4 5 ] , on the other hand , use the 2D Faster R - CNN [ 6 9 ] and VGG - 1 6 [ 7 7 ] , pre - trained on the 2D ImageNet database [ 7 2 ] , to position 2D bounding boxes around possible objects with high accuracy and efficiency .", "ner": [["2D Faster R - CNN", "Method"], ["VGG - 1 6", "Method"], ["2D ImageNet database", "Dataset"]], "rel": [["2D Faster R - CNN", "Trained-With", "2D ImageNet database"], ["VGG - 1 6", "Trained-With", "2D ImageNet database"]], "rel_plus": [["2D Faster R - CNN:Method", "Trained-With", "2D ImageNet database:Dataset"], ["VGG - 1 6:Method", "Trained-With", "2D ImageNet database:Dataset"]]}
{"doc_id": "198147921", "sentence": "They then generalize the multiscale combinatorial grouping ( MCG ) algorithm [ 5 , 6 1 ] to RGB - D images for region proposal and ranking .", "ner": [["multiscale combinatorial grouping", "Method"], ["MCG", "Method"], ["region proposal", "Task"], ["ranking", "Task"]], "rel": [["MCG", "Synonym-Of", "multiscale combinatorial grouping"], ["multiscale combinatorial grouping", "Used-For", "region proposal"], ["multiscale combinatorial grouping", "Used-For", "ranking"]], "rel_plus": [["MCG:Method", "Synonym-Of", "multiscale combinatorial grouping:Method"], ["multiscale combinatorial grouping:Method", "Used-For", "region proposal:Task"], ["multiscale combinatorial grouping:Method", "Used-For", "ranking:Task"]]}
{"doc_id": "198147921", "sentence": "Note that both the hand - crafted features as well as the region recognition and ranking algorithms can be replaced with deep learning techniques , as in [ 6 2 ] .", "ner": [["hand - crafted features", "Method"], ["region recognition", "Task"], ["ranking", "Task"], ["deep learning techniques", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "198147921", "sentence": "Finally , a network that is similar to the region proposal network of [ 6 8 ] was used to generate region proposals from the bird 's eye view map .", "ner": [["region proposal network", "Method"], ["region proposals", "Task"]], "rel": [["region proposal network", "Used-For", "region proposals"]], "rel_plus": [["region proposal network:Method", "Used-For", "region proposals:Task"]]}
{"doc_id": "198147921", "sentence": "The 3D scene is then processed with a fully 3D convolutional network , called a 3D Amodal Region Proposal Network , which generates region proposals in the form of 3D bounding boxes at two different scales .", "ner": [["fully 3D convolutional network", "Method"], ["3D Amodal Region Proposal Network", "Method"], ["region proposals", "Task"]], "rel": [["3D Amodal Region Proposal Network", "SubClass-Of", "fully 3D convolutional network"], ["3D Amodal Region Proposal Network", "Used-For", "region proposals"]], "rel_plus": [["3D Amodal Region Proposal Network:Method", "SubClass-Of", "fully 3D convolutional network:Method"], ["3D Amodal Region Proposal Network:Method", "Used-For", "region proposals:Task"]]}
{"doc_id": "198147921", "sentence": "Multi - scale RPNs allow the detection of objects of different sizes .", "ner": [["Multi - scale RPNs", "Method"], ["detection", "Task"]], "rel": [["Multi - scale RPNs", "Used-For", "detection"]], "rel_plus": [["Multi - scale RPNs:Method", "Used-For", "detection:Task"]]}
{"doc_id": "198147921", "sentence": "To leverage this sparsity , Engelcke et al. [ 1 8 ] extended the approach of Song et al. [ 8 0 ] by replacing the SVM ensemble with a 3D CNN , which operates on voxelized 3D grids .", "ner": [["SVM", "Method"], ["3D CNN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "198147921", "sentence": "Once region proposals have been generated , the next step is to classify these regions into whether they correspond to the objects we want to detect or not , and subsequently refine the detection by estimating the accurate location , extent , and pose ( position and orientation ) of each object 's bounding box .", "ner": [["region proposals", "Task"], ["detection", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "198147921", "sentence": "The former is a classification problem , which has been well solved using Object Recognition Networks ( ORNs ) [ 6 , 1 7 , 7 5 ] .", "ner": [["classification", "Task"], ["Object Recognition Networks", "Method"], ["ORNs", "Method"]], "rel": [["Object Recognition Networks", "Used-For", "classification"], ["ORNs", "Synonym-Of", "Object Recognition Networks"]], "rel_plus": [["Object Recognition Networks:Method", "Used-For", "classification:Task"], ["ORNs:Method", "Synonym-Of", "Object Recognition Networks:Method"]]}
{"doc_id": "198147921", "sentence": "The point cloud is then fed into a 3D convolutional network , termed VoxNet , which outputs the class label of the region .", "ner": [["3D convolutional network", "Method"], ["VoxNet", "Method"]], "rel": [["VoxNet", "SubClass-Of", "3D convolutional network"]], "rel_plus": [["VoxNet:Method", "SubClass-Of", "3D convolutional network:Method"]]}
{"doc_id": "198147921", "sentence": "For each 3D proposal , the 3D volume from depth is fed to a 3D ConvNet , and the 2D colour patch ( the 2D projection of the 3D proposal ) is fed to a 2D ConvNet ( based on VGG and pre - trained on ImageNet ) .", "ner": [["3D ConvNet", "Method"], ["2D ConvNet", "Method"], ["VGG", "Method"], ["ImageNet", "Dataset"]], "rel": [["VGG", "Part-Of", "2D ConvNet"], ["VGG", "Trained-With", "ImageNet"]], "rel_plus": [["VGG:Method", "Part-Of", "2D ConvNet:Method"], ["VGG:Method", "Trained-With", "ImageNet:Dataset"]]}
{"doc_id": "198147921", "sentence": "Their approach replaces the 3D ConvNet of Song and Xia [ 8 1 ] with a 2D ConvNet that processes the depth map .", "ner": [["3D ConvNet", "Method"], ["2D ConvNet", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "198147921", "sentence": "Chen et al. [ 1 2 ] proposed a Multi - View 3D network ( MV 3 D ) , a region - based fusion network , which combines features from multiple views .", "ner": [["Multi - View 3D network", "Method"], ["MV 3 D", "Method"], ["region - based fusion network", "Method"]], "rel": [["MV 3 D", "Synonym-Of", "Multi - View 3D network"], ["Multi - View 3D network", "SubClass-Of", "region - based fusion network"]], "rel_plus": [["MV 3 D:Method", "Synonym-Of", "Multi - View 3D network:Method"], ["Multi - View 3D network:Method", "SubClass-Of", "region - based fusion network:Method"]]}
{"doc_id": "198147921", "sentence": "They first detect and segment object instances in the scene and then use a convolutional neural network ( CNN ) to predict the coarse pose of the object .", "ner": [["convolutional neural network", "Method"], ["CNN", "Method"]], "rel": [["CNN", "Synonym-Of", "convolutional neural network"]], "rel_plus": [["CNN:Method", "Synonym-Of", "convolutional neural network:Method"]]}
{"doc_id": "198147921", "sentence": "The Iterative Closest Point ( ICP ) algorithm [ 7 1 ] is then used to align 3D CAD models to these 3D points .", "ner": [["Iterative Closest Point", "Method"], ["ICP", "Method"]], "rel": [["ICP", "Synonym-Of", "Iterative Closest Point"]], "rel_plus": [["ICP:Method", "Synonym-Of", "Iterative Closest Point:Method"]]}
{"doc_id": "198147921", "sentence": "This is usually achieved using a region recognition network , which has two branches of fully connected layers ; one for recognition and another one for bounding box regression [ 1 2 , 1 5 ] .", "ner": [["region recognition network", "Method"], ["fully connected layers", "Method"], ["recognition", "Task"], ["bounding box regression", "Task"]], "rel": [["fully connected layers", "Part-Of", "region recognition network"], ["fully connected layers", "Used-For", "recognition"], ["fully connected layers", "Used-For", "bounding box regression"]], "rel_plus": [["fully connected layers:Method", "Part-Of", "region recognition network:Method"], ["fully connected layers:Method", "Used-For", "recognition:Task"], ["fully connected layers:Method", "Used-For", "bounding box regression:Task"]]}
{"doc_id": "198147921", "sentence": "In general , the region proposal network ( RPN ) and the object recognition network ( ORN ) operate separately .", "ner": [["region proposal network", "Method"], ["RPN", "Method"], ["object recognition network", "Method"], ["ORN", "Method"]], "rel": [["RPN", "Synonym-Of", "region proposal network"], ["ORN", "Synonym-Of", "object recognition network"]], "rel_plus": [["RPN:Method", "Synonym-Of", "region proposal network:Method"], ["ORN:Method", "Synonym-Of", "object recognition network:Method"]]}
{"doc_id": "198147921", "sentence": "The ORN then refines the detection by discarding regions that do not correspond to the objects of interest .", "ner": [["ORN", "Method"], ["detection", "Task"]], "rel": [["ORN", "Used-For", "detection"]], "rel_plus": [["ORN:Method", "Used-For", "detection:Task"]]}
{"doc_id": "198147921", "sentence": "A common loss function that has been used for classification ( in the RPN as well as in the ORN ) is the softmax regression loss .", "ner": [["classification", "Task"], ["RPN", "Method"], ["ORN", "Method"], ["softmax", "Method"]], "rel": [["softmax", "Used-For", "classification"], ["softmax", "Part-Of", "RPN"], ["softmax", "Part-Of", "ORN"]], "rel_plus": [["softmax:Method", "Used-For", "classification:Task"], ["softmax:Method", "Part-Of", "RPN:Method"], ["softmax:Method", "Part-Of", "ORN:Method"]]}
{"doc_id": "198147921", "sentence": "Note that some datasets , such as the PASCAL 3 D+ , are particularly suitable for testing the robustness of various algorithms to occlusions , since an emphasis was placed on gathering data with occlusions . [ 5 6 ] Cluttered driving scenarios recorded in and around Karlsruhe in Germany . 4 0 0 annotated dynamic scenes from the raw KITTI dataset .", "ner": [["PASCAL 3 D+", "Dataset"], ["KITTI", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "198147921", "sentence": "PASCAL 3 D+ [ 8 9 ] Vehicular and indoor objects ( augments the PASCAL VOC dataset [ 1 9 ] ) . 1 2 object categories with 3 , 0 0 0 instances per category .", "ner": [["PASCAL 3 D+", "Dataset"], ["PASCAL VOC", "Dataset"]], "rel": [["PASCAL 3 D+", "Compare-With", "PASCAL VOC"]], "rel_plus": [["PASCAL 3 D+:Dataset", "Compare-With", "PASCAL VOC:Dataset"]]}
{"doc_id": "198147921", "sentence": "ModelNet 1 0 [ 8 7 ] Object aligned 3D CAD models for the 1 0 most common object categories found in the SUN 2 0 1 2 database [ 9 0 ] . 9 , 7 9 8 total instances split amongst 1 0 object categories , each with their own test/train split .", "ner": [["ModelNet 1 0", "Dataset"], ["SUN 2 0 1 2", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "198147921", "sentence": "Object detection usually involves two tasks ; the first is to assess whether the object exists in the RGB - D image ( classification ) and the second is to exactly localize the object in the image ( localization ) .", "ner": [["Object detection", "Task"], ["classification", "Task"], ["localization", "Task"]], "rel": [["classification", "SubTask-Of", "Object detection"], ["localization", "SubTask-Of", "Object detection"]], "rel_plus": [["classification:Task", "SubTask-Of", "Object detection:Task"], ["localization:Task", "SubTask-Of", "Object detection:Task"]]}
{"doc_id": "198147921", "sentence": "The Deep Sliding Shapes model for amodal 3D object detection in RGB - D images [ 8 1 ] extends its predecessor [ 8 0 ] , which used hand - crafted features and SVMs .", "ner": [["Deep Sliding Shapes", "Method"], ["3D object detection", "Task"], ["SVMs", "Method"]], "rel": [["SVMs", "Part-Of", "Deep Sliding Shapes"], ["Deep Sliding Shapes", "Used-For", "3D object detection"]], "rel_plus": [["SVMs:Method", "Part-Of", "Deep Sliding Shapes:Method"], ["Deep Sliding Shapes:Method", "Used-For", "3D object detection:Task"]]}
{"doc_id": "198147921", "sentence": "Non - maximum suppression with an IoU constraint of less than 0. 3 5 is enforced on the RPN output to reduce the number of RoIs .", "ner": [["Non - maximum suppression", "Method"], ["RPN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "198147921", "sentence": "Each RoI is then projected to 2D and fed to a VGG - 1 9 - based deep feature extractor [ 7 7 ] , which produces the class labels as well as the 3D bounding boxes of the detected objects . [ 1 0 ] i - LIDS 4 9 2 . 1 1 Considering only 5 categories from the NYUDv 2 dataset . 2 3D annotations for the NYUDv 2 dataset were improved in [ 1 5 ] and this improved dataset was used to calculate performance . 3 Considering only 1 0 categories from the SUN RGB - D dataset . 4 Scenario 1 of the i - LIDS dataset .", "ner": [["VGG - 1 9", "Method"], ["NYUDv 2", "Dataset"], ["NYUDv 2", "Dataset"], ["SUN RGB - D", "Dataset"], ["i - LIDS", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "198147921", "sentence": "The framework is composed of an RPN , an ORN , and a Scene Recognition Network ( SRN ) .", "ner": [["RPN", "Method"], ["ORN", "Method"], ["Scene Recognition Network", "Method"], ["SRN", "Method"]], "rel": [["SRN", "Synonym-Of", "Scene Recognition Network"]], "rel_plus": [["SRN:Method", "Synonym-Of", "Scene Recognition Network:Method"]]}
{"doc_id": "198147921", "sentence": "The ORN , which achieved a mean average precision ( mAP ) of 5 2 . 4 % on the SUN RGB - D dataset , outperformed Faster R - CNN [ 6 9 ] , RGB - D RCNN [ 2 9 ] , and DPM [ 2 1 ] .", "ner": [["ORN", "Method"], ["SUN RGB - D", "Dataset"], ["Faster R - CNN", "Method"], ["RGB - D RCNN", "Method"], ["DPM", "Method"]], "rel": [["ORN", "Evaluated-With", "SUN RGB - D"], ["Faster R - CNN", "Evaluated-With", "SUN RGB - D"], ["Faster R - CNN", "Evaluated-With", "SUN RGB - D"], ["RGB - D RCNN", "Evaluated-With", "SUN RGB - D"], ["DPM", "Evaluated-With", "SUN RGB - D"], ["ORN", "Compare-With", "Faster R - CNN"], ["ORN", "Compare-With", "RGB - D RCNN"], ["ORN", "Compare-With", "DPM"]], "rel_plus": [["ORN:Method", "Evaluated-With", "SUN RGB - D:Dataset"], ["Faster R - CNN:Method", "Evaluated-With", "SUN RGB - D:Dataset"], ["Faster R - CNN:Method", "Evaluated-With", "SUN RGB - D:Dataset"], ["RGB - D RCNN:Method", "Evaluated-With", "SUN RGB - D:Dataset"], ["DPM:Method", "Evaluated-With", "SUN RGB - D:Dataset"], ["ORN:Method", "Compare-With", "Faster R - CNN:Method"], ["ORN:Method", "Compare-With", "RGB - D RCNN:Method"], ["ORN:Method", "Compare-With", "DPM:Method"]]}
{"doc_id": "198147921", "sentence": "Another example is the object detection pipeline of Qi et al. [ 6 2 ] , which produces the full extents of an object 's bounding box in 3D from RGB - D images by using four sub - networks , namely : 3 6 3 . 7 MV 3 D [ 1 2 ] KITTI ( valid ) 3 5 5 . 1 MV 3 D [ 1 2 ] KITTI ( test ) 3 7 9 . 8 3D FCN [ 4 8 ] KITTI ( test ) 3 6 8 . 2 Using only 1 0 categories from the SUN RGB - D dataset . 3 Using the hard , cars subset of the KITTI dataset . 3 Using the hard , cars subset of the KITTI dataset . 4 Results taken from [ 3 0 ] . 5 Uses ResNet - 1 0 1 . 6 Using the PASCAL Visual Object Classes ( VOC ) evaluation . \u2022 A joint 2D RPN/ORN .", "ner": [["object detection", "Task"], ["MV 3 D", "Method"], ["KITTI ( valid )", "Dataset"], ["MV 3 D", "Method"], ["KITTI ( test )", "Dataset"], ["3D FCN", "Method"], ["KITTI ( test )", "Dataset"], ["SUN RGB - D", "Dataset"], ["KITTI", "Dataset"], ["KITTI", "Dataset"], ["ResNet - 1 0 1", "Method"], ["PASCAL Visual Object Classes", "Dataset"], ["VOC", "Dataset"]], "rel": [["MV 3 D", "Evaluated-With", "KITTI ( valid )"], ["MV 3 D", "Evaluated-With", "KITTI ( test )"], ["3D FCN", "Evaluated-With", "KITTI ( test )"], ["VOC", "Synonym-Of", "PASCAL Visual Object Classes"]], "rel_plus": [["MV 3 D:Method", "Evaluated-With", "KITTI ( valid ):Dataset"], ["MV 3 D:Method", "Evaluated-With", "KITTI ( test ):Dataset"], ["3D FCN:Method", "Evaluated-With", "KITTI ( test ):Dataset"], ["VOC:Dataset", "Synonym-Of", "PASCAL Visual Object Classes:Dataset"]]}
{"doc_id": "198147921", "sentence": "It generates 2D region proposals from the RGB image , and classifies them into one of the n c object categories . \u2022 A PointNet - based network .", "ner": [["2D region proposals", "Task"], ["A PointNet - based network", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "198147921", "sentence": "It performs 3D instance segmentation of the point clouds within 3D frustums extended from the proposed regions . \u2022 A light - weight regression PointNet ( T - Net ) .", "ner": [["3D instance segmentation", "Task"], ["light - weight regression PointNet", "Method"], ["T - Net", "Method"]], "rel": [["T - Net", "Synonym-Of", "light - weight regression PointNet"]], "rel_plus": [["T - Net:Method", "Synonym-Of", "light - weight regression PointNet:Method"]]}
{"doc_id": "198147921", "sentence": "The approach simultaneously trains the 3D instance segmentation PointNet , the TNet , and the amodal box estimation PointNet , using a loss function that is defined as a weighted sum of the losses of the individual subnetworks .", "ner": [["3D instance segmentation", "Task"], ["PointNet", "Method"], ["TNet", "Method"], ["PointNet", "Method"]], "rel": [["PointNet", "Used-For", "3D instance segmentation"], ["TNet", "Used-For", "3D instance segmentation"]], "rel_plus": [["PointNet:Method", "Used-For", "3D instance segmentation:Task"], ["TNet:Method", "Used-For", "3D instance segmentation:Task"]]}
{"doc_id": "198147921", "sentence": "However , many situations , e.g. robust grasping , image editing , and accurate robot navigation , require the accurate detection of object boundaries .", "ner": [["robust grasping", "Task"], ["image editing", "Task"], ["robot navigation", "Task"], ["detection", "Task"]], "rel": [["detection", "Used-For", "robust grasping"], ["detection", "Used-For", "image editing"], ["detection", "Used-For", "robot navigation"]], "rel_plus": [["detection:Task", "Used-For", "robust grasping:Task"], ["detection:Task", "Used-For", "image editing:Task"], ["detection:Task", "Used-For", "robot navigation:Task"]]}
{"doc_id": "146120936", "sentence": "Its design is critical for dense prediction tasks such as object detection and semantic/instance segmentation .", "ner": [["dense prediction tasks", "Task"], ["object detection", "Task"], ["semantic/instance segmentation", "Task"]], "rel": [["object detection", "SubTask-Of", "dense prediction tasks"], ["semantic/instance segmentation", "SubTask-Of", "dense prediction tasks"]], "rel_plus": [["object detection:Task", "SubTask-Of", "dense prediction tasks:Task"], ["semantic/instance segmentation:Task", "SubTask-Of", "dense prediction tasks:Task"]]}
{"doc_id": "146120936", "sentence": "In this work , we propose Content - Aware ReAssembly of FEatures ( CARAFE ) , a universal , lightweight and highly effective operator to fulfill this goal .", "ner": [["Content - Aware ReAssembly of FEatures", "Method"], ["CARAFE", "Method"]], "rel": [["CARAFE", "Synonym-Of", "Content - Aware ReAssembly of FEatures"]], "rel_plus": [["CARAFE:Method", "Synonym-Of", "Content - Aware ReAssembly of FEatures:Method"]]}
{"doc_id": "146120936", "sentence": "Instead of using a fixed kernel for all samples ( e.g. deconvolution ) , CARAFE enables instance - specific content - aware handling , which generates adaptive kernels on - the - fly . ( 3 ) Lightweight and fast to compute .", "ner": [["deconvolution", "Method"], ["CARAFE", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "146120936", "sentence": "We conduct comprehensive evaluations on standard benchmarks in object detection , instance/semantic segmentation and inpainting .", "ner": [["object detection", "Task"], ["instance/semantic segmentation", "Task"], ["inpainting", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "146120936", "sentence": "On the one hand , for the decoders in dense prediction tasks ( e.g. super resolution [ 6 , 1 7 ] , inpainting [ 1 1 , 2 9 ] and semantic segmentation [ 3 9 , 4 ] ) , the high - level/low - res feature map is upsampled to match the high - resolution supervision .", "ner": [["dense prediction tasks", "Task"], ["super resolution", "Task"], ["inpainting", "Task"], ["semantic segmentation", "Task"]], "rel": [["super resolution", "SubTask-Of", "dense prediction tasks"], ["inpainting", "SubTask-Of", "dense prediction tasks"], ["semantic segmentation", "SubTask-Of", "dense prediction tasks"]], "rel_plus": [["super resolution:Task", "SubTask-Of", "dense prediction tasks:Task"], ["inpainting:Task", "SubTask-Of", "dense prediction tasks:Task"], ["semantic segmentation:Task", "SubTask-Of", "dense prediction tasks:Task"]]}
{"doc_id": "146120936", "sentence": "On the other hand , feature upsampling is also involved in fusing a highlevel/low - res feature map with a low - level/high - res feature map , which is widely adopted in many state - of - the - art architectures , e.g. , Feature Pyramid Network [ 1 8 ] , U - Net [ 3 1 ] Reassembly Center Reassembled Region Upsample and Stacked Hourglass [ 2 6 ] .", "ner": [["feature upsampling", "Method"], ["Feature Pyramid Network", "Method"], ["U - Net", "Method"]], "rel": [["feature upsampling", "Used-For", "Feature Pyramid Network"], ["feature upsampling", "Used-For", "U - Net"]], "rel_plus": [["feature upsampling:Method", "Used-For", "Feature Pyramid Network:Method"], ["feature upsampling:Method", "Used-For", "U - Net:Method"]]}
{"doc_id": "146120936", "sentence": "To this end , we propose a lightweight yet highly effective operator , called ContentAware ReAssembly of Features ( CARAFE ) .", "ner": [["ContentAware ReAssembly of Features", "Method"], ["CARAFE", "Method"]], "rel": [["CARAFE", "Synonym-Of", "ContentAware ReAssembly of Features"]], "rel_plus": [["CARAFE:Method", "Synonym-Of", "ContentAware ReAssembly of Features:Method"]]}
{"doc_id": "146120936", "sentence": "After upsampled by CARAFE , a feature map can represent the shape of an object more accurately , so that the model can predict better instance segmentation results .", "ner": [["CARAFE", "Method"], ["instance segmentation", "Task"]], "rel": [["CARAFE", "Used-For", "instance segmentation"]], "rel_plus": [["CARAFE:Method", "Used-For", "instance segmentation:Task"]]}
{"doc_id": "146120936", "sentence": "To demonstrate the universal effectiveness of CARAFE , we conduct comprehensive evaluations across a wide range of dense prediction tasks , i.e. , object detection , instance segmentation , semantic segmentation , image inpainting , with mainstream architectures .", "ner": [["CARAFE", "Method"], ["dense prediction tasks", "Task"], ["object detection", "Task"], ["instance segmentation", "Task"], ["semantic segmentation", "Task"], ["image inpainting", "Task"]], "rel": [["object detection", "Used-For", "dense prediction tasks"], ["instance segmentation", "Used-For", "dense prediction tasks"], ["semantic segmentation", "Used-For", "dense prediction tasks"], ["image inpainting", "Used-For", "dense prediction tasks"]], "rel_plus": [["object detection:Task", "Used-For", "dense prediction tasks:Task"], ["instance segmentation:Task", "Used-For", "dense prediction tasks:Task"], ["semantic segmentation:Task", "Used-For", "dense prediction tasks:Task"], ["image inpainting:Task", "Used-For", "dense prediction tasks:Task"]]}
{"doc_id": "146120936", "sentence": "CARAFE can boost the performance of Faster RCNN [ 3 0 ] [ 4 3 , 4 4 ] val in semantic segmentation , and improves Global&Local [ 1 1 ] by 1. 1 dB of PSNR on Places [ 4 2 ] val in image inpainting .", "ner": [["CARAFE", "Method"], ["Faster RCNN", "Method"], ["semantic segmentation", "Task"], ["Global&Local", "Method"], ["Places", "Dataset"], ["image inpainting", "Task"]], "rel": [["CARAFE", "Part-Of", "Faster RCNN"], ["Faster RCNN", "Used-For", "semantic segmentation"], ["Faster RCNN", "Compare-With", "Global&Local"], ["Faster RCNN", "Evaluated-With", "Places"], ["Global&Local", "Evaluated-With", "Places"], ["Faster RCNN", "Used-For", "image inpainting"], ["Places", "Benchmark-For", "image inpainting"], ["Global&Local", "Used-For", "image inpainting"]], "rel_plus": [["CARAFE:Method", "Part-Of", "Faster RCNN:Method"], ["Faster RCNN:Method", "Used-For", "semantic segmentation:Task"], ["Faster RCNN:Method", "Compare-With", "Global&Local:Method"], ["Faster RCNN:Method", "Evaluated-With", "Places:Dataset"], ["Global&Local:Method", "Evaluated-With", "Places:Dataset"], ["Faster RCNN:Method", "Used-For", "image inpainting:Task"], ["Places:Dataset", "Benchmark-For", "image inpainting:Task"], ["Global&Local:Method", "Used-For", "image inpainting:Task"]]}
{"doc_id": "146120936", "sentence": "When upsampling an H \u00d7 W feature map with 2 5 6 channels by a factor of two , the introduced computational overhead by CARAFE is only H * W * 1 9 9 k FLOPs , vs. , H * W * 1 1 8 0 k FLOPs of deconvolution .", "ner": [["CARAFE", "Method"], ["deconvolution", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "146120936", "sentence": "Recently , [ 2 3 ] proposed guided upsampling ( GUM ) , which performs interpolation by sampling pixels with learnable offsets .", "ner": [["guided upsampling", "Method"], ["GUM", "Method"]], "rel": [["GUM", "Synonym-Of", "guided upsampling"]], "rel_plus": [["GUM:Method", "Synonym-Of", "guided upsampling:Method"]]}
{"doc_id": "146120936", "sentence": "Within the realms of super - resolution and denoising , some other works [ 2 4 , 1 4 , 9 ] also explore using learnable kernels spatially in low - level vision .", "ner": [["super - resolution", "Task"], ["denoising", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "146120936", "sentence": "Object detection is the task of localizing objects with bounding - boxes , instance segmentation further requires the prediction of instance - wise masks .", "ner": [["Object detection", "Task"], ["localizing objects with bounding - boxes", "Task"], ["instance segmentation", "Task"]], "rel": [["localizing objects with bounding - boxes", "SubTask-Of", "Object detection"], ["instance segmentation", "SubTask-Of", "Object detection"]], "rel_plus": [["localizing objects with bounding - boxes:Task", "SubTask-Of", "Object detection:Task"], ["instance segmentation:Task", "SubTask-Of", "Object detection:Task"]]}
{"doc_id": "146120936", "sentence": "Faster - RCNN [ 3 0 ] introduces Region Proposal Network ( RPN ) for end - to - end training , which is further improved by the guided anchoring scheme [ 3 4 ] . [ 1 8 , 2 1 , 1 5 , 4 1 , 2 8 ] exploits multi - scale feature pyramids to deal with objects at different scales .", "ner": [["Faster - RCNN", "Method"], ["Region Proposal Network", "Method"], ["RPN", "Method"]], "rel": [["Region Proposal Network", "Part-Of", "Faster - RCNN"], ["RPN", "Synonym-Of", "Region Proposal Network"]], "rel_plus": [["Region Proposal Network:Method", "Part-Of", "Faster - RCNN:Method"], ["RPN:Method", "Synonym-Of", "Region Proposal Network:Method"]]}
{"doc_id": "146120936", "sentence": "By adding extra mask prediction branches , Mask - RCNN [ 8 ] and its variants [ 1 , 1 0 ] yield promising pixel - level results .", "ner": [["Mask - RCNN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "146120936", "sentence": "PSPNet [ 3 9 ] introduces spatial pooling at multiple grid scales . and UperNet [ 3 5 ] designs a more generalized framework based on PSPNet .", "ner": [["PSPNet", "Method"], ["spatial pooling", "Method"], ["UperNet", "Method"], ["PSPNet", "Method"]], "rel": [["spatial pooling", "Part-Of", "PSPNet"]], "rel_plus": [["spatial pooling:Method", "Part-Of", "PSPNet:Method"]]}
{"doc_id": "146120936", "sentence": "Feature upsampling is a key operator in many modern convolutional network architectures developed for tasks including object detection , instance segmentation , and scene parsing .", "ner": [["object detection", "Task"], ["instance segmentation", "Task"], ["scene parsing", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "146120936", "sentence": "In this work , we propose the content - aware reassembly of features ( CARAFE ) to upsample a feature map .", "ner": [["content - aware reassembly of features", "Method"], ["CARAFE", "Method"]], "rel": [["CARAFE", "Synonym-Of", "content - aware reassembly of features"]], "rel_plus": [["CARAFE:Method", "Synonym-Of", "content - aware reassembly of features:Method"]]}
{"doc_id": "146120936", "sentence": "Thanks to the content information , CARAFE can use an adaptive and optimized reassembly kernel in different locations and achieve better performance than mainstream upsampling operators , e.g. interpolations or deconvolution .", "ner": [["CARAFE", "Method"], ["upsampling operators", "Method"], ["interpolations", "Method"], ["deconvolution", "Method"]], "rel": [["interpolations", "SubClass-Of", "upsampling operators"], ["deconvolution", "SubClass-Of", "upsampling operators"], ["CARAFE", "Compare-With", "upsampling operators"], ["CARAFE", "Compare-With", "interpolations"], ["CARAFE", "Compare-With", "deconvolution"]], "rel_plus": [["interpolations:Method", "SubClass-Of", "upsampling operators:Method"], ["deconvolution:Method", "SubClass-Of", "upsampling operators:Method"], ["CARAFE:Method", "Compare-With", "upsampling operators:Method"], ["CARAFE:Method", "Compare-With", "interpolations:Method"], ["CARAFE:Method", "Compare-With", "deconvolution:Method"]]}
{"doc_id": "146120936", "sentence": "Here we discuss the relations between CARAFE and dynamic filter [ 1 3 ] , spatial attention [ 3 ] , spatial transformer [ 1 2 ] and deformable convolution [ 5 ] , which share similar design philosophy but with different focuses .", "ner": [["CARAFE", "Method"], ["dynamic filter", "Method"], ["spatial attention", "Method"], ["spatial transformer", "Method"], ["deformable convolution", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "146120936", "sentence": "Both dynamic filter and CARAFE are content - aware operators , but a fundamental difference between them lies at their kernel generation process .", "ner": [["dynamic filter", "Method"], ["CARAFE", "Method"], ["content - aware operators", "Method"]], "rel": [["dynamic filter", "SubClass-Of", "content - aware operators"], ["CARAFE", "SubClass-Of", "content - aware operators"]], "rel_plus": [["dynamic filter:Method", "SubClass-Of", "content - aware operators:Method"], ["CARAFE:Method", "SubClass-Of", "content - aware operators:Method"]]}
{"doc_id": "146120936", "sentence": "In summary , spatial attention is a rescaling operator with point - wise guidance while CARAFE is a reassembly operator with region - wise local guidance .", "ner": [["spatial attention", "Method"], ["CARAFE", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "146120936", "sentence": "Spatial attention can be seen as a special case of CARAFE where the reassembly kernel size is 1 , regardless of the kernel normalizer .", "ner": [["Spatial attention", "Method"], ["CARAFE", "Method"], ["kernel normalizer", "Method"]], "rel": [["Spatial attention", "SubClass-Of", "CARAFE"]], "rel_plus": [["Spatial attention:Method", "SubClass-Of", "CARAFE:Method"]]}
{"doc_id": "146120936", "sentence": "Spatial Transformer Networks ( STN ) .", "ner": [["Spatial Transformer Networks", "Method"], ["STN", "Method"]], "rel": [["STN", "Synonym-Of", "Spatial Transformer Networks"]], "rel_plus": [["STN:Method", "Synonym-Of", "Spatial Transformer Networks:Method"]]}
{"doc_id": "146120936", "sentence": "Deformable Convolutional Networks ( DCN ) .", "ner": [["Deformable Convolutional Networks", "Method"], ["DCN", "Method"]], "rel": [["DCN", "Synonym-Of", "Deformable Convolutional Networks"]], "rel_plus": [["DCN:Method", "Synonym-Of", "Deformable Convolutional Networks:Method"]]}
{"doc_id": "146120936", "sentence": "Similar to dynamic filter , it is also a heavy parametric operator with 2 4 times more computational cost than CARAFE .", "ner": [["dynamic filter", "Method"], ["CARAFE", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "146120936", "sentence": "With negligible additional parameters , CARAFE benefits state - of - the - art methods in both highlevel and low - level tasks , such as object detection , instance segmentation , semantic segmentation and image inpainting .", "ner": [["CARAFE", "Method"], ["object detection", "Task"], ["instance segmentation", "Task"], ["semantic segmentation", "Task"], ["image inpainting", "Task"]], "rel": [["CARAFE", "Used-For", "object detection"], ["CARAFE", "Used-For", "instance segmentation"], ["CARAFE", "Used-For", "semantic segmentation"], ["CARAFE", "Used-For", "image inpainting"]], "rel_plus": [["CARAFE:Method", "Used-For", "object detection:Task"], ["CARAFE:Method", "Used-For", "instance segmentation:Task"], ["CARAFE:Method", "Used-For", "semantic segmentation:Task"], ["CARAFE:Method", "Used-For", "image inpainting:Task"]]}
{"doc_id": "146120936", "sentence": "Feature Pyramid Network ( FPN ) is an important and effective architecture in the field of object detection and instance segmentation .", "ner": [["Feature Pyramid Network", "Method"], ["FPN", "Method"], ["object detection", "Task"], ["instance segmentation", "Task"]], "rel": [["FPN", "Synonym-Of", "Feature Pyramid Network"], ["Feature Pyramid Network", "Used-For", "object detection"], ["Feature Pyramid Network", "Used-For", "instance segmentation"]], "rel_plus": [["FPN:Method", "Synonym-Of", "Feature Pyramid Network:Method"], ["Feature Pyramid Network:Method", "Used-For", "object detection:Task"], ["Feature Pyramid Network:Method", "Used-For", "instance segmentation:Task"]]}
{"doc_id": "146120936", "sentence": "It significantly improves the performance of popular frameworks like Faster R - CNN and Mask R - CNN .", "ner": [["Faster R - CNN", "Method"], ["Mask R - CNN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "146120936", "sentence": "FPN constructs feature pyramids of strong semantics with the top - down pathway and lateral connections .", "ner": [["FPN", "Method"], ["feature pyramids", "Method"], ["top - down pathway", "Method"], ["lateral connections", "Method"]], "rel": [["feature pyramids", "Part-Of", "FPN"], ["top - down pathway", "Part-Of", "feature pyramids"], ["lateral connections", "Part-Of", "feature pyramids"]], "rel_plus": [["feature pyramids:Method", "Part-Of", "FPN:Method"], ["top - down pathway:Method", "Part-Of", "feature pyramids:Method"], ["lateral connections:Method", "Part-Of", "feature pyramids:Method"]]}
{"doc_id": "146120936", "sentence": "In addition to the FPN structure , Mask R - CNN adopts a deconvolution layer at the end of mask head .", "ner": [["FPN", "Method"], ["Mask R - CNN", "Method"], ["deconvolution layer", "Method"]], "rel": [["deconvolution layer", "Part-Of", "Mask R - CNN"]], "rel_plus": [["deconvolution layer:Method", "Part-Of", "Mask R - CNN:Method"]]}
{"doc_id": "146120936", "sentence": "We can also use CARAFE to replace the deconvolution layer , resulting in even less computational cost .", "ner": [["CARAFE", "Method"], ["deconvolution", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "146120936", "sentence": "UperNet is a strong baseline for semantic segmentation .", "ner": [["UperNet", "Method"], ["semantic segmentation", "Task"]], "rel": [["UperNet", "Used-For", "semantic segmentation"]], "rel_plus": [["UperNet:Method", "Used-For", "semantic segmentation:Task"]]}
{"doc_id": "146120936", "sentence": "It uses upsampling in the following three components , i.e. , PPM , FPN , FUSE .", "ner": [["PPM", "Method"], ["FPN", "Method"], ["FUSE", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "146120936", "sentence": "Pyramid Pooling Module ( PPM ) .", "ner": [["Pyramid Pooling Module", "Method"], ["PPM", "Method"]], "rel": [["PPM", "Synonym-Of", "Pyramid Pooling Module"]], "rel_plus": [["PPM:Method", "Synonym-Of", "Pyramid Pooling Module:Method"]]}
{"doc_id": "146120936", "sentence": "PPM is the key component in PSPNet that hierarchically down - samples an input feature map into multiple scales { 1 \u00d7 1 , 2 \u00d7 2 , 3 \u00d7 3 , 6 \u00d7 6 } , and then upsamples them back to the original sizes with bilinear interpolation .", "ner": [["PPM", "Method"], ["PSPNet", "Method"]], "rel": [["PPM", "Part-Of", "PSPNet"]], "rel_plus": [["PPM:Method", "Part-Of", "PSPNet:Method"]]}
{"doc_id": "146120936", "sentence": "Feature Pyramid Network ( FPN ) .", "ner": [["Feature Pyramid Network", "Method"], ["FPN", "Method"]], "rel": [["FPN", "Synonym-Of", "Feature Pyramid Network"]], "rel_plus": [["FPN:Method", "Synonym-Of", "Feature Pyramid Network:Method"]]}
{"doc_id": "146120936", "sentence": "Similar to detection models , UperNet also adopts FPN to enrich the feature semantics .", "ner": [["UperNet", "Method"], ["FPN", "Method"]], "rel": [["FPN", "Part-Of", "UperNet"]], "rel_plus": [["FPN:Method", "Part-Of", "UperNet:Method"]]}
{"doc_id": "146120936", "sentence": "Multi - level Feature Fusion ( FUSE ) .", "ner": [["Multi - level Feature Fusion", "Method"], ["FUSE", "Method"]], "rel": [["FUSE", "Synonym-Of", "Multi - level Feature Fusion"]], "rel_plus": [["FUSE:Method", "Synonym-Of", "Multi - level Feature Fusion:Method"]]}
{"doc_id": "146120936", "sentence": "UperNet proposes a multi - level feature fusion module after the FPN .", "ner": [["UperNet", "Method"], ["multi - level feature fusion module", "Method"], ["FPN", "Method"]], "rel": [["FPN", "Part-Of", "UperNet"], ["multi - level feature fusion module", "Part-Of", "UperNet"]], "rel_plus": [["FPN:Method", "Part-Of", "UperNet:Method"], ["multi - level feature fusion module:Method", "Part-Of", "UperNet:Method"]]}
{"doc_id": "146120936", "sentence": "The U - net architecture is popular among recent proposed image inpainting methods , such as Global&Local [ 1 1 ] and Partial Conv [ 2 0 ] .", "ner": [["U - net", "Method"], ["image inpainting", "Task"], ["Global&Local", "Method"], ["Partial Conv", "Method"]], "rel": [["U - net", "Used-For", "image inpainting"], ["Global&Local", "Used-For", "image inpainting"], ["Partial Conv", "Used-For", "image inpainting"]], "rel_plus": [["U - net:Method", "Used-For", "image inpainting:Task"], ["Global&Local:Method", "Used-For", "image inpainting:Task"], ["Partial Conv:Method", "Used-For", "image inpainting:Task"]]}
{"doc_id": "146120936", "sentence": "As for Partial Conv , we can conveniently keep the mask propagation in CARAFE by updating the mask with our content - aware reassembly kernels .   Datasets & Evaluation Metrics .", "ner": [["Partial Conv", "Method"], ["CARAFE", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "146120936", "sentence": "Object Detection and Instance Segmentation .", "ner": [["Object Detection", "Task"], ["Instance Segmentation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "146120936", "sentence": "We adopt the ADE 2 0 k benchmark to evaluate our method in the semantic segmentation task .", "ner": [["ADE 2 0 k", "Dataset"], ["semantic segmentation", "Task"]], "rel": [["ADE 2 0 k", "Benchmark-For", "semantic segmentation"]], "rel_plus": [["ADE 2 0 k:Dataset", "Benchmark-For", "semantic segmentation:Task"]]}
{"doc_id": "146120936", "sentence": "Places dataset is adopted for image inpainting .", "ner": [["Places dataset", "Dataset"], ["image inpainting", "Task"]], "rel": [["Places dataset", "Benchmark-For", "image inpainting"]], "rel_plus": [["Places dataset:Dataset", "Benchmark-For", "image inpainting:Task"]]}
{"doc_id": "146120936", "sentence": "Object Detection and Instance Segmentation .", "ner": [["Object Detection", "Task"], ["Instance Segmentation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "146120936", "sentence": "We evaluate CARAFE on Faster RCNN and Mask RCNN with the ResNet - 5 0 w/ FPN backbone , and follow the 1x training schedule settings as Detectron [ 7 ] and MMDetection [ 2 ] .", "ner": [["CARAFE", "Method"], ["Faster RCNN", "Method"], ["Mask RCNN", "Method"], ["ResNet - 5 0", "Method"], ["FPN", "Method"], ["Detectron", "Method"], ["MMDetection", "Method"]], "rel": [["CARAFE", "Part-Of", "Faster RCNN"], ["ResNet - 5 0", "Part-Of", "Faster RCNN"], ["MMDetection", "Part-Of", "Faster RCNN"], ["Detectron", "Part-Of", "Faster RCNN"], ["CARAFE", "Part-Of", "Mask RCNN"], ["ResNet - 5 0", "Part-Of", "Mask RCNN"], ["Detectron", "Part-Of", "Mask RCNN"], ["MMDetection", "Part-Of", "Mask RCNN"], ["FPN", "Part-Of", "ResNet - 5 0"]], "rel_plus": [["CARAFE:Method", "Part-Of", "Faster RCNN:Method"], ["ResNet - 5 0:Method", "Part-Of", "Faster RCNN:Method"], ["MMDetection:Method", "Part-Of", "Faster RCNN:Method"], ["Detectron:Method", "Part-Of", "Faster RCNN:Method"], ["CARAFE:Method", "Part-Of", "Mask RCNN:Method"], ["ResNet - 5 0:Method", "Part-Of", "Mask RCNN:Method"], ["Detectron:Method", "Part-Of", "Mask RCNN:Method"], ["MMDetection:Method", "Part-Of", "Mask RCNN:Method"], ["FPN:Method", "Part-Of", "ResNet - 5 0:Method"]]}
{"doc_id": "146120936", "sentence": "Image Inpainting We adopt Global&Local [ 1 1 ] and Paritial Conv [ 2 0 ] as baseline methods to evaluate CARAFE .", "ner": [["Image Inpainting", "Task"], ["Global&Local", "Method"], ["Paritial Conv", "Method"], ["CARAFE", "Method"]], "rel": [["Global&Local", "Used-For", "Image Inpainting"], ["Paritial Conv", "Used-For", "Image Inpainting"], ["CARAFE", "Part-Of", "Global&Local"], ["CARAFE", "Part-Of", "Paritial Conv"]], "rel_plus": [["Global&Local:Method", "Used-For", "Image Inpainting:Task"], ["Paritial Conv:Method", "Used-For", "Image Inpainting:Task"], ["CARAFE:Method", "Part-Of", "Global&Local:Method"], ["CARAFE:Method", "Part-Of", "Paritial Conv:Method"]]}
{"doc_id": "146120936", "sentence": "Object Detection & Instance Segmentation .", "ner": [["Object Detection", "Task"], ["Instance Segmentation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "146120936", "sentence": "We first evaluate our method by substituting the nearest neighbor interpolation in FPN with CARAFE for both Faster RCNN and Mask RCNN , and the deconvolution layer in the mask head for Mask RCNN .", "ner": [["nearest neighbor interpolation", "Method"], ["FPN", "Method"], ["CARAFE", "Method"], ["Faster RCNN", "Method"], ["Mask RCNN", "Method"], ["deconvolution", "Method"], ["Mask RCNN", "Method"]], "rel": [["CARAFE", "Part-Of", "FPN"], ["nearest neighbor interpolation", "Part-Of", "FPN"], ["FPN", "Part-Of", "Faster RCNN"], ["FPN", "Part-Of", "Mask RCNN"], ["deconvolution", "Part-Of", "Mask RCNN"], ["CARAFE", "Part-Of", "Mask RCNN"]], "rel_plus": [["CARAFE:Method", "Part-Of", "FPN:Method"], ["nearest neighbor interpolation:Method", "Part-Of", "FPN:Method"], ["FPN:Method", "Part-Of", "Faster RCNN:Method"], ["FPN:Method", "Part-Of", "Mask RCNN:Method"], ["deconvolution:Method", "Part-Of", "Mask RCNN:Method"], ["CARAFE:Method", "Part-Of", "Mask RCNN:Method"]]}
{"doc_id": "146120936", "sentence": "As shown in Table 1 , CARAFE improves Faster RCNN by 1. 2 % on bbox AP , and Mask RCNN by 1. 3 % on mask AP .", "ner": [["CARAFE", "Method"], ["Faster RCNN", "Method"], ["Mask RCNN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "146120936", "sentence": "We visualize the feature maps in the top - down pathway of FPN and compare CARAFE with the baseline , i.e. , nearest neighbor interpolation .", "ner": [["FPN", "Method"], ["CARAFE", "Method"], ["nearest neighbor interpolation", "Method"]], "rel": [["CARAFE", "Compare-With", "nearest neighbor interpolation"]], "rel_plus": [["CARAFE:Method", "Compare-With", "nearest neighbor interpolation:Method"]]}
{"doc_id": "146120936", "sentence": "In Figure 4 , we show some examples of instance segmentation results comparing the baseline and CARAFE .", "ner": [["instance segmentation", "Task"], ["CARAFE", "Method"]], "rel": [["CARAFE", "Used-For", "instance segmentation"]], "rel_plus": [["CARAFE:Method", "Used-For", "instance segmentation:Task"]]}
{"doc_id": "146120936", "sentence": "To investigate the effectiveness of different upsampling operators , we perform extensive experiments on Faster RCNN by using different operators to perform upsampling in FPN .", "ner": [["Faster RCNN", "Method"], ["FPN", "Method"]], "rel": [["FPN", "Part-Of", "Faster RCNN"]], "rel_plus": [["FPN:Method", "Part-Of", "Faster RCNN:Method"]]}
{"doc_id": "146120936", "sentence": "For ' N.C. ' and ' B.C. ' , which respectively indicate ' Nearest + Conv ' and ' Bilinear + Conv ' , we add an extra 3 \u00d7 3 convolution layer after the corresponding upsampling . ' Deconv ' , ' Pixel Shuffle ' ( indicated as ' P.S. ' ) , ' GUM ' are three representative learning based upsampling methods .", "ner": [["N.C.", "Method"], ["B.C.", "Method"], ["Nearest + Conv", "Method"], ["Bilinear + Conv", "Method"], ["3 \u00d7 3 convolution layer", "Method"], ["Deconv", "Method"], ["Pixel Shuffle", "Method"], ["P.S.", "Method"], ["GUM", "Method"], ["representative learning based upsampling", "Method"]], "rel": [["N.C.", "Synonym-Of", "Nearest + Conv"], ["B.C.", "Synonym-Of", "Bilinear + Conv"], ["P.S.", "Synonym-Of", "Pixel Shuffle"], ["Deconv", "SubClass-Of", "representative learning based upsampling"], ["Pixel Shuffle", "SubClass-Of", "representative learning based upsampling"], ["GUM", "SubClass-Of", "representative learning based upsampling"]], "rel_plus": [["N.C.:Method", "Synonym-Of", "Nearest + Conv:Method"], ["B.C.:Method", "Synonym-Of", "Bilinear + Conv:Method"], ["P.S.:Method", "Synonym-Of", "Pixel Shuffle:Method"], ["Deconv:Method", "SubClass-Of", "representative learning based upsampling:Method"], ["Pixel Shuffle:Method", "SubClass-Of", "representative learning based upsampling:Method"], ["GUM:Method", "SubClass-Of", "representative learning based upsampling:Method"]]}
{"doc_id": "146120936", "sentence": "The results of ' Nearest + Conv ' and ' Bilinear + Conv ' show that extra parameters do not lead to a significant gain . ' Deconv ' , ' Pixel Shuffle ' , ' GUM ' and ' Spatial Attention ' obtain inferior performance to CARAFE , indicating that the design of effective upsampling operators is critical .", "ner": [["Nearest + Conv", "Method"], ["Bilinear + Conv", "Method"], ["Deconv", "Method"], ["Pixel Shuffle", "Method"], ["GUM", "Method"], ["Spatial Attention", "Method"], ["CARAFE", "Method"], ["upsampling operators", "Method"]], "rel": [["Spatial Attention", "Compare-With", "CARAFE"], ["GUM", "Compare-With", "CARAFE"], ["Pixel Shuffle", "Compare-With", "CARAFE"], ["Deconv", "Compare-With", "CARAFE"], ["CARAFE", "SubClass-Of", "upsampling operators"]], "rel_plus": [["Spatial Attention:Method", "Compare-With", "CARAFE:Method"], ["GUM:Method", "Compare-With", "CARAFE:Method"], ["Pixel Shuffle:Method", "Compare-With", "CARAFE:Method"], ["Deconv:Method", "Compare-With", "CARAFE:Method"], ["CARAFE:Method", "SubClass-Of", "upsampling operators:Method"]]}
{"doc_id": "146120936", "sentence": "In typical Mask R - CNN , a deconvolution layer is adopted to upsample the RoI features by 2x .", "ner": [["Mask R - CNN", "Method"], ["deconvolution layer", "Method"]], "rel": [["deconvolution layer", "Part-Of", "Mask R - CNN"]], "rel_plus": [["deconvolution layer:Method", "Part-Of", "Mask R - CNN:Method"]]}
{"doc_id": "146120936", "sentence": "For a fair comparison , we do not make any changes to FPN , and only replace the deconvolution layer with various operators .", "ner": [["FPN", "Method"], ["deconvolution layer", "Method"]], "rel": [["deconvolution layer", "Part-Of", "FPN"]], "rel_plus": [["deconvolution layer:Method", "Part-Of", "FPN:Method"]]}
{"doc_id": "146120936", "sentence": "CARAFE achieves the best performance in instance segmentation among these methods .", "ner": [["CARAFE", "Method"], ["instance segmentation", "Task"]], "rel": [["CARAFE", "Used-For", "instance segmentation"]], "rel_plus": [["CARAFE:Method", "Used-For", "instance segmentation:Task"]]}
{"doc_id": "146120936", "sentence": "In Table 4 , we report the object detection and instance segmentation results of adopting CARAFE in FPN and mask head on Mask RCNN respectively .", "ner": [["object detection", "Task"], ["instance segmentation", "Task"], ["CARAFE", "Method"], ["FPN", "Method"], ["Mask RCNN", "Method"]], "rel": [["FPN", "Used-For", "object detection"], ["Mask RCNN", "Used-For", "object detection"], ["FPN", "Used-For", "instance segmentation"], ["Mask RCNN", "Used-For", "instance segmentation"], ["CARAFE", "Part-Of", "FPN"], ["CARAFE", "Part-Of", "Mask RCNN"]], "rel_plus": [["FPN:Method", "Used-For", "object detection:Task"], ["Mask RCNN:Method", "Used-For", "object detection:Task"], ["FPN:Method", "Used-For", "instance segmentation:Task"], ["Mask RCNN:Method", "Used-For", "instance segmentation:Task"], ["CARAFE:Method", "Part-Of", "FPN:Method"], ["CARAFE:Method", "Part-Of", "Mask RCNN:Method"]]}
{"doc_id": "146120936", "sentence": "We replace the upsamplers in UperNet with CARAFE and evaluate the results on ADE 2 0 k benchmark .", "ner": [["upsamplers", "Method"], ["UperNet", "Method"], ["CARAFE", "Method"], ["ADE 2 0 k", "Dataset"]], "rel": [["CARAFE", "Part-Of", "UperNet"], ["upsamplers", "Part-Of", "UperNet"], ["UperNet", "Evaluated-With", "ADE 2 0 k"]], "rel_plus": [["CARAFE:Method", "Part-Of", "UperNet:Method"], ["upsamplers:Method", "Part-Of", "UperNet:Method"], ["UperNet:Method", "Evaluated-With", "ADE 2 0 k:Dataset"]]}
{"doc_id": "146120936", "sentence": "Note that UperNet with CARAFE also achieves better performance than recent strong baselines such as PSPNet [ 3 9 ] and PSANet [ 4 0 ] .", "ner": [["UperNet", "Method"], ["CARAFE", "Method"], ["PSPNet", "Method"], ["PSANet", "Method"]], "rel": [["CARAFE", "Part-Of", "UperNet"], ["UperNet", "Compare-With", "PSPNet"], ["UperNet", "Compare-With", "PSANet"]], "rel_plus": [["CARAFE:Method", "Part-Of", "UperNet:Method"], ["UperNet:Method", "Compare-With", "PSPNet:Method"], ["UperNet:Method", "Compare-With", "PSANet:Method"]]}
{"doc_id": "146120936", "sentence": "We show that CARAFE is also effective in low - level tasks such as image inpainting .", "ner": [["CARAFE", "Method"], ["image inpainting", "Task"]], "rel": [["CARAFE", "Used-For", "image inpainting"]], "rel_plus": [["CARAFE:Method", "Used-For", "image inpainting:Task"]]}
{"doc_id": "146120936", "sentence": "By replacing the upsampling operators with CARAFE in two strong baselines Global&Local [ 1 1 ] and Partial Conv [ 2 0 ] , we observe significant improvements for both methods .", "ner": [["upsampling operators", "Method"], ["CARAFE", "Method"], ["Global&Local", "Method"], ["Partial Conv", "Method"]], "rel": [["upsampling operators", "Part-Of", "Global&Local"], ["CARAFE", "Part-Of", "Global&Local"], ["upsampling operators", "Part-Of", "Partial Conv"], ["CARAFE", "Part-Of", "Partial Conv"]], "rel_plus": [["upsampling operators:Method", "Part-Of", "Global&Local:Method"], ["CARAFE:Method", "Part-Of", "Global&Local:Method"], ["upsampling operators:Method", "Part-Of", "Partial Conv:Method"], ["CARAFE:Method", "Part-Of", "Partial Conv:Method"]]}
{"doc_id": "146120936", "sentence": "Other than the softmax function , we also test other alternatives in the kernel normalizer , such as sigmoid or sigmoid with normalization .", "ner": [["softmax", "Method"], ["kernel normalizer", "Method"], ["sigmoid", "Method"], ["sigmoid with normalization", "Method"]], "rel": [["sigmoid", "SubClass-Of", "kernel normalizer"], ["sigmoid with normalization", "SubClass-Of", "kernel normalizer"]], "rel_plus": [["sigmoid:Method", "SubClass-Of", "kernel normalizer:Method"], ["sigmoid with normalization:Method", "SubClass-Of", "kernel normalizer:Method"]]}
{"doc_id": "146120936", "sentence": "As shown in Table 1 0 , ' Softmax ' and ' Sigmoid Normalized ' have the same performance and better than ' Sigmoid ' , which shows that it is crucial to normalize the reassembly kernel to be summed to 1 .", "ner": [["Softmax", "Method"], ["Sigmoid Normalized", "Method"], ["Sigmoid", "Method"]], "rel": [["Softmax", "Compare-With", "Sigmoid"], ["Sigmoid Normalized", "Compare-With", "Sigmoid"]], "rel_plus": [["Softmax:Method", "Compare-With", "Sigmoid:Method"], ["Sigmoid Normalized:Method", "Compare-With", "Sigmoid:Method"]]}
{"doc_id": "146120936", "sentence": "With a trained Mask RCNN model adopting CARAFE as the upsampling operator , we visualize the reassembling process in Figure 5 .", "ner": [["Mask RCNN", "Method"], ["CARAFE", "Method"], ["upsampling operator", "Method"]], "rel": [["CARAFE", "Part-Of", "Mask RCNN"], ["CARAFE", "SubClass-Of", "upsampling operator"]], "rel_plus": [["CARAFE:Method", "Part-Of", "Mask RCNN:Method"], ["CARAFE:Method", "SubClass-Of", "upsampling operator:Method"]]}
{"doc_id": "146120936", "sentence": "We have presented Content - Aware ReAssembly of FEatures ( CARAFE ) , a universal , lightweight and highly effective upsampling operator .", "ner": [["Content - Aware ReAssembly of FEatures", "Method"], ["CARAFE", "Method"]], "rel": [["CARAFE", "Synonym-Of", "Content - Aware ReAssembly of FEatures"]], "rel_plus": [["CARAFE:Method", "Synonym-Of", "Content - Aware ReAssembly of FEatures:Method"]]}
{"doc_id": "146120936", "sentence": "It consistently boosts the performances on standard benchmarks in object detection , instance/semantic segmentation and inpainting by 1. 2 % AP , 1. 3 % AP , 1. 8 % mIoU , 1. 1 dB , respectively .", "ner": [["object detection", "Task"], ["instance/semantic segmentation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "146120936", "sentence": "Future directions include exploring the applicability of CARAFE in low - level vision tasks such as image restoration and super - resolution .", "ner": [["CARAFE", "Method"], ["image restoration", "Task"], ["super - resolution", "Task"]], "rel": [["CARAFE", "Used-For", "image restoration"], ["CARAFE", "Used-For", "super - resolution"]], "rel_plus": [["CARAFE:Method", "Used-For", "image restoration:Task"], ["CARAFE:Method", "Used-For", "super - resolution:Task"]]}
{"doc_id": "208548469", "sentence": "Deep neural networks have been playing an essential role in the task of Visual Question Answering ( VQA ) .", "ner": [["Visual Question Answering", "Task"], ["VQA", "Task"]], "rel": [["VQA", "Synonym-Of", "Visual Question Answering"]], "rel_plus": [["VQA:Task", "Synonym-Of", "Visual Question Answering:Task"]]}
{"doc_id": "208548469", "sentence": "In VQA , the attack can target the image and/or the proposed query question , dubbed main question , and yet there is a lack of proper analysis of this aspect of VQA .", "ner": [["VQA", "Task"], ["VQA", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "208548469", "sentence": "We cast this ranking problem as a LASSO optimization problem .", "ner": [["ranking", "Task"], ["LASSO optimization problem", "Task"]], "rel": [["LASSO optimization problem", "SubTask-Of", "ranking"]], "rel_plus": [["LASSO optimization problem:Task", "SubTask-Of", "ranking:Task"]]}
{"doc_id": "208548469", "sentence": "Visual Question Answering ( VQA ) is one of the most challenging computer vision tasks in which an algorithm is given a natural language question about an image and is tasked with producing a natural language answer for that question - image pair .", "ner": [["Visual Question Answering", "Task"], ["VQA", "Task"], ["computer vision", "Task"]], "rel": [["VQA", "Synonym-Of", "Visual Question Answering"], ["Visual Question Answering", "SubTask-Of", "computer vision"]], "rel_plus": [["VQA:Task", "Synonym-Of", "Visual Question Answering:Task"], ["Visual Question Answering:Task", "SubTask-Of", "computer vision:Task"]]}
{"doc_id": "208548469", "sentence": "The Noise Generator takes a plain text main question ( MQ ) and a plain text basic question dataset ( BQD ) as input .", "ner": [["Noise Generator", "Method"], ["main question", "Dataset"], ["MQ", "Dataset"], ["basic question dataset", "Dataset"], ["BQD", "Dataset"]], "rel": [["basic question dataset", "Used-For", "Noise Generator"], ["main question", "Used-For", "Noise Generator"], ["MQ", "Synonym-Of", "main question"], ["BQD", "Synonym-Of", "basic question dataset"]], "rel_plus": [["basic question dataset:Dataset", "Used-For", "Noise Generator:Method"], ["main question:Dataset", "Used-For", "Noise Generator:Method"], ["MQ:Dataset", "Synonym-Of", "main question:Dataset"], ["BQD:Dataset", "Synonym-Of", "basic question dataset:Dataset"]]}
{"doc_id": "208548469", "sentence": "It starts by ranking the basic questions in BQD by their similarity to MQ using a text similarity ranking method .", "ner": [["BQD", "Dataset"], ["MQ", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "208548469", "sentence": "To improve the question ranking quality , we propose a new method formulated using LASSO optimization and compare it to other rankings produced by the commonly used textual similarity measures .", "ner": [["LASSO", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "208548469", "sentence": "Then , we do perform this comparison to rank our proposed BQDs , General Basic Question Dataset ( GBQD ) and Yes/No Basic Question Dataset ( YNBQD ) .", "ner": [["BQDs", "Dataset"], ["General Basic Question Dataset", "Dataset"], ["GBQD", "Dataset"], ["Yes/No Basic Question Dataset", "Dataset"], ["YNBQD", "Dataset"]], "rel": [["GBQD", "Synonym-Of", "General Basic Question Dataset"], ["YNBQD", "Synonym-Of", "Yes/No Basic Question Dataset"]], "rel_plus": [["GBQD:Dataset", "Synonym-Of", "General Basic Question Dataset:Dataset"], ["YNBQD:Dataset", "Synonym-Of", "Yes/No Basic Question Dataset:Dataset"]]}
{"doc_id": "208548469", "sentence": "Finally , we conduct extensive experiments to compare our proposed LASSO ranking method with the other metrics in BQD ranking .", "ner": [["LASSO ranking method", "Method"], ["BQD ranking", "Task"]], "rel": [["LASSO ranking method", "Used-For", "BQD ranking"]], "rel_plus": [["LASSO ranking method:Method", "Used-For", "BQD ranking:Task"]]}
{"doc_id": "208548469", "sentence": "In this paper , our main contributions are summarized as follows : \u2022 We introduce two large - scale basic questions datasets and make available two datasets for VQA robustness evaluation . \u2022 We propose a novel method to measure the robustness of VQA models and test it on six different stateof - the - art VQA models . \u2022 We propose a new LASSO - based text similarity ranking method and show that it outperforms seven popular similarity metrics .", "ner": [["VQA", "Task"], ["VQA", "Task"], ["VQA", "Task"], ["LASSO - based text similarity ranking method", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "208548469", "sentence": "Furthermore , in sections 4 and 5 , we present the various analyses on our proposed General Basic Question Dataset ( GBQD ) and Yes/No Basic Question Dataset ( YNBQD ) Huang et al ( 2 0 1 9 ) .", "ner": [["General Basic Question Dataset", "Dataset"], ["GBQD", "Dataset"], ["Yes/No Basic Question Dataset", "Dataset"], ["YNBQD", "Dataset"]], "rel": [["GBQD", "Synonym-Of", "General Basic Question Dataset"], ["YNBQD", "Synonym-Of", "Yes/No Basic Question Dataset"]], "rel_plus": [["GBQD:Dataset", "Synonym-Of", "General Basic Question Dataset:Dataset"], ["YNBQD:Dataset", "Synonym-Of", "Yes/No Basic Question Dataset:Dataset"]]}
{"doc_id": "208548469", "sentence": "The above works involve different fields including natural language progressing ( NLP ) , computer vision , and machine learning .", "ner": [["natural language progressing", "Task"], ["NLP", "Task"], ["computer vision", "Task"], ["machine learning", "Method"]], "rel": [["NLP", "Synonym-Of", "natural language progressing"]], "rel_plus": [["NLP:Task", "Synonym-Of", "natural language progressing:Task"]]}
{"doc_id": "208548469", "sentence": "Sentence evaluation metrics have been widely used in several areas such as video captioning Yu et al ( 2 0 1 6 ) and text summarization Barzilay and Elhadad ( 1 9 9 9 ) .", "ner": [["video captioning", "Task"], ["text summarization", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "208548469", "sentence": "The R score -our proposed robustness measure -is generated based the \" Accuracy Generated by Ranked Basic Question Dataset \" and \" Accuracy Generated by Clean VQA Testing Set \" .", "ner": [["Ranked Basic Question Dataset", "Dataset"], ["Clean VQA Testing Set", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "208548469", "sentence": "Then , we have two choices , GBQD and YNBQD , of Basic Question Dataset and eight different choices of question ranking methods .", "ner": [["GBQD", "Dataset"], ["YNBQD", "Dataset"], ["Basic Question", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "208548469", "sentence": "If a new Basic Question Dataset or ranking method is proposed in the future , we can add them into our proposed method .", "ner": [["Basic Question Dataset", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "208548469", "sentence": "BLEU ( BiLingual Evaluation Understudy ) Papineni et al ( 2 0 0 2 ) is one of the most popular metrics in machine translation based on precision .", "ner": [["BLEU", "Method"], ["BiLingual Evaluation Understudy", "Method"], ["machine translation", "Task"]], "rel": [["BiLingual Evaluation Understudy", "Synonym-Of", "BLEU"], ["BLEU", "Used-For", "machine translation"]], "rel_plus": [["BiLingual Evaluation Understudy:Method", "Synonym-Of", "BLEU:Method"], ["BLEU:Method", "Used-For", "machine translation:Task"]]}
{"doc_id": "208548469", "sentence": "METEOR Banerjee and Lavie ( 2 0 0 5 ) is based on the harmonic mean of unigram precision and recall , and it can handle the stemming and synonym matching , which is designated to fix problems found with BLEU and produces a better correlation with translations by human experts .", "ner": [["METEOR", "Method"], ["stemming and synonym matching", "Task"], ["BLEU", "Method"]], "rel": [["METEOR", "Used-For", "stemming and synonym matching"]], "rel_plus": [["METEOR:Method", "Used-For", "stemming and synonym matching:Task"]]}
{"doc_id": "208548469", "sentence": "Regarding the difference between METEOR and BLEU , METEOR evaluates the correlation at the sentence and segment level whereas BLEU looks for correlations at the corpus level .", "ner": [["METEOR", "Method"], ["BLEU", "Method"], ["METEOR", "Method"], ["BLEU", "Method"]], "rel": [["METEOR", "Compare-With", "BLEU"]], "rel_plus": [["METEOR:Method", "Compare-With", "BLEU:Method"]]}
{"doc_id": "208548469", "sentence": "ROUGE ( Recall Oriented Understudy of Gisting Evaluation ) Lin ( 2 0 0 4 ) is another popular recall - based metric in the text summarization community , and it tends to reward the longer sentences with the higher recall .", "ner": [["ROUGE", "Method"], ["Recall Oriented Understudy of Gisting Evaluation", "Method"], ["text summarization", "Task"]], "rel": [["ROUGE", "Synonym-Of", "Recall Oriented Understudy of Gisting Evaluation"]], "rel_plus": [["ROUGE:Method", "Synonym-Of", "Recall Oriented Understudy of Gisting Evaluation:Method"]]}
{"doc_id": "208548469", "sentence": "CIDEr Vedantam et al ( 2 0 1 5 ) , a consensus - based metric , rewards a sentence for being similar to the majority of descriptions written by the human expert and this metric is mostly used in the image captioning community .", "ner": [["CIDEr", "Method"], ["image captioning", "Task"]], "rel": [["CIDEr", "Used-For", "image captioning"]], "rel_plus": [["CIDEr:Method", "Used-For", "image captioning:Task"]]}
{"doc_id": "208548469", "sentence": "In our experiments , we take all of the metrics above and our proposed LASSO ranking approach to rank BQs and compare their BQ ranking performance .", "ner": [["LASSO ranking", "Method"], ["BQs", "Dataset"], ["BQ ranking", "Method"]], "rel": [["BQ ranking", "Evaluated-With", "BQs"], ["LASSO ranking", "Evaluated-With", "BQs"], ["LASSO ranking", "Compare-With", "BQ ranking"]], "rel_plus": [["BQ ranking:Method", "Evaluated-With", "BQs:Dataset"], ["LASSO ranking:Method", "Evaluated-With", "BQs:Dataset"], ["LASSO ranking:Method", "Compare-With", "BQ ranking:Method"]]}
{"doc_id": "208548469", "sentence": "Some commonly used techniques , such as encoding and decoding , in the image captioning task Xu et al ( 2 0 1 5 ) ; Karpathy and Fei - Fei ( 2 0 1 5 ) ; Vinyals et al ( 2 0 1 5 ) ; Fang et al ( 2 0 1 5 ) are also used in the VQA task .", "ner": [["image captioning", "Task"], ["VQA", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "208548469", "sentence": "The authors of Vinyals et al ( 2 0 1 5 ) exploit a convolutional neural networks model to extract the high - level image features , and then give an LSTM unit these features as the first input .", "ner": [["convolutional neural networks", "Method"], ["LSTM", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "208548469", "sentence": "Also , although BLEU is commonly used to evaluate the result of the image captioning task , it is n't the most proper metric to evaluate the quality of the image captioning result because of its innate property .", "ner": [["BLEU", "Method"], ["image captioning", "Task"], ["image captioning", "Task"]], "rel": [["BLEU", "Used-For", "image captioning"]], "rel_plus": [["BLEU:Method", "Used-For", "image captioning:Task"]]}
{"doc_id": "208548469", "sentence": "In VQA , we have two types of inputs with different modalities including the question sentence and image , so VQA is a multimodal task .", "ner": [["VQA", "Task"], ["VQA", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "208548469", "sentence": "The authors of Kiros et al ( 2 0 1 4 ) ; Lin et al ( 2 0 1 5 ) have shown that the bilinear interaction between two embedding spaces is very successful in deep learning for fine - grained classification and multimodal language modeling .", "ner": [["deep learning", "Method"], ["fine - grained classification", "Task"], ["multimodal language modeling", "Task"]], "rel": [["deep learning", "Used-For", "fine - grained classification"], ["deep learning", "Used-For", "multimodal language modeling"]], "rel_plus": [["deep learning:Method", "Used-For", "fine - grained classification:Task"], ["deep learning:Method", "Used-For", "multimodal language modeling:Task"]]}
{"doc_id": "208548469", "sentence": "The authors of Kim et al ( 2 0 1 7 ) propose a tensor - based method , Multimodal Low - rank Bi - linear ( MLB ) pooling , to parameterize the full bilinear interactions between image and question sentence embedding spaces .", "ner": [["Multimodal Low - rank Bi - linear", "Method"], ["MLB", "Method"], ["sentence embedding", "Task"]], "rel": [["MLB", "Synonym-Of", "Multimodal Low - rank Bi - linear"]], "rel_plus": [["MLB:Method", "Synonym-Of", "Multimodal Low - rank Bi - linear:Method"]]}
{"doc_id": "208548469", "sentence": "In Ren et al ( 2 0 1 5 a ) , the authors exploit Recurrent Neural Networks ( RNN ) and Convolutional Neural Networks ( CNN ) to build a question generation algorithm , but it sometimes generates questions with invalid grammar .", "ner": [["Recurrent Neural Networks", "Method"], ["RNN", "Method"], ["Convolutional Neural Networks", "Method"], ["CNN", "Method"], ["question generation", "Task"]], "rel": [["RNN", "Synonym-Of", "Recurrent Neural Networks"], ["CNN", "Synonym-Of", "Convolutional Neural Networks"], ["Recurrent Neural Networks", "Used-For", "question generation"], ["Convolutional Neural Networks", "Used-For", "question generation"]], "rel_plus": [["RNN:Method", "Synonym-Of", "Recurrent Neural Networks:Method"], ["CNN:Method", "Synonym-Of", "Convolutional Neural Networks:Method"], ["Recurrent Neural Networks:Method", "Used-For", "question generation:Task"], ["Convolutional Neural Networks:Method", "Used-For", "question generation:Task"]]}
{"doc_id": "208548469", "sentence": "The authors of ; Gao et al ( 2 0 1 5 ) ; Malinowski et al ( 2 0 1 7 ) exploit RNN to combine the word and image features for the VQA task .", "ner": [["RNN", "Method"], ["VQA", "Task"]], "rel": [["RNN", "Used-For", "VQA"]], "rel_plus": [["RNN:Method", "Used-For", "VQA:Task"]]}
{"doc_id": "208548469", "sentence": "Gated Recurrent Unit ( GRU ) Chung et al ( 2 0 1 4 ) is another variant of RNN , and the authors of Noh et al ( 2 0 1 6 b ) use it to encode an input question .", "ner": [["Gated Recurrent Unit", "Method"], ["GRU", "Method"], ["RNN", "Method"]], "rel": [["GRU", "Synonym-Of", "Gated Recurrent Unit"], ["Gated Recurrent Unit", "SubClass-Of", "RNN"]], "rel_plus": [["GRU:Method", "Synonym-Of", "Gated Recurrent Unit:Method"], ["Gated Recurrent Unit:Method", "SubClass-Of", "RNN:Method"]]}
{"doc_id": "208548469", "sentence": "To the best of our knowledge , DAQUAR ( DAtaset for QUestion Answering on Real - world images ) dataset Malinowski and Fritz ( 2 0 1 4 a ) is the first proposed dataset , which contains about 1 2 . 5 thousand manually annotated question - answer pairs on about 1 4 4 9 indoor scenes Silberman et al ( 2 0 1 2 ) .", "ner": [["DAQUAR", "Dataset"], ["DAtaset for QUestion Answering on Real - world images", "Dataset"]], "rel": [["DAQUAR", "Synonym-Of", "DAtaset for QUestion Answering on Real - world images"]], "rel_plus": [["DAQUAR:Dataset", "Synonym-Of", "DAtaset for QUestion Answering on Real - world images:Dataset"]]}
{"doc_id": "208548469", "sentence": "After the introduction of DAQUAR , three other VQA datasets based on MS - COCO Lin et al ( 2 0 1 4 ) have been proposed , namely Ren et al ( 2 0 1 5 b ) ; Antol et al ( 2 0 1 5 ) ; Gao et al ( 2 0 1 5 ) .", "ner": [["DAQUAR", "Dataset"], ["VQA", "Task"], ["MS - COCO", "Dataset"]], "rel": [["DAQUAR", "Benchmark-For", "VQA"]], "rel_plus": [["DAQUAR:Dataset", "Benchmark-For", "VQA:Task"]]}
{"doc_id": "208548469", "sentence": "The VQA test set answers are not released because of the VQA challenge workshop .", "ner": [["VQA test set", "Dataset"], ["VQA", "Task"]], "rel": [["VQA test set", "Benchmark-For", "VQA"]], "rel_plus": [["VQA test set:Dataset", "Benchmark-For", "VQA:Task"]]}
{"doc_id": "208548469", "sentence": "In Yu et al ( 2 0 1 5 ) , the authors try to simplify the evaluation of the performance of VQA models by introducing Visual Madlibs , a multiple choice question answering ( QA ) by filling the blanks task .", "ner": [["VQA", "Task"], ["Visual Madlibs", "Dataset"], ["question answering", "Task"], ["QA", "Task"]], "rel": [["Visual Madlibs", "Benchmark-For", "VQA"], ["QA", "Synonym-Of", "question answering"], ["Visual Madlibs", "Benchmark-For", "question answering"]], "rel_plus": [["Visual Madlibs:Dataset", "Benchmark-For", "VQA:Task"], ["QA:Task", "Synonym-Of", "question answering:Task"], ["Visual Madlibs:Dataset", "Benchmark-For", "question answering:Task"]]}
{"doc_id": "208548469", "sentence": "The authors of the Visual 7 W dataset Zhu et al ( 2 0 1 6 ) have built question - answer pairs based on the Visual Genome dataset Krishna et al ( 2 0 1 7 ) , and it contains around 3 3 0 0 0 0 natural language questions .", "ner": [["Visual 7 W", "Dataset"], ["Visual Genome", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "208548469", "sentence": "In contrast to the other datasets such as VQA or DAQUAR , the Visual Genome dataset focuses on the so - called six Ws , namely what , where , when , who , why , and how , which can be answered with a text - based sentence .", "ner": [["VQA", "Dataset"], ["DAQUAR", "Dataset"], ["Visual Genome", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "208548469", "sentence": "Then , the Visual 7 W contains multiple - choice answers similar to Visual Madlibs Yu et al ( 2 0 1 5 ) .", "ner": [["Visual 7 W", "Dataset"], ["Visual Madlibs", "Dataset"]], "rel": [["Visual 7 W", "Compare-With", "Visual Madlibs"]], "rel_plus": [["Visual 7 W:Dataset", "Compare-With", "Visual Madlibs:Dataset"]]}
{"doc_id": "208548469", "sentence": "In Nag Chowdhury et al ( 2 0 1 6 ) , the authors have proposed Xplore - M - Ego , which is a dataset of images with natural language queries , a media retrieval system , and collective memories .", "ner": [["Xplore - M - Ego", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "208548469", "sentence": "There is another task , called video question answering , which is related to VQA .", "ner": [["video question answering", "Task"], ["VQA", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "208548469", "sentence": "In this work , we propose GBQD and YNBQD robustness - based datasets .", "ner": [["GBQD", "Dataset"], ["YNBQD", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "208548469", "sentence": "The VQA module contains the model we want to do robustness analysis on , while the Noise Generator utilizes eight ranking methods , namely BLEU - 1 , BLEU - 2 , BLEU - 3 , BLEU - 4 , ROUGE , CIDEr , ME - TEOR , and our proposed LASSO ranking method , to generate noise for a given main question .", "ner": [["VQA", "Task"], ["Noise Generator", "Method"], ["ranking methods", "Method"], ["BLEU - 1", "Method"], ["BLEU - 2", "Method"], ["BLEU - 3", "Method"], ["BLEU - 4", "Method"], ["ROUGE", "Method"], ["CIDEr", "Method"], ["ME - TEOR", "Method"], ["LASSO ranking method", "Method"]], "rel": [["ranking methods", "Part-Of", "Noise Generator"], ["BLEU - 1", "SubClass-Of", "ranking methods"], ["BLEU - 2", "SubClass-Of", "ranking methods"], ["BLEU - 3", "SubClass-Of", "ranking methods"], ["BLEU - 4", "SubClass-Of", "ranking methods"], ["ROUGE", "SubClass-Of", "ranking methods"], ["CIDEr", "SubClass-Of", "ranking methods"], ["ME - TEOR", "SubClass-Of", "ranking methods"], ["LASSO ranking method", "SubClass-Of", "ranking methods"]], "rel_plus": [["ranking methods:Method", "Part-Of", "Noise Generator:Method"], ["BLEU - 1:Method", "SubClass-Of", "ranking methods:Method"], ["BLEU - 2:Method", "SubClass-Of", "ranking methods:Method"], ["BLEU - 3:Method", "SubClass-Of", "ranking methods:Method"], ["BLEU - 4:Method", "SubClass-Of", "ranking methods:Method"], ["ROUGE:Method", "SubClass-Of", "ranking methods:Method"], ["CIDEr:Method", "SubClass-Of", "ranking methods:Method"], ["ME - TEOR:Method", "SubClass-Of", "ranking methods:Method"], ["LASSO ranking method:Method", "SubClass-Of", "ranking methods:Method"]]}
{"doc_id": "208548469", "sentence": "Word 2 Vec Mikolov et al ( 2 0 1 3 ) , GloVe Pennington et al ( 2 0 1 4 ) and Skip - thoughts Kiros et al ( 2 0 1 5 ) are popular text encoders .", "ner": [["Word 2 Vec", "Method"], ["GloVe", "Method"], ["text encoders", "Method"]], "rel": [["GloVe", "SubClass-Of", "text encoders"], ["Word 2 Vec", "SubClass-Of", "text encoders"]], "rel_plus": [["GloVe:Method", "SubClass-Of", "text encoders:Method"], ["Word 2 Vec:Method", "SubClass-Of", "text encoders:Method"]]}
{"doc_id": "208548469", "sentence": "The Skip - thoughts model exploits an RNN encoder with GRU Chung et al ( 2 0 1 4 ) activations , which maps an English sentence , i.e. , q i , into a feature vector v \u2208 R 4 8 0 0 .", "ner": [["Skip - thoughts model", "Method"], ["RNN encoder", "Method"], ["GRU", "Method"]], "rel": [["RNN encoder", "Used-For", "Skip - thoughts model"], ["GRU", "Part-Of", "RNN encoder"]], "rel_plus": [["RNN encoder:Method", "Used-For", "Skip - thoughts model:Method"], ["GRU:Method", "Part-Of", "RNN encoder:Method"]]}
{"doc_id": "208548469", "sentence": "As we have mentioned in the Introduction , the existing textual similarity measures , such as BLEU , CIDEr , METEOR , and ROUGE , can not effectively capture the semantic similarity .", "ner": [["textual similarity measures", "Method"], ["BLEU", "Method"], ["CIDEr", "Method"], ["METEOR", "Method"], ["ROUGE", "Method"], ["semantic similarity", "Task"]], "rel": [["BLEU", "SubClass-Of", "textual similarity measures"], ["CIDEr", "SubClass-Of", "textual similarity measures"], ["METEOR", "SubClass-Of", "textual similarity measures"], ["ROUGE", "SubClass-Of", "textual similarity measures"]], "rel_plus": [["BLEU:Method", "SubClass-Of", "textual similarity measures:Method"], ["CIDEr:Method", "SubClass-Of", "textual similarity measures:Method"], ["METEOR:Method", "SubClass-Of", "textual similarity measures:Method"], ["ROUGE:Method", "SubClass-Of", "textual similarity measures:Method"]]}
{"doc_id": "208548469", "sentence": "To develop our basic question dataset ( BQD ) , we combine the unique questions in the training and validation datasets of the most popular VQA dataset Antol et al ( 2 0 1 5 ) and we use the testing dataset as our main question candidates .", "ner": [["basic question dataset", "Dataset"], ["BQD", "Dataset"], ["VQA dataset", "Dataset"]], "rel": [["BQD", "Synonym-Of", "basic question dataset"]], "rel_plus": [["BQD:Dataset", "Synonym-Of", "basic question dataset:Dataset"]]}
{"doc_id": "208548469", "sentence": "Also , we need to do \" question sentences preprocessing \" , in particular , making sure that none of the main questions is contained in our basic question dataset , as otherwise , LASSO modelling can not give a useful ranking .", "ner": [["basic question dataset", "Dataset"], ["LASSO", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "208548469", "sentence": "Additionally , because most of the VQA models have the highest accuracy performance in answering yes/no questions , we argue that yes/no questions are the simplest questions for VQA models in the sense of accuracy .", "ner": [["VQA", "Task"], ["VQA", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "208548469", "sentence": "Hence , we also create a Yes/No Basic Question dataset based on the aforementioned basic question generation approach .", "ner": [["Yes/No Basic Question dataset", "Dataset"], ["question generation", "Task"]], "rel": [["Yes/No Basic Question dataset", "Benchmark-For", "question generation"]], "rel_plus": [["Yes/No Basic Question dataset:Dataset", "Benchmark-For", "question generation:Task"]]}
{"doc_id": "208548469", "sentence": "In our work , based on the LASSO - based ranking method , we propose two large - scale basic question datasets , General Basic Question Dataset and Yes/No Basic Question Dataset .", "ner": [["LASSO - based ranking", "Method"], ["basic question datasets", "Dataset"], ["General Basic Question", "Dataset"], ["Yes/No Basic Question Dataset", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "208548469", "sentence": "The proposed General and Yes/No BQ datasets , with the format { Image , M Q , 2 1 ( BQ + corresponding similarity score ) } , contain 8 1 , 4 3 4 images from the testing images of MS COCO dataset Lin et al ( 2 0 1 4 ) and 2 4 4 , 3 0 2 main questions from the testing questions of VQA dataset ( open - ended task ) Antol et al ( 2 0 1 5 ) .", "ner": [["General and Yes/No BQ datasets", "Dataset"], ["MS COCO", "Dataset"], ["VQA dataset", "Dataset"]], "rel": [["MS COCO", "SubClass-Of", "General and Yes/No BQ datasets"], ["VQA dataset", "SubClass-Of", "General and Yes/No BQ datasets"]], "rel_plus": [["MS COCO:Dataset", "SubClass-Of", "General and Yes/No BQ datasets:Dataset"], ["VQA dataset:Dataset", "SubClass-Of", "General and Yes/No BQ datasets:Dataset"]]}
{"doc_id": "208548469", "sentence": "Furthermore , our General and Yes/No basic questions are extracted from the validation and training questions of VQA dataset ( open - ended task ) and the corresponding similarity scores of General and Yes/No BQ are generated by our LASSO ranking approach .", "ner": [["General and Yes/No basic questions", "Dataset"], ["VQA dataset", "Dataset"], ["General and Yes/No BQ", "Dataset"], ["LASSO ranking", "Method"]], "rel": [["VQA dataset", "SubClass-Of", "General and Yes/No basic questions"]], "rel_plus": [["VQA dataset:Dataset", "SubClass-Of", "General and Yes/No basic questions:Dataset"]]}
{"doc_id": "208548469", "sentence": "That is to say , in our GBQD and YNBQD , there are 5 , 1 3 0 , 3 4 2 ( General BQ + corresponding similarity score ) tuplets and 5 , 1 3 0 , 3 4 2 ( Yes/No BQ + corresponding similarity score ) tuplets .", "ner": [["GBQD", "Dataset"], ["YNBQD", "Dataset"], ["General BQ", "Dataset"], ["Yes/No BQ", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "208548469", "sentence": "To analyze the robustness of a VQA model , we first measure the accuracy of the model on the clean VQA dataset Antol et al ( 2 0 1 5 ) and we call it Acc vqa .", "ner": [["VQA", "Task"], ["VQA dataset", "Dataset"]], "rel": [["VQA dataset", "Benchmark-For", "VQA"]], "rel_plus": [["VQA dataset:Dataset", "Benchmark-For", "VQA:Task"]]}
{"doc_id": "208548469", "sentence": "We conduct the experiments on GBQD , YNBQD and VQA dataset Antol et al ( 2 0 1 5 ) .", "ner": [["GBQD", "Dataset"], ["YNBQD", "Dataset"], ["VQA dataset", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "208548469", "sentence": "The VQA dataset is based on the MS COCO dataset Lin et al ( 2 0 1 4 ) , and it includes 2 4 8 , 3 4 9 training , 1 2 1 , 5 1 2 validation and 2 4 4 , 3 0 2 testing questions .", "ner": [["VQA dataset", "Dataset"], ["MS COCO", "Dataset"]], "rel": [["MS COCO", "SubClass-Of", "VQA dataset"]], "rel_plus": [["MS COCO:Dataset", "SubClass-Of", "VQA dataset:Dataset"]]}
{"doc_id": "208548469", "sentence": "Regarding the GBQD and YNBQD , please refer to the Details of the Proposed Basic Question Dataset section .", "ner": [["GBQD", "Dataset"], ["YNBQD", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "208548469", "sentence": "Because the similarity scores are negligible after top 2 1 ranked BQs , we only collect the top 2 1 ranked General and Yes/No BQs and put them into our GBQD and YNBQD .", "ner": [["General and Yes/No BQs", "Dataset"], ["GBQD", "Dataset"], ["YNBQD", "Dataset"]], "rel": [["General and Yes/No BQs", "SubClass-Of", "GBQD"], ["General and Yes/No BQs", "SubClass-Of", "YNBQD"]], "rel_plus": [["General and Yes/No BQs:Dataset", "SubClass-Of", "GBQD:Dataset"], ["General and Yes/No BQs:Dataset", "SubClass-Of", "YNBQD:Dataset"]]}
{"doc_id": "208548469", "sentence": "We compare the performance of LASSO - based ranking method with non - LASSObased ranking methods including seven popular sen - tence evaluation metrics Papineni et al ( 2 0 0 2 ) ; Vedantam et al ( 2 0 1 5 ) ; Lin ( 2 0 0 4 ) ; Banerjee and Lavie ( 2 0 0 5 ) , namely BLEU - 1 , BLEU - 2 , BLEU - 3 , BLEU - 4 , ROUGE , CIDEr and METEOR that are also used to measure the similarity score between MQ and BQs .", "ner": [["LASSO - based ranking", "Method"], ["LASSObased ranking methods", "Method"], ["sen - tence evaluation metrics", "Method"], ["BLEU - 1", "Method"], ["BLEU - 2", "Method"], ["BLEU - 3", "Method"], ["BLEU - 4", "Method"], ["ROUGE", "Method"], ["CIDEr", "Method"], ["METEOR", "Method"], ["MQ", "Dataset"], ["BQs", "Dataset"]], "rel": [["LASSO - based ranking", "Compare-With", "LASSObased ranking methods"], ["BLEU - 1", "SubClass-Of", "sen - tence evaluation metrics"], ["BLEU - 2", "SubClass-Of", "sen - tence evaluation metrics"], ["BLEU - 3", "SubClass-Of", "sen - tence evaluation metrics"], ["BLEU - 4", "SubClass-Of", "sen - tence evaluation metrics"], ["ROUGE", "SubClass-Of", "sen - tence evaluation metrics"], ["CIDEr", "SubClass-Of", "sen - tence evaluation metrics"], ["METEOR", "SubClass-Of", "sen - tence evaluation metrics"]], "rel_plus": [["LASSO - based ranking:Method", "Compare-With", "LASSObased ranking methods:Method"], ["BLEU - 1:Method", "SubClass-Of", "sen - tence evaluation metrics:Method"], ["BLEU - 2:Method", "SubClass-Of", "sen - tence evaluation metrics:Method"], ["BLEU - 3:Method", "SubClass-Of", "sen - tence evaluation metrics:Method"], ["BLEU - 4:Method", "SubClass-Of", "sen - tence evaluation metrics:Method"], ["ROUGE:Method", "SubClass-Of", "sen - tence evaluation metrics:Method"], ["CIDEr:Method", "SubClass-Of", "sen - tence evaluation metrics:Method"], ["METEOR:Method", "SubClass-Of", "sen - tence evaluation metrics:Method"]]}
{"doc_id": "208548469", "sentence": "Similar to the setup for building the General Basic Question Dataset ( GBQD ) , we build a general basic question dataset for each metric .", "ner": [["General Basic Question Dataset", "Dataset"], ["GBQD", "Dataset"]], "rel": [["GBQD", "Synonym-Of", "General Basic Question Dataset"]], "rel_plus": [["GBQD:Dataset", "Synonym-Of", "General Basic Question Dataset:Dataset"]]}
{"doc_id": "208548469", "sentence": "Table 2 : The table shows the six state - of - the - art pretrained VQA models evaluation results on the GBQD and VQA dataset . \" - \" indicates the results are not available , \" -std \" represents the accuracy of VQA model evaluated on the complete testing set of GBQD and VQA dataset and \" -dev \" indicates the accuracy of VQA model evaluated on the partial testing set of GBQD and VQA dataset .", "ner": [["pretrained VQA models", "Method"], ["GBQD", "Dataset"], ["VQA dataset", "Dataset"], ["VQA model", "Method"], ["GBQD", "Dataset"], ["VQA dataset", "Dataset"], ["VQA model", "Method"], ["GBQD", "Dataset"], ["VQA dataset", "Dataset"]], "rel": [["pretrained VQA models", "Evaluated-With", "GBQD"], ["pretrained VQA models", "Evaluated-With", "VQA dataset"], ["VQA model", "Evaluated-With", "GBQD"], ["VQA model", "Evaluated-With", "VQA dataset"], ["VQA model", "Evaluated-With", "GBQD"], ["VQA model", "Evaluated-With", "VQA dataset"]], "rel_plus": [["pretrained VQA models:Method", "Evaluated-With", "GBQD:Dataset"], ["pretrained VQA models:Method", "Evaluated-With", "VQA dataset:Dataset"], ["VQA model:Method", "Evaluated-With", "GBQD:Dataset"], ["VQA model:Method", "Evaluated-With", "VQA dataset:Dataset"], ["VQA model:Method", "Evaluated-With", "GBQD:Dataset"], ["VQA model:Method", "Evaluated-With", "VQA dataset:Dataset"]]}
{"doc_id": "208548469", "sentence": "Note that when we replace the GBQD by YNBQD and do the same experiment ( refer to Figure 5 -(b) - 1 and Figure 5 -(b) - 2 ) , the trends are similar to those in GBQD .", "ner": [["GBQD", "Dataset"], ["YNBQD", "Dataset"], ["GBQD", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "208548469", "sentence": "However , based on Figure 6 , we discover that the accuracy of these 7 similarity met - rics , { ( BLEU 1 ... 4 , ROU GE , CIDEr , M ET EOR ) } , are less monotonous and much more random from the first partition to the seventh partition .", "ner": [["similarity met - rics", "Method"], ["BLEU", "Method"], ["ROU GE", "Method"], ["CIDEr", "Method"], ["M ET EOR", "Method"]], "rel": [["BLEU", "SubClass-Of", "similarity met - rics"], ["ROU GE", "SubClass-Of", "similarity met - rics"], ["CIDEr", "SubClass-Of", "similarity met - rics"], ["M ET EOR", "SubClass-Of", "similarity met - rics"]], "rel_plus": [["BLEU:Method", "SubClass-Of", "similarity met - rics:Method"], ["ROU GE:Method", "SubClass-Of", "similarity met - rics:Method"], ["CIDEr:Method", "SubClass-Of", "similarity met - rics:Method"], ["M ET EOR:Method", "SubClass-Of", "similarity met - rics:Method"]]}
{"doc_id": "208548469", "sentence": "According to the above , we see that the rankings by these 7 sentence similarity metrics are not effective in this context . ( ii ) Which VQA model is the most robust ?", "ner": [["sentence similarity metrics", "Method"], ["VQA model", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "208548469", "sentence": "Referring to Table 4 , HAV , HAR , MUA and MLB are attentionbased models whereas LQI and MU are not .", "ner": [["HAV", "Method"], ["HAR", "Method"], ["MUA", "Method"], ["MLB", "Method"], ["LQI", "Method"], ["MU", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "208548469", "sentence": "However , when we consider MU and MUA in Table 4 ( R score 2 ) , the non - attention - based model ( MU ) is more robust than the attention - based model ( MUA ) .", "ner": [["MU", "Method"], ["MUA", "Method"], ["MU", "Method"], ["MUA", "Method"]], "rel": [["MU", "Compare-With", "MUA"]], "rel_plus": [["MU:Method", "Compare-With", "MUA:Method"]]}
{"doc_id": "208548469", "sentence": "Note that the difference between MU and MUA is only the attention mechanism .", "ner": [["MU", "Method"], ["MUA", "Method"]], "rel": [["MU", "Compare-With", "MUA"]], "rel_plus": [["MU:Method", "Compare-With", "MUA:Method"]]}
{"doc_id": "208548469", "sentence": "Yet , in Table 4 ( R score 1 ) , MUA is more robust than MU .", "ner": [["MUA", "Method"], ["MU", "Method"]], "rel": [["MUA", "Compare-With", "MU"]], "rel_plus": [["MUA:Method", "Compare-With", "MU:Method"]]}
{"doc_id": "208548469", "sentence": "Finally , based on Table 4 , we conclude that HieCoAtt Lu et al ( 2 0 1 6 ) is the most robust VQA model .", "ner": [["HieCoAtt", "Method"], ["VQA model", "Method"]], "rel": [["HieCoAtt", "SubClass-Of", "VQA model"]], "rel_plus": [["HieCoAtt:Method", "SubClass-Of", "VQA model:Method"]]}
{"doc_id": "208548469", "sentence": "Since the HieCoAtt model with co - attention mechanism which repeatedly exploits the text and image information to guide each other , it makes VQA models more robust Lu et al ( 2 0 1 6 ) ; Huang et al ( 2 0 1 9 ) .", "ner": [["HieCoAtt", "Method"], ["co - attention mechanism", "Method"], ["VQA models", "Method"]], "rel": [["co - attention mechanism", "Part-Of", "HieCoAtt"], ["HieCoAtt", "SubClass-Of", "VQA models"]], "rel_plus": [["co - attention mechanism:Method", "Part-Of", "HieCoAtt:Method"], ["HieCoAtt:Method", "SubClass-Of", "VQA models:Method"]]}
{"doc_id": "208548469", "sentence": "Based on our experimental result , we know that HieCoAtt is the most robust VQA model , and this ( 2 0 1 5 ) datasets .", "ner": [["HieCoAtt", "Method"], ["VQA model", "Method"]], "rel": [["HieCoAtt", "SubClass-Of", "VQA model"]], "rel_plus": [["HieCoAtt:Method", "SubClass-Of", "VQA model:Method"]]}
{"doc_id": "208548469", "sentence": "These results are based on our proposed LASSO BQ ranking method .", "ner": [["LASSO BQ ranking", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "208548469", "sentence": "Figure 5 in our paper , our LASSO ranking method performance is better than those seven ranking methods . motivates us to conduct the extended experiments for this model . ( iii ) Can basic questions directly help the accuracy of the HieCoAtt model ?", "ner": [["LASSO ranking method", "Method"], ["HieCoAtt", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "208548469", "sentence": "If we do not have the step of question sentences preprocessing , the LASSO ranking method will generate some random ranking result .", "ner": [["LASSO ranking method", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "208548469", "sentence": "For convenience , we take the same HieCoAtt VQA model to demonstrate what the random ranking is .", "ner": [["HieCoAtt VQA model", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "208548469", "sentence": "To compare with our proposed LASSO basic question ranking method , we also conduct the basic question ranking experiments using the seven aforementioned text similarity metrics on the same basic question candidate dataset .", "ner": [["LASSO basic question ranking method", "Method"], ["question ranking", "Task"]], "rel": [["LASSO basic question ranking method", "Used-For", "question ranking"]], "rel_plus": [["LASSO basic question ranking method:Method", "Used-For", "question ranking:Task"]]}
{"doc_id": "208548469", "sentence": "As for our LASSO ranking method , the ranking performance is quite effective , despite its simplicity .", "ner": [["LASSO ranking method", "Method"], ["ranking", "Task"]], "rel": [["LASSO ranking method", "Used-For", "ranking"]], "rel_plus": [["LASSO ranking method:Method", "Used-For", "ranking:Task"]]}
{"doc_id": "208548469", "sentence": "Note that , in practice , we will directly use our proposed datasets to test the robustness of VQA models without running the LASSO ranking method again , so the computational complexity of LASSO ranking method is not an issue in this case . ( vi ) Is the ranking in semantic meaning effective ?", "ner": [["VQA models", "Method"], ["LASSO ranking method", "Method"], ["LASSO ranking method", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "208548469", "sentence": "In the LASSO BQ ranking method , the semantic meaning of a question can not be ranked very accurately but it still works quite well .", "ner": [["LASSO BQ ranking", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "208548469", "sentence": "This is primarily due to the Antol et al ( 2 0 1 5 ) without question sentences preprocessing . \" - \" indicates the results are not available , \" -std \" means that the VQA model is evaluated by the complete testing set of BQD and VQA dataset , and \" -dev \" means that the VQA model is evaluated by the partial testing set of BQD and VQA dataset .", "ner": [["VQA model", "Method"], ["BQD", "Dataset"], ["VQA dataset", "Dataset"], ["VQA model", "Method"], ["BQD", "Dataset"], ["VQA dataset", "Dataset"]], "rel": [["VQA model", "Evaluated-With", "BQD"], ["VQA model", "Evaluated-With", "VQA dataset"], ["VQA model", "Evaluated-With", "BQD"], ["VQA model", "Evaluated-With", "VQA dataset"]], "rel_plus": [["VQA model:Method", "Evaluated-With", "BQD:Dataset"], ["VQA model:Method", "Evaluated-With", "VQA dataset:Dataset"], ["VQA model:Method", "Evaluated-With", "BQD:Dataset"], ["VQA model:Method", "Evaluated-With", "VQA dataset:Dataset"]]}
{"doc_id": "208548469", "sentence": "We believe that if more semantic encoders are developed in the future , the LASSO ranking method can readily make use of them to produce more semantically driven ranking .", "ner": [["LASSO ranking method", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "208548469", "sentence": "Although the semantic meaning ranking by LASSO ranking method is not very accurate , it is still acceptable .", "ner": [["semantic meaning ranking", "Task"], ["LASSO ranking method", "Method"]], "rel": [["LASSO ranking method", "Used-For", "semantic meaning ranking"]], "rel_plus": [["LASSO ranking method:Method", "Used-For", "semantic meaning ranking:Task"]]}
{"doc_id": "208548469", "sentence": "We provide some BQ ranking results using our LASSO ranking method in Figure 4 and Table 1 . ( vii ) What affects the quality of BQs ?", "ner": [["BQ ranking", "Task"], ["LASSO ranking method", "Method"], ["BQs", "Dataset"]], "rel": [["LASSO ranking method", "Used-For", "BQ ranking"]], "rel_plus": [["LASSO ranking method:Method", "Used-For", "BQ ranking:Task"]]}
{"doc_id": "208548469", "sentence": "We provide some ranking examples based on LASSO ranking method in Table 1 to show the quality of BQs when \u03bb = 1 0 \u2212 6 . ( viii ) Extended experiments on YNBQD dataset .", "ner": [["LASSO ranking method", "Method"], ["BQs", "Dataset"], ["YNBQD", "Dataset"]], "rel": [["LASSO ranking method", "Used-For", "BQs"]], "rel_plus": [["LASSO ranking method:Method", "Used-For", "BQs:Dataset"]]}
{"doc_id": "208548469", "sentence": "Although we have done the basic question ranking experiments by the seven different text similarity metrics , BLEU 1 ... 4 , ROU GE , CIDEr , and M ET EOR , on GBQD , we have n't done such ranking experiments by those metrics on YNBQD .", "ner": [["question ranking", "Task"], ["text similarity metrics", "Method"], ["BLEU 1 ... 4", "Method"], ["ROU GE", "Method"], ["CIDEr", "Method"], ["M ET EOR", "Method"], ["GBQD", "Dataset"], ["YNBQD", "Dataset"]], "rel": [["BLEU 1 ... 4", "SubClass-Of", "text similarity metrics"], ["ROU GE", "SubClass-Of", "text similarity metrics"], ["CIDEr", "SubClass-Of", "text similarity metrics"], ["M ET EOR", "SubClass-Of", "text similarity metrics"]], "rel_plus": [["BLEU 1 ... 4:Method", "SubClass-Of", "text similarity metrics:Method"], ["ROU GE:Method", "SubClass-Of", "text similarity metrics:Method"], ["CIDEr:Method", "SubClass-Of", "text similarity metrics:Method"], ["M ET EOR:Method", "SubClass-Of", "text similarity metrics:Method"]]}
{"doc_id": "208548469", "sentence": "Note that in Figure   9 -(a ) , the grey shade denotes BLEU - 1 , blue shade denotes BLEU - 2 , orange shade denotes BLEU - 3 , purple shade denotes BLEU - 4 and green shade denotes ROUGE .", "ner": [["BLEU - 1", "Method"], ["BLEU - 2", "Method"], ["BLEU - 3", "Method"], ["BLEU - 4", "Method"], ["ROUGE", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "208548469", "sentence": "CIDEr and METEOR in Figure 9 -(b ) and Figure 9 -(c ) , respectively .", "ner": [["CIDEr", "Method"], ["METEOR", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "208548469", "sentence": "Based on Figure 9 , Figure 6 , and Figure   5 , we conclude that the proposed LASSO ranking method performance is better than those seven ranking methods on both YNBQD and GBQD datasets .", "ner": [["LASSO ranking method", "Method"], ["YNBQD", "Dataset"], ["GBQD", "Dataset"]], "rel": [["LASSO ranking method", "Evaluated-With", "YNBQD"], ["LASSO ranking method", "Evaluated-With", "GBQD"]], "rel_plus": [["LASSO ranking method:Method", "Evaluated-With", "YNBQD:Dataset"], ["LASSO ranking method:Method", "Evaluated-With", "GBQD:Dataset"]]}
{"doc_id": "208548469", "sentence": "In this section , we present state - of - the - art VQA models among our six tested VQA models , Antol et al ( 2 0 1 5 ) ; Lu et al ( 2 0 1 6 ) ; Ben - younes et al ( 2 0 1 7 ) ; Kim et al ( 2 0 1 7 ) , in different senses .", "ner": [["VQA models", "Method"], ["VQA models", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "208548469", "sentence": "According to Table 4 , we observe that \" HieCoAtt ( Alt , VGG 1 9 ) \" model has the highest R score 1 , 0. 4 8 .", "ner": [["HieCoAtt", "Method"], ["VGG 1 9", "Method"]], "rel": [["VGG 1 9", "Part-Of", "HieCoAtt"]], "rel_plus": [["VGG 1 9:Method", "Part-Of", "HieCoAtt:Method"]]}
{"doc_id": "208548469", "sentence": "Furthermore , \" HieCoAtt ( Alt , Resnet 2 0 0 ) \" has the highest R score 2 , 0. 5 3 .", "ner": [["HieCoAtt", "Method"], ["Resnet 2 0 0", "Method"]], "rel": [["Resnet 2 0 0", "Part-Of", "HieCoAtt"]], "rel_plus": [["Resnet 2 0 0:Method", "Part-Of", "HieCoAtt:Method"]]}
{"doc_id": "208548469", "sentence": "Therefore , for GBQD , \" HieCoAtt ( Alt , VGG 1 9 ) \" model is the state - of - the - art VQA model among our six tested VQA models in the sense of robustness .", "ner": [["GBQD", "Dataset"], ["HieCoAtt", "Method"], ["VGG 1 9", "Method"], ["VQA model", "Method"], ["VQA models", "Method"]], "rel": [["HieCoAtt", "Evaluated-With", "GBQD"], ["VGG 1 9", "Part-Of", "HieCoAtt"], ["HieCoAtt", "SubClass-Of", "VQA model"]], "rel_plus": [["HieCoAtt:Method", "Evaluated-With", "GBQD:Dataset"], ["VGG 1 9:Method", "Part-Of", "HieCoAtt:Method"], ["HieCoAtt:Method", "SubClass-Of", "VQA model:Method"]]}
{"doc_id": "208548469", "sentence": "However , for YNBQD , \" HieCoAtt ( Alt , Resnet 2 0 0 ) \" model is the state - of - the - art VQA model among our six tested VQA models in the sense of robustness .", "ner": [["YNBQD", "Dataset"], ["HieCoAtt", "Method"], ["Resnet 2 0 0", "Method"], ["VQA model", "Method"], ["VQA models", "Method"]], "rel": [["HieCoAtt", "Evaluated-With", "YNBQD"], ["Resnet 2 0 0", "Part-Of", "HieCoAtt"], ["HieCoAtt", "SubClass-Of", "VQA model"]], "rel_plus": [["HieCoAtt:Method", "Evaluated-With", "YNBQD:Dataset"], ["Resnet 2 0 0:Method", "Part-Of", "HieCoAtt:Method"], ["HieCoAtt:Method", "SubClass-Of", "VQA model:Method"]]}
{"doc_id": "208548469", "sentence": "Table 2 , we discover that \" MUTAN with Attention \" model in Table 2 -(d ) has the highest accuracy , 6 5 . 7 7 , and \" LSTM Q+I \" has the lowest accuracy , 5 8 . 1 8 .", "ner": [["MUTAN with Attention", "Method"], ["LSTM Q+I", "Method"]], "rel": [["MUTAN with Attention", "Compare-With", "LSTM Q+I"]], "rel_plus": [["MUTAN with Attention:Method", "Compare-With", "LSTM Q+I:Method"]]}
{"doc_id": "208548469", "sentence": "Therefore , \" MUTAN with Attention \" model is the state - of - the - art VQA model among our six tested VQA models in the sense of accuracy .", "ner": [["MUTAN with Attention", "Method"], ["VQA model", "Method"], ["VQA models", "Method"]], "rel": [["MUTAN with Attention", "SubClass-Of", "VQA model"]], "rel_plus": [["MUTAN with Attention:Method", "SubClass-Of", "VQA model:Method"]]}
{"doc_id": "208548469", "sentence": "Also , these results imply that the attention - based VQA model has higher accuracy than the non - attention - based one .   In this work , we propose a novel method comprised of a number of components namely , large - scale General Basic Question Dataset , Yes/No Basic Question Dataset and robustness measure ( R score ) for measuring the robustness of VQA models .", "ner": [["attention - based VQA model", "Method"], ["General Basic Question Dataset", "Dataset"], ["Yes/No Basic Question Dataset", "Dataset"], ["VQA models", "Method"]], "rel": [["VQA models", "Evaluated-With", "General Basic Question Dataset"], ["VQA models", "Evaluated-With", "Yes/No Basic Question Dataset"]], "rel_plus": [["VQA models:Method", "Evaluated-With", "General Basic Question Dataset:Dataset"], ["VQA models:Method", "Evaluated-With", "Yes/No Basic Question Dataset:Dataset"]]}
{"doc_id": "208548469", "sentence": "Moreover , based on our proposed General and Yes/No Basic Question Datasets and R score , we show that our LASSO BQ ranking method has the better ranking performance among most of the popular text evaluation metrics .", "ner": [["General and Yes/No Basic Question", "Dataset"], ["LASSO BQ ranking", "Method"], ["ranking", "Task"]], "rel": [["LASSO BQ ranking", "Evaluated-With", "General and Yes/No Basic Question"], ["LASSO BQ ranking", "Used-For", "ranking"]], "rel_plus": [["LASSO BQ ranking:Method", "Evaluated-With", "General and Yes/No Basic Question:Dataset"], ["LASSO BQ ranking:Method", "Used-For", "ranking:Task"]]}
{"doc_id": "208548469", "sentence": "Finally , we have some new methods to evaluate the robustness of VQA models , so how to build a robust and accurate VQA model will be interesting future work .", "ner": [["VQA models", "Method"], ["VQA model", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "208548469", "sentence": "Table 1 3 : The table shows the six state - of - the - art pretrained VQA models evaluation results on the GBQD and VQA dataset . \" - \" indicates the results are not available , \" -std \" represents the accuracy of VQA model evaluated on the complete testing set of GBQD and VQA dataset and \" -dev \" indicates the accuracy of VQA model evaluated on the partial testing set of GBQD and VQA dataset .", "ner": [["pretrained VQA models", "Method"], ["GBQD", "Dataset"], ["VQA dataset", "Dataset"], ["VQA model", "Method"], ["GBQD", "Dataset"], ["VQA dataset", "Dataset"], ["VQA model", "Method"], ["GBQD", "Dataset"], ["VQA dataset", "Dataset"]], "rel": [["pretrained VQA models", "Evaluated-With", "GBQD"], ["pretrained VQA models", "Evaluated-With", "VQA dataset"], ["VQA model", "Evaluated-With", "GBQD"], ["VQA model", "Evaluated-With", "VQA dataset"], ["VQA model", "Evaluated-With", "GBQD"], ["VQA model", "Evaluated-With", "VQA dataset"]], "rel_plus": [["pretrained VQA models:Method", "Evaluated-With", "GBQD:Dataset"], ["pretrained VQA models:Method", "Evaluated-With", "VQA dataset:Dataset"], ["VQA model:Method", "Evaluated-With", "GBQD:Dataset"], ["VQA model:Method", "Evaluated-With", "VQA dataset:Dataset"], ["VQA model:Method", "Evaluated-With", "GBQD:Dataset"], ["VQA model:Method", "Evaluated-With", "VQA dataset:Dataset"]]}
{"doc_id": "208548469", "sentence": "The table shows the six state - of - the - art pretrained VQA models evaluation results on the GBQD and VQA dataset . \" - \" indicates the results are not available , \" -std \" represents the accuracy of VQA model evaluated on the complete testing set of GBQD and VQA dataset and \" -dev \" indicates the accuracy of VQA model evaluated on the partial testing set of GBQD and VQA dataset .", "ner": [["pretrained VQA models", "Method"], ["GBQD", "Dataset"], ["VQA dataset", "Dataset"], ["VQA model", "Method"], ["GBQD", "Dataset"], ["VQA dataset", "Dataset"], ["VQA model", "Method"], ["GBQD", "Dataset"], ["VQA dataset", "Dataset"]], "rel": [["pretrained VQA models", "Evaluated-With", "GBQD"], ["pretrained VQA models", "Evaluated-With", "VQA dataset"], ["VQA model", "Evaluated-With", "GBQD"], ["VQA model", "Evaluated-With", "VQA dataset"], ["VQA model", "Evaluated-With", "GBQD"], ["VQA model", "Evaluated-With", "VQA dataset"]], "rel_plus": [["pretrained VQA models:Method", "Evaluated-With", "GBQD:Dataset"], ["pretrained VQA models:Method", "Evaluated-With", "VQA dataset:Dataset"], ["VQA model:Method", "Evaluated-With", "GBQD:Dataset"], ["VQA model:Method", "Evaluated-With", "VQA dataset:Dataset"], ["VQA model:Method", "Evaluated-With", "GBQD:Dataset"], ["VQA model:Method", "Evaluated-With", "VQA dataset:Dataset"]]}
{"doc_id": "208548469", "sentence": "Table 2 0 : The table shows the six state - of - the - art pretrained VQA models evaluation results on the YNBQD and VQA dataset . \" - \" indicates the results are not available , \" -std \" represents the accuracy of VQA model evaluated on the complete testing set of YNBQD and VQA dataset and \" -dev \" indicates the accuracy of VQA model evaluated on the partial testing set of YNBQD and VQA dataset .", "ner": [["VQA", "Method"], ["YNBQD", "Dataset"], ["VQA dataset", "Dataset"], ["VQA model", "Method"], ["YNBQD", "Dataset"], ["VQA dataset", "Dataset"], ["VQA model", "Method"], ["YNBQD", "Dataset"], ["VQA dataset", "Dataset"]], "rel": [["VQA", "Evaluated-With", "YNBQD"], ["VQA", "Evaluated-With", "VQA dataset"], ["VQA model", "Evaluated-With", "YNBQD"], ["VQA model", "Evaluated-With", "VQA dataset"], ["VQA model", "Evaluated-With", "YNBQD"], ["VQA model", "Evaluated-With", "VQA dataset"]], "rel_plus": [["VQA:Method", "Evaluated-With", "YNBQD:Dataset"], ["VQA:Method", "Evaluated-With", "VQA dataset:Dataset"], ["VQA model:Method", "Evaluated-With", "YNBQD:Dataset"], ["VQA model:Method", "Evaluated-With", "VQA dataset:Dataset"], ["VQA model:Method", "Evaluated-With", "YNBQD:Dataset"], ["VQA model:Method", "Evaluated-With", "VQA dataset:Dataset"]]}
{"doc_id": "208548469", "sentence": "Table 2 1 : The table shows the six state - of - the - art pretrained VQA models evaluation results on the YNBQD and VQA dataset . \" - \" indicates the results are not available , \" -std \" represents the accuracy of VQA model evaluated on the complete testing set of YNBQD and VQA dataset and \" -dev \" indicates the accuracy of VQA model evaluated on the partial testing set of YNBQD and VQA dataset .", "ner": [["pretrained VQA models", "Method"], ["YNBQD", "Dataset"], ["VQA dataset", "Dataset"], ["VQA model", "Method"], ["YNBQD", "Dataset"], ["VQA dataset", "Dataset"], ["VQA model", "Method"], ["YNBQD", "Dataset"], ["VQA dataset", "Dataset"]], "rel": [["pretrained VQA models", "Evaluated-With", "YNBQD"], ["pretrained VQA models", "Evaluated-With", "VQA dataset"], ["VQA model", "Evaluated-With", "YNBQD"], ["VQA model", "Evaluated-With", "VQA dataset"], ["VQA model", "Evaluated-With", "YNBQD"], ["VQA model", "Evaluated-With", "VQA dataset"]], "rel_plus": [["pretrained VQA models:Method", "Evaluated-With", "YNBQD:Dataset"], ["pretrained VQA models:Method", "Evaluated-With", "VQA dataset:Dataset"], ["VQA model:Method", "Evaluated-With", "YNBQD:Dataset"], ["VQA model:Method", "Evaluated-With", "VQA dataset:Dataset"], ["VQA model:Method", "Evaluated-With", "YNBQD:Dataset"], ["VQA model:Method", "Evaluated-With", "VQA dataset:Dataset"]]}
{"doc_id": "209862890", "sentence": "Existing state of the art neural entity linking models employ attention - based bag - of - words context model and pre - trained entity embeddings bootstrapped from word embeddings to assess topic level context compatibility .", "ner": [["entity embeddings", "Task"], ["word embeddings", "Task"]], "rel": [["word embeddings", "Used-For", "entity embeddings"]], "rel_plus": [["word embeddings:Task", "Used-For", "entity embeddings:Task"]]}
{"doc_id": "209862890", "sentence": "To tackle this problem , we propose to inject latent entity type information into the entity embeddings based on pre - trained BERT .", "ner": [["entity embeddings", "Task"], ["BERT", "Method"]], "rel": [["BERT", "Used-For", "entity embeddings"]], "rel_plus": [["BERT:Method", "Used-For", "entity embeddings:Task"]]}
{"doc_id": "209862890", "sentence": "Our model significantly outperforms the state - of - the - art entity linking models on standard benchmark ( AIDA - CoNLL ) .", "ner": [["entity linking", "Task"], ["AIDA - CoNLL", "Dataset"]], "rel": [["AIDA - CoNLL", "Benchmark-For", "entity linking"]], "rel_plus": [["AIDA - CoNLL:Dataset", "Benchmark-For", "entity linking:Task"]]}
{"doc_id": "209862890", "sentence": "Entity Linking ( EL ) is the task of disambiguating textual mentions to their corresponding entities in a reference knowledge base ( e.g. , Wikipedia ) .", "ner": [["Entity Linking", "Task"], ["EL", "Task"], ["Wikipedia", "Dataset"]], "rel": [["EL", "Synonym-Of", "Entity Linking"]], "rel_plus": [["EL:Task", "Synonym-Of", "Entity Linking:Task"]]}
{"doc_id": "209862890", "sentence": "An accurate entity linking system is crucial for many knowledge related tasks such as question answering ( Yih et al. 2 0 1 5 ) and information extraction ( Hoffmann et al. 2 0 1 1 ) .", "ner": [["entity linking", "Task"], ["question answering", "Task"], ["information extraction", "Task"]], "rel": [["entity linking", "Used-For", "question answering"], ["entity linking", "Used-For", "information extraction"]], "rel_plus": [["entity linking:Task", "Used-For", "question answering:Task"], ["entity linking:Task", "Used-For", "information extraction:Task"]]}
{"doc_id": "209862890", "sentence": "Such state - of - the - art entity linking models ( Ganea and Hofmann 2 0 1 7 ; Le and Titov 2 0 1 8) employ attention - based bag - of - words context model and pre - trained entity embeddings bootstrapped from word embeddings to assess topic level context compatibility .", "ner": [["entity linking models", "Method"], ["attention - based bag - of - words context model", "Method"], ["entity embeddings", "Task"], ["word embeddings", "Task"]], "rel": [["attention - based bag - of - words context model", "Used-For", "entity linking models"], ["word embeddings", "Used-For", "entity embeddings"]], "rel_plus": [["attention - based bag - of - words context model:Method", "Used-For", "entity linking models:Method"], ["word embeddings:Task", "Used-For", "entity embeddings:Task"]]}
{"doc_id": "209862890", "sentence": "Specifically , we apply pre - trained BERT ( Devlin et al. 2 0 1 9 ) to represent the entity context and build a shared entity representation by aggregating all the entity contexts linking to the same entity via average pooling .", "ner": [["BERT", "Method"], ["average pooling", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "209862890", "sentence": "This can leverage both the pre - trained entity embeddings from BERT and the domain adaption capability of BERT via fine - tuning .", "ner": [["entity embeddings", "Task"], ["BERT", "Method"], ["domain adaption", "Method"], ["BERT", "Method"]], "rel": [["BERT", "Used-For", "entity embeddings"], ["BERT", "Used-For", "domain adaption"]], "rel_plus": [["BERT:Method", "Used-For", "entity embeddings:Task"], ["BERT:Method", "Used-For", "domain adaption:Method"]]}
{"doc_id": "209862890", "sentence": "We conduct entity linking experiments on standard benchmark datasets : AIDA - CoNLL and five out - domain test sets .", "ner": [["entity linking", "Task"], ["AIDA - CoNLL", "Dataset"]], "rel": [["AIDA - CoNLL", "Benchmark-For", "entity linking"]], "rel_plus": [["AIDA - CoNLL:Dataset", "Benchmark-For", "entity linking:Task"]]}
{"doc_id": "209862890", "sentence": "Our contributions can be summarized as follows . \u2022 We show current state - of - the - art ( SOTA ) neural entity linking models based on attention - based bag - of - words context model often produce type errors and analyze the possible causes . \u2022 We propose a novel entity embedding method based on pre - trained BERT to better capture latent entity type information . \u2022 We integrate a BERT - based entity similarity into the local model of a SOTA model ( Ganea and Hofmann 2 0 1 7 ) . \u2022 We verify the effectiveness of our model on standard benchmark datasets and achieve significant improvement over the baseline .", "ner": [["entity embedding", "Task"], ["BERT", "Method"], ["BERT - based entity similarity", "Method"]], "rel": [["BERT", "Used-For", "entity embedding"]], "rel_plus": [["BERT:Method", "Used-For", "entity embedding:Task"]]}
{"doc_id": "209862890", "sentence": "Due to potentially very large entity space ( e.g. Wikipedia has more than 4 million entities ) , standard entity linking is often divided into two stages : candidate generation which chooses potential candidates C i = ( e i 1 , ... , e ili ) using a heuristic and entity disambiguation which learns to select the best entity from the candidates using a statistical model .", "ner": [["entity linking", "Task"], ["entity disambiguation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "209862890", "sentence": "Next , we introduce the general formulation of entity linking problem with a focus on the well known DeepED model ( Ganea and Hofmann 2 0 1 7 ) .", "ner": [["entity linking", "Task"], ["DeepED", "Method"]], "rel": [["DeepED", "Used-For", "entity linking"]], "rel_plus": [["DeepED:Method", "Used-For", "entity linking:Task"]]}
{"doc_id": "209862890", "sentence": "General Formulation An entity linking model integrating both local and global features can be formulated as a conditional random field .", "ner": [["entity linking", "Task"], ["conditional random field", "Method"]], "rel": [["conditional random field", "Used-For", "entity linking"]], "rel_plus": [["conditional random field:Method", "Used-For", "entity linking:Task"]]}
{"doc_id": "209862890", "sentence": "Our work focuses on improving entity linking by capturing latent entity type information with BERT .", "ner": [["entity linking", "Task"], ["BERT", "Method"]], "rel": [["BERT", "Used-For", "entity linking"]], "rel_plus": [["BERT:Method", "Used-For", "entity linking:Task"]]}
{"doc_id": "209862890", "sentence": "Entity Embedding The entity linking task is essentially a zero - shot task where the answer of test cases may not exist in the training data . 3 So we need to build a shared entity embedding space for all entities which allows neural entity linking models to generalize to both seen and unseen entities during test time .", "ner": [["Entity Embedding", "Task"], ["entity linking", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "209862890", "sentence": "Type Information Previous work attempt to integrate type information into the entity linking task mostly by jointly modeling named entity recognition and entity linking .", "ner": [["entity linking", "Task"], ["named entity recognition", "Task"], ["entity linking", "Task"]], "rel": [["named entity recognition", "SubTask-Of", "entity linking"], ["entity linking", "SubTask-Of", "entity linking"]], "rel_plus": [["named entity recognition:Task", "SubTask-Of", "entity linking:Task"], ["entity linking:Task", "SubTask-Of", "entity linking:Task"]]}
{"doc_id": "209862890", "sentence": "Specifically , a line of work ( Durrett and Klein 2 0 1 4 ; Luo et al. 2 0 1 5 ; Nguyen , Theobald , and Weikum 2 0 1 6 ) jointly model entity linking and named entity recognition to capture the mutual dependency between them using structured CRF .", "ner": [["entity linking", "Task"], ["named entity recognition", "Task"], ["CRF", "Method"]], "rel": [["CRF", "Used-For", "entity linking"], ["CRF", "Used-For", "named entity recognition"]], "rel_plus": [["CRF:Method", "Used-For", "entity linking:Task"], ["CRF:Method", "Used-For", "named entity recognition:Task"]]}
{"doc_id": "209862890", "sentence": "Applications of BERT Since the advent of the wellknown BERT models ( Devlin et al. 2 0 1 9 ) , it has been applied successfully to and has achieved state - of - the - art performance on many NLP tasks .", "ner": [["BERT", "Method"], ["BERT", "Method"], ["NLP", "Task"]], "rel": [["BERT", "Used-For", "NLP"]], "rel_plus": [["BERT:Method", "Used-For", "NLP:Task"]]}
{"doc_id": "209862890", "sentence": "The main challenges which the entity linking task has over other tasks e.g. sentence classification , named entity recognition , where BERT has been applied are : ( 1 ) a very large label space , i.e. every mention has many target entities and ( 2 ) the zero - shot nature of the entity linking task .", "ner": [["entity linking", "Task"], ["sentence classification", "Task"], ["named entity recognition", "Task"], ["BERT", "Method"], ["entity linking", "Task"]], "rel": [["sentence classification", "SubTask-Of", "entity linking"], ["named entity recognition", "SubTask-Of", "entity linking"], ["BERT", "Used-For", "sentence classification"], ["BERT", "Used-For", "named entity recognition"]], "rel_plus": [["sentence classification:Task", "SubTask-Of", "entity linking:Task"], ["named entity recognition:Task", "SubTask-Of", "entity linking:Task"], ["BERT:Method", "Used-For", "sentence classification:Task"], ["BERT:Method", "Used-For", "named entity recognition:Task"]]}
{"doc_id": "209862890", "sentence": "To tackle this problem , we introduce a novel method to build entity embeddings from BERT by modeling the immediate context of an entity .", "ner": [["entity embeddings", "Task"], ["BERT", "Method"]], "rel": [["BERT", "Used-For", "entity embeddings"]], "rel_plus": [["BERT:Method", "Used-For", "entity embeddings:Task"]]}
{"doc_id": "209862890", "sentence": "Our model consists of two phrases : ( 1 ) Build entity embeddings from BERT ( 2 ) Add a BERT - based entity similarity component to the local model .", "ner": [["entity embeddings", "Task"], ["BERT", "Method"], ["BERT - based entity similarity component", "Method"]], "rel": [["BERT", "Used-For", "entity embeddings"]], "rel_plus": [["BERT:Method", "Used-For", "entity embeddings:Task"]]}
{"doc_id": "209862890", "sentence": "As will be shown in the analysis section , the entity embeddings from BERT better capture entity type information than those from Ganea and Hofmann ( 2 0 1 7 ) .", "ner": [["entity embeddings", "Task"], ["BERT", "Method"]], "rel": [["BERT", "Used-For", "entity embeddings"]], "rel_plus": [["BERT:Method", "Used-For", "entity embeddings:Task"]]}
{"doc_id": "209862890", "sentence": "To capture latent entity type information , we design a BERT - based entity similarity score \u03a8 BERT ( e , c ) .", "ner": [["BERT - based entity similarity score", "Method"], ["\u03a8 BERT ( e , c )", "Method"]], "rel": [["\u03a8 BERT ( e , c )", "Synonym-Of", "BERT - based entity similarity score"]], "rel_plus": [["\u03a8 BERT ( e , c ):Method", "Synonym-Of", "BERT - based entity similarity score:Method"]]}
{"doc_id": "209862890", "sentence": "Finally , as for the local disambiguation model , we integrate the BERT - based entity similarity \u03a8 BERT ( e , c ) with the local context score \u03a8 long ( e , c ) ( defined in Equation 2 ) and the priorp(e|m i ) with two fully connected layers of 1 0 0 hidden units and ReLU non - linearities following the same feature composition methods as Ganea and Hofmann ( 2 0 1 7 ) .", "ner": [["local disambiguation model", "Method"], ["BERT - based entity similarity", "Method"], ["\u03a8 BERT ( e , c )", "Method"], ["local context score", "Method"], ["\u03a8 long ( e , c )", "Method"], ["priorp(e|m i )", "Method"], ["fully connected layers", "Method"], ["ReLU non - linearities", "Method"]], "rel": [["BERT - based entity similarity", "Part-Of", "local disambiguation model"], ["local context score", "Part-Of", "local disambiguation model"], ["priorp(e|m i )", "Part-Of", "local disambiguation model"], ["ReLU non - linearities", "Part-Of", "local disambiguation model"], ["\u03a8 BERT ( e , c )", "Synonym-Of", "BERT - based entity similarity"], ["\u03a8 long ( e , c )", "Synonym-Of", "local context score"], ["fully connected layers", "Part-Of", "priorp(e|m i )"]], "rel_plus": [["BERT - based entity similarity:Method", "Part-Of", "local disambiguation model:Method"], ["local context score:Method", "Part-Of", "local disambiguation model:Method"], ["priorp(e|m i ):Method", "Part-Of", "local disambiguation model:Method"], ["ReLU non - linearities:Method", "Part-Of", "local disambiguation model:Method"], ["\u03a8 BERT ( e , c ):Method", "Synonym-Of", "BERT - based entity similarity:Method"], ["\u03a8 long ( e , c ):Method", "Synonym-Of", "local context score:Method"], ["fully connected layers:Method", "Part-Of", "priorp(e|m i ):Method"]]}
{"doc_id": "209862890", "sentence": "For out - domain setting , we evaluate the model trained with AIDA - CoNLL on five popular out - domain test sets : MSNBC , AQUAINT , ACE 2 0 0 4 datasets cleaned and updated by Guo and Barbosa ( 2 0 1 6 ) and WNED - CWEB ( CWEB ) , WNED - WIKI ( WIKI ) automatically extracted from ClueWeb and Wikipedia ( Guo and Barbosa 2 0 1 6 ) .", "ner": [["AIDA - CoNLL", "Dataset"], ["MSNBC", "Dataset"], ["AQUAINT", "Dataset"], ["ACE 2 0 0 4", "Dataset"], ["WNED - CWEB", "Dataset"], ["CWEB", "Dataset"], ["WNED - WIKI", "Dataset"], ["WIKI", "Dataset"], ["ClueWeb", "Dataset"], ["Wikipedia", "Dataset"]], "rel": [["CWEB", "Synonym-Of", "WNED - CWEB"], ["WIKI", "Synonym-Of", "WNED - WIKI"], ["WNED - CWEB", "SubClass-Of", "ClueWeb"], ["WNED - WIKI", "SubClass-Of", "Wikipedia"]], "rel_plus": [["CWEB:Dataset", "Synonym-Of", "WNED - CWEB:Dataset"], ["WIKI:Dataset", "Synonym-Of", "WNED - WIKI:Dataset"], ["WNED - CWEB:Dataset", "SubClass-Of", "ClueWeb:Dataset"], ["WNED - WIKI:Dataset", "SubClass-Of", "Wikipedia:Dataset"]]}
{"doc_id": "209862890", "sentence": "To verify the contribution of our proposed BERT - based entity embeddings , we also compare with a straightforward baseline which directly replaces the encoder of Ganea and Hofmann ( 2 0 1 7 ) utilizing pre - trained BERT .", "ner": [["BERT", "Method"], ["entity embeddings", "Task"], ["BERT", "Method"]], "rel": [["BERT", "Used-For", "entity embeddings"]], "rel_plus": [["BERT:Method", "Used-For", "entity embeddings:Task"]]}
{"doc_id": "209862890", "sentence": "To do so , we introduce a 7 6 8 \u00d7 3 0 0 dimensional matrix W which projects BERT - based context representation c into Ganea and Hofmann ( 2 0 1 7 ) 's entity embeddings space when calculating the similarity score .", "ner": [["BERT - based context representation", "Method"], ["Ganea and Hofmann ( 2 0 1 7 ) 's entity embeddings", "Method"]], "rel": [["BERT - based context representation", "Part-Of", "Ganea and Hofmann ( 2 0 1 7 ) 's entity embeddings"]], "rel_plus": [["BERT - based context representation:Method", "Part-Of", "Ganea and Hofmann ( 2 0 1 7 ) 's entity embeddings:Method"]]}
{"doc_id": "209862890", "sentence": "For each entity , we randomly sam - 7 https://github.com/dalab/deep - ed/ Methods AIDA - B Local models priorp(e|m ) 7 1 . 9 Lazic et al. ( 2 0 1 5 ) 8 6 . 4 Globerson et al. ( 2 0 1 6 ) 8 7 . 9 Yamada et al. ( 2 0 1 6 ) 8 7 . 2 Ganea and Hofmann ( 2 0 1 7 ) 8 8 . 8 Ganea and Hofmann ( 2 0 1 7 ) ( reproduce ) 8 8 . 7 5 \u00b1 0. 3 0 BERT - Entity - Sim ( local ) 9 0 . 0 6 \u00b1 0. 2 2", "ner": [["Ganea and Hofmann", "Method"], ["BERT - Entity - Sim ( local )", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "209862890", "sentence": "Local & Global models Huang , Heck , and Ji ( 2 0 1 5 ) 8 6 . 6 Ganea et al. ( 2 0 1 6 ) 8 7 . 6 Chisholm and Hachey ( 2 0 1 5 ) 8 8 . 7 Guo and Barbosa ( 2 0 1 6 ) 8 9 . 0 Globerson et al. ( 2 0 1 6 ) 9 1 . 0 Yamada et al. ( 2 0 1 6 ) 9 1 . 5 Ganea and Hofmann ( 2 0 1 7 ) 9 2 . 2 2 \u00b1 0. 1 4 Le and Titov ( 2 0 1 8) 9 3 . ple at most 1 0 0 anchor contexts from Wikipedia 8 to build the entity representation from BERT .", "ner": [["Local & Global models", "Method"], ["Chisholm and Hachey", "Method"], ["Guo and Barbosa", "Method"], ["Ganea and Hofmann", "Method"], ["BERT", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "209862890", "sentence": "We discard any articles appearing in WIKI dataset when building the entity representation from BERT .", "ner": [["WIKI", "Dataset"], ["BERT", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "209862890", "sentence": "We use the PyTorch implementation of pre - trained BERT models 9 and choose the BERT - base - cased version .", "ner": [["BERT", "Method"], ["BERT - base - cased", "Method"]], "rel": [["BERT - base - cased", "SubClass-Of", "BERT"]], "rel_plus": [["BERT - base - cased:Method", "SubClass-Of", "BERT:Method"]]}
{"doc_id": "209862890", "sentence": "Empirically , we found that it is helpful to set parameters in BERT a small initial learning rate and not BERT related parameters a larger initial learning rate to avoid the whole model biasing toward the BERT feature and disregarding other model components .", "ner": [["BERT", "Method"], ["BERT", "Method"], ["BERT", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "209862890", "sentence": "In our experiments , pre - trained BERT model is fine - tuned with initial learning rate 1 0 \u2212 5 whereas not BERT related parameters are trained with 1 0 \u2212 3 .", "ner": [["BERT", "Method"], ["BERT", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "209862890", "sentence": "Table 1 shows the micro F 1 scores on in - domain AIDA - B dataset of the SOTA methods and ours , which all use Wikipedia and YAGO mention - entity index .", "ner": [["AIDA - B", "Dataset"], ["Wikipedia", "Dataset"], ["YAGO", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "209862890", "sentence": "Moreover , BERT+G&Hs embeddings performs significantly worse than the baseline ( Ganea and Hofmann 2 0 1 7 ) and our proposed BERT - Entity - Sim model .", "ner": [["BERT+G&Hs", "Method"], ["BERT - Entity - Sim", "Method"]], "rel": [["BERT+G&Hs", "Compare-With", "BERT - Entity - Sim"]], "rel_plus": [["BERT+G&Hs:Method", "Compare-With", "BERT - Entity - Sim:Method"]]}
{"doc_id": "209862890", "sentence": "Ganea and Hofmanns entity embeddings are bootstrapped from word embeddings which mainly capture topic level entity relatedness , while BERT - based context representation is derived from BERT which naturally captures type information .", "ner": [["Ganea and Hofmanns entity embeddings", "Method"], ["word embeddings", "Method"], ["BERT - based context representation", "Method"], ["BERT", "Method"]], "rel": [["Ganea and Hofmanns entity embeddings", "Used-For", "word embeddings"], ["BERT", "Used-For", "BERT - based context representation"]], "rel_plus": [["Ganea and Hofmanns entity embeddings:Method", "Used-For", "word embeddings:Method"], ["BERT:Method", "Used-For", "BERT - based context representation:Method"]]}
{"doc_id": "209862890", "sentence": "We conduct experiment analysis to answer the following questions : \u2022 Do the entity embeddings from BERT better capture latent entity type information than that of Ganea and Hofmann ( 2 0 1 7 ) ? \u2022 Does the proposed model correct the type errors in the baseline ( Ganea and Hofmann 2 0 1 7 ) ? \u2022 Can straightforward integration of state - of - the - art fine grained entity typing systems improve entity linking performance ? \u2022 Can better global model further boost the performance of the proposed model ?", "ner": [["entity embeddings", "Method"], ["BERT", "Method"], ["Ganea and Hofmann", "Method"], ["fine grained entity typing systems", "Method"], ["entity linking", "Task"]], "rel": [["BERT", "Used-For", "entity embeddings"], ["entity embeddings", "Compare-With", "Ganea and Hofmann"], ["fine grained entity typing systems", "Used-For", "entity linking"]], "rel_plus": [["BERT:Method", "Used-For", "entity embeddings:Method"], ["entity embeddings:Method", "Compare-With", "Ganea and Hofmann:Method"], ["fine grained entity typing systems:Method", "Used-For", "entity linking:Task"]]}
{"doc_id": "209862890", "sentence": "Table 5 : Performance of two state - of - the - art fine grained entity typing systems on AIDA - CoNLL development set order to verify our claim that the entity embeddings from BERT better capture entity type information than those from Ganea and Hofmann ( 2 0 1 7 ) , we carry out an entity type prediction task based on its entity embedding .", "ner": [["fine grained entity typing systems", "Method"], ["AIDA - CoNLL", "Dataset"], ["entity embeddings", "Task"], ["BERT", "Method"], ["Ganea and Hofmann", "Method"], ["entity type prediction", "Task"], ["entity embedding", "Task"]], "rel": [["fine grained entity typing systems", "Evaluated-With", "AIDA - CoNLL"], ["BERT", "Used-For", "entity embeddings"], ["BERT", "Compare-With", "Ganea and Hofmann"], ["entity embedding", "Used-For", "entity type prediction"]], "rel_plus": [["fine grained entity typing systems:Method", "Evaluated-With", "AIDA - CoNLL:Dataset"], ["BERT:Method", "Used-For", "entity embeddings:Task"], ["BERT:Method", "Compare-With", "Ganea and Hofmann:Method"], ["entity embedding:Task", "Used-For", "entity type prediction:Task"]]}
{"doc_id": "209862890", "sentence": "For each entity , we obtain its entity types from three typing systems : FIGER ( Ling and Weld 2 0 1 2 ) , BBN ( Weischedel and Brunstein 2 0 0 5 ) and OntoNotes fine ( Gillick et al. 2 0 1 4 ) via the entity type mapping provided by Zhou et al. ( 2 0 1 8) .", "ner": [["FIGER", "Dataset"], ["BBN", "Dataset"], ["OntoNotes", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "209862890", "sentence": "As shown in Table 3 , our proposed entity embedding from BERT significantly outperforms the entity embedding proposed by Ganea and Hofmann ( 2 0 1 7 ) on three typing sys - tems FIGER , BBN and OntoNotes fine .", "ner": [["entity embedding", "Task"], ["BERT", "Method"], ["entity embedding", "Task"], ["Ganea and Hofmann", "Method"], ["FIGER", "Dataset"], ["BBN", "Dataset"], ["OntoNotes", "Dataset"]], "rel": [["BERT", "Used-For", "entity embedding"], ["Ganea and Hofmann", "Used-For", "entity embedding"], ["BERT", "Compare-With", "Ganea and Hofmann"], ["BERT", "Evaluated-With", "FIGER"], ["Ganea and Hofmann", "Evaluated-With", "FIGER"], ["BERT", "Evaluated-With", "BBN"], ["Ganea and Hofmann", "Evaluated-With", "BBN"], ["BERT", "Evaluated-With", "OntoNotes"], ["Ganea and Hofmann", "Evaluated-With", "OntoNotes"]], "rel_plus": [["BERT:Method", "Used-For", "entity embedding:Task"], ["Ganea and Hofmann:Method", "Used-For", "entity embedding:Task"], ["BERT:Method", "Compare-With", "Ganea and Hofmann:Method"], ["BERT:Method", "Evaluated-With", "FIGER:Dataset"], ["Ganea and Hofmann:Method", "Evaluated-With", "FIGER:Dataset"], ["BERT:Method", "Evaluated-With", "BBN:Dataset"], ["Ganea and Hofmann:Method", "Evaluated-With", "BBN:Dataset"], ["BERT:Method", "Evaluated-With", "OntoNotes:Dataset"], ["Ganea and Hofmann:Method", "Evaluated-With", "OntoNotes:Dataset"]]}
{"doc_id": "209862890", "sentence": "In the predict setting , we use two state - of - the - art fine grained entity typing systems : 1 ) Ultrafine ( Choi et al. 2 0 1 8) which predicts types in ultra - fine type sets ; 2 ) ZOE ( Zhou et al. 2 0 1 8 ) which can predict types in FIGER type sets .", "ner": [["fine grained entity typing systems", "Method"], ["Ultrafine", "Method"], ["ZOE", "Method"], ["FIGER", "Dataset"]], "rel": [["Ultrafine", "SubClass-Of", "fine grained entity typing systems"], ["ZOE", "SubClass-Of", "fine grained entity typing systems"], ["Ultrafine", "Evaluated-With", "FIGER"], ["ZOE", "Evaluated-With", "FIGER"]], "rel_plus": [["Ultrafine:Method", "SubClass-Of", "fine grained entity typing systems:Method"], ["ZOE:Method", "SubClass-Of", "fine grained entity typing systems:Method"], ["Ultrafine:Method", "Evaluated-With", "FIGER:Dataset"], ["ZOE:Method", "Evaluated-With", "FIGER:Dataset"]]}
{"doc_id": "209862890", "sentence": "As we can see from both Table 1 and Table 2 , in the oracle setting , the best model outperforms all the state - of - theart entity linking models by a large margin , even surpass Le and Titov ( 2 0 1 8) by 3. 2 8 F 1 points on AIDA - CoNLL test set .", "ner": [["entity linking", "Task"], ["AIDA - CoNLL", "Dataset"]], "rel": [["AIDA - CoNLL", "Benchmark-For", "entity linking"]], "rel_plus": [["AIDA - CoNLL:Dataset", "Benchmark-For", "entity linking:Task"]]}
{"doc_id": "209862890", "sentence": "This result shows that a better type prediction system can further improve upon the state - of - the - state entity linking systems .", "ner": [["type prediction", "Task"], ["entity linking", "Task"]], "rel": [["type prediction", "Used-For", "entity linking"]], "rel_plus": [["type prediction:Task", "Used-For", "entity linking:Task"]]}
{"doc_id": "209862890", "sentence": "To verify this , we measure the performance of the two typing systems on AIDA - CoNLL development set . 1 3 As shown in Table 5 , the ultra - fine entity typing system ( Choi et al. 2 0 1 8) only achieves 2 6 . 5 2 % F 1 mi score while the ZOE system ( Zhou et al. 2 0 1 8 ) achieves 6 6 . 1 2 % F 1 mi score 1 4 which are insufficient to improve state - of - the - art entity linking system with more than 9 2 % F 1 score .", "ner": [["AIDA - CoNLL", "Dataset"], ["ultra - fine entity typing system", "Method"], ["ZOE", "Method"], ["entity linking", "Task"]], "rel": [["ultra - fine entity typing system", "Compare-With", "ZOE"], ["ZOE", "Used-For", "entity linking"], ["ultra - fine entity typing system", "Used-For", "entity linking"]], "rel_plus": [["ultra - fine entity typing system:Method", "Compare-With", "ZOE:Method"], ["ZOE:Method", "Used-For", "entity linking:Task"], ["ultra - fine entity typing system:Method", "Used-For", "entity linking:Task"]]}
{"doc_id": "209862890", "sentence": "In this paper , we propose to improve entity linking by capturing latent entity type information with BERT .", "ner": [["entity linking", "Task"], ["BERT", "Method"]], "rel": [["BERT", "Used-For", "entity linking"]], "rel_plus": [["BERT:Method", "Used-For", "entity linking:Task"]]}
{"doc_id": "209862890", "sentence": "Firstly , we build entity embeddings from BERT by averaging all the context representation extracted from pre - trained BERT .", "ner": [["entity embeddings", "Task"], ["BERT", "Method"], ["BERT", "Method"]], "rel": [["BERT", "Used-For", "entity embeddings"]], "rel_plus": [["BERT:Method", "Used-For", "entity embeddings:Task"]]}
{"doc_id": "209862890", "sentence": "Given an entity e , we firstly retrieve its entity embedding e , then compute the probability for each type in the typeset T : where \u03c3 is the sigmoid function , w j and b are respectively the weight and bias parameter .", "ner": [["entity embedding", "Task"], ["sigmoid", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210861282", "sentence": "We propose UniPose , a unified framework for human pose estimation , based on our \" Waterfall \" Atrous Spatial Pooling architecture , that achieves state - of - art - results on several pose estimation metrics .", "ner": [["UniPose", "Method"], ["human pose estimation", "Task"], ["\" Waterfall \" Atrous Spatial Pooling", "Method"], ["pose estimation", "Task"]], "rel": [["pose estimation", "Used-For", "UniPose"], ["\" Waterfall \" Atrous Spatial Pooling", "Used-For", "UniPose"], ["UniPose", "Used-For", "human pose estimation"]], "rel_plus": [["pose estimation:Task", "Used-For", "UniPose:Method"], ["\" Waterfall \" Atrous Spatial Pooling:Method", "Used-For", "UniPose:Method"], ["UniPose:Method", "Used-For", "human pose estimation:Task"]]}
{"doc_id": "210861282", "sentence": "Current pose estimation methods utilizing standard CNN architectures heavily rely on statistical postprocessing or predefined anchor poses for joint localization .", "ner": [["pose estimation", "Task"], ["CNN", "Method"], ["statistical postprocessing", "Method"], ["predefined anchor poses", "Method"], ["joint localization", "Task"]], "rel": [["CNN", "Used-For", "pose estimation"], ["statistical postprocessing", "Used-For", "pose estimation"], ["predefined anchor poses", "Used-For", "pose estimation"], ["predefined anchor poses", "Used-For", "joint localization"], ["statistical postprocessing", "Used-For", "joint localization"]], "rel_plus": [["CNN:Method", "Used-For", "pose estimation:Task"], ["statistical postprocessing:Method", "Used-For", "pose estimation:Task"], ["predefined anchor poses:Method", "Used-For", "pose estimation:Task"], ["predefined anchor poses:Method", "Used-For", "joint localization:Task"], ["statistical postprocessing:Method", "Used-For", "joint localization:Task"]]}
{"doc_id": "210861282", "sentence": "UniPose incorporates contextual segmentation and joint localization to estimate the human pose in a single stage , with high accuracy , without relying on statistical postprocessing methods .", "ner": [["UniPose", "Method"], ["contextual segmentation", "Task"], ["joint localization", "Task"]], "rel": [["contextual segmentation", "Used-For", "UniPose"], ["joint localization", "Used-For", "UniPose"]], "rel_plus": [["contextual segmentation:Task", "Used-For", "UniPose:Method"], ["joint localization:Task", "Used-For", "UniPose:Method"]]}
{"doc_id": "210861282", "sentence": "Additionally , our method is extended to UniPose - LSTM for multi - frame processing and achieves state - of - the - art results for temporal pose estimation in Video .", "ner": [["UniPose - LSTM", "Method"], ["multi - frame processing", "Task"], ["temporal pose estimation in Video", "Task"]], "rel": [["UniPose - LSTM", "Used-For", "multi - frame processing"], ["UniPose - LSTM", "Used-For", "temporal pose estimation in Video"]], "rel_plus": [["UniPose - LSTM:Method", "Used-For", "multi - frame processing:Task"], ["UniPose - LSTM:Method", "Used-For", "temporal pose estimation in Video:Task"]]}
{"doc_id": "210861282", "sentence": "Our results on multiple datasets demonstrate that UniPose , with a ResNet backbone and Waterfall module , is a robust and efficient architecture for pose estimation obtaining state - of - the - art results in single person pose detection for both single images and videos .", "ner": [["UniPose", "Method"], ["ResNet", "Method"], ["Waterfall", "Method"], ["pose estimation", "Task"], ["person pose detection", "Task"]], "rel": [["ResNet", "Part-Of", "UniPose"], ["Waterfall", "Part-Of", "UniPose"], ["UniPose", "Used-For", "pose estimation"], ["UniPose", "Used-For", "person pose detection"]], "rel_plus": [["ResNet:Method", "Part-Of", "UniPose:Method"], ["Waterfall:Method", "Part-Of", "UniPose:Method"], ["UniPose:Method", "Used-For", "pose estimation:Task"], ["UniPose:Method", "Used-For", "person pose detection:Task"]]}
{"doc_id": "210861282", "sentence": "Human pose estimation is an important task in computer vision with applications in activity recognition [ 5 1 ] , human computer interaction [ 4 1 ] , animation [ 3 ] , gaming [ 3 9 ] , health [ 8 ] , and sports [ 4 8 ] .", "ner": [["Human pose estimation", "Task"], ["computer vision", "Task"], ["activity recognition", "Task"], ["human computer interaction", "Task"]], "rel": [["Human pose estimation", "SubTask-Of", "computer vision"], ["Human pose estimation", "Used-For", "activity recognition"], ["Human pose estimation", "Used-For", "human computer interaction"]], "rel_plus": [["Human pose estimation:Task", "SubTask-Of", "computer vision:Task"], ["Human pose estimation:Task", "Used-For", "activity recognition:Task"], ["Human pose estimation:Task", "Used-For", "human computer interaction:Task"]]}
{"doc_id": "210861282", "sentence": "Motivated by advances in semantic segmentation architectures [ 1 2 ] , [ 5 3 ] , [ 3 3 ] , we propose a unified pose estimation framework , called UniPose , that consists of only one stage and obtains accurate results without postprocessing .", "ner": [["semantic segmentation", "Task"], ["pose estimation", "Task"], ["UniPose", "Method"]], "rel": [["UniPose", "Used-For", "pose estimation"]], "rel_plus": [["UniPose:Method", "Used-For", "pose estimation:Task"]]}
{"doc_id": "210861282", "sentence": "A main component of our architecture is the Waterfall Atrous Spatial Pooling ( WASP ) module which combines the cascaded approach for Atrous Convolution with the larger FOV obtained from parallel configuration from the Atrous Spatial Pyramid Pooling ( ASPP ) module [ 1 1 ] .", "ner": [["Waterfall Atrous Spatial Pooling", "Method"], ["WASP", "Method"], ["Atrous Convolution with the larger FOV", "Method"], ["Atrous Spatial Pyramid Pooling", "Method"], ["ASPP", "Method"]], "rel": [["WASP", "Synonym-Of", "Waterfall Atrous Spatial Pooling"], ["Atrous Spatial Pyramid Pooling", "Used-For", "Atrous Convolution with the larger FOV"], ["Waterfall Atrous Spatial Pooling", "Used-For", "Atrous Convolution with the larger FOV"], ["ASPP", "Synonym-Of", "Atrous Spatial Pyramid Pooling"]], "rel_plus": [["WASP:Method", "Synonym-Of", "Waterfall Atrous Spatial Pooling:Method"], ["Atrous Spatial Pyramid Pooling:Method", "Used-For", "Atrous Convolution with the larger FOV:Method"], ["Waterfall Atrous Spatial Pooling:Method", "Used-For", "Atrous Convolution with the larger FOV:Method"], ["ASPP:Method", "Synonym-Of", "Atrous Spatial Pyramid Pooling:Method"]]}
{"doc_id": "210861282", "sentence": "Examples of pose estimation obtained with our UniPose method are shown in Figure 1 .", "ner": [["pose estimation", "Task"], ["UniPose", "Method"]], "rel": [["UniPose", "Used-For", "pose estimation"]], "rel_plus": [["UniPose:Method", "Used-For", "pose estimation:Task"]]}
{"doc_id": "210861282", "sentence": "The main contributions of this paper are the following . state - of - the - art results for single person human pose estimation . \u2022 Our Waterfall module increases the receptive field of the network by combining the benefits of cascade atrous convolutions with multiple FOV in a parallel architecture inspired by the spatial pyramid approach . \u2022 The proposed UniPose method determines both the locations of joints and the bounding box for person detection , eliminating the need for separate branches in the network . \u2022 We extend the Waterfall based approach to UniPose - LSTM by adopting a linear sequential LSTM configuration and obtain state - of - the - art results for temporal human pose estimation in video .", "ner": [["single person human pose estimation", "Task"], ["Waterfall", "Method"], ["atrous convolutions", "Method"], ["FOV", "Method"], ["UniPose", "Method"], ["person detection", "Task"], ["UniPose - LSTM", "Method"], ["linear sequential LSTM", "Method"], ["human pose estimation", "Task"]], "rel": [["Waterfall", "Used-For", "atrous convolutions"], ["FOV", "Part-Of", "atrous convolutions"], ["UniPose", "Used-For", "person detection"], ["linear sequential LSTM", "Part-Of", "UniPose - LSTM"], ["UniPose - LSTM", "Used-For", "human pose estimation"]], "rel_plus": [["Waterfall:Method", "Used-For", "atrous convolutions:Method"], ["FOV:Method", "Part-Of", "atrous convolutions:Method"], ["UniPose:Method", "Used-For", "person detection:Task"], ["linear sequential LSTM:Method", "Part-Of", "UniPose - LSTM:Method"], ["UniPose - LSTM:Method", "Used-For", "human pose estimation:Task"]]}
{"doc_id": "210861282", "sentence": "Traditional methods for human pose estimation focused on the detection of joints , and consequently pose , via techniques that explored the geometry between joints in the target image [ 3 5 ] , [ 5 2 ] , and [ 4 5 ] .", "ner": [["human pose estimation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "210861282", "sentence": "In recent years , methods relying on Convolutional Neural Networks ( CNNs ) achieved superior results [ 4 3 ] , [ 7 ] , [ 3 8 ] .", "ner": [["Convolutional Neural Networks", "Method"], ["CNNs", "Method"]], "rel": [["CNNs", "Synonym-Of", "Convolutional Neural Networks"]], "rel_plus": [["CNNs:Method", "Synonym-Of", "Convolutional Neural Networks:Method"]]}
{"doc_id": "210861282", "sentence": "The popular Convolutional Pose Machine ( CPM ) [ 4 9 ] took an approach that refined joint detection via a set of stages in the network .", "ner": [["Convolutional Pose Machine", "Method"], ["CPM", "Method"]], "rel": [["CPM", "Synonym-Of", "Convolutional Pose Machine"]], "rel_plus": [["CPM:Method", "Synonym-Of", "Convolutional Pose Machine:Method"]]}
{"doc_id": "210861282", "sentence": "Stacked hourglass networks [ 2 8 ] utilized cascades of the hourglass structure for the pose estimation task .", "ner": [["Stacked hourglass networks", "Method"], ["pose estimation", "Task"]], "rel": [["Stacked hourglass networks", "Used-For", "pose estimation"]], "rel_plus": [["Stacked hourglass networks:Method", "Used-For", "pose estimation:Task"]]}
{"doc_id": "210861282", "sentence": "Building upon [ 4 9 ] , Yan et al. integrated the concept of Part Affinity Fields ( PAF ) , resulting in the OpenPose method [ 7 ] .", "ner": [["Part Affinity Fields", "Method"], ["PAF", "Method"], ["OpenPose", "Method"]], "rel": [["PAF", "Synonym-Of", "Part Affinity Fields"], ["Part Affinity Fields", "Part-Of", "OpenPose"]], "rel_plus": [["PAF:Method", "Synonym-Of", "Part Affinity Fields:Method"], ["Part Affinity Fields:Method", "Part-Of", "OpenPose:Method"]]}
{"doc_id": "210861282", "sentence": "PAF uses the detection of more significant joints to better estimate the prediction of less significant joints .", "ner": [["PAF", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210861282", "sentence": "The original backbone is augmented by the Hourglass Residual Units ( HRU ) with the goal of increasing the receptive FOV .", "ner": [["Hourglass Residual Units", "Method"], ["HRU", "Method"], ["FOV", "Method"]], "rel": [["HRU", "Synonym-Of", "Hourglass Residual Units"], ["FOV", "Part-Of", "Hourglass Residual Units"]], "rel_plus": [["HRU:Method", "Synonym-Of", "Hourglass Residual Units:Method"], ["FOV:Method", "Part-Of", "Hourglass Residual Units:Method"]]}
{"doc_id": "210861282", "sentence": "Post processing with Conditional Random Fields ( CRF ) is used to assemble the relations between detected joints .", "ner": [["Conditional Random Fields", "Method"], ["CRF", "Method"]], "rel": [["CRF", "Synonym-Of", "Conditional Random Fields"]], "rel_plus": [["CRF:Method", "Synonym-Of", "Conditional Random Fields:Method"]]}
{"doc_id": "210861282", "sentence": "The High - Resolution Network ( HRNet ) [ 4 3 ] includes both high and low resolution representations .", "ner": [["High - Resolution Network", "Method"], ["HRNet", "Method"]], "rel": [["HRNet", "Synonym-Of", "High - Resolution Network"]], "rel_plus": [["HRNet:Method", "Synonym-Of", "High - Resolution Network:Method"]]}
{"doc_id": "210861282", "sentence": "HRNet benefits from the larger FOV of multi resolution , a capability that we achieve in a simpler fashion with our WASP module .", "ner": [["HRNet", "Method"], ["FOV", "Method"], ["WASP", "Method"]], "rel": [["WASP", "Part-Of", "HRNet"], ["FOV", "Part-Of", "HRNet"]], "rel_plus": [["WASP:Method", "Part-Of", "HRNet:Method"], ["FOV:Method", "Part-Of", "HRNet:Method"]]}
{"doc_id": "210861282", "sentence": "DeepPose [ 4 7 ] utilizes a cascade of deep CNNs and locates body joints via regression .", "ner": [["DeepPose", "Method"], ["CNNs", "Method"]], "rel": [["CNNs", "Part-Of", "DeepPose"]], "rel_plus": [["CNNs:Method", "Part-Of", "DeepPose:Method"]]}
{"doc_id": "210861282", "sentence": "The Cascade Prediction Fusion ( CPF ) [ 5 4 ] uses graphical components in order to exploit the context for pose estimation .", "ner": [["Cascade Prediction Fusion", "Method"], ["CPF", "Method"], ["pose estimation", "Task"]], "rel": [["CPF", "Part-Of", "Cascade Prediction Fusion"], ["Cascade Prediction Fusion", "Used-For", "pose estimation"]], "rel_plus": [["CPF:Method", "Part-Of", "Cascade Prediction Fusion:Method"], ["Cascade Prediction Fusion:Method", "Used-For", "pose estimation:Task"]]}
{"doc_id": "210861282", "sentence": "Similarly , the Cascade Feature Aggregation ( CFA ) [ 4 2 ] aims to use semantic information to detect pose with a cascade approach .", "ner": [["Cascade Feature Aggregation", "Method"], ["CFA", "Method"]], "rel": [["CFA", "Synonym-Of", "Cascade Feature Aggregation"]], "rel_plus": [["CFA:Method", "Synonym-Of", "Cascade Feature Aggregation:Method"]]}
{"doc_id": "210861282", "sentence": "The Location , Classification , and Regression network ( LCR - Net ) [ 3 8 ] extends pose estimation to 3D space via depth regression .", "ner": [["Location , Classification , and Regression network", "Method"], ["LCR - Net", "Method"], ["pose estimation", "Task"], ["3D space via depth regression", "Task"]], "rel": [["LCR - Net", "Synonym-Of", "Location , Classification , and Regression network"], ["Location , Classification , and Regression network", "Used-For", "pose estimation"], ["Location , Classification , and Regression network", "Used-For", "3D space via depth regression"]], "rel_plus": [["LCR - Net:Method", "Synonym-Of", "Location , Classification , and Regression network:Method"], ["Location , Classification , and Regression network:Method", "Used-For", "pose estimation:Task"], ["Location , Classification , and Regression network:Method", "Used-For", "3D space via depth regression:Task"]]}
{"doc_id": "210861282", "sentence": "LCR - Net relies on a Detectron backbone [ 1 7 ] for the detection of human joint locations .", "ner": [["LCR - Net", "Method"], ["Detectron", "Method"], ["detection of human joint locations", "Task"]], "rel": [["Detectron", "Part-Of", "LCR - Net"], ["LCR - Net", "Used-For", "detection of human joint locations"]], "rel_plus": [["Detectron:Method", "Part-Of", "LCR - Net:Method"], ["LCR - Net:Method", "Used-For", "detection of human joint locations:Task"]]}
{"doc_id": "210861282", "sentence": "In a different approach for 3D pose estimation , the MonoCap method for human capture [ 5 6 ] couples a CNN with a geometric prior in order to statistically determine the third dimension for the pose using the Expectation - Maximization algorithm .", "ner": [["3D pose estimation", "Task"], ["MonoCap", "Method"], ["human capture", "Task"], ["CNN", "Method"], ["Expectation - Maximization algorithm", "Method"]], "rel": [["CNN", "Part-Of", "MonoCap"], ["Expectation - Maximization algorithm", "Part-Of", "MonoCap"], ["MonoCap", "Used-For", "human capture"]], "rel_plus": [["CNN:Method", "Part-Of", "MonoCap:Method"], ["Expectation - Maximization algorithm:Method", "Part-Of", "MonoCap:Method"], ["MonoCap:Method", "Used-For", "human capture:Task"]]}
{"doc_id": "210861282", "sentence": "LightTrack [ 3 0 ] , for instance , relies on a separate YOLO [ 3 7 ] architecture to perform the detection of subjects prior to detecting joints .", "ner": [["LightTrack", "Method"], ["YOLO", "Method"]], "rel": [["YOLO", "Part-Of", "LightTrack"]], "rel_plus": [["YOLO:Method", "Part-Of", "LightTrack:Method"]]}
{"doc_id": "210861282", "sentence": "In a different feamework , LCR - Net [ 3 8 ] has different branches for the detection using Detectron [ 1 7 ] and the arrangement of joints during classification .", "ner": [["LCR - Net", "Method"], ["detection", "Task"], ["Detectron", "Method"], ["classification", "Task"]], "rel": [["Detectron", "Part-Of", "LCR - Net"], ["LCR - Net", "Used-For", "detection"], ["LCR - Net", "Used-For", "classification"]], "rel_plus": [["Detectron:Method", "Part-Of", "LCR - Net:Method"], ["LCR - Net:Method", "Used-For", "detection:Task"], ["LCR - Net:Method", "Used-For", "classification:Task"]]}
{"doc_id": "210861282", "sentence": "Deepflow [ 5 0 ] used optical flow to better connect predictions between frames in a more continuous detection .", "ner": [["Deepflow", "Method"], ["optical flow", "Method"], ["detection", "Task"]], "rel": [["optical flow", "Part-Of", "Deepflow"], ["Deepflow", "Used-For", "detection"]], "rel_plus": [["optical flow:Method", "Part-Of", "Deepflow:Method"], ["Deepflow:Method", "Used-For", "detection:Task"]]}
{"doc_id": "210861282", "sentence": "A similar concept was adopted by the LSTM Pose Machine [ 2 7 ] approach , where the LSTM was utilized as the memory augmentation of the network .", "ner": [["LSTM Pose Machine", "Method"], ["LSTM", "Method"]], "rel": [["LSTM", "Part-Of", "LSTM Pose Machine"]], "rel_plus": [["LSTM:Method", "Part-Of", "LSTM Pose Machine:Method"]]}
{"doc_id": "210861282", "sentence": "Recurrent 3D Pose Sequence Machines ( RSPM ) [ 2 4 ] used LSTMs in the regression from 2D to 3D , to obtain better correspondence during the regression .", "ner": [["Recurrent 3D Pose Sequence Machines", "Method"], ["RSPM", "Method"], ["LSTMs", "Method"], ["regression", "Task"]], "rel": [["RSPM", "Synonym-Of", "Recurrent 3D Pose Sequence Machines"], ["LSTMs", "Part-Of", "Recurrent 3D Pose Sequence Machines"], ["Recurrent 3D Pose Sequence Machines", "Used-For", "regression"]], "rel_plus": [["RSPM:Method", "Synonym-Of", "Recurrent 3D Pose Sequence Machines:Method"], ["LSTMs:Method", "Part-Of", "Recurrent 3D Pose Sequence Machines:Method"], ["Recurrent 3D Pose Sequence Machines:Method", "Used-For", "regression:Task"]]}
{"doc_id": "210861282", "sentence": "DeepSign [ 1 6 ] applied transfer learning on a pretrained CNN for joint detection during sign language .", "ner": [["DeepSign", "Method"], ["transfer learning", "Task"], ["CNN", "Method"], ["joint detection during sign language", "Task"]], "rel": [["transfer learning", "Used-For", "DeepSign"], ["DeepSign", "Used-For", "joint detection during sign language"]], "rel_plus": [["transfer learning:Task", "Used-For", "DeepSign:Method"], ["DeepSign:Method", "Used-For", "joint detection during sign language:Task"]]}
{"doc_id": "210861282", "sentence": "An important challenge with both semantic segmentation and pose estimation methods incorporating CNN layers is the significant reduction of resolution caused by pooling .", "ner": [["semantic segmentation", "Task"], ["pose estimation", "Task"], ["CNN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210861282", "sentence": "Fully Convolutional Networks ( FCN ) [ 2 6 ] [ 2 6 ] addressed the resolution reduction problem by deploying upsampling strategies across deconvolution layers .", "ner": [["Fully Convolutional Networks", "Method"], ["FCN", "Method"], ["deconvolution", "Method"]], "rel": [["FCN", "Synonym-Of", "Fully Convolutional Networks"], ["deconvolution", "Part-Of", "Fully Convolutional Networks"]], "rel_plus": [["FCN:Method", "Synonym-Of", "Fully Convolutional Networks:Method"], ["deconvolution:Method", "Part-Of", "Fully Convolutional Networks:Method"]]}
{"doc_id": "210861282", "sentence": "A popular technique in semantic segmentation is the use of dilated or Atrous or dilated convolutions [ 1 1 ] .", "ner": [["semantic segmentation", "Task"], ["dilated", "Method"], ["Atrous", "Method"], ["dilated convolutions", "Method"]], "rel": [["dilated", "Used-For", "semantic segmentation"], ["Atrous", "Used-For", "semantic segmentation"], ["dilated convolutions", "Used-For", "semantic segmentation"]], "rel_plus": [["dilated:Method", "Used-For", "semantic segmentation:Task"], ["Atrous:Method", "Used-For", "semantic segmentation:Task"], ["dilated convolutions:Method", "Used-For", "semantic segmentation:Task"]]}
{"doc_id": "210861282", "sentence": "Motivated by the success of the Spatial Pyramids applied on pooling operations [ 1 9 ] , the ASPP architecture was successfully incorporated in DeepLab [ 1 1 ] for semantic segmentation .", "ner": [["Spatial Pyramids", "Method"], ["pooling operations", "Method"], ["ASPP", "Method"], ["DeepLab", "Method"], ["semantic segmentation", "Task"]], "rel": [["Spatial Pyramids", "Part-Of", "pooling operations"], ["ASPP", "Part-Of", "DeepLab"], ["DeepLab", "Used-For", "semantic segmentation"], ["ASPP", "Used-For", "semantic segmentation"]], "rel_plus": [["Spatial Pyramids:Method", "Part-Of", "pooling operations:Method"], ["ASPP:Method", "Part-Of", "DeepLab:Method"], ["DeepLab:Method", "Used-For", "semantic segmentation:Task"], ["ASPP:Method", "Used-For", "semantic segmentation:Task"]]}
{"doc_id": "210861282", "sentence": "The ASPP approach assembles atrous convolutions in four parallel branches with different rates , that are combined by fast bilinear interpolation with an additional factor of eight .", "ner": [["ASPP", "Method"], ["atrous convolutions", "Method"]], "rel": [["atrous convolutions", "Part-Of", "ASPP"]], "rel_plus": [["atrous convolutions:Method", "Part-Of", "ASPP:Method"]]}
{"doc_id": "210861282", "sentence": "The increase in resolution and FOV in the ASPP network can be beneficial for a contextual detection of body parts during pose estimation .", "ner": [["FOV", "Method"], ["ASPP", "Method"], ["pose estimation", "Task"]], "rel": [["FOV", "Part-Of", "ASPP"], ["ASPP", "Used-For", "pose estimation"]], "rel_plus": [["FOV:Method", "Part-Of", "ASPP:Method"], ["ASPP:Method", "Used-For", "pose estimation:Task"]]}
{"doc_id": "210861282", "sentence": "We propose UniPose , a unified architecture for pose estimation , that exploits the large FOV generated by atrous convolutions combined with cascade of convolutions in a \" Waterfall \" configuration .", "ner": [["UniPose", "Method"], ["pose estimation", "Task"], ["FOV", "Method"], ["atrous convolutions", "Method"], ["convolutions", "Method"], ["Waterfall", "Method"]], "rel": [["Waterfall", "Part-Of", "UniPose"], ["UniPose", "Used-For", "pose estimation"], ["FOV", "Part-Of", "atrous convolutions"], ["convolutions", "Part-Of", "Waterfall"], ["atrous convolutions", "Part-Of", "Waterfall"]], "rel_plus": [["Waterfall:Method", "Part-Of", "UniPose:Method"], ["UniPose:Method", "Used-For", "pose estimation:Task"], ["FOV:Method", "Part-Of", "atrous convolutions:Method"], ["convolutions:Method", "Part-Of", "Waterfall:Method"], ["atrous convolutions:Method", "Part-Of", "Waterfall:Method"]]}
{"doc_id": "210861282", "sentence": "The input image is initially fed into a deep CNN , in this case ResNet - 1 0 1 , with the final layers replaced by a WASP module .", "ner": [["CNN", "Method"], ["ResNet - 1 0 1", "Method"], ["WASP", "Method"]], "rel": [["ResNet - 1 0 1", "SubClass-Of", "CNN"], ["WASP", "Part-Of", "CNN"], ["WASP", "Part-Of", "ResNet - 1 0 1"]], "rel_plus": [["ResNet - 1 0 1:Method", "SubClass-Of", "CNN:Method"], ["WASP:Method", "Part-Of", "CNN:Method"], ["WASP:Method", "Part-Of", "ResNet - 1 0 1:Method"]]}
{"doc_id": "210861282", "sentence": "We next provide the motivation for the development of the WASP module and contrast it with traditional deconvolutions in [ 2 6 ] and the ASPP architecture in [ 1 1 ] .   The WASP module generates an efficient multi - scale representation that helps UniPose achieve state - of - the - art results .", "ner": [["WASP", "Method"], ["traditional deconvolutions", "Method"], ["ASPP", "Method"], ["WASP", "Method"], ["UniPose", "Method"]], "rel": [["WASP", "Compare-With", "traditional deconvolutions"], ["WASP", "Compare-With", "ASPP"], ["WASP", "Part-Of", "UniPose"]], "rel_plus": [["WASP:Method", "Compare-With", "traditional deconvolutions:Method"], ["WASP:Method", "Compare-With", "ASPP:Method"], ["WASP:Method", "Part-Of", "UniPose:Method"]]}
{"doc_id": "210861282", "sentence": "The WASP architecture , shown in Figure 4 , is designed to leverage both the larger FOV of the ASPP configuration and the reduced size of the cascade approach .", "ner": [["WASP", "Method"], ["FOV", "Method"], ["ASPP", "Method"]], "rel": [["ASPP", "Used-For", "WASP"], ["FOV", "Part-Of", "ASPP"]], "rel_plus": [["ASPP:Method", "Used-For", "WASP:Method"], ["FOV:Method", "Part-Of", "ASPP:Method"]]}
{"doc_id": "210861282", "sentence": "The inspiration for WASP was to combine the benefits of the ASPP [ 1 1 ] , Cascade [ 1 2 ] , and Res 2 Net [ 1 5 ] modules .", "ner": [["WASP", "Method"], ["ASPP", "Method"], ["Cascade", "Method"], ["Res 2 Net", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210861282", "sentence": "WASP relies on atrous convolutions , which are fundamental to ASPP , to maintain a large FOV .", "ner": [["WASP", "Method"], ["atrous convolutions", "Method"], ["ASPP", "Method"], ["FOV", "Method"]], "rel": [["atrous convolutions", "Part-Of", "WASP"], ["atrous convolutions", "Part-Of", "ASPP"], ["atrous convolutions", "Used-For", "FOV"]], "rel_plus": [["atrous convolutions:Method", "Part-Of", "WASP:Method"], ["atrous convolutions:Method", "Part-Of", "ASPP:Method"], ["atrous convolutions:Method", "Used-For", "FOV:Method"]]}
{"doc_id": "210861282", "sentence": "Furthermore , WASP incorporates multi - scale features inspired by the Res 2 Net architecture and other multi - scale approaches .", "ner": [["WASP", "Method"], ["Res 2 Net", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210861282", "sentence": "In contrast to ASPP and Res 2 Net , WASP does not immediately parallelize the input stream .", "ner": [["ASPP", "Method"], ["Res 2 Net", "Method"], ["WASP", "Method"]], "rel": [["WASP", "Compare-With", "ASPP"], ["WASP", "Compare-With", "Res 2 Net"]], "rel_plus": [["WASP:Method", "Compare-With", "ASPP:Method"], ["WASP:Method", "Compare-With", "Res 2 Net:Method"]]}
{"doc_id": "210861282", "sentence": "WASP also goes beyond the cascade approach by combining the streams from all its branches and average pooling of the original input to achieve a multiscale representation .", "ner": [["WASP", "Method"], ["average pooling", "Method"]], "rel": [["average pooling", "Part-Of", "WASP"]], "rel_plus": [["average pooling:Method", "Part-Of", "WASP:Method"]]}
{"doc_id": "210861282", "sentence": "WASP is designed with the goal of reducing the number of parameters in order to deal with memory constraints and overcome the main limitation of atrous convolutions .", "ner": [["WASP", "Method"], ["atrous convolutions", "Method"]], "rel": [["WASP", "Used-For", "atrous convolutions"]], "rel_plus": [["WASP:Method", "Used-For", "atrous convolutions:Method"]]}
{"doc_id": "210861282", "sentence": "The atrous convolutions in WASP start with a small rate of 6 , which consistently increases in subsequent branches ( rates of 6 , 1 2 , 1 8 , 2 4 ) .", "ner": [["atrous convolutions", "Method"], ["WASP", "Method"]], "rel": [["atrous convolutions", "Part-Of", "WASP"]], "rel_plus": [["atrous convolutions:Method", "Part-Of", "WASP:Method"]]}
{"doc_id": "210861282", "sentence": "The WASP module is utilized in the UniPose architecture of Figure 2 for pose estimation .", "ner": [["WASP", "Method"], ["UniPose", "Method"], ["pose estimation", "Task"]], "rel": [["WASP", "Part-Of", "UniPose"], ["UniPose", "Used-For", "pose estimation"]], "rel_plus": [["WASP:Method", "Part-Of", "UniPose:Method"], ["UniPose:Method", "Used-For", "pose estimation:Task"]]}
{"doc_id": "210861282", "sentence": "The decoder receives 2 5 6 feature maps from WASP and 2 5 6 low level feature maps from the first block of the ResNet backbone .", "ner": [["WASP", "Method"], ["ResNet", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210861282", "sentence": "After a max pooling operation to match the dimensions of the inputs , the feature maps are concatenated and processed through convolutional layers , dropout layers , and a final bilinear interpolation to resize to the original input size .", "ner": [["max pooling", "Method"], ["convolutional layers", "Method"], ["dropout", "Method"], ["bilinear interpolation", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210861282", "sentence": "The UniPose architecture was modified to UniPose - LSTM for pose estimation in video .", "ner": [["UniPose", "Method"], ["UniPose - LSTM", "Method"], ["pose estimation", "Task"]], "rel": [["UniPose - LSTM", "SubClass-Of", "UniPose"], ["UniPose - LSTM", "Used-For", "pose estimation"]], "rel_plus": [["UniPose - LSTM:Method", "SubClass-Of", "UniPose:Method"], ["UniPose - LSTM:Method", "Used-For", "pose estimation:Task"]]}
{"doc_id": "210861282", "sentence": "To operate in video processing mode , the UniPose architecture is augmented by an LSTM module that receives the final heatmaps from the previous frame along with the decoder heatmaps from the current frame .", "ner": [["UniPose", "Method"], ["LSTM", "Method"]], "rel": [["LSTM", "Part-Of", "UniPose"]], "rel_plus": [["LSTM:Method", "Part-Of", "UniPose:Method"]]}
{"doc_id": "210861282", "sentence": "The pipeline of UniPose - LSTM is shown in Figure 5 .", "ner": [["UniPose - LSTM", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210861282", "sentence": "This network includes CNN layers following the LSTM to generate the final heatmaps used for joint detection .", "ner": [["CNN", "Method"], ["LSTM", "Method"], ["joint detection", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210861282", "sentence": "The UniPose - LSTM configuration allows the network to use information from the previously processed frames , without significantly increasing the total size of the network .", "ner": [["UniPose - LSTM", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210861282", "sentence": "For both the single image and video configurations , our network uses identical ResNet - 1 0 1 backbone , WASP module , and decoder .", "ner": [["ResNet - 1 0 1", "Method"], ["WASP", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210861282", "sentence": "Two of the datasets are composed of single images : Leeds Sports Pose ( LSP ) [ 2 2 ] and MPII [ 2 ] ; and two datasets are composed of video sequences : Penn Action [ 5 5 ] and BBC Pose [ 1 0 ] .", "ner": [["Leeds Sports Pose", "Dataset"], ["LSP", "Dataset"], ["MPII", "Dataset"], ["Penn Action", "Dataset"], ["BBC Pose", "Dataset"]], "rel": [["LSP", "Synonym-Of", "Leeds Sports Pose"]], "rel_plus": [["LSP:Dataset", "Synonym-Of", "Leeds Sports Pose:Dataset"]]}
{"doc_id": "210861282", "sentence": "The Leeds Sports Pose ( LSP ) dataset [ 2 2 ] was initially used for single person pose estimation .", "ner": [["Leeds Sports Pose", "Dataset"], ["LSP", "Dataset"], ["single person pose estimation", "Task"]], "rel": [["LSP", "Synonym-Of", "Leeds Sports Pose"], ["Leeds Sports Pose", "Benchmark-For", "single person pose estimation"]], "rel_plus": [["LSP:Dataset", "Synonym-Of", "Leeds Sports Pose:Dataset"], ["Leeds Sports Pose:Dataset", "Benchmark-For", "single person pose estimation:Task"]]}
{"doc_id": "210861282", "sentence": "The LSP dataset includes lower variation in the data , allowing a good initial assessment of the network performance for the task of single person pose estimation .", "ner": [["LSP", "Dataset"], ["single person pose estimation", "Task"]], "rel": [["LSP", "Benchmark-For", "single person pose estimation"]], "rel_plus": [["LSP:Dataset", "Benchmark-For", "single person pose estimation:Task"]]}
{"doc_id": "210861282", "sentence": "Compared to state of the art , our methods achieved superior performance in several datasets , for both single frame pose estimation with UniPose and video pose estimation with UniPose - LSTM , including the specific task of pose estimation on sign language videos .", "ner": [["single frame pose estimation", "Task"], ["UniPose", "Method"], ["video pose estimation", "Task"], ["UniPose - LSTM", "Method"], ["pose estimation on sign language videos", "Task"]], "rel": [["UniPose", "Used-For", "single frame pose estimation"], ["UniPose - LSTM", "Used-For", "video pose estimation"]], "rel_plus": [["UniPose:Method", "Used-For", "single frame pose estimation:Task"], ["UniPose - LSTM:Method", "Used-For", "video pose estimation:Task"]]}
{"doc_id": "210861282", "sentence": "The PCP method introduces a bias due to the stronger penalization for the detection of smaller limbs ( i.e. arm in comparison to torso ) , since they naturally have a shorter distance , and consequently a smaller threshold for detection .", "ner": [], "rel": [], "rel_plus": []}
{"doc_id": "210861282", "sentence": "For that reason , the batch size utilized varied from high amounts for lower resolution datasets ( e.g. LSP ) to smaller batches of 4 for datasets such as the BBC Pose [ 1 0 ] .", "ner": [["LSP", "Dataset"], ["BBC Pose", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "210861282", "sentence": "Differently than methods such as CPM [ 4 9 ] , UniPose is able to detect the body joints with high confidence in a single iteration , instead of going through several stages or iterations in the network .", "ner": [["CPM", "Method"], ["UniPose", "Method"]], "rel": [["CPM", "Compare-With", "UniPose"]], "rel_plus": [["CPM:Method", "Compare-With", "UniPose:Method"]]}
{"doc_id": "210861282", "sentence": "Examples of pose estimation for subjects from LSP dataset are shown in Figure 6 .", "ner": [["pose estimation", "Task"], ["LSP", "Dataset"]], "rel": [["LSP", "Benchmark-For", "pose estimation"]], "rel_plus": [["LSP:Dataset", "Benchmark-For", "pose estimation:Task"]]}
{"doc_id": "210861282", "sentence": "Challenging conditions include the detection of joints in limbs that are not sufficiently separated and occlude each other . - 8 5 . 2 % DeepPose [ 4 7 ] 6 1 % -Poselet [ 3 5 ] 5 6 % -Tian et al. [ 4 5 ] 5 6 % - We next perform training and testing in the larger MPII dataset [ 2 ] , focusing on single person detection .", "ner": [["DeepPose", "Method"], ["MPII", "Dataset"], ["single person detection", "Task"]], "rel": [["MPII", "Benchmark-For", "single person detection"]], "rel_plus": [["MPII:Dataset", "Benchmark-For", "single person detection:Task"]]}
{"doc_id": "210861282", "sentence": "We used \" Detectron 2 \" [ 1 7 ] for segmentation and detection of all the individuals in the image , followed by the UniPose method to detect the pose of the selected individual .", "ner": [["Detectron 2", "Method"], ["segmentation", "Task"], ["detection", "Task"], ["UniPose", "Method"]], "rel": [["Detectron 2", "Used-For", "segmentation"], ["Detectron 2", "Used-For", "detection"]], "rel_plus": [["Detectron 2:Method", "Used-For", "segmentation:Task"], ["Detectron 2:Method", "Used-For", "detection:Task"]]}
{"doc_id": "210861282", "sentence": "UniPose achieves a PCKh detection rate of 9 2 . 7 % and outperformed other methods for single person pose estimation .", "ner": [["UniPose", "Method"], ["single person pose estimation", "Task"]], "rel": [["UniPose", "Used-For", "single person pose estimation"]], "rel_plus": [["UniPose:Method", "Used-For", "single person pose estimation:Task"]]}
{"doc_id": "210861282", "sentence": "Examples of pose estimation with UniPose in the MPII dataset are shown in Figure 7 .", "ner": [["pose estimation", "Task"], ["UniPose", "Method"], ["MPII", "Dataset"]], "rel": [["UniPose", "Used-For", "pose estimation"]], "rel_plus": [["UniPose:Method", "Used-For", "pose estimation:Task"]]}
{"doc_id": "210861282", "sentence": "These examples illustrate that UniPose deals effectively with occlusion , e.g. in the case of the horse rider .   PCKh@ 0 . 5 for MPII UniPose ( ours ) 9 2 . 7 % 8 - Stack HG [ 5 4 ] 9 2 . 5 % Deeply - Learned Models [ 4 4 ] 9 2 . 3 % Structure - Aware [ 2 3 ] 9 2 . 0 % Improvement Multi - Stage [ 4 2 ] 9 0 . 1 % CPM [ 4 9 ] 8 8 . 5 % Table 2 .", "ner": [["UniPose", "Method"], ["MPII UniPose", "Method"], ["CPM", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210861282", "sentence": "Pose estimation results and comparisons with other methods for the MPII dataset .", "ner": [["Pose estimation", "Task"], ["MPII", "Dataset"]], "rel": [["MPII", "Benchmark-For", "Pose estimation"]], "rel_plus": [["MPII:Dataset", "Benchmark-For", "Pose estimation:Task"]]}
{"doc_id": "210861282", "sentence": "Table 3 shows the results for our UniPose - LSTM in the Penn Action dataset [ 5 5 ] .", "ner": [["UniPose - LSTM", "Method"], ["Penn Action", "Dataset"]], "rel": [["UniPose - LSTM", "Evaluated-With", "Penn Action"]], "rel_plus": [["UniPose - LSTM:Method", "Evaluated-With", "Penn Action:Dataset"]]}
{"doc_id": "210861282", "sentence": "Our results show a significant improvement over previous state - of - the - art methods by the application of UniPose - LSTM in the temporal mode with 5 consecutive frames .", "ner": [["UniPose - LSTM", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210861282", "sentence": "PCK for Penn Action UniPose - LSTM ( ours ) 9 9 . 3 % LSTM - PM [ 2 7 ] 9 7 . 7 % CPM [ 4 9 ] 9 7 . 1 % Thin - Slicing [ 4 0 ] 9 6 . 5 % N - best [ 3 2 ] 9 1 . 8 % Iqbal [ 2 0 ] 8 1 . 1 % Table 3 .", "ner": [["Penn Action UniPose - LSTM", "Method"], ["LSTM - PM", "Method"], ["CPM", "Method"], ["Thin - Slicing", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210861282", "sentence": "Pose estimation results and comparisons with other methods for the Penn Action dataset .", "ner": [["Pose estimation", "Task"], ["Penn Action", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "210861282", "sentence": "Our UniPose network leverages the memory capability of the LSTM by incorporating 5 consecutive frames .", "ner": [["UniPose", "Method"], ["LSTM", "Method"]], "rel": [["LSTM", "Part-Of", "UniPose"]], "rel_plus": [["LSTM:Method", "Part-Of", "UniPose:Method"]]}
{"doc_id": "210861282", "sentence": "UniPose - LSTM results for the Penn Action dataset for different number of frames used by the LSTM .", "ner": [["UniPose - LSTM", "Method"], ["Penn Action", "Dataset"], ["LSTM", "Method"]], "rel": [["UniPose - LSTM", "Evaluated-With", "Penn Action"]], "rel_plus": [["UniPose - LSTM:Method", "Evaluated-With", "Penn Action:Dataset"]]}
{"doc_id": "210861282", "sentence": "UniPose - LSTM significantly outperforms the older methods by achieving a PCKh of 9 8 . 9 % .", "ner": [["UniPose - LSTM", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210861282", "sentence": "In order to obtain results from another method for comparison , we trained CPM for the BBC Pose dataset , obtaining a PCK of 9 7 . 6 % , which is below the performance of UniPose - LSTM .   PCKh@ 0 . 5 for BBC Pose UniPose - LSTM ( ours ) 9 8 . 9 % CPM [ 4 9 ] 9 7 . 6 % Charles et al. [ 9 ] 7 4 . 9 % Buehler et al. [ 5 ] 6 7 . 5 % Table 5 .", "ner": [["CPM", "Method"], ["BBC Pose", "Dataset"], ["UniPose - LSTM", "Method"], ["BBC Pose UniPose - LSTM", "Method"], ["CPM", "Method"]], "rel": [["CPM", "Trained-With", "BBC Pose"], ["CPM", "Compare-With", "UniPose - LSTM"], ["CPM", "Compare-With", "BBC Pose UniPose - LSTM"], ["CPM", "Compare-With", "CPM"]], "rel_plus": [["CPM:Method", "Trained-With", "BBC Pose:Dataset"], ["CPM:Method", "Compare-With", "UniPose - LSTM:Method"], ["CPM:Method", "Compare-With", "BBC Pose UniPose - LSTM:Method"], ["CPM:Method", "Compare-With", "CPM:Method"]]}
{"doc_id": "210861282", "sentence": "Pose estimation results and comparisons with other methods for the BBC Pose dataset Figure 9 shows examples of pose estimation and bounding box detections for subjects in the BBC dataset .", "ner": [["Pose estimation", "Task"], ["BBC Pose", "Dataset"], ["pose estimation", "Task"], ["BBC", "Dataset"]], "rel": [["BBC Pose", "Benchmark-For", "Pose estimation"]], "rel_plus": [["BBC Pose:Dataset", "Benchmark-For", "Pose estimation:Task"]]}
{"doc_id": "210861282", "sentence": "We presented the UniPose and UniPose - LSTM architectures for pose estimation in single images and videos , respectively .", "ner": [["UniPose", "Method"], ["UniPose - LSTM", "Method"], ["pose estimation", "Task"]], "rel": [["UniPose - LSTM", "Used-For", "pose estimation"], ["UniPose", "Used-For", "pose estimation"]], "rel_plus": [["UniPose - LSTM:Method", "Used-For", "pose estimation:Task"], ["UniPose:Method", "Used-For", "pose estimation:Task"]]}
{"doc_id": "210861282", "sentence": "The UniPose pipeline utilizes the WASP module that features a waterfall flow with a cascade of atrous convolutions and multi - scale representations .", "ner": [["UniPose", "Method"], ["WASP", "Method"], ["waterfall flow", "Method"], ["a cascade of atrous convolutions", "Method"]], "rel": [["WASP", "Part-Of", "UniPose"], ["waterfall flow", "Part-Of", "WASP"], ["a cascade of atrous convolutions", "Part-Of", "WASP"]], "rel_plus": [["WASP:Method", "Part-Of", "UniPose:Method"], ["waterfall flow:Method", "Part-Of", "WASP:Method"], ["a cascade of atrous convolutions:Method", "Part-Of", "WASP:Method"]]}
{"doc_id": "210861282", "sentence": "The results of UniPose and UniPose - LSTM demonstrated superior performance compared to state - of - the - art methods for several datasets , i.e. , LSP , MPII , Penn Action and BBC Pose , using various metrics .", "ner": [["UniPose", "Method"], ["UniPose - LSTM", "Method"], ["LSP", "Dataset"], ["MPII", "Dataset"], ["Penn Action", "Dataset"], ["BBC Pose", "Dataset"]], "rel": [["UniPose - LSTM", "Evaluated-With", "LSP"], ["UniPose", "Evaluated-With", "LSP"], ["UniPose - LSTM", "Evaluated-With", "MPII"], ["UniPose", "Evaluated-With", "MPII"], ["UniPose - LSTM", "Evaluated-With", "Penn Action"], ["UniPose", "Evaluated-With", "Penn Action"], ["UniPose - LSTM", "Evaluated-With", "BBC Pose"], ["UniPose", "Evaluated-With", "BBC Pose"]], "rel_plus": [["UniPose - LSTM:Method", "Evaluated-With", "LSP:Dataset"], ["UniPose:Method", "Evaluated-With", "LSP:Dataset"], ["UniPose - LSTM:Method", "Evaluated-With", "MPII:Dataset"], ["UniPose:Method", "Evaluated-With", "MPII:Dataset"], ["UniPose - LSTM:Method", "Evaluated-With", "Penn Action:Dataset"], ["UniPose:Method", "Evaluated-With", "Penn Action:Dataset"], ["UniPose - LSTM:Method", "Evaluated-With", "BBC Pose:Dataset"], ["UniPose:Method", "Evaluated-With", "BBC Pose:Dataset"]]}
{"doc_id": "210713911", "sentence": "Recent studies in image classification have demonstrated a variety of techniques for improving the performance of Convolutional Neural Networks ( CNNs ) .", "ner": [["image classification", "Task"], ["Convolutional Neural Networks", "Method"], ["CNNs", "Method"]], "rel": [["Convolutional Neural Networks", "Used-For", "image classification"], ["CNNs", "Synonym-Of", "Convolutional Neural Networks"]], "rel_plus": [["Convolutional Neural Networks:Method", "Used-For", "image classification:Task"], ["CNNs:Method", "Synonym-Of", "Convolutional Neural Networks:Method"]]}
{"doc_id": "210713911", "sentence": "In this study , we carry out extensive experiments to validate that carefully assembling these techniques and applying them to basic CNN models ( e.g. ResNet and MobileNet ) can improve the accuracy and robustness of the models while minimizing the loss of throughput .", "ner": [["CNN", "Method"], ["ResNet", "Method"], ["MobileNet", "Method"]], "rel": [["ResNet", "SubClass-Of", "CNN"], ["MobileNet", "SubClass-Of", "CNN"]], "rel_plus": [["ResNet:Method", "SubClass-Of", "CNN:Method"], ["MobileNet:Method", "SubClass-Of", "CNN:Method"]]}
{"doc_id": "210713911", "sentence": "Our proposed assembled ResNet - 5 0 shows improvements in top - 1 accuracy from 7 6 . 3 \\% to 8 2 . 7 8 \\% , mCE from 7 6 . 0 \\% to 4 8 . 9 \\% and mFR from 5 7 . 7 \\% to 3 2 . 3 \\% on ILSVRC 2 0 1 2 validation set .", "ner": [["ResNet - 5 0", "Method"], ["ILSVRC 2 0 1 2 validation set", "Dataset"]], "rel": [["ResNet - 5 0", "Evaluated-With", "ILSVRC 2 0 1 2 validation set"]], "rel_plus": [["ResNet - 5 0:Method", "Evaluated-With", "ILSVRC 2 0 1 2 validation set:Dataset"]]}
{"doc_id": "210713911", "sentence": "To verify the performance improvement in transfer learning , fine grained classification and image retrieval tasks were tested on several public datasets and showed that the improvement to backbone network performance boosted transfer learning performance significantly .", "ner": [["transfer learning", "Task"], ["fine grained classification", "Task"], ["image retrieval", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "210713911", "sentence": "Our approach achieved 1st place in the iFood Competition Fine - Grained Visual Recognition at CVPR 2 0 1 9 , and the source code and trained models are available at https://github.com/clovaai/assembled - cnn Since the introduction of AlexNet [ 2 0 ] , many studies have mainly focused on designing new network architectures for image classification to increase accuracy .", "ner": [["iFood Competition Fine - Grained Visual Recognition", "Task"], ["AlexNet", "Method"], ["image classification", "Task"]], "rel": [["AlexNet", "Used-For", "image classification"]], "rel_plus": [["AlexNet:Method", "Used-For", "image classification:Task"]]}
{"doc_id": "210713911", "sentence": "For example , new architectures such as Inception [ 3 2 ] , ResNet [ 9 ] , DenseNet [ 1 5 ] , NASNet [ 4 1 ] , MNASNet [ 3 3 ] and Efficient - Net [ 3 4 ] have been proposed .", "ner": [["Inception", "Method"], ["ResNet", "Method"], ["DenseNet", "Method"], ["NASNet", "Method"], ["MNASNet", "Method"], ["Efficient - Net", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210713911", "sentence": "Inception introduced new modules into the network with convolution layers of different kernel sizes .", "ner": [["Inception", "Method"], ["convolution", "Method"]], "rel": [["convolution", "Part-Of", "Inception"]], "rel_plus": [["convolution:Method", "Part-Of", "Inception:Method"]]}
{"doc_id": "210713911", "sentence": "ResNet utilized the concept of skip con - Model top - 1 mCE throughput EfficientNet B 4 [ 3 4 ] + AutoAugment [ 4 ] 8 3 . 0 6 0 . 7 9 5 EfficientNet B 6 [ 3 4 ] + AutoAugment [ 4 ] 8 4 . 2 6 0 . 6 2 8 EfficientNet B 7 [ 3 4 ] + AutoAugment [ 4 ] 8 4 . 5 5 9 . 4 1 6 ResNet - 5 0 [ Table 1 .", "ner": [["ResNet", "Method"], ["skip con", "Method"], ["EfficientNet B 4", "Method"], ["AutoAugment", "Method"], ["EfficientNet B 6", "Method"], ["AutoAugment", "Method"], ["EfficientNet B 7", "Method"], ["AutoAugment", "Method"], ["ResNet - 5 0", "Method"]], "rel": [["skip con", "Part-Of", "ResNet"]], "rel_plus": [["skip con:Method", "Part-Of", "ResNet:Method"]]}
{"doc_id": "210713911", "sentence": "In addition , in the area of AutoML , network design was automatically decided to create models such as NASNet and MNASNet .", "ner": [["AutoML", "Task"], ["NASNet", "Method"], ["MNASNet", "Method"]], "rel": [["NASNet", "Used-For", "AutoML"], ["MNASNet", "Used-For", "AutoML"]], "rel_plus": [["NASNet:Method", "Used-For", "AutoML:Task"], ["MNASNet:Method", "Used-For", "AutoML:Task"]]}
{"doc_id": "210713911", "sentence": "The resulting performance of EfficientNet for ImageNet top - 1 accuracy was greatly improved relative to AlexNet .", "ner": [["EfficientNet", "Method"], ["ImageNet", "Dataset"], ["AlexNet", "Method"]], "rel": [["EfficientNet", "Used-For", "ImageNet"], ["EfficientNet", "Compare-With", "AlexNet"]], "rel_plus": [["EfficientNet:Method", "Used-For", "ImageNet:Dataset"], ["EfficientNet:Method", "Compare-With", "AlexNet:Method"]]}
{"doc_id": "210713911", "sentence": "As a result of using these tricks , ImageNet validation top - 1 accuracy of ResNet - 5 0 improved from 7 5 . 3 % to 7 9 . 2 9 % .", "ner": [["ImageNet validation", "Dataset"], ["ResNet - 5 0", "Method"]], "rel": [["ResNet - 5 0", "Evaluated-With", "ImageNet validation"]], "rel_plus": [["ResNet - 5 0:Method", "Evaluated-With", "ImageNet validation:Dataset"]]}
{"doc_id": "210713911", "sentence": "Regularization is a method that prevents overfitting by increasing the training data through data augmentation processes such as Au - toAugment [ 4 ] and Mixup [ 3 9 ] , or by limiting the complexity of the CNN with processes such as Dropout [ 3 1 ] , and DropBlock [ 5 ] .", "ner": [["data augmentation", "Method"], ["Au - toAugment", "Method"], ["Mixup", "Method"], ["CNN", "Method"], ["Dropout", "Method"], ["DropBlock", "Method"]], "rel": [["Au - toAugment", "SubClass-Of", "data augmentation"], ["Mixup", "SubClass-Of", "data augmentation"], ["Dropout", "Part-Of", "CNN"], ["DropBlock", "Part-Of", "CNN"]], "rel_plus": [["Au - toAugment:Method", "SubClass-Of", "data augmentation:Method"], ["Mixup:Method", "SubClass-Of", "data augmentation:Method"], ["Dropout:Method", "Part-Of", "CNN:Method"], ["DropBlock:Method", "Part-Of", "CNN:Method"]]}
{"doc_id": "210713911", "sentence": "By organizing the existing CNN - related techniques for image classification , we find techniques that can be assembled into a single CNN .", "ner": [["CNN", "Method"], ["image classification", "Task"], ["CNN", "Method"]], "rel": [["CNN", "Used-For", "image classification"]], "rel_plus": [["CNN:Method", "Used-For", "image classification:Task"]]}
{"doc_id": "210713911", "sentence": "ResNet - D ResNet - D is a minor adjustment to the vanilla ResNet network architecture model proposed by He et al. [ 1 0 ] .", "ner": [["ResNet - D", "Method"], ["ResNet - D", "Method"], ["ResNet", "Method"]], "rel": [["ResNet - D", "SubClass-Of", "ResNet"]], "rel_plus": [["ResNet - D:Method", "SubClass-Of", "ResNet:Method"]]}
{"doc_id": "210713911", "sentence": "Second , a 2 \u00d7 2 average pooling layer has been added with a stride of 2 before the convolution ( Green ) .", "ner": [["2 \u00d7 2 average pooling", "Method"], ["convolution", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210713911", "sentence": "Last , a large 7x 7 convolution has been replaced with three smaller 3x 3 convolutions in Stem layer ( Red ) .", "ner": [["7x 7 convolution", "Method"], ["3x 3 convolutions", "Method"], ["Stem layer", "Method"]], "rel": [["7x 7 convolution", "Part-Of", "Stem layer"]], "rel_plus": [["7x 7 convolution:Method", "Part-Of", "Stem layer:Method"]]}
{"doc_id": "210713911", "sentence": "Channel Attention ( SE , SK ) We examine two tweaks in relation to channel attention .", "ner": [["Channel Attention", "Method"], ["SE", "Method"], ["SK", "Method"], ["channel attention", "Method"]], "rel": [["SE", "SubClass-Of", "Channel Attention"], ["SK", "SubClass-Of", "Channel Attention"]], "rel_plus": [["SE:Method", "SubClass-Of", "Channel Attention:Method"], ["SK:Method", "SubClass-Of", "Channel Attention:Method"]]}
{"doc_id": "210713911", "sentence": "First , Squeeze and Excitation ( SE ) network [ 1 4 ] focuses on enhancing the representational capacity of the network by modeling channel - wise relationships .", "ner": [["Squeeze and Excitation", "Method"], ["SE", "Method"]], "rel": [["SE", "Synonym-Of", "Squeeze and Excitation"]], "rel_plus": [["SE:Method", "Synonym-Of", "Squeeze and Excitation:Method"]]}
{"doc_id": "210713911", "sentence": "Second , Selective Kernel ( SK ) [ 2 1 ] is used , is inspired by the fact that the receptive sizes of neurons in the human visual cortex are different from each other .", "ner": [["Selective Kernel", "Method"], ["SK", "Method"]], "rel": [["SK", "Synonym-Of", "Selective Kernel"]], "rel_plus": [["SK:Method", "Synonym-Of", "Selective Kernel:Method"]]}
{"doc_id": "210713911", "sentence": "SK unit has multiple branches with different kernel sizes , and all branches are fused using softmax attention .", "ner": [["SK", "Method"], ["softmax attention", "Method"]], "rel": [["softmax attention", "Part-Of", "SK"]], "rel_plus": [["softmax attention:Method", "Part-Of", "SK:Method"]]}
{"doc_id": "210713911", "sentence": "The original SK generates multiple paths with 3x 3 and 5x 5 convolutions , but we instead use two 3x 3 convolutions to split the given feature map .", "ner": [["SK", "Method"], ["5x 5 convolutions", "Method"], ["3x 3 convolutions", "Method"]], "rel": [["3x 3 convolutions", "Part-Of", "SK"], ["5x 5 convolutions", "Part-Of", "SK"]], "rel_plus": [["3x 3 convolutions:Method", "Part-Of", "SK:Method"], ["5x 5 convolutions:Method", "Part-Of", "SK:Method"]]}
{"doc_id": "210713911", "sentence": "This is because two convolutions of the same kernel size can be replaced by a convolution with twice as many channels , thereby lowering the inference cost .", "ner": [["convolutions", "Method"], ["convolution", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210713911", "sentence": "Figure 3 shows an SK unit that replaces two branches with one convolution operation .", "ner": [["SK", "Method"], ["convolution", "Method"]], "rel": [["convolution", "Part-Of", "SK"]], "rel_plus": [["convolution:Method", "Part-Of", "SK:Method"]]}
{"doc_id": "210713911", "sentence": "R 5 0 is a simple notation for ResNet - 5 0 .", "ner": [["R 5 0", "Method"], ["ResNet - 5 0", "Method"]], "rel": [["R 5 0", "Synonym-Of", "ResNet - 5 0"]], "rel_plus": [["R 5 0:Method", "Synonym-Of", "ResNet - 5 0:Method"]]}
{"doc_id": "210713911", "sentence": "Compared with SK , SE has higher throughput but lower accuracy ( C 1 in Table 2 ) .", "ner": [["SK", "Method"], ["SE", "Method"]], "rel": [["SE", "Compare-With", "SK"]], "rel_plus": [["SE:Method", "Compare-With", "SK:Method"]]}
{"doc_id": "210713911", "sentence": "Comparing R 5 0 +SK \u2020 ( C 3 ) to R 5 0 +SK with 3x 3 and 5x 5 kernels ( C 2 ) , the top - 1 accuracy only differs by 0.0 8 % ( 7 8 . 0 0 and 7 7 . 9 2 ) , but the throughput is significantly different ( 3 2 6 and 3 8 2 ) .", "ner": [["R 5 0 +SK \u2020 ( C 3 )", "Method"], ["R 5 0 +SK with 3x 3 and 5x 5 kernels ( C 2 )", "Method"]], "rel": [["R 5 0 +SK \u2020 ( C 3 )", "Compare-With", "R 5 0 +SK with 3x 3 and 5x 5 kernels ( C 2 )"]], "rel_plus": [["R 5 0 +SK \u2020 ( C 3 ):Method", "Compare-With", "R 5 0 +SK with 3x 3 and 5x 5 kernels ( C 2 ):Method"]]}
{"doc_id": "210713911", "sentence": "Applying both SE and SK ( C 5 ) not only decreases accuracy by 0. 4 2 ( from 7 7 . 9 2 to 7 7 . 5 0 ) , but also decreases inference throughput by 3 7 ( from 3 8 2 to 3 4 5 ) .", "ner": [["SE", "Method"], ["SK ( C 5 )", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210713911", "sentence": "For a better trade - off between top - 1 accuracy and throughput , R 5 0 +SK \u2020 is preferred .   Anti - Alias Downsampling ( AA ) CNN models for image classification are known to be very vulnerable to small amounts of distortion [ 3 7 ] .", "ner": [["R 5 0 +SK \u2020", "Method"], ["Anti - Alias Downsampling", "Method"], ["AA", "Method"], ["CNN", "Method"], ["image classification", "Task"]], "rel": [["AA", "Part-Of", "Anti - Alias Downsampling"], ["Anti - Alias Downsampling", "Part-Of", "CNN"], ["CNN", "Used-For", "image classification"]], "rel_plus": [["AA:Method", "Part-Of", "Anti - Alias Downsampling:Method"], ["Anti - Alias Downsampling:Method", "Part-Of", "CNN:Method"], ["CNN:Method", "Used-For", "image classification:Task"]]}
{"doc_id": "210713911", "sentence": "In the original paper , AA applies to max - pooling , projection - conv , and strided - conv of ResNet .", "ner": [["AA", "Method"], ["max - pooling", "Method"], ["projection - conv", "Method"], ["strided - conv", "Method"], ["ResNet", "Method"]], "rel": [["AA", "Used-For", "max - pooling"], ["AA", "Used-For", "projection - conv"], ["AA", "Used-For", "strided - conv"], ["max - pooling", "Part-Of", "ResNet"], ["projection - conv", "Part-Of", "ResNet"], ["strided - conv", "Part-Of", "ResNet"]], "rel_plus": [["AA:Method", "Used-For", "max - pooling:Method"], ["AA:Method", "Used-For", "projection - conv:Method"], ["AA:Method", "Used-For", "strided - conv:Method"], ["max - pooling:Method", "Part-Of", "ResNet:Method"], ["projection - conv:Method", "Part-Of", "ResNet:Method"], ["strided - conv:Method", "Part-Of", "ResNet:Method"]]}
{"doc_id": "210713911", "sentence": "We also observe that applying AA to max - pooling degrades throughput significantly ( A 1 - 3 ) .", "ner": [["AA", "Method"], ["max - pooling", "Method"]], "rel": [["AA", "Used-For", "max - pooling"]], "rel_plus": [["AA:Method", "Used-For", "max - pooling:Method"]]}
{"doc_id": "210713911", "sentence": "Big Little Network ( BL ) BigLittleNet [ 3 ] applies multiple branches with different resolutions while aiming at reducing computational cost and increasing accuracy .", "ner": [["Big Little Network", "Method"], ["BL", "Method"], ["BigLittleNet", "Method"]], "rel": [["BL", "Synonym-Of", "Big Little Network"]], "rel_plus": [["BL:Method", "Synonym-Of", "Big Little Network:Method"]]}
{"doc_id": "210713911", "sentence": "We use \u03b1= 2 and \u03b2= 4 for ResNet - 5 0 and use \u03b1= 1 and \u03b2= 2 for ResNet - 1 5 2 .", "ner": [["ResNet - 5 0", "Method"], ["ResNet - 1 5 2", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210713911", "sentence": "AutoAugment ( Autoaug ) AutoAugment [ 4 ] is a data augmentation procedure which learns augmentation strategies from data .", "ner": [["AutoAugment", "Method"], ["Autoaug", "Method"], ["AutoAugment", "Method"], ["data augmentation", "Method"]], "rel": [["Autoaug", "Synonym-Of", "AutoAugment"], ["AutoAugment", "SubClass-Of", "data augmentation"]], "rel_plus": [["Autoaug:Method", "Synonym-Of", "AutoAugment:Method"], ["AutoAugment:Method", "SubClass-Of", "data augmentation:Method"]]}
{"doc_id": "210713911", "sentence": "It uses reinforcement learning to choose a sequence of image augmentation operations with the best accuracy by searching in a discrete search space of their probability of application and magnitude .", "ner": [["reinforcement learning", "Method"], ["image augmentation", "Task"]], "rel": [["reinforcement learning", "Used-For", "image augmentation"]], "rel_plus": [["reinforcement learning:Method", "Used-For", "image augmentation:Task"]]}
{"doc_id": "210713911", "sentence": "We borrow the augmentation policy found by Autoaug on ImageNet ILSVRC - 2 0 1 2 4 .", "ner": [["Autoaug", "Method"], ["ImageNet ILSVRC - 2 0 1 2", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "210713911", "sentence": "Mixup Mixup [ 3 9 ] creates one example by interpolating two examples of the training set for data augmentation .", "ner": [["Mixup", "Method"], ["Mixup", "Method"], ["data augmentation", "Method"]], "rel": [["Mixup", "SubClass-Of", "data augmentation"]], "rel_plus": [["Mixup:Method", "SubClass-Of", "data augmentation:Method"]]}
{"doc_id": "210713911", "sentence": "Configuration top - 1 R 5 0 D ( E 2 ) LS 7 7 . 3 7 R 5 0 D + Mixup ( type= 2 ) 7 8 . 8 5 R 5 0 D ( E 3 ) + Mixup ( type= 1 ) 7 9 . 1 0 Table 4 .", "ner": [["R 5 0 D ( E 2 ) LS", "Method"], ["R 5 0 D + Mixup", "Method"], ["R 5 0 D ( E 3 ) + Mixup", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210713911", "sentence": "DropBlock Dropout [ 3 1 ] is a popular technique for regularizing deep neural networks .", "ner": [["DropBlock", "Method"], ["Dropout", "Method"], ["deep neural networks", "Method"]], "rel": [["Dropout", "Part-Of", "deep neural networks"]], "rel_plus": [["Dropout:Method", "Part-Of", "deep neural networks:Method"]]}
{"doc_id": "210713911", "sentence": "However , Dropout does not work well for extremely deep networks such as ResNet [ 7 ] .", "ner": [["Dropout", "Method"], ["ResNet", "Method"]], "rel": [["Dropout", "Part-Of", "ResNet"]], "rel_plus": [["Dropout:Method", "Part-Of", "ResNet:Method"]]}
{"doc_id": "210713911", "sentence": "We apply Drop - Block to Stages 3 and 4 for ResNet - 5 0 and linearly decay the keep_prob hyperparameter from 1. 0 to 0. 9 during training .", "ner": [["Drop - Block", "Method"], ["ResNet - 5 0", "Method"]], "rel": [["Drop - Block", "Part-Of", "ResNet - 5 0"]], "rel_plus": [["Drop - Block:Method", "Part-Of", "ResNet - 5 0:Method"]]}
{"doc_id": "210713911", "sentence": "If CNN trains to minimize cross entropy with this hard one hot encoding target , the logits of the last fully connected layer of CNN grow to infinity , which leads to over - fitting [ 1 0 ] .", "ner": [["CNN", "Method"], ["cross entropy", "Method"], ["fully connected layer", "Method"], ["CNN", "Method"]], "rel": [["cross entropy", "Part-Of", "CNN"], ["fully connected layer", "Part-Of", "CNN"]], "rel_plus": [["cross entropy:Method", "Part-Of", "CNN:Method"], ["fully connected layer:Method", "Part-Of", "CNN:Method"]]}
{"doc_id": "210713911", "sentence": "Because our model uses Mixup and KD techniques together , the teacher network should also be applied to Mixup .", "ner": [["Mixup", "Method"], ["KD", "Method"], ["Mixup", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210713911", "sentence": "We used AmoebaNet - A as a teacher with 8 3 . 9 % of ImageNet validation top - 1 accuracy .", "ner": [["AmoebaNet - A", "Method"], ["ImageNet validation", "Dataset"]], "rel": [["AmoebaNet - A", "Used-For", "ImageNet validation"]], "rel_plus": [["AmoebaNet - A:Method", "Used-For", "ImageNet validation:Dataset"]]}
{"doc_id": "210713911", "sentence": "Adding ResNet - D to the baseline model improves top - 1 accuracy by 0. 5 % ( from 7 6 . 8 7 to 7 7 . 3 7 ) ( M 1 in Table 6 ) , and adding SK tweaks improves accuracy by 1. 4 6 % ( from 7 7 . 3 7 to 7 8 . 8 3 ) ( M 2 ) .", "ner": [["ResNet - D", "Method"], ["SK", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210713911", "sentence": "In Table 2 , We show that the accuracy is increased by 1. 6 2 % when SK is independently applied to ResNet ( from 7 6 . 3 0 to 7 7 . 9 2 ) .", "ner": [["SK", "Method"], ["ResNet", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210713911", "sentence": "Stacking ResNet - D and SK increases the top - 1 accuracy gains almost in equal measure to the sum of the performance gains of applying ResNet - D and SK separately .", "ner": [["ResNet - D", "Method"], ["SK", "Method"], ["ResNet - D", "Method"], ["SK", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210713911", "sentence": "Applying BL to R 5 0 D+SK improves top - 1 accuracy by 0. 4 4 % ( from 7 9 . 2 7 to 7 8 . 8 3 ) ( M 3 ) .", "ner": [["BL", "Method"], ["R 5 0 D+SK", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210713911", "sentence": "Ablation study for assembling the network tweaks and regularization with ResNet - 5 0 on ImageNet ILSVRC 2 0 1 2 dataset .", "ner": [["ResNet - 5 0", "Method"], ["ImageNet ILSVRC 2 0 1 2", "Dataset"]], "rel": [["ResNet - 5 0", "Used-For", "ImageNet ILSVRC 2 0 1 2"]], "rel_plus": [["ResNet - 5 0:Method", "Used-For", "ImageNet ILSVRC 2 0 1 2:Dataset"]]}
{"doc_id": "210713911", "sentence": "As with other experiments , the inference throughput measurements of EfficientNet were performed on a single NVIDIA Tesla P 4 0 using official EfficientNet code [ 1 7 ] .", "ner": [["EfficientNet", "Method"], ["EfficientNet", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210713911", "sentence": "KD \u2020 uses EfficientNet - B 7 instead of AmoebaNet as a teacher model . olution for training .", "ner": [["KD", "Method"], ["EfficientNet - B 7", "Method"], ["AmoebaNet", "Method"]], "rel": [["EfficientNet - B 7", "Used-For", "KD"]], "rel_plus": [["EfficientNet - B 7:Method", "Used-For", "KD:Method"]]}
{"doc_id": "210713911", "sentence": "Applying AA to the R 5 0 D+SK+BL improves top - 1 accuracy by 0. 1 2 % ( from 7 9 . 2 7 to 7 9 . 3 9 ) and decreases throughput by 4 7 ( from 3 5 9 to 3 1 2 ) ( M 4 ) .", "ner": [["AA", "Method"], ["R 5 0 D+SK+BL", "Method"]], "rel": [["AA", "Used-For", "R 5 0 D+SK+BL"]], "rel_plus": [["AA:Method", "Used-For", "R 5 0 D+SK+BL:Method"]]}
{"doc_id": "210713911", "sentence": "Because AA is a network structure designed for robustness to image distortion , the top - 1 accuracy does not reliably determine the AA effect .", "ner": [["AA", "Method"], ["AA", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210713911", "sentence": "For example , applying Mixup , DropBlock , KD , and Autoaug individually improves top 1 /mCE 0. 7 5 %/ 6 . 0 8 % , 0. 6 9 %/ 1 . 8 4 % , 0. 2 9 %/ 1 . 2 6 % , and 0.0 9 %/ 4 . 1 4 % respectively .", "ner": [["Mixup", "Method"], ["DropBlock", "Method"], ["KD", "Method"], ["Autoaug", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210713911", "sentence": "Replacing SE with SK improves performance by 1. 0 % and 4. 3 % for the top - 1 and mCE , respectively ( E 6 ) .", "ner": [["SE", "Method"], ["SK", "Method"]], "rel": [["SE", "Compare-With", "SK"]], "rel_plus": [["SE:Method", "Compare-With", "SK:Method"]]}
{"doc_id": "210713911", "sentence": "In Table 2 , when SE is changed to SK without regularization , the accuracy increases by 0. 5 % .", "ner": [["SE", "Method"], ["SK without regularization", "Method"]], "rel": [["SE", "Compare-With", "SK without regularization"]], "rel_plus": [["SE:Method", "Compare-With", "SK without regularization:Method"]]}
{"doc_id": "210713911", "sentence": "Compared to SK without regularization , replacing SE with SK with regularization leads to nearly double the accuracy improvement .", "ner": [["SK without regularization", "Method"], ["SE with SK with regularization", "Method"]], "rel": [["SK without regularization", "Compare-With", "SE with SK with regularization"]], "rel_plus": [["SK without regularization:Method", "Compare-With", "SE with SK with regularization:Method"]]}
{"doc_id": "210713911", "sentence": "This means that SK is more complementary for regularization techniques than SE .", "ner": [["SK", "Method"], ["SE", "Method"]], "rel": [["SK", "Compare-With", "SE"]], "rel_plus": [["SK:Method", "Compare-With", "SE:Method"]]}
{"doc_id": "210713911", "sentence": "AA also shows higher performance gain in mCE relative to top - 1 ( E 1 0 ) , which agrees with AA being used as a network tweak to make the CNN robust for image translations as claimed in [ 4 0 ] .", "ner": [["AA", "Method"], ["AA", "Method"], ["CNN", "Method"]], "rel": [["AA", "Used-For", "CNN"]], "rel_plus": [["AA:Method", "Used-For", "CNN:Method"]]}
{"doc_id": "210713911", "sentence": "We also experiment with ResNet - 1 5 2 for comparison as E 1 2 , we call this model Assemble - ResNet - 1 5 2 .", "ner": [["ResNet - 1 5 2", "Method"], ["Assemble - ResNet - 1 5 2", "Method"]], "rel": [["ResNet - 1 5 2", "Synonym-Of", "Assemble - ResNet - 1 5 2"]], "rel_plus": [["ResNet - 1 5 2:Method", "Synonym-Of", "Assemble - ResNet - 1 5 2:Method"]]}
{"doc_id": "210713911", "sentence": "Ablation study of transfer learning with the Food - 1 0 1 dataset .", "ner": [["transfer learning", "Task"], ["Food - 1 0 1", "Dataset"]], "rel": [["Food - 1 0 1", "Benchmark-For", "transfer learning"]], "rel_plus": [["Food - 1 0 1:Dataset", "Benchmark-For", "transfer learning:Task"]]}
{"doc_id": "210713911", "sentence": "REG means that regularization techniques \" LS + Mixup + Drop - Block + KD + Autoaug \" are applied during backbone training .", "ner": [["LS + Mixup + Drop - Block + KD + Autoaug", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210713911", "sentence": "The Food - 1 0 1 mCE is not normalized by AlexNet 's errors .", "ner": [["Food - 1 0 1", "Dataset"], ["AlexNet", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210713911", "sentence": "We use the augmentation policy which is found by Autoaug on CIFAR - 1 0 in these experiments [ 4 ] .", "ner": [["Autoaug", "Method"], ["CIFAR - 1 0", "Dataset"]], "rel": [["Autoaug", "Evaluated-With", "CIFAR - 1 0"]], "rel_plus": [["Autoaug:Method", "Evaluated-With", "CIFAR - 1 0:Dataset"]]}
{"doc_id": "210713911", "sentence": "The state - of - the - art Models ResNet - 5 0 -tuned [ 1 8 ] ResNet - 5 0 Assemble - ResNet - FGVC - 5 0 Table 9 .", "ner": [["ResNet - 5 0", "Method"], ["ResNet - 5 0 Assemble - ResNet - FGVC - 5 0", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210713911", "sentence": "Transfer learning for FGVC task with our model and comparison with other state - of - the - art models .", "ner": [["Transfer learning", "Task"], ["FGVC", "Task"]], "rel": [["Transfer learning", "Used-For", "FGVC"]], "rel_plus": [["Transfer learning:Task", "Used-For", "FGVC:Task"]]}
{"doc_id": "210713911", "sentence": "ImageNet - based transfer learning results are only compared with a single crop .", "ner": [["ImageNet - based transfer learning", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "210713911", "sentence": "Assemble - ResNet - FGVC - 5 0 means that the final F 1 3 model in Table 8 .", "ner": [["Assemble - ResNet - FGVC - 5 0", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210713911", "sentence": "We did not tune hyperparameters because we aim to compare between ResNet - 5 0 and Assemble - ResNet - FGVC - 5 0 rather than to achieve the state - of - the - art performance .", "ner": [["ResNet - 5 0", "Method"], ["Assemble - ResNet - FGVC - 5 0", "Method"]], "rel": [["ResNet - 5 0", "Compare-With", "Assemble - ResNet - FGVC - 5 0"]], "rel_plus": [["ResNet - 5 0:Method", "Compare-With", "Assemble - ResNet - FGVC - 5 0:Method"]]}
{"doc_id": "210713911", "sentence": "Assemble - ResNet - FGVC - 5 0 not only boosts the performance of ResNet - 5 0 significantly , but also leads to performance comparable to heavyweight state - of - the - art models for all datasets .", "ner": [["Assemble - ResNet - FGVC - 5 0", "Method"], ["ResNet - 5 0", "Method"]], "rel": [["Assemble - ResNet - FGVC - 5 0", "Compare-With", "ResNet - 5 0"]], "rel_plus": [["Assemble - ResNet - FGVC - 5 0:Method", "Compare-With", "ResNet - 5 0:Method"]]}
{"doc_id": "210713911", "sentence": "To do this , we performed an ablation study on the Food - 1 0 1 [ 2 ] dataset , which is the largest public fine - grained visual classification ( FGVC ) dataset .", "ner": [["Food - 1 0 1", "Dataset"], ["fine - grained visual classification", "Task"], ["FGVC", "Task"]], "rel": [["FGVC", "Synonym-Of", "fine - grained visual classification"], ["Food - 1 0 1", "Benchmark-For", "fine - grained visual classification"]], "rel_plus": [["FGVC:Task", "Synonym-Of", "fine - grained visual classification:Task"], ["Food - 1 0 1:Dataset", "Benchmark-For", "fine - grained visual classification:Task"]]}
{"doc_id": "210713911", "sentence": "The basic experiment setup and hyperparameters that differ from the backbone training are : \u2022 Initial learning rate reduced from 0. 1 to 0.0 1 . \u2022 Weight decay is set to 0.0 0 1 . \u2022 Momentum for BN is set to max( 1 \u2212 1 0 /s , 0. 9 ) . \u2022 Keep probability of DropBlock starts at 0. 9 and decreases linearly to 0. 7 at the end of training \u2022 The training epoch is set differently for each dataset and is indicated in Appendix B. As shown in Table 8 , stacking network tweaks and regularization techniques have steadily improved both top - 1 accuracy and mCE for the transfer learning task on the Food - 1 0 1 dataset .", "ner": [["Weight decay", "Method"], ["Momentum", "Method"], ["BN", "Method"], ["DropBlock", "Method"], ["transfer learning", "Task"], ["Food - 1 0 1", "Dataset"]], "rel": [["Momentum", "Part-Of", "BN"], ["Food - 1 0 1", "Benchmark-For", "transfer learning"]], "rel_plus": [["Momentum:Method", "Part-Of", "BN:Method"], ["Food - 1 0 1:Dataset", "Benchmark-For", "transfer learning:Task"]]}
{"doc_id": "210713911", "sentence": "We use the same network structure in F 4 - F 1 3 , but for F 9 - F 1 3 , they have regularization such as Mixup , DropBlock , KD and Autoaug on the backbone .", "ner": [["Mixup", "Method"], ["DropBlock", "Method"], ["KD", "Method"], ["Autoaug", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210713911", "sentence": "For convenience , we call the final F 1 3 model in Table 8 as Assemble - ResNet - FGVC - 5 0 .", "ner": [["Assemble - ResNet - FGVC - 5 0", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210713911", "sentence": "We also have evaluated Assemble - ResNet - FGVC - 5 0 in Table 8 on the following datasets : Stanford Cars [ 1 9 ] , CUB - 2 0 0 - 2 0 1 l [ 3 5 ] , Oxford 1 0 2 Flowers [ 2 5 ] , Oxford - IIIT Pets [ 2 6 ] , FGVC - Aircraft [ 2 3 ] , and Food - 1 0 1 [ 2 ] .", "ner": [["Assemble - ResNet - FGVC - 5 0", "Method"], ["Stanford Cars", "Dataset"], ["CUB - 2 0 0 - 2 0 1 l", "Dataset"], ["Oxford 1 0 2 Flowers", "Dataset"], ["Oxford - IIIT Pets", "Dataset"], ["FGVC - Aircraft", "Dataset"], ["Food - 1 0 1", "Dataset"]], "rel": [["Assemble - ResNet - FGVC - 5 0", "Evaluated-With", "Stanford Cars"], ["Assemble - ResNet - FGVC - 5 0", "Evaluated-With", "CUB - 2 0 0 - 2 0 1 l"], ["Assemble - ResNet - FGVC - 5 0", "Evaluated-With", "Oxford 1 0 2 Flowers"], ["Assemble - ResNet - FGVC - 5 0", "Evaluated-With", "Oxford - IIIT Pets"], ["Assemble - ResNet - FGVC - 5 0", "Evaluated-With", "FGVC - Aircraft"], ["Assemble - ResNet - FGVC - 5 0", "Evaluated-With", "Food - 1 0 1"]], "rel_plus": [["Assemble - ResNet - FGVC - 5 0:Method", "Evaluated-With", "Stanford Cars:Dataset"], ["Assemble - ResNet - FGVC - 5 0:Method", "Evaluated-With", "CUB - 2 0 0 - 2 0 1 l:Dataset"], ["Assemble - ResNet - FGVC - 5 0:Method", "Evaluated-With", "Oxford 1 0 2 Flowers:Dataset"], ["Assemble - ResNet - FGVC - 5 0:Method", "Evaluated-With", "Oxford - IIIT Pets:Dataset"], ["Assemble - ResNet - FGVC - 5 0:Method", "Evaluated-With", "FGVC - Aircraft:Dataset"], ["Assemble - ResNet - FGVC - 5 0:Method", "Evaluated-With", "Food - 1 0 1:Dataset"]]}
{"doc_id": "210713911", "sentence": "The statistics for each dataset are as shown in Appendix C. We borrow the same training settings from Kornblith et al. [ 1 8 ] and fine - tuned new datasets from Assemble - ResNet - 5 0 Im - ageNet checkpoint .", "ner": [["Assemble - ResNet - 5 0 Im - ageNet", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210713911", "sentence": "Compared to EfficientNet [ 3 4 ] and AmoebaNet - B [ 1 6 ] which are state - of - the - art model for image classification tasks .", "ner": [["EfficientNet", "Method"], ["AmoebaNet - B", "Method"], ["image classification", "Task"]], "rel": [["EfficientNet", "Compare-With", "AmoebaNet - B"], ["EfficientNet", "Used-For", "image classification"], ["AmoebaNet - B", "Used-For", "image classification"]], "rel_plus": [["EfficientNet:Method", "Compare-With", "AmoebaNet - B:Method"], ["EfficientNet:Method", "Used-For", "image classification:Task"], ["AmoebaNet - B:Method", "Used-For", "image classification:Task"]]}
{"doc_id": "210713911", "sentence": "Our Assemble - ResNet - FGVC - 5 0 model achieves comparable accuracy with 2 0 x faster inference throughput than the existing state - of - the - art models .", "ner": [["Assemble - ResNet", "Method"], ["FGVC - 5 0", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "210713911", "sentence": "We also conducted an ablation study on three standard fine - grained image retrieval ( IR ) datasets : Stanford Online Products ( SOP ) [ 3 0 ] , CUB 2 0 0 [ 3 5 ] and CARS 1 9 6 [ 1 9 ] .", "ner": [["image retrieval", "Task"], ["IR", "Task"], ["Stanford Online Products", "Dataset"], ["SOP", "Dataset"], ["CUB 2 0 0", "Dataset"], ["CARS 1 9 6", "Dataset"]], "rel": [["IR", "Synonym-Of", "image retrieval"], ["Stanford Online Products", "Evaluated-With", "image retrieval"], ["CUB 2 0 0", "Evaluated-With", "image retrieval"], ["CARS 1 9 6", "Evaluated-With", "image retrieval"], ["SOP", "Synonym-Of", "Stanford Online Products"]], "rel_plus": [["IR:Task", "Synonym-Of", "image retrieval:Task"], ["Stanford Online Products:Dataset", "Evaluated-With", "image retrieval:Task"], ["CUB 2 0 0:Dataset", "Evaluated-With", "image retrieval:Task"], ["CARS 1 9 6:Dataset", "Evaluated-With", "image retrieval:Task"], ["SOP:Dataset", "Synonym-Of", "Stanford Online Products:Dataset"]]}
{"doc_id": "210713911", "sentence": "The basic experiment setup and hyperparameters are as follows . \u2022 Image preprocessing resizes to 2 2 4 x 2 2 4 without maintaining aspect ratio with probability 0. 5 and resize to 2 5 6 x 2 5 6 and random crop to 2 2 4 x 2 2 4 with probability 0. 5 . \u2022 Data augmentation includes random flip with 0. 5 probability . \u2022 Momentum for BN is set to max( 1 \u2212 1 0 /s , 0. 9 ) . \u2022 Weight decay is set to 0.0 0 0 5 . \u2022 The training epoch , batch size , learning rate decay and assembling configuration is set differently for each dataset .", "ner": [["Data augmentation", "Method"], ["Momentum", "Method"], ["BN", "Method"], ["Weight decay", "Method"]], "rel": [["Momentum", "Part-Of", "BN"]], "rel_plus": [["Momentum:Method", "Part-Of", "BN:Method"]]}
{"doc_id": "210713911", "sentence": "We will describe the settings in the Appendix D. On top of that , cosine - softmax based losses were used for image retrieval .", "ner": [["cosine - softmax based losses", "Method"], ["image retrieval", "Task"]], "rel": [["cosine - softmax based losses", "Used-For", "image retrieval"]], "rel_plus": [["cosine - softmax based losses:Method", "Used-For", "image retrieval:Task"]]}
{"doc_id": "210713911", "sentence": "In this work , we use ArcFace [ 6 ] loss with a margin of 0. 3 and use generalized mean - pooling ( GeM ) [ 2 8 ] for a pooling method without performing downsampling at Stage 4 of backbone networks because it has better performance for the image retrieval task .", "ner": [["ArcFace", "Method"], ["generalized mean - pooling", "Method"], ["GeM", "Method"], ["image retrieval", "Task"]], "rel": [["GeM", "Synonym-Of", "generalized mean - pooling"], ["generalized mean - pooling", "Used-For", "image retrieval"]], "rel_plus": [["GeM:Method", "Synonym-Of", "generalized mean - pooling:Method"], ["generalized mean - pooling:Method", "Used-For", "image retrieval:Task"]]}
{"doc_id": "210713911", "sentence": "In contradiction to our results on FGVC , the particular combination of network tweaks and regularization that worked well on the SOP dataset were different from that for FGVC datasets .", "ner": [["FGVC", "Task"], ["SOP", "Dataset"], ["FGVC", "Task"]], "rel": [["SOP", "Benchmark-For", "FGVC"]], "rel_plus": [["SOP:Dataset", "Benchmark-For", "FGVC:Task"]]}
{"doc_id": "210713911", "sentence": "Comparing S 2 - 4 , we see that BL and AA did not work well on the SOP dataset .", "ner": [["BL", "Method"], ["AA", "Method"], ["SOP", "Dataset"]], "rel": [["AA", "Evaluated-With", "SOP"], ["BL", "Evaluated-With", "SOP"]], "rel_plus": [["AA:Method", "Evaluated-With", "SOP:Dataset"], ["BL:Method", "Evaluated-With", "SOP:Dataset"]]}
{"doc_id": "210713911", "sentence": "Of the regularizers , DropBlock works well , but Autoaug do not improve the recall@ 1 performance ( S 2 and S 5 , 6 ) .", "ner": [["DropBlock", "Method"], ["Autoaug", "Method"]], "rel": [["DropBlock", "Compare-With", "Autoaug"]], "rel_plus": [["DropBlock:Method", "Compare-With", "Autoaug:Method"]]}
{"doc_id": "210713911", "sentence": "There is also a significant performance improvement on CUB 2 0 0 and CARS 1 9 6 datasets .", "ner": [["CUB 2 0 0", "Dataset"], ["CARS 1 9 6", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "210713911", "sentence": "Transfer learning for IR task with our method .", "ner": [["Transfer learning", "Task"], ["IR", "Task"]], "rel": [["Transfer learning", "Used-For", "IR"]], "rel_plus": [["Transfer learning:Task", "Used-For", "IR:Task"]]}
{"doc_id": "210713911", "sentence": "The best configurations for each dataset are specified in Appendix D. Assemble - ResNet - IR - 5 0 boosts the performance of ResNet - 5 0 significantly for all IR datasets .", "ner": [["ResNet - IR - 5 0", "Method"], ["ResNet - 5 0", "Method"], ["IR", "Task"]], "rel": [["ResNet - IR - 5 0", "SubClass-Of", "ResNet - 5 0"], ["ResNet - IR - 5 0", "Used-For", "IR"]], "rel_plus": [["ResNet - IR - 5 0:Method", "SubClass-Of", "ResNet - 5 0:Method"], ["ResNet - IR - 5 0:Method", "Used-For", "IR:Task"]]}
{"doc_id": "210713911", "sentence": "In this paper , we show that assembling various techniques for CNNs to single convolutional networks leads to improvements of top - 1 accuracy and mCE on the ImageNet ILSVRC 2 0 1 2 validation dataset .", "ner": [["CNNs", "Method"], ["convolutional networks", "Method"], ["ImageNet ILSVRC 2 0 1 2 validation dataset", "Dataset"]], "rel": [["CNNs", "Evaluated-With", "ImageNet ILSVRC 2 0 1 2 validation dataset"]], "rel_plus": [["CNNs:Method", "Evaluated-With", "ImageNet ILSVRC 2 0 1 2 validation dataset:Dataset"]]}
{"doc_id": "210713911", "sentence": "Our approach has also improved performance consistently on transfer learning such as FGVC and image retrieval tasks .", "ner": [["transfer learning", "Task"], ["FGVC", "Task"], ["image retrieval", "Task"]], "rel": [["FGVC", "SubTask-Of", "transfer learning"], ["image retrieval", "SubTask-Of", "transfer learning"]], "rel_plus": [["FGVC:Task", "SubTask-Of", "transfer learning:Task"], ["image retrieval:Task", "SubTask-Of", "transfer learning:Task"]]}
{"doc_id": "210713911", "sentence": "For example , we already are planning to reassemble various new studies such as AugMix [ 1 2 ] and ECA - net [ 3 6 ] , which were recently published .", "ner": [["AugMix", "Method"], ["ECA - net", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210713911", "sentence": "We expect that there will be further improvements if we change the vanilla backbone to a more powerful backbone such as EfficientNet [ 3 4 ] instead of ResNet , which we leave as future works .   We observe in several experiments that FLOPS is not proportional to the inference speed of the actual GPU .", "ner": [["EfficientNet", "Method"], ["ResNet", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210713911", "sentence": "Food - 1 0 1 1 0 0 Stanford Cars 1, 0 0 0 Oxford - Flowers 1, 0 0 0 FGVC Aircraft 8 0 0 Oxford - IIIT Pets 1, 3 0 0 We use the same hyperparameters for as all datasets as possible for transfer learning .", "ner": [["Food - 1 0 1", "Dataset"], ["Stanford Cars", "Dataset"], ["Oxford - Flowers", "Dataset"], ["FGVC Aircraft", "Dataset"], ["Oxford - IIIT Pets", "Dataset"], ["transfer learning", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "4319457", "sentence": "Generative adversarial networks ( GANs ) have demonstrated to be successful at generating realistic real - world images .", "ner": [["Generative adversarial networks", "Method"], ["GANs", "Method"]], "rel": [["GANs", "Synonym-Of", "Generative adversarial networks"]], "rel_plus": [["GANs:Method", "Synonym-Of", "Generative adversarial networks:Method"]]}
{"doc_id": "4319457", "sentence": "The experiments with the Capsule Network as discriminator result in generated images of a lower quality , compared to a standard convolutional neural network .", "ner": [["Capsule Network", "Method"], ["discriminator", "Method"], ["convolutional neural network", "Method"]], "rel": [["Capsule Network", "Used-For", "discriminator"], ["Capsule Network", "Compare-With", "convolutional neural network"]], "rel_plus": [["Capsule Network:Method", "Used-For", "discriminator:Method"], ["Capsule Network:Method", "Compare-With", "convolutional neural network:Method"]]}
{"doc_id": "4319457", "sentence": "Generative Adversarial Networks ( GANs ) [ Goodfellow et al. , 2 0 1 4 ] are a subclass of generative models that have received a lot of attention because of their ability to generate realistic high quality images .", "ner": [["Generative Adversarial Networks", "Method"], ["GANs", "Method"], ["generative models", "Method"]], "rel": [["GANs", "Synonym-Of", "Generative Adversarial Networks"], ["Generative Adversarial Networks", "SubClass-Of", "generative models"]], "rel_plus": [["GANs:Method", "Synonym-Of", "Generative Adversarial Networks:Method"], ["Generative Adversarial Networks:Method", "SubClass-Of", "generative models:Method"]]}
{"doc_id": "4319457", "sentence": "More recently , people started to apply the GAN framework to other problems , such as text generation and image - to - image translation [ Zhang et al. , 2 0 1 7 , Isola et al. , 2 0 1 6 .", "ner": [["GAN", "Method"], ["text generation", "Task"], ["image - to - image translation", "Task"]], "rel": [["GAN", "Used-For", "text generation"], ["GAN", "Used-For", "image - to - image translation"]], "rel_plus": [["GAN:Method", "Used-For", "text generation:Task"], ["GAN:Method", "Used-For", "image - to - image translation:Task"]]}
{"doc_id": "4319457", "sentence": "We compare standard GANs [ Goodfellow et al. , 2 0 1 4 ] with conditional GANs [ Mirza and Osindero , 2 0 1 4 ] , and supervised networks with unsupervised networks [ Chen et al. , 2 0 1 6 ] .", "ner": [["GANs", "Method"], ["conditional GANs", "Method"], ["supervised networks", "Method"], ["unsupervised networks", "Method"]], "rel": [["GANs", "Compare-With", "conditional GANs"], ["GANs", "Compare-With", "supervised networks"], ["GANs", "Compare-With", "unsupervised networks"]], "rel_plus": [["GANs:Method", "Compare-With", "conditional GANs:Method"], ["GANs:Method", "Compare-With", "supervised networks:Method"], ["GANs:Method", "Compare-With", "unsupervised networks:Method"]]}
{"doc_id": "4319457", "sentence": "Finally , we compare a deep convolutional neural network with the novel Capsule Network as parameterization of the discriminator .", "ner": [["deep convolutional neural network", "Method"], ["Capsule Network", "Method"], ["parameterization", "Method"], ["discriminator", "Method"]], "rel": [["deep convolutional neural network", "Compare-With", "Capsule Network"], ["Capsule Network", "Used-For", "parameterization"], ["parameterization", "Part-Of", "discriminator"]], "rel_plus": [["deep convolutional neural network:Method", "Compare-With", "Capsule Network:Method"], ["Capsule Network:Method", "Used-For", "parameterization:Method"], ["parameterization:Method", "Part-Of", "discriminator:Method"]]}
{"doc_id": "4319457", "sentence": "A conditional GAN is a supervised network that needs for each data point x a label y. This network has many similarities with InfoGAN [ Chen et al. , 2 0 1 6 ] , an unsupervised conditional GAN , which we will introduce in the next section .", "ner": [["conditional GAN", "Method"], ["supervised network", "Method"], ["InfoGAN", "Method"], ["unsupervised conditional GAN", "Method"]], "rel": [["conditional GAN", "SubClass-Of", "supervised network"]], "rel_plus": [["conditional GAN:Method", "SubClass-Of", "supervised network:Method"]]}
{"doc_id": "4319457", "sentence": "In contrast to the conditional GAN , the discriminator does not know about c ( or y in the case of the conditional GAN ) .", "ner": [["conditional GAN", "Method"], ["conditional GAN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "4319457", "sentence": "In the work of an alternative value function named Wasserstein GAN ( WGAN ) is proposed .", "ner": [["Wasserstein GAN", "Method"], ["WGAN", "Method"]], "rel": [["WGAN", "Synonym-Of", "Wasserstein GAN"]], "rel_plus": [["WGAN:Method", "Synonym-Of", "Wasserstein GAN:Method"]]}
{"doc_id": "4319457", "sentence": "Gulrajani et al. [ 2 0 1 7 ] propose to use a gradient penalty to enforce the Lipschitz constraint , this method is referred to as WGAN with gradient penalty ( WGAN - GP ) .", "ner": [["WGAN with gradient penalty", "Method"], ["WGAN - GP", "Method"]], "rel": [["WGAN - GP", "Synonym-Of", "WGAN with gradient penalty"]], "rel_plus": [["WGAN - GP:Method", "Synonym-Of", "WGAN with gradient penalty:Method"]]}
{"doc_id": "4319457", "sentence": "Note that when using either the condition GAN or infoGAN , we need to encode an additional variable ( y or c respectively ) .", "ner": [["condition GAN", "Method"], ["infoGAN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "4319457", "sentence": "In the original paper that introduced GANs [ Goodfellow et al. , 2 0 1 4 ] the authors use a multilayer perceptron for both the generator and discriminator .", "ner": [["GANs", "Method"], ["multilayer perceptron", "Method"], ["generator", "Method"], ["discriminator", "Method"]], "rel": [["multilayer perceptron", "Part-Of", "GANs"], ["multilayer perceptron", "Used-For", "generator"], ["multilayer perceptron", "Used-For", "discriminator"]], "rel_plus": [["multilayer perceptron:Method", "Part-Of", "GANs:Method"], ["multilayer perceptron:Method", "Used-For", "generator:Method"], ["multilayer perceptron:Method", "Used-For", "discriminator:Method"]]}
{"doc_id": "4319457", "sentence": "Convolutional neural networks ( CNNs ) are widely used for supervised learning in computer vision problems .", "ner": [["Convolutional neural networks", "Method"], ["CNNs", "Method"], ["computer vision", "Task"]], "rel": [["CNNs", "Synonym-Of", "Convolutional neural networks"], ["Convolutional neural networks", "Used-For", "computer vision"]], "rel_plus": [["CNNs:Method", "Synonym-Of", "Convolutional neural networks:Method"], ["Convolutional neural networks:Method", "Used-For", "computer vision:Task"]]}
{"doc_id": "4319457", "sentence": "In more recent work [ Radford et al. , 2 0 1 5 ] the GAN setup was succesfully combined with a CNN ( DCGAN ) to create state of the art results .", "ner": [["GAN", "Method"], ["CNN", "Method"], ["DCGAN", "Method"]], "rel": [["GAN", "Part-Of", "DCGAN"], ["CNN", "Part-Of", "DCGAN"]], "rel_plus": [["GAN:Method", "Part-Of", "DCGAN:Method"], ["CNN:Method", "Part-Of", "DCGAN:Method"]]}
{"doc_id": "4319457", "sentence": "Radford et al. [ 2 0 1 5 ] proposed several improvements such as using strided convolutions , batch normalization [ Ioffe and Szegedy , 2 0 1 5 ] , and the ReLU activation function .", "ner": [["strided convolutions", "Method"], ["batch normalization", "Method"], ["ReLU activation", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "4319457", "sentence": "Part of this problem is caused by the invariant detection of features , CNNs process the likelihood of features without subsequently processing the properties of these features ( e.g. angle or size ) .", "ner": [["CNNs", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "4319457", "sentence": "In our experiments we make use of two datasets , MNIST [ LeCun et al. , 1 9 9 8 ] and CelebA [ Liu et al. , 2 0 1 5 ] .", "ner": [["MNIST", "Dataset"], ["CelebA", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "4319457", "sentence": "For the Adam optimizer we used 0. 5 for \u03b2 1 and 0. 9 for \u03b2 2 when training WGAN - GP , otherwise we used 0. 5 and 0. 9 9 respectively .", "ner": [["Adam optimizer", "Method"], ["WGAN - GP", "Method"]], "rel": [["Adam optimizer", "Part-Of", "WGAN - GP"]], "rel_plus": [["Adam optimizer:Method", "Part-Of", "WGAN - GP:Method"]]}
{"doc_id": "4319457", "sentence": "In all experiments with the MNIST dataset we used a z vector of dimension 6 4 , whereas with the CelebA dataset we used vectors of dimension 1 2 8 .", "ner": [["MNIST", "Dataset"], ["CelebA", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "4319457", "sentence": "The second layer has an output dimension that matches the predetermined dimension of the latent code z. When we train a conditional GAN or infoGAN , the output dimension of network E is equivalent to the total dimensions of the latent vector and vectors y or c. Note that in this context the latent vector z is not really a noise vector .", "ner": [["conditional GAN", "Method"], ["infoGAN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "4319457", "sentence": "Our network has a single output capsule because the task is binary classification ( differentiate real from generated images ) , whereas Sabour et al. [ 2 0 1 7 ] used 1 0 output capsules because they tried to classify digits in the MNIST dataset .", "ner": [["binary classification", "Task"], ["MNIST", "Dataset"]], "rel": [["MNIST", "Benchmark-For", "binary classification"]], "rel_plus": [["MNIST:Dataset", "Benchmark-For", "binary classification:Task"]]}
{"doc_id": "4319457", "sentence": "The first layer is a standard convolutional layer with 1 2 8 convolutional filters with size 9 \u00d7 9 , a stride of 1 , and the ReLU activation function .", "ner": [["convolutional layer", "Method"], ["convolutional filters", "Method"], ["ReLU activation", "Method"]], "rel": [["convolutional filters", "Part-Of", "convolutional layer"], ["ReLU activation", "Part-Of", "convolutional layer"]], "rel_plus": [["convolutional filters:Method", "Part-Of", "convolutional layer:Method"], ["ReLU activation:Method", "Part-Of", "convolutional layer:Method"]]}
{"doc_id": "4319457", "sentence": "FC stands for a fully - connected layer , BN denotes batch normalization , and t - conv is short for transposed convolution . z - dim is 6 4 when using MNIST , for CelebA it is 1 2 8 .", "ner": [["FC", "Method"], ["fully - connected layer", "Method"], ["BN", "Method"], ["batch normalization", "Method"], ["t - conv", "Method"], ["transposed convolution", "Method"], ["MNIST", "Dataset"], ["CelebA", "Dataset"]], "rel": [["fully - connected layer", "Synonym-Of", "FC"], ["batch normalization", "Synonym-Of", "BN"], ["transposed convolution", "Synonym-Of", "t - conv"]], "rel_plus": [["fully - connected layer:Method", "Synonym-Of", "FC:Method"], ["batch normalization:Method", "Synonym-Of", "BN:Method"], ["transposed convolution:Method", "Synonym-Of", "t - conv:Method"]]}
{"doc_id": "4319457", "sentence": "We found that using the WGAN - GP training , the results are of higher quality compared to the standard GAN training objective .", "ner": [["WGAN - GP", "Method"], ["GAN", "Method"]], "rel": [["WGAN - GP", "Compare-With", "GAN"]], "rel_plus": [["WGAN - GP:Method", "Compare-With", "GAN:Method"]]}
{"doc_id": "4319457", "sentence": "When training the conditional GAN we could use either the standard GAN objective or the WGAN - GP objective .", "ner": [["conditional GAN", "Method"], ["GAN objective", "Method"], ["WGAN - GP objective", "Method"]], "rel": [["GAN objective", "Part-Of", "conditional GAN"], ["WGAN - GP objective", "Part-Of", "conditional GAN"]], "rel_plus": [["GAN objective:Method", "Part-Of", "conditional GAN:Method"], ["WGAN - GP objective:Method", "Part-Of", "conditional GAN:Method"]]}
{"doc_id": "4319457", "sentence": "We found that , similar to the unconditional GAN , the results are better when using WGAN - GP .", "ner": [["unconditional GAN", "Method"], ["WGAN - GP", "Method"]], "rel": [["unconditional GAN", "Compare-With", "WGAN - GP"]], "rel_plus": [["unconditional GAN:Method", "Compare-With", "WGAN - GP:Method"]]}
{"doc_id": "4319457", "sentence": "The results of the conditional GAN for the CelebA dataset are shown in Figure 1 0 , where we made use of the binary attributes blond hair , eyeglasses , and male .", "ner": [["conditional GAN", "Method"], ["CelebA", "Dataset"]], "rel": [["conditional GAN", "Evaluated-With", "CelebA"]], "rel_plus": [["conditional GAN:Method", "Evaluated-With", "CelebA:Dataset"]]}
{"doc_id": "4319457", "sentence": "Figures 2 2 and 2 3 show more results for the conditional GAN using the CelebA dataset .", "ner": [["conditional GAN", "Method"], ["CelebA", "Dataset"]], "rel": [["conditional GAN", "Evaluated-With", "CelebA"]], "rel_plus": [["conditional GAN:Method", "Evaluated-With", "CelebA:Dataset"]]}
{"doc_id": "4319457", "sentence": "Figures 1 1 and 1 2 show the results of the encoding and reconstruction for MNIST and CelebA , respectively .", "ner": [["encoding", "Task"], ["reconstruction", "Task"], ["MNIST", "Dataset"], ["CelebA", "Dataset"]], "rel": [["MNIST", "Benchmark-For", "encoding"], ["CelebA", "Benchmark-For", "encoding"], ["MNIST", "Benchmark-For", "reconstruction"], ["CelebA", "Benchmark-For", "reconstruction"]], "rel_plus": [["MNIST:Dataset", "Benchmark-For", "encoding:Task"], ["CelebA:Dataset", "Benchmark-For", "encoding:Task"], ["MNIST:Dataset", "Benchmark-For", "reconstruction:Task"], ["CelebA:Dataset", "Benchmark-For", "reconstruction:Task"]]}
{"doc_id": "4319457", "sentence": "Figure 1 1 : Image encoding and reconstruction for the MNIST dataset .", "ner": [["Image encoding", "Task"], ["reconstruction", "Task"], ["MNIST", "Dataset"]], "rel": [["MNIST", "Benchmark-For", "Image encoding"], ["MNIST", "Benchmark-For", "reconstruction"]], "rel_plus": [["MNIST:Dataset", "Benchmark-For", "Image encoding:Task"], ["MNIST:Dataset", "Benchmark-For", "reconstruction:Task"]]}
{"doc_id": "4319457", "sentence": "Figure 1 2 : Image encoding and reconstruction for the CelebA dataset .", "ner": [["Image encoding", "Task"], ["reconstruction", "Task"], ["CelebA", "Dataset"]], "rel": [["CelebA", "Benchmark-For", "Image encoding"], ["CelebA", "Benchmark-For", "reconstruction"]], "rel_plus": [["CelebA:Dataset", "Benchmark-For", "Image encoding:Task"], ["CelebA:Dataset", "Benchmark-For", "reconstruction:Task"]]}
{"doc_id": "4319457", "sentence": "We found that the standard GAN can be unstable , using WGAN - GP results in more stable training and the generated images are of a higher quality .", "ner": [["GAN", "Method"], ["WGAN - GP", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "4319457", "sentence": "We compared two conditional GANs , namely the standard conditional GAN that is supervised and InfoGAN , an unsupervised network .", "ner": [["conditional GANs", "Method"], ["conditional GAN", "Method"], ["InfoGAN", "Method"]], "rel": [["conditional GAN", "SubClass-Of", "conditional GANs"], ["InfoGAN", "SubClass-Of", "conditional GANs"], ["conditional GAN", "Compare-With", "InfoGAN"]], "rel_plus": [["conditional GAN:Method", "SubClass-Of", "conditional GANs:Method"], ["InfoGAN:Method", "SubClass-Of", "conditional GANs:Method"], ["conditional GAN:Method", "Compare-With", "InfoGAN:Method"]]}
{"doc_id": "4319457", "sentence": "We also used an encoder network that makes it possible to create a reconstruction of an image , independent of the used GAN variant .", "ner": [["GAN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "4319457", "sentence": "The reconstructions for MNIST are seemingly perfect , whereas for the CelebA dataset the reconstructions are good but blurry .", "ner": [["MNIST", "Dataset"], ["CelebA", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "4319457", "sentence": "Using this technique we can apply dimensionality reduction , similar to a variational autoencoder [ Kingma and Welling , 2 0 1 3 ] .", "ner": [["dimensionality reduction", "Method"], ["variational autoencoder", "Method"]], "rel": [["dimensionality reduction", "Compare-With", "variational autoencoder"]], "rel_plus": [["dimensionality reduction:Method", "Compare-With", "variational autoencoder:Method"]]}
{"doc_id": "4319457", "sentence": "We could even extend this , using the discriminator in a GAN as a measure for the reconstruction objective , as introduced by Larsen et al. [ 2 0 1 6 ] .", "ner": [["discriminator", "Method"], ["GAN", "Method"], ["reconstruction", "Task"]], "rel": [["discriminator", "Part-Of", "GAN"], ["discriminator", "Used-For", "reconstruction"]], "rel_plus": [["discriminator:Method", "Part-Of", "GAN:Method"], ["discriminator:Method", "Used-For", "reconstruction:Task"]]}
{"doc_id": "4319457", "sentence": "More experiments with methods that improve the stability of GANs are needed , such as spectral normalization [ Takeru Miyato , 2 0 1 8 ] and using two discriminators [ Nguyen et al. , 2 0 1 7 ] .", "ner": [["GANs", "Method"], ["spectral normalization", "Method"], ["discriminators", "Method"]], "rel": [["discriminators", "Part-Of", "GANs"], ["spectral normalization", "Part-Of", "GANs"]], "rel_plus": [["discriminators:Method", "Part-Of", "GANs:Method"], ["spectral normalization:Method", "Part-Of", "GANs:Method"]]}
{"doc_id": "4319457", "sentence": "An example for bilinear interpolation on the MNIST dataset is given in Figure 1 6 , examples for the CelebA dataset are shown in Figures 1 8 and 1 9 .", "ner": [["bilinear interpolation", "Method"], ["MNIST", "Dataset"], ["CelebA", "Dataset"]], "rel": [["bilinear interpolation", "Used-For", "MNIST"], ["bilinear interpolation", "Used-For", "CelebA"]], "rel_plus": [["bilinear interpolation:Method", "Used-For", "MNIST:Dataset"], ["bilinear interpolation:Method", "Used-For", "CelebA:Dataset"]]}
{"doc_id": "202750230", "sentence": "Overparameterized transformer networks have obtained state of the art results in various natural language processing tasks , such as machine translation , language modeling , and question answering .", "ner": [["natural language processing", "Task"], ["machine translation", "Task"], ["language modeling", "Task"], ["question answering", "Task"]], "rel": [["machine translation", "SubTask-Of", "natural language processing"], ["language modeling", "SubTask-Of", "natural language processing"], ["question answering", "SubTask-Of", "natural language processing"]], "rel_plus": [["machine translation:Task", "SubTask-Of", "natural language processing:Task"], ["language modeling:Task", "SubTask-Of", "natural language processing:Task"], ["question answering:Task", "SubTask-Of", "natural language processing:Task"]]}
{"doc_id": "202750230", "sentence": "In this work , we explore LayerDrop , a form of structured dropout , which has a regularization effect during training and allows for efficient pruning at inference time .", "ner": [["LayerDrop", "Method"], ["dropout", "Method"]], "rel": [["LayerDrop", "SubClass-Of", "dropout"]], "rel_plus": [["LayerDrop:Method", "SubClass-Of", "dropout:Method"]]}
{"doc_id": "202750230", "sentence": "We demonstrate the effectiveness of our approach by improving the state of the art on machine translation , language modeling , summarization , question answering , and language understanding benchmarks .", "ner": [["machine translation", "Task"], ["language modeling", "Task"], ["summarization", "Task"], ["question answering", "Task"], ["language understanding", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "202750230", "sentence": "Transformer architectures ( Vaswani et al. , 2 0 1 7 ) have become the dominant architecture in natural language processing , with state - of - the - art performance across a variety of tasks , including machine translation ( Vaswani et al. , 2 0 1 7 ; Ott et al. , 2 0 1 8) , language modeling Baevski & Auli , 2 0 1 8) and sentence representation ( Devlin et al. , 2 0 1 8 ; .", "ner": [["Transformer", "Method"], ["natural language processing", "Task"], ["machine translation", "Task"], ["language modeling", "Task"], ["sentence representation", "Task"]], "rel": [["Transformer", "Used-For", "natural language processing"], ["Transformer", "Used-For", "machine translation"], ["Transformer", "Used-For", "language modeling"], ["Transformer", "Used-For", "sentence representation"]], "rel_plus": [["Transformer:Method", "Used-For", "natural language processing:Task"], ["Transformer:Method", "Used-For", "machine translation:Task"], ["Transformer:Method", "Used-For", "language modeling:Task"], ["Transformer:Method", "Used-For", "sentence representation:Task"]]}
{"doc_id": "202750230", "sentence": "The core of our method is to sample small sub - networks from the larger model during training by randomly dropping model weights as in Dropout ( Hinton et al. , 2 0 1 2 ) or DropConnect ( Wan et al. , 2 0 1 3 ) .", "ner": [["Dropout", "Method"], ["DropConnect", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202750230", "sentence": "We validate our findings on a variety of competitive benchmarks , namely WMT 1 4 English - German for machine translation , WikiText - 1 0 3 ( Merity et al. , 2 0 1 6 ) for language modeling , CNN - Dailymail ( Hermann et al. , 2 0 1 5 ) for abstractive summarization , ELI 5 for long form question answering , and several natural language understanding tasks ( Wang et al. , 2 0 1 9 ) for sentence representation .", "ner": [["WMT 1 4 English - German", "Dataset"], ["machine translation", "Task"], ["WikiText - 1 0 3", "Dataset"], ["language modeling", "Task"], ["CNN - Dailymail", "Dataset"], ["abstractive summarization", "Task"], ["ELI 5", "Dataset"], ["long form question answering", "Task"], ["natural language understanding", "Task"], ["sentence representation", "Task"]], "rel": [["WMT 1 4 English - German", "Benchmark-For", "machine translation"], ["WikiText - 1 0 3", "Benchmark-For", "language modeling"], ["CNN - Dailymail", "Benchmark-For", "abstractive summarization"], ["ELI 5", "Benchmark-For", "long form question answering"], ["natural language understanding", "Used-For", "sentence representation"]], "rel_plus": [["WMT 1 4 English - German:Dataset", "Benchmark-For", "machine translation:Task"], ["WikiText - 1 0 3:Dataset", "Benchmark-For", "language modeling:Task"], ["CNN - Dailymail:Dataset", "Benchmark-For", "abstractive summarization:Task"], ["ELI 5:Dataset", "Benchmark-For", "long form question answering:Task"], ["natural language understanding:Task", "Used-For", "sentence representation:Task"]]}
{"doc_id": "202750230", "sentence": "Overall , applying Layer - Drop to Transformer networks provides the following key advantages : \u2022 LayerDrop regularizes very deep Transformers and stabilizes their training , leading to stateof - the - art performance across a variety of benchmarks . \u2022 Small and efficient models of any depth can be extracted automatically at test time from a single large pre - trained model , without the need for finetuning . \u2022 LayerDrop is as simple to implement as dropout .", "ner": [["Layer - Drop", "Method"], ["Transformer", "Method"], ["LayerDrop", "Method"], ["Transformers", "Method"], ["large pre - trained model", "Method"], ["LayerDrop", "Method"], ["dropout", "Method"]], "rel": [["Layer - Drop", "Part-Of", "Transformer"], ["LayerDrop", "Part-Of", "Transformers"], ["LayerDrop", "SubClass-Of", "dropout"]], "rel_plus": [["Layer - Drop:Method", "Part-Of", "Transformer:Method"], ["LayerDrop:Method", "Part-Of", "Transformers:Method"], ["LayerDrop:Method", "SubClass-Of", "dropout:Method"]]}
{"doc_id": "202750230", "sentence": "Our approach is a form of Dropout ( Srivastava et al. , 2 0 1 4 ) applied to model weights instead of activations , as in DropConnect ( Wan et al. , 2 0 1 3 ) .", "ner": [["Dropout", "Method"], ["DropConnect", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202750230", "sentence": "Gomez et al. ( 2 0 1 8) propose a targeted Dropout and DropConnect , where they learn the drop rate of the weights to match a targeted pruning scheme .", "ner": [["targeted Dropout", "Method"], ["DropConnect", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202750230", "sentence": "Concurrently to this work , Pham et al. ( 2 0 1 9 ) applied Stochastic Depth to train very deep Transformers for speech and show the benefits of its regularization effect .", "ner": [["Stochastic Depth", "Method"], ["Transformers", "Method"]], "rel": [["Stochastic Depth", "Part-Of", "Transformers"]], "rel_plus": [["Stochastic Depth:Method", "Part-Of", "Transformers:Method"]]}
{"doc_id": "202750230", "sentence": "Structured pruning has been used in some NLP applications , such as machine translation ( See et al. , 2 0 1 6 ) , text classification and language modeling ( Murray & Chiang , 2 0 1 5 ) .", "ner": [["NLP", "Task"], ["machine translation", "Task"], ["text classification", "Task"], ["language modeling", "Task"]], "rel": [["machine translation", "SubTask-Of", "NLP"], ["text classification", "SubTask-Of", "NLP"], ["language modeling", "SubTask-Of", "NLP"]], "rel_plus": [["machine translation:Task", "SubTask-Of", "NLP:Task"], ["text classification:Task", "SubTask-Of", "NLP:Task"], ["language modeling:Task", "SubTask-Of", "NLP:Task"]]}
{"doc_id": "202750230", "sentence": "Reducing the memory footprint of Transformer architectures and BERT in particular is an active subject of research .", "ner": [["Transformer", "Method"], ["BERT", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202750230", "sentence": "Each sub - layer is followed by a AddNorm operation that is a residual connection ( He et al. , 2 0 1 6 ) and a layer normalization ( Ba et al. , 2 0 1 6 ) .", "ner": [["AddNorm operation", "Method"], ["residual connection", "Method"], ["layer normalization", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202750230", "sentence": "This is inspired by the Stochastic Depth approach of Huang et al. ( 2 0 1 6 ) used to train very deep ResNets ( He et al. , 2 0 1 5 ) .", "ner": [["Stochastic Depth", "Method"], ["ResNets", "Method"]], "rel": [["Stochastic Depth", "Part-Of", "ResNets"]], "rel_plus": [["Stochastic Depth:Method", "Part-Of", "ResNets:Method"]]}
{"doc_id": "202750230", "sentence": "We apply our method to a variety of sequence modeling tasks : neural machine translation , language modeling , summarization , long form question answering , and various natural language understanding tasks .", "ner": [["sequence modeling", "Task"], ["neural machine translation", "Task"], ["language modeling", "Task"], ["summarization", "Task"], ["long form question answering", "Task"], ["natural language understanding", "Task"]], "rel": [["neural machine translation", "SubTask-Of", "sequence modeling"], ["language modeling", "SubTask-Of", "sequence modeling"], ["summarization", "SubTask-Of", "sequence modeling"], ["long form question answering", "SubTask-Of", "sequence modeling"], ["natural language understanding", "SubTask-Of", "sequence modeling"]], "rel_plus": [["neural machine translation:Task", "SubTask-Of", "sequence modeling:Task"], ["language modeling:Task", "SubTask-Of", "sequence modeling:Task"], ["summarization:Task", "SubTask-Of", "sequence modeling:Task"], ["long form question answering:Task", "SubTask-Of", "sequence modeling:Task"], ["natural language understanding:Task", "SubTask-Of", "sequence modeling:Task"]]}
{"doc_id": "202750230", "sentence": "We experiment on the WMT English - German machine translation benchmark using the Transformer Big architecture .", "ner": [["WMT English - German machine translation", "Dataset"], ["Transformer Big architecture", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202750230", "sentence": "We use the dataset of 4. 5 M en - de sentence pairs from WMT 1 6 ( Vaswani et al. , 2 0 1 7 ) for training , newstest 2 0 1 3 for validation , and newstest 2 0 1 4 for test .", "ner": [["WMT 1 6", "Dataset"], ["newstest 2 0 1 3", "Dataset"], ["newstest 2 0 1 4", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "202750230", "sentence": "We optimize the dropout value within the range { 0. 1 , 0. 2 , 0. 5 } on the validation set and set the LayerDrop rate p to 0. 2 .", "ner": [["dropout", "Method"], ["LayerDrop", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202750230", "sentence": "We experiment on the Wikitext - 1 0 3 language modeling benchmark ( Merity et al. , 2 0 1 6 ) which contains 1 0 0 M tokens and a large vocabulary size of 2 6 0 K .", "ner": [["Wikitext - 1 0 3 language modeling", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "202750230", "sentence": "We set the LayerDrop rate p to 0. 2 and tune the standard dropout parameter in { 0. 1 , 0. 2 , 0. 3 } on the validation set .", "ner": [["LayerDrop", "Method"], ["dropout", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202750230", "sentence": "We adopt the Transformer base architecture and training schedule from and experiment on the CNN - Dailymail multi - sentence summarization benchmark .", "ner": [["Transformer", "Method"], ["CNN - Dailymail", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "202750230", "sentence": "We use two datasets : Bookscorpus + Wiki from and the larger combination of Bookscorpus + OpenWebText + CC - News + Stories .", "ner": [["Bookscorpus + Wiki", "Dataset"], ["Bookscorpus + OpenWebText + CC - News + Stories", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "202750230", "sentence": "Specifically , we evaluate accuracy on MRPC ( Dolan & Brockett , 2 0 0 5 ) , QNLI ( Rajpurkar et al. , 2 0 1 6 ) , MNLI ( Williams et al. , 2 0 1 8) , and SST 2 ( Socher et al. , 2 0 1 3 ) .   Language Modeling .", "ner": [["MRPC", "Dataset"], ["QNLI", "Dataset"], ["MNLI", "Dataset"], ["SST 2", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "202750230", "sentence": "In Table 2 , we show the impact of LayerDrop on the performance of a Transformer network trained in the setting of Adaptive Inputs ( Baevski & Auli , 2 0 1 8) .", "ner": [["LayerDrop", "Method"], ["Transformer", "Method"]], "rel": [["LayerDrop", "Part-Of", "Transformer"]], "rel_plus": [["LayerDrop:Method", "Part-Of", "Transformer:Method"]]}
{"doc_id": "202750230", "sentence": "Adding LayerDrop to a 1 6 layer Transformer improves the performance by 0. 4 perplexity , matching the state - of - the - art results of Transformer - XL .", "ner": [["LayerDrop", "Method"], ["Transformer", "Method"], ["Transformer - XL", "Method"]], "rel": [["LayerDrop", "Part-Of", "Transformer"], ["Transformer", "Compare-With", "Transformer - XL"]], "rel_plus": [["LayerDrop:Method", "Part-Of", "Transformer:Method"], ["Transformer:Method", "Compare-With", "Transformer - XL:Method"]]}
{"doc_id": "202750230", "sentence": "Our 4 0 layer Transformer with LayerDrop further improves the state of the art by 0. 6 points .", "ner": [["Transformer", "Method"], ["LayerDrop", "Method"]], "rel": [["LayerDrop", "Part-Of", "Transformer"]], "rel_plus": [["LayerDrop:Method", "Part-Of", "Transformer:Method"]]}
{"doc_id": "202750230", "sentence": "Very deep Transformers are typically hard to train because of instability and memory usage , and they are prone to overfitting on a small dataset like Wikitext - 1 0 3 .", "ner": [["Transformers", "Method"], ["Wikitext - 1 0 3", "Dataset"]], "rel": [["Transformers", "Trained-With", "Wikitext - 1 0 3"]], "rel_plus": [["Transformers:Method", "Trained-With", "Wikitext - 1 0 3:Dataset"]]}
{"doc_id": "202750230", "sentence": "In a second set of experiments , we look at the impact of LayerDrop on pre - training for sentence representation models and subsequent finetuning on multiple natural language understanding tasks .", "ner": [["LayerDrop", "Method"], ["sentence representation", "Task"], ["natural language understanding", "Task"]], "rel": [["LayerDrop", "Used-For", "sentence representation"], ["LayerDrop", "Used-For", "natural language understanding"]], "rel_plus": [["LayerDrop:Method", "Used-For", "sentence representation:Task"], ["LayerDrop:Method", "Used-For", "natural language understanding:Task"]]}
{"doc_id": "202750230", "sentence": "We compare our models to a variant of BERT for sentence representations , called RoBERTa , and analyze the results of finetuning for data adaptation on MNLI , MRPC , QNLI , and SST 2 .", "ner": [["BERT", "Method"], ["sentence representations", "Task"], ["RoBERTa", "Method"], ["MNLI", "Dataset"], ["MRPC", "Dataset"], ["QNLI", "Dataset"], ["SST 2", "Dataset"]], "rel": [["RoBERTa", "SubClass-Of", "BERT"], ["BERT", "Used-For", "sentence representations"], ["RoBERTa", "Trained-With", "MNLI"], ["RoBERTa", "Trained-With", "MRPC"], ["RoBERTa", "Trained-With", "QNLI"], ["RoBERTa", "Trained-With", "SST 2"]], "rel_plus": [["RoBERTa:Method", "SubClass-Of", "BERT:Method"], ["BERT:Method", "Used-For", "sentence representations:Task"], ["RoBERTa:Method", "Trained-With", "MNLI:Dataset"], ["RoBERTa:Method", "Trained-With", "MRPC:Dataset"], ["RoBERTa:Method", "Trained-With", "QNLI:Dataset"], ["RoBERTa:Method", "Trained-With", "SST 2:Dataset"]]}
{"doc_id": "202750230", "sentence": "We compare the performance of the large architecture on the BooksCorpus+Wiki dataset used in BERT .", "ner": [["BooksCorpus+Wiki", "Dataset"], ["BERT", "Method"]], "rel": [["BERT", "Evaluated-With", "BooksCorpus+Wiki"]], "rel_plus": [["BERT:Method", "Evaluated-With", "BooksCorpus+Wiki:Dataset"]]}
{"doc_id": "202750230", "sentence": "Comparing fixed model size and training data , LayerDrop can improve the performance of RoBERTa on several tasks .", "ner": [["LayerDrop", "Method"], ["RoBERTa", "Method"]], "rel": [["LayerDrop", "Part-Of", "RoBERTa"]], "rel_plus": [["LayerDrop:Method", "Part-Of", "RoBERTa:Method"]]}
{"doc_id": "202750230", "sentence": "In Figure 2 , we investigate the impact of the number of pruned decoder layers on the performance of a Transformer for language modeling , neural machine translation , and summarization .", "ner": [["Transformer", "Method"], ["language modeling", "Task"], ["neural machine translation", "Task"], ["summarization", "Task"]], "rel": [["Transformer", "Used-For", "language modeling"], ["Transformer", "Used-For", "neural machine translation"], ["Transformer", "Used-For", "summarization"]], "rel_plus": [["Transformer:Method", "Used-For", "language modeling:Task"], ["Transformer:Method", "Used-For", "neural machine translation:Task"], ["Transformer:Method", "Used-For", "summarization:Task"]]}
{"doc_id": "202750230", "sentence": "We compare three different settings : standard Transformer models trained without LayerDrop but subsequently pruned , standard Transformer models trained from scratch to each desired depth , and lastly our approach : pruning layers of a Transformer trained with Layer - Drop .", "ner": [["Transformer", "Method"], ["LayerDrop", "Method"], ["Transformer", "Method"], ["Transformer", "Method"], ["Layer - Drop", "Method"]], "rel": [["Layer - Drop", "Part-Of", "Transformer"]], "rel_plus": [["Layer - Drop:Method", "Part-Of", "Transformer:Method"]]}
{"doc_id": "202750230", "sentence": "For completeness , dropping layers of a deep Transformer trained without LayerDrop performs poorly as it was not trained to be robust to missing layers .", "ner": [["Transformer", "Method"], ["LayerDrop", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202750230", "sentence": "In Table 7 ( left ) , we compare pruning Transformers trained with LayerDrop to different approaches used to create smaller , shallower models .", "ner": [["Transformers", "Method"], ["LayerDrop", "Method"]], "rel": [["LayerDrop", "Part-Of", "Transformers"]], "rel_plus": [["LayerDrop:Method", "Part-Of", "Transformers:Method"]]}
{"doc_id": "202750230", "sentence": "We compare to BERT base and RoBERTa base trained from scratch with 6 and 3 layers as well as recent work on distillation , called DistilBERT ( Sanh , 2 0 1 9 ) .", "ner": [["BERT base", "Method"], ["RoBERTa base", "Method"], ["DistilBERT", "Method"]], "rel": [["DistilBERT", "SubClass-Of", "BERT base"], ["DistilBERT", "SubClass-Of", "RoBERTa base"]], "rel_plus": [["DistilBERT:Method", "SubClass-Of", "BERT base:Method"], ["DistilBERT:Method", "SubClass-Of", "RoBERTa base:Method"]]}
{"doc_id": "202750230", "sentence": "We analyze both BERT and RoBERTa models as the vocabulary is not the same due to differences in subword tokenization , which affects performance .", "ner": [["BERT", "Method"], ["RoBERTa", "Method"]], "rel": [["BERT", "Compare-With", "RoBERTa"]], "rel_plus": [["BERT:Method", "Compare-With", "RoBERTa:Method"]]}
{"doc_id": "202750230", "sentence": "DistilBERT occasionally performs worse than BERT of the same size trained from scratch , which confirms the findings of training small models from scratch .", "ner": [["DistilBERT", "Method"], ["BERT", "Method"]], "rel": [["DistilBERT", "Compare-With", "BERT"]], "rel_plus": [["DistilBERT:Method", "Compare-With", "BERT:Method"]]}
{"doc_id": "202750230", "sentence": "Our approach , however , obtains results better than BERT and RoBERTa trained from scratch .", "ner": [["BERT", "Method"], ["RoBERTa", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202750230", "sentence": "Further , our method does not need any post - processing : we simply prune every other layer of our RoBERTa model that has been pre - trained with LayerDrop and finetune the small models on each of the downstream tasks , following standard procedure .", "ner": [["RoBERTa", "Method"], ["LayerDrop", "Method"]], "rel": [["LayerDrop", "Part-Of", "RoBERTa"]], "rel_plus": [["LayerDrop:Method", "Part-Of", "RoBERTa:Method"]]}
{"doc_id": "202750230", "sentence": "In a variety of text generation and pre - training tasks , we show that LayerDrop enables and stabilizes the training of substantially deeper networks and simultaneously allows for the extraction of models of various depths with strong performance .", "ner": [["text generation", "Task"], ["pre - training", "Task"], ["LayerDrop", "Method"]], "rel": [["LayerDrop", "Used-For", "text generation"], ["LayerDrop", "Used-For", "pre - training"]], "rel_plus": [["LayerDrop:Method", "Used-For", "text generation:Task"], ["LayerDrop:Method", "Used-For", "pre - training:Task"]]}
{"doc_id": "202750230", "sentence": "A. 1 ADDITIONAL IMPLEMENTATION DETAILS WMT en - de : We model a 3 2 K joint byte - pair encoding .", "ner": [["WMT en - de", "Dataset"], ["byte - pair encoding", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202750230", "sentence": "IWSLT de - en : Training smaller models : We train the 6 and 3 layer RoBERTa models following the same settings , but using the smaller number of layers and without LayerDrop .", "ner": [["IWSLT de - en", "Dataset"], ["RoBERTa", "Method"], ["LayerDrop", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202750230", "sentence": "Training larger models : We train the 4 8 layer RoBERTa model with 0. 5 LayerDrop so only 2 4 layers on average are active during a forward pass .", "ner": [["RoBERTa", "Method"], ["LayerDrop", "Method"]], "rel": [["LayerDrop", "Part-Of", "RoBERTa"]], "rel_plus": [["LayerDrop:Method", "Part-Of", "RoBERTa:Method"]]}
{"doc_id": "202750230", "sentence": "Pruning : When pruning RoBERTa models , we use the Every Other Layer strategy and finetune without LayerDrop for the smaller models .", "ner": [["RoBERTa", "Method"], ["LayerDrop", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202750230", "sentence": "IWSLT Table 6 displays results on the IWSLT de - en dataset .", "ner": [["IWSLT", "Dataset"], ["IWSLT de - en", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "202750230", "sentence": "We see small improvement , likely as the network is small and already has a large quantity of regularization with dropout , attention dropout , and weight decay .", "ner": [["dropout", "Method"], ["attention dropout", "Method"], ["weight decay", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202750230", "sentence": "The Transformer is not the state of the art architecture , and there remains a large gap between the Transformer and the DynamicConv model proposed by Wu et al. ( 2 0 1 9 ) .", "ner": [["Transformer", "Method"], ["Transformer", "Method"], ["DynamicConv", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202750230", "sentence": "The numerical values corresponding to the pruned 6 and 3 layer RoBERTa + LayerDrop models are shown in Table 7 .", "ner": [["RoBERTa + LayerDrop", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "54447105", "sentence": "We aim at benefiting from several datasets with different categories but without additional labelling , not only to increase the number of categories detected , but also to take advantage from transfer learning and to enhance domain independence .    Our dataset merging procedure starts with training several initial Faster R - CNN on the different datasets while considering the complementary datasets ' images for domain adaptation .", "ner": [["transfer learning", "Task"], ["Faster R - CNN", "Method"], ["domain adaptation", "Task"]], "rel": [["Faster R - CNN", "Used-For", "domain adaptation"]], "rel_plus": [["Faster R - CNN:Method", "Used-For", "domain adaptation:Task"]]}
{"doc_id": "54447105", "sentence": "The joint training handles unsafe targets with a new classification loss called SoftSig in a softly supervised way .    Experimental results show that in the case of fashion detection for images in the wild , merging Modanet with COCO increases the final performance from 4 5 . 5 % to 5 7 . 4 % in mAP .", "ner": [["classification", "Task"], ["SoftSig", "Method"], ["fashion detection", "Task"], ["Modanet", "Dataset"], ["COCO", "Dataset"]], "rel": [["SoftSig", "Used-For", "classification"], ["COCO", "Benchmark-For", "fashion detection"], ["Modanet", "Benchmark-For", "fashion detection"]], "rel_plus": [["SoftSig:Method", "Used-For", "classification:Task"], ["COCO:Dataset", "Benchmark-For", "fashion detection:Task"], ["Modanet:Dataset", "Benchmark-For", "fashion detection:Task"]]}
{"doc_id": "54447105", "sentence": "Applying our soft distillation to the task of detection with domain shift between GTA and Cityscapes enables to beat the state - of - the - art by 5. 3 points .", "ner": [["detection", "Task"], ["GTA", "Dataset"], ["Cityscapes", "Dataset"]], "rel": [["GTA", "Benchmark-For", "detection"], ["Cityscapes", "Benchmark-For", "detection"]], "rel_plus": [["GTA:Dataset", "Benchmark-For", "detection:Task"], ["Cityscapes:Dataset", "Benchmark-For", "detection:Task"]]}
{"doc_id": "54447105", "sentence": "Convolutional Neural Networks ( CNNs ) [ 3 4 , 2 6 ] has become the default method for any computer vision task , and is widely used for problems such as image classification , semantic segmentation or visual relationship detection .", "ner": [["Convolutional Neural Networks", "Method"], ["CNNs", "Method"], ["computer vision", "Task"], ["image classification", "Task"], ["semantic segmentation", "Task"], ["visual relationship detection", "Task"]], "rel": [["CNNs", "Synonym-Of", "Convolutional Neural Networks"], ["Convolutional Neural Networks", "Used-For", "computer vision"], ["Convolutional Neural Networks", "Used-For", "image classification"], ["Convolutional Neural Networks", "Used-For", "semantic segmentation"], ["Convolutional Neural Networks", "Used-For", "visual relationship detection"]], "rel_plus": [["CNNs:Method", "Synonym-Of", "Convolutional Neural Networks:Method"], ["Convolutional Neural Networks:Method", "Used-For", "computer vision:Task"], ["Convolutional Neural Networks:Method", "Used-For", "image classification:Task"], ["Convolutional Neural Networks:Method", "Used-For", "semantic segmentation:Task"], ["Convolutional Neural Networks:Method", "Used-For", "visual relationship detection:Task"]]}
{"doc_id": "54447105", "sentence": "One of the key computer vision task is certainly object detection .", "ner": [["computer vision", "Task"], ["object detection", "Task"]], "rel": [["object detection", "SubTask-Of", "computer vision"]], "rel_plus": [["object detection:Task", "SubTask-Of", "computer vision:Task"]]}
{"doc_id": "54447105", "sentence": "Best - performing detectors are Fully Supervised Detectors ( FSDs ): instance annotations are needed for each object in each image , composed of a category and its location .", "ner": [["Fully Supervised Detectors", "Method"], ["FSDs", "Method"]], "rel": [["FSDs", "Synonym-Of", "Fully Supervised Detectors"]], "rel_plus": [["FSDs:Method", "Synonym-Of", "Fully Supervised Detectors:Method"]]}
{"doc_id": "54447105", "sentence": "The problem of training CNNs for classification on small datasets is usually tackled by transfer learning methods [ 5 8 ] : the network is pretrained on a large labeled dataset such as Imagenet [ 1 8 ] , and fine - tuned on the task at hand while preserving the original capabilities [ 3 6 ] .", "ner": [["CNNs", "Method"], ["classification", "Task"], ["transfer learning methods", "Method"], ["Imagenet", "Dataset"]], "rel": [["CNNs", "Used-For", "classification"], ["transfer learning methods", "Used-For", "classification"]], "rel_plus": [["CNNs:Method", "Used-For", "classification:Task"], ["transfer learning methods:Method", "Used-For", "classification:Task"]]}
{"doc_id": "54447105", "sentence": "On the detection task , transfer learning methods have also been investigated using COCO [ 3 9 ] for pretraining in Mask R - CNN [ 2 5 ] .", "ner": [["detection", "Task"], ["transfer learning methods", "Method"], ["COCO", "Dataset"], ["Mask R - CNN", "Method"]], "rel": [["transfer learning methods", "Used-For", "detection"], ["Mask R - CNN", "Trained-With", "COCO"]], "rel_plus": [["transfer learning methods:Method", "Used-For", "detection:Task"], ["Mask R - CNN:Method", "Trained-With", "COCO:Dataset"]]}
{"doc_id": "54447105", "sentence": "The distribution mismatch and the considerable domain shift between the two domains lead to a significant performance drop [ 4 ] , and domain adaptation ( DA ) is needed [ 5 0 , 1 2 , 2 9 , 5 7 ] .", "ner": [["domain adaptation", "Method"], ["DA", "Method"]], "rel": [["DA", "Synonym-Of", "domain adaptation"]], "rel_plus": [["DA:Method", "Synonym-Of", "domain adaptation:Method"]]}
{"doc_id": "54447105", "sentence": "However , categories in COCO and Modanet are not the same ( only bag and tie are in both ) .", "ner": [["COCO", "Dataset"], ["Modanet", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "54447105", "sentence": "If a Faster R - CNN is trained on the naive concatenation of the two datasets , dresses instances in COCO 's images are considered as background and deteriorate the performances .", "ner": [["Faster R - CNN", "Method"], ["COCO", "Dataset"]], "rel": [["Faster R - CNN", "Trained-With", "COCO"]], "rel_plus": [["Faster R - CNN:Method", "Trained-With", "COCO:Dataset"]]}
{"doc_id": "54447105", "sentence": "To avoid a new labelling step , we will instead use predicted dresses ( with our FSD trained on Modanet ) as targets on COCO .", "ner": [["FSD", "Method"], ["Modanet", "Dataset"], ["COCO", "Dataset"]], "rel": [["FSD", "Trained-With", "Modanet"]], "rel_plus": [["FSD:Method", "Trained-With", "Modanet:Dataset"]]}
{"doc_id": "54447105", "sentence": "The contributions of this paper are threefold : ( 1 ) we introduce a dataset merging procedure that enables the fusion of several datasets with heterogeneous categories : without any additional labelling , we benefit from large open source detection datasets ( 2 ) our self training approach handles unsafe predictions with a custom classification loss SoftSig ( 3 ) we combine self - training and data distillation for domain adaptation .", "ner": [["detection", "Task"], ["classification loss", "Method"], ["SoftSig", "Method"], ["self - training", "Method"], ["data distillation", "Method"], ["domain adaptation", "Task"]], "rel": [["SoftSig", "SubClass-Of", "classification loss"], ["data distillation", "Used-For", "domain adaptation"], ["self - training", "Used-For", "domain adaptation"]], "rel_plus": [["SoftSig:Method", "SubClass-Of", "classification loss:Method"], ["data distillation:Method", "Used-For", "domain adaptation:Task"], ["self - training:Method", "Used-For", "domain adaptation:Task"]]}
{"doc_id": "54447105", "sentence": "Merging COCO [ 3 9 ] and Modanet [ 6 9 ] increases performance for object detection of fashion garments on OpenImages [ 3 3 ] from 4 5 . 5 % to 5 7 . 4 % in mAP@ 0 . 5 .", "ner": [["COCO", "Dataset"], ["Modanet", "Dataset"], ["object detection", "Task"], ["OpenImages", "Dataset"]], "rel": [["OpenImages", "Benchmark-For", "object detection"], ["COCO", "Used-For", "object detection"], ["Modanet", "Used-For", "object detection"]], "rel_plus": [["OpenImages:Dataset", "Benchmark-For", "object detection:Task"], ["COCO:Dataset", "Used-For", "object detection:Task"], ["Modanet:Dataset", "Used-For", "object detection:Task"]]}
{"doc_id": "54447105", "sentence": "For ablation study , we use the synthetic dataset SIM 1 0 k to learn how to detect cars on real - world images , without using the car annotations from Cityscapes .", "ner": [["SIM 1 0 k", "Dataset"], ["Cityscapes", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "54447105", "sentence": "Finally , our soft distillation procedure can be applied to detection with domain adaptation , and beat the state - of - the - art by 5. 3 points , from 3 9 . 6 to 4 4 . 9 on the task of learning from SIM 1 0 k to Cityscapes .", "ner": [["detection", "Task"], ["domain adaptation", "Task"], ["SIM 1 0 k", "Dataset"], ["Cityscapes", "Dataset"]], "rel": [["SIM 1 0 k", "Benchmark-For", "detection"], ["Cityscapes", "Benchmark-For", "domain adaptation"]], "rel_plus": [["SIM 1 0 k:Dataset", "Benchmark-For", "detection:Task"], ["Cityscapes:Dataset", "Benchmark-For", "domain adaptation:Task"]]}
{"doc_id": "54447105", "sentence": "Object detection Many approaches have been developed recently for fast and accurate detection [ 4 0 , 5 1 , 1 7 , 3 8 , 3 5 , 2 2 , 2 1 , 5 3 , 2 5 ] .", "ner": [["Object detection", "Task"], ["detection", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "54447105", "sentence": "We selected the Faster R - CNN framework [ 5 3 ] for its state - of - the - art performance , and for better comparability since this is the architecture mostly used in the context of domain adaptation detection .", "ner": [["Faster R - CNN", "Method"], ["domain adaptation detection", "Task"]], "rel": [["Faster R - CNN", "Used-For", "domain adaptation detection"]], "rel_plus": [["Faster R - CNN:Method", "Used-For", "domain adaptation detection:Task"]]}
{"doc_id": "54447105", "sentence": "Classification datasets [ 1 8 , 6 1 ] with millions of images and numerous categories enabled the boost in accuracies of CNNs [ 5 9 ] .", "ner": [["Classification", "Task"], ["CNNs", "Method"]], "rel": [["CNNs", "Used-For", "Classification"]], "rel_plus": [["CNNs:Method", "Used-For", "Classification:Task"]]}
{"doc_id": "54447105", "sentence": "However , the biggest fully supervised detection and segmentation datasets [ 3 9 , 6 9 , 3 0 , 1 5 ] are still on the order of hundreds of thousands of images .", "ner": [["detection", "Task"], ["segmentation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "54447105", "sentence": "Huge classification datasets are used for pretraining [ 5 9 ] , or even during training to expand the number of classes detected in Yolo 9 0 0 0 [ 5 1 ] , by using image - level class annotations as weak supervision for object detection .", "ner": [["classification", "Task"], ["Yolo 9 0 0 0", "Dataset"], ["object detection", "Task"]], "rel": [["Yolo 9 0 0 0", "Benchmark-For", "classification"]], "rel_plus": [["Yolo 9 0 0 0:Dataset", "Benchmark-For", "classification:Task"]]}
{"doc_id": "54447105", "sentence": "As far as we know , Kemnitz et al. [ 3 1 ] is the only approach for merging detection datasets with heterogeneous label subsets ( for semantic segmentation of medical images ) .", "ner": [["detection", "Task"], ["semantic segmentation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "54447105", "sentence": "It was applied to object detection [ 5 4 , 4 8 , 2 ] : large unlabeled datasets improved performance on object and human keypoint detection tasks .", "ner": [["object detection", "Task"], ["human keypoint detection", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "54447105", "sentence": "Domain adaptation can be optimized in an adversarial training manner with a Gradient Reversal Layer ( GRL ) .", "ner": [["Domain adaptation", "Method"], ["Gradient Reversal Layer", "Method"], ["GRL", "Method"]], "rel": [["Gradient Reversal Layer", "Used-For", "Domain adaptation"], ["GRL", "Synonym-Of", "Gradient Reversal Layer"]], "rel_plus": [["Gradient Reversal Layer:Method", "Used-For", "Domain adaptation:Method"], ["GRL:Method", "Synonym-Of", "Gradient Reversal Layer:Method"]]}
{"doc_id": "54447105", "sentence": "Ganin and Lempitsky [ 1 9 ] introduced this layer to force the CNN to maximize a domain classification loss while minimizing the usual label prediction loss [ 4 1 , 5 6 , 6 2 ] .", "ner": [["CNN", "Method"], ["domain classification loss", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "54447105", "sentence": "Domain adaptation has been studied only recently for problems such as detection [ 6 0 , 6 6 , 2 4 , 5 0 , 1 3 , 6 8 ] .", "ner": [["Domain adaptation", "Method"], ["detection", "Task"]], "rel": [["Domain adaptation", "Used-For", "detection"]], "rel_plus": [["Domain adaptation:Method", "Used-For", "detection:Task"]]}
{"doc_id": "54447105", "sentence": "They added 3 new terms in the training loss : ( 1 ) an instance level domain adaptation loss , to align the ROIs features distribution ( 2 ) an image level domain adaptation loss , to eliminate the domain distribution mismatch on the image level ( 3 ) a consistency regularization .", "ner": [["domain adaptation loss", "Method"], ["domain adaptation loss", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "54447105", "sentence": "As each dataset has its own bias , we train two different DA Faster R - CNN [ 1 2 ] domain adapted to the complementary unlabeled images .", "ner": [["DA Faster R - CNN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "54447105", "sentence": "The first DA Faster R - CNN Detect a learns to detect C a on I a , and has access to images I b ( without seeing G b ) to minimize the semantic shift between the features extracted from the two domains .", "ner": [["DA Faster R - CNN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "54447105", "sentence": "Similarly , the second DA Faster R - CNN Detect b learns to detect C b on I b , and has access to images I a ( without seeing G a ) .", "ner": [["DA Faster R - CNN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "54447105", "sentence": "A prediction with a high detection score is more trustworthy than another one with a lower score : the classification predicted score can be used as proxy for annotation quality [ 4 8 ] .", "ner": [["detection", "Task"], ["classification", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "54447105", "sentence": "To train the RPN , a binary class label for foreground/background classification is usually assigned to each anchor .", "ner": [["RPN", "Method"], ["foreground/background classification", "Task"]], "rel": [["RPN", "Used-For", "foreground/background classification"]], "rel_plus": [["RPN:Method", "Used-For", "foreground/background classification:Task"]]}
{"doc_id": "54447105", "sentence": "In conclusion , the anchors that match with unsafe predictions will contribute neither to the classification loss nor to the regression loss of the RPN .", "ner": [["classification", "Task"], ["RPN", "Method"]], "rel": [["RPN", "Used-For", "classification"]], "rel_plus": [["RPN:Method", "Used-For", "classification:Task"]]}
{"doc_id": "54447105", "sentence": "We named this loss SoftSig because it combines Softmax and Sigmoid activation fonctions while handling Soft Signs .", "ner": [["SoftSig", "Method"], ["Softmax", "Method"], ["Sigmoid activation", "Method"], ["Soft Signs", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "54447105", "sentence": "A SGD optimizer with momentum 0. 9 is used with a batch of 2 images , one from the first domain and the other from the second domain , without data augmentation .", "ner": [["SGD", "Method"], ["momentum", "Method"], ["data augmentation", "Method"]], "rel": [["momentum", "Part-Of", "SGD"]], "rel_plus": [["momentum:Method", "Part-Of", "SGD:Method"]]}
{"doc_id": "54447105", "sentence": "We conduct three main experiments : ( 4. 2 ) we show that our dataset merging procedure improves generalization results for fashion detection for images in the wild , ( 4. 3 ) we add a new category in a dataset without relabelling and ( 4. 4 ) we learn from synthetic data with domain adaptation using our soft distillation loss .", "ner": [["fashion detection", "Task"], ["domain adaptation", "Method"], ["soft distillation loss", "Method"]], "rel": [["soft distillation loss", "Part-Of", "domain adaptation"]], "rel_plus": [["soft distillation loss:Method", "Part-Of", "domain adaptation:Method"]]}
{"doc_id": "54447105", "sentence": "We leverage the currently biggest detection dataset with objects in context , COCO [ 3 9 ] , with 8 0 objects ( such as glasses , firehydrant ) labeled on 1 1 7 , 2 8 1 training images .", "ner": [["detection", "Task"], ["COCO", "Dataset"]], "rel": [["COCO", "Benchmark-For", "detection"]], "rel_plus": [["COCO:Dataset", "Benchmark-For", "detection:Task"]]}
{"doc_id": "54447105", "sentence": "As COCO is twice as big as Modanet , the fashion images will be sampled 2 times more .", "ner": [["COCO", "Dataset"], ["Modanet", "Dataset"]], "rel": [["COCO", "Compare-With", "Modanet"]], "rel_plus": [["COCO:Dataset", "Compare-With", "Modanet:Dataset"]]}
{"doc_id": "54447105", "sentence": "It was expected for bag and tie as OMNIA benefits from additional annotations from COCO ( even though the labelling rules do not perfectly match ) and we have a 6. 3 points gain ( + 1 2 . 8 % ) in average on all other categories .", "ner": [["OMNIA", "Method"], ["COCO", "Dataset"]], "rel": [["OMNIA", "Trained-With", "COCO"]], "rel_plus": [["OMNIA:Method", "Trained-With", "COCO:Dataset"]]}
{"doc_id": "54447105", "sentence": "The Domain Adaptive [ 1 2 ] trained on Modanet and adapted to COCO does not generalize well to OpenImages ( see Figure 3 ) .", "ner": [["Domain Adaptive", "Method"], ["Modanet", "Dataset"], ["COCO", "Dataset"], ["OpenImages", "Dataset"]], "rel": [["Domain Adaptive", "Trained-With", "Modanet"], ["Domain Adaptive", "Trained-With", "COCO"], ["Domain Adaptive", "Evaluated-With", "OpenImages"]], "rel_plus": [["Domain Adaptive:Method", "Trained-With", "Modanet:Dataset"], ["Domain Adaptive:Method", "Trained-With", "COCO:Dataset"], ["Domain Adaptive:Method", "Evaluated-With", "OpenImages:Dataset"]]}
{"doc_id": "54447105", "sentence": "The Hard Distillation is trained similarly to [ 4 8 ] and use COCO images as unlabelled data for bootstrapping .", "ner": [["Hard Distillation", "Method"], ["COCO", "Dataset"]], "rel": [["Hard Distillation", "Trained-With", "COCO"]], "rel_plus": [["Hard Distillation:Method", "Trained-With", "COCO:Dataset"]]}
{"doc_id": "54447105", "sentence": "Fashion detection results on OpenImages .", "ner": [["detection", "Task"], ["OpenImages", "Dataset"]], "rel": [["OpenImages", "Benchmark-For", "detection"]], "rel_plus": [["OpenImages:Dataset", "Benchmark-For", "detection:Task"]]}
{"doc_id": "54447105", "sentence": "This gain is not linear but logarithmic : a small subset of COCO is already useful , even more for the validation dataset with random images from OpenImages .", "ner": [["COCO", "Dataset"], ["OpenImages", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "54447105", "sentence": "SIM 1 0 k [ 3 0 ] provides 1 0 , 0 0 0 training images with 5 8 , 7 0 1 cars automatically annotated from the video game Grand Theft Auto .", "ner": [["SIM 1 0 k", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "54447105", "sentence": "We apply the dataset merging procedure on Cityscapes and SIM 1 0 K , with a learning rate of 0.0 0 2 for 1 5 k iterations which is then reduced to 0.0 0 0 2 for another 1 5 k iterations .", "ner": [["Cityscapes", "Dataset"], ["SIM 1 0 K", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "54447105", "sentence": "The Domain Adaptive trained on SIM 1 0 k reaches 3 9 . 6 : predictions of this model on Cityscapes are used as targets in the final training .", "ner": [["Domain Adaptive", "Method"], ["SIM 1 0 k", "Dataset"], ["Cityscapes", "Dataset"]], "rel": [["Domain Adaptive", "Trained-With", "SIM 1 0 k"], ["Domain Adaptive", "Evaluated-With", "Cityscapes"]], "rel_plus": [["Domain Adaptive:Method", "Trained-With", "SIM 1 0 k:Dataset"], ["Domain Adaptive:Method", "Evaluated-With", "Cityscapes:Dataset"]]}
{"doc_id": "54447105", "sentence": "KITTI [ 2 0 ] contains 7, 4 8 1 real - world images , but with a different data collecting method than Cityscapes : the performances in Cityscapes do not perfectly match those in KITTI .", "ner": [["KITTI", "Dataset"], ["Cityscapes", "Dataset"], ["Cityscapes", "Dataset"], ["KITTI", "Dataset"]], "rel": [["KITTI", "Compare-With", "Cityscapes"]], "rel_plus": [["KITTI:Dataset", "Compare-With", "Cityscapes:Dataset"]]}
{"doc_id": "54447105", "sentence": "Learning to detect car from 2 domains , SIM 1 0 k and Cityscapes , induces a better domain invariance .", "ner": [["SIM 1 0 k", "Dataset"], ["Cityscapes", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "54447105", "sentence": "We also notice that our SoftSig detector has a better cars AP than the Hard Distillation and a better truck AP than when we do not sample unsafe regions .", "ner": [["SoftSig", "Method"], ["Hard Distillation", "Method"]], "rel": [["SoftSig", "Compare-With", "Hard Distillation"]], "rel_plus": [["SoftSig:Method", "Compare-With", "Hard Distillation:Method"]]}
{"doc_id": "54447105", "sentence": "Domain adaptation results on Cityscapes and KITTI .", "ner": [["Domain adaptation", "Method"], ["Cityscapes", "Dataset"], ["KITTI", "Dataset"]], "rel": [["Domain adaptation", "Evaluated-With", "Cityscapes"], ["Domain adaptation", "Evaluated-With", "KITTI"]], "rel_plus": [["Domain adaptation:Method", "Evaluated-With", "Cityscapes:Dataset"], ["Domain adaptation:Method", "Evaluated-With", "KITTI:Dataset"]]}
{"doc_id": "54447105", "sentence": "SIM 1 0 k is the source dataset . car predictions Pcar from detector ( 1 ) on Cityscapes are used for self - training .", "ner": [["SIM 1 0 k", "Dataset"], ["Cityscapes", "Dataset"], ["self - training", "Method"]], "rel": [["self - training", "Trained-With", "Cityscapes"]], "rel_plus": [["self - training:Method", "Trained-With", "Cityscapes:Dataset"]]}
{"doc_id": "54447105", "sentence": "The training data consists of two parts : ( 1 ) the source dataset ( SIM 1 0 k ) where images and their associated instances annotations are provided and ( 2 ) the target dataset ( Cityscapes ) with images without supervision .", "ner": [["SIM 1 0 k", "Dataset"], ["Cityscapes", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "54447105", "sentence": "Our procedure aims at detecting robustly on the Cityscapes validation by using unlabeled images from Cityscapes training .", "ner": [["Cityscapes", "Dataset"], ["Cityscapes", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "54447105", "sentence": "Results on Cityscapes and KITTI .", "ner": [["Cityscapes", "Dataset"], ["KITTI", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "210164920", "sentence": "From the autonomous car driving to medical diagnosis , the requirement of the task of image segmentation is everywhere .", "ner": [["autonomous car driving", "Task"], ["medical diagnosis", "Task"], ["image segmentation", "Task"]], "rel": [["image segmentation", "Used-For", "autonomous car driving"], ["image segmentation", "Used-For", "medical diagnosis"]], "rel_plus": [["image segmentation:Task", "Used-For", "autonomous car driving:Task"], ["image segmentation:Task", "Used-For", "medical diagnosis:Task"]]}
{"doc_id": "210164920", "sentence": "Segmentation of an image is one of the indispensable tasks in computer vision .", "ner": [["Segmentation", "Task"], ["computer vision", "Task"]], "rel": [["Segmentation", "SubTask-Of", "computer vision"]], "rel_plus": [["Segmentation:Task", "SubTask-Of", "computer vision:Task"]]}
{"doc_id": "210164920", "sentence": "Basically , image segmentation can be of two types : semantic segmentation and instance segmentation .", "ner": [["image segmentation", "Task"], ["semantic segmentation", "Task"], ["instance segmentation", "Task"]], "rel": [["semantic segmentation", "SubTask-Of", "image segmentation"], ["instance segmentation", "SubTask-Of", "image segmentation"]], "rel_plus": [["semantic segmentation:Task", "SubTask-Of", "image segmentation:Task"], ["instance segmentation:Task", "SubTask-Of", "image segmentation:Task"]]}
{"doc_id": "210164920", "sentence": "In the recent era , the success of deep convolutional neural networks ( CNN ) has influenced the field of segmentation greatly and gave us various successful models to date .", "ner": [["convolutional neural networks", "Method"], ["CNN", "Method"], ["segmentation", "Task"]], "rel": [["CNN", "Synonym-Of", "convolutional neural networks"], ["convolutional neural networks", "Used-For", "segmentation"]], "rel_plus": [["CNN:Method", "Synonym-Of", "convolutional neural networks:Method"], ["convolutional neural networks:Method", "Used-For", "segmentation:Task"]]}
{"doc_id": "210164920", "sentence": "In this survey , we are going to take a glance at the evolution of both semantic and instance segmentation work based on CNN .", "ner": [["semantic and instance segmentation", "Task"], ["CNN", "Method"]], "rel": [["CNN", "Used-For", "semantic and instance segmentation"]], "rel_plus": [["CNN:Method", "Used-For", "semantic and instance segmentation:Task"]]}
{"doc_id": "210164920", "sentence": "Among different deep learning models , convolutional neural network(CNN ) [ 3 , 4 , 5 ] has shown outstanding performance in different high level computer vision task such as image classification [ 6 , 7 , 8 , 9 , 1 0 , 1 1 , 1 2 , 1 3 , 1 4 , 1 5 ] , object detection [ 1 6 , 1 7 , 1 8 , 1 9 , 2 0 , 2 1 , 2 2 , 2 3 , 2 4 , 2 5 , 2 6 , 2 7 , 2 8 ] etc .", "ner": [["deep learning models", "Method"], ["convolutional neural network(CNN )", "Method"], ["computer vision", "Task"], ["image classification", "Task"], ["object detection", "Task"]], "rel": [["convolutional neural network(CNN )", "SubClass-Of", "deep learning models"], ["image classification", "SubTask-Of", "computer vision"], ["object detection", "SubTask-Of", "computer vision"], ["convolutional neural network(CNN )", "Used-For", "computer vision"], ["convolutional neural network(CNN )", "Used-For", "image classification"], ["convolutional neural network(CNN )", "Used-For", "object detection"]], "rel_plus": [["convolutional neural network(CNN ):Method", "SubClass-Of", "deep learning models:Method"], ["image classification:Task", "SubTask-Of", "computer vision:Task"], ["object detection:Task", "SubTask-Of", "computer vision:Task"], ["convolutional neural network(CNN ):Method", "Used-For", "computer vision:Task"], ["convolutional neural network(CNN ):Method", "Used-For", "image classification:Task"], ["convolutional neural network(CNN ):Method", "Used-For", "object detection:Task"]]}
{"doc_id": "210164920", "sentence": "Though the advent and success of AlexNet [ 6 ] turned the field of computer vision towards CNN from traditional machine learning algorithms .", "ner": [["AlexNet", "Method"], ["computer vision", "Task"], ["CNN", "Method"], ["machine learning", "Method"]], "rel": [["AlexNet", "Used-For", "computer vision"], ["CNN", "SubClass-Of", "machine learning"]], "rel_plus": [["AlexNet:Method", "Used-For", "computer vision:Task"], ["CNN:Method", "SubClass-Of", "machine learning:Method"]]}
{"doc_id": "210164920", "sentence": "In 1 9 8 9 , Yann LeCun turned the theoretical idea of Neocognitron into a practical one called LeNet - 5 [ 3 2 ] .", "ner": [["Neocognitron", "Method"], ["LeNet - 5", "Method"]], "rel": [["Neocognitron", "Part-Of", "LeNet - 5"]], "rel_plus": [["Neocognitron:Method", "Part-Of", "LeNet - 5:Method"]]}
{"doc_id": "210164920", "sentence": "LeNet - 5 was the first CNN developed for recognizing handwritten digits .", "ner": [["LeNet - 5", "Method"], ["CNN", "Method"], ["recognizing handwritten digits", "Task"]], "rel": [["LeNet - 5", "SubClass-Of", "CNN"], ["LeNet - 5", "Used-For", "recognizing handwritten digits"], ["CNN", "Used-For", "recognizing handwritten digits"]], "rel_plus": [["LeNet - 5:Method", "SubClass-Of", "CNN:Method"], ["LeNet - 5:Method", "Used-For", "recognizing handwritten digits:Task"], ["CNN:Method", "Used-For", "recognizing handwritten digits:Task"]]}
{"doc_id": "210164920", "sentence": "The invention of LeNet - 5 paved the way for the continuous success of CNN in various high - level computer vision tasks as well as motivated researchers to explore the capabilities of such networks for pixel - level classification problems like image segmentation .", "ner": [["LeNet - 5", "Method"], ["CNN", "Method"], ["computer vision", "Task"], ["pixel - level classification", "Task"], ["image segmentation", "Task"]], "rel": [["LeNet - 5", "SubClass-Of", "CNN"], ["CNN", "Used-For", "computer vision"], ["LeNet - 5", "Used-For", "computer vision"], ["image segmentation", "SubTask-Of", "pixel - level classification"]], "rel_plus": [["LeNet - 5:Method", "SubClass-Of", "CNN:Method"], ["CNN:Method", "Used-For", "computer vision:Task"], ["LeNet - 5:Method", "Used-For", "computer vision:Task"], ["image segmentation:Task", "SubTask-Of", "pixel - level classification:Task"]]}
{"doc_id": "210164920", "sentence": "From the autonomous car driving [ 3 5 ] to medical diagnosis [ 3 6 , 3 7 ] , the requirement of the task of image segmentation is everywhere .", "ner": [["autonomous car driving", "Task"], ["medical diagnosis", "Task"], ["image segmentation", "Task"]], "rel": [["image segmentation", "Used-For", "autonomous car driving"], ["image segmentation", "Used-For", "medical diagnosis"]], "rel_plus": [["image segmentation:Task", "Used-For", "autonomous car driving:Task"], ["image segmentation:Task", "Used-For", "medical diagnosis:Task"]]}
{"doc_id": "210164920", "sentence": "In this paper , we have tried to give a survey of different image segmentation models based on CNN .", "ner": [["image segmentation", "Task"], ["CNN", "Method"]], "rel": [["CNN", "Used-For", "image segmentation"]], "rel_plus": [["CNN:Method", "Used-For", "image segmentation:Task"]]}
{"doc_id": "210164920", "sentence": "Semantic segmentation and instance segmentation of an image are discussed .", "ner": [["Semantic segmentation", "Task"], ["instance segmentation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "210164920", "sentence": "Also , different aspects of those models are presented in tabular form for clear understanding . \u2022 Gives taxonomy and survey of the evolution of CNN based image segmentation . \u2022 Explores elaborately some CNN based popular state - of - the - art segmen - tation models . \u2022 Compares training details of those models to have a clear view of hyperparameter tuning . \u2022 Compares the performance metrics of those state - of - the - art models on different datasets .", "ner": [["CNN", "Method"], ["image segmentation", "Task"], ["CNN", "Method"]], "rel": [["CNN", "Used-For", "image segmentation"]], "rel_plus": [["CNN:Method", "Used-For", "image segmentation:Task"]]}
{"doc_id": "210164920", "sentence": "In sections 3 and 4 , semantic segmentation and instance segmentation works are discussed respectively with some subsections .", "ner": [["semantic segmentation", "Task"], ["instance segmentation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "210164920", "sentence": "The paper is concluded in section 6 .   In computer vision , image segmentation is a way of segregating a digital image into multiple regions according to the different properties of pixels .", "ner": [["computer vision", "Task"], ["image segmentation", "Task"]], "rel": [["image segmentation", "SubTask-Of", "computer vision"]], "rel_plus": [["image segmentation:Task", "SubTask-Of", "computer vision:Task"]]}
{"doc_id": "210164920", "sentence": "Unlike classification and object detection , it is typically a low - level or pixellevel vision task as the spatial information of an image is very important for segmenting different regions semantically .", "ner": [["classification", "Task"], ["object detection", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "210164920", "sentence": "Mainly , image segmentation is of two types : semantic segmentation and instance segmentation .", "ner": [["image segmentation", "Task"], ["semantic segmentation", "Task"], ["instance segmentation", "Task"]], "rel": [["semantic segmentation", "SubTask-Of", "image segmentation"], ["instance segmentation", "SubTask-Of", "image segmentation"]], "rel_plus": [["semantic segmentation:Task", "SubTask-Of", "image segmentation:Task"], ["instance segmentation:Task", "SubTask-Of", "image segmentation:Task"]]}
{"doc_id": "210164920", "sentence": "Also , there is another type called panoptic segmentation [ 4 0 ] which is the unified version of two basic segmentation processes .", "ner": [["panoptic segmentation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "210164920", "sentence": "Various researchers are addressing this task in different way using traditional machine learning algorithms like in [ 4 1 , 4 2 , 4 3 ] with the help various technique such as thresholding [ 4 4 ] , region growing [ 4 5 , 4 6 ] , edge detection [ 4 7 , 4 8 , 4 9 ] , clustering [ 5 0 , 5 1 , 5 2 , 5 3 , 5 4 , 5 5 , 5 6 , 5 7 ] , super - pixel [ 5 8 , 5 9 ] etc for years .", "ner": [["machine learning", "Method"], ["thresholding", "Method"], ["region growing", "Method"], ["edge detection", "Method"], ["clustering", "Method"], ["super - pixel", "Method"]], "rel": [["thresholding", "Used-For", "machine learning"], ["region growing", "Used-For", "machine learning"], ["edge detection", "Used-For", "machine learning"], ["clustering", "Used-For", "machine learning"], ["super - pixel", "Used-For", "machine learning"]], "rel_plus": [["thresholding:Method", "Used-For", "machine learning:Method"], ["region growing:Method", "Used-For", "machine learning:Method"], ["edge detection:Method", "Used-For", "machine learning:Method"], ["clustering:Method", "Used-For", "machine learning:Method"], ["super - pixel:Method", "Used-For", "machine learning:Method"]]}
{"doc_id": "210164920", "sentence": "Most of the successful works are based on handcrafted machine learning features such as HOG [ 6 0 , 6 1 , 6 2 , 6 3 ] , SIFT [ 6 4 , 6 5 ] etc .", "ner": [["handcrafted machine learning features", "Method"], ["HOG", "Method"], ["SIFT", "Method"]], "rel": [["HOG", "SubClass-Of", "handcrafted machine learning features"], ["SIFT", "SubClass-Of", "handcrafted machine learning features"]], "rel_plus": [["HOG:Method", "SubClass-Of", "handcrafted machine learning features:Method"], ["SIFT:Method", "SubClass-Of", "handcrafted machine learning features:Method"]]}
{"doc_id": "210164920", "sentence": "First of all , feature engineering needs domain expertise and the success of those machine learning - based models was slowed down around the era when deep learning was started to take over the world of computer vision .", "ner": [["feature engineering", "Task"], ["machine learning", "Method"], ["deep learning", "Method"], ["computer vision", "Task"]], "rel": [["deep learning", "Used-For", "computer vision"]], "rel_plus": [["deep learning:Method", "Used-For", "computer vision:Task"]]}
{"doc_id": "210164920", "sentence": "Among different deep learning algorithms , CNN got tremendous success in different fields of computer vision as well as grab the area of image segmentation [ 6 6 , 6 7 , 6 8 ] .", "ner": [["CNN", "Method"], ["computer vision", "Task"], ["image segmentation", "Task"]], "rel": [["CNN", "Used-For", "computer vision"], ["image segmentation", "SubTask-Of", "computer vision"], ["CNN", "Used-For", "image segmentation"]], "rel_plus": [["CNN:Method", "Used-For", "computer vision:Task"], ["image segmentation:Task", "SubTask-Of", "computer vision:Task"], ["CNN:Method", "Used-For", "image segmentation:Task"]]}
{"doc_id": "210164920", "sentence": "After the success of AlexNet in 2 0 1 2 , we have got different successful semantic segmentation models based on CNN .", "ner": [["AlexNet", "Method"], ["semantic segmentation", "Task"], ["CNN", "Method"]], "rel": [["AlexNet", "Used-For", "semantic segmentation"], ["CNN", "Used-For", "semantic segmentation"]], "rel_plus": [["AlexNet:Method", "Used-For", "semantic segmentation:Task"], ["CNN:Method", "Used-For", "semantic segmentation:Task"]]}
{"doc_id": "210164920", "sentence": "In this section , we are going to survey the evolution of CNN based semantic segmentation models .", "ner": [["CNN", "Method"], ["semantic segmentation", "Task"]], "rel": [["CNN", "Used-For", "semantic segmentation"]], "rel_plus": [["CNN:Method", "Used-For", "semantic segmentation:Task"]]}
{"doc_id": "210164920", "sentence": "In addition , we are going to bring up here an elaborate exploration of some state - of - the - art models .   The application of CNN in semantic segmentation models has started with a huge diversity .", "ner": [["CNN", "Method"], ["semantic segmentation", "Task"]], "rel": [["CNN", "Used-For", "semantic segmentation"]], "rel_plus": [["CNN:Method", "Used-For", "semantic segmentation:Task"]]}
{"doc_id": "210164920", "sentence": "In [ 7 0 ] , the authors have used multi - scale CNN for scene labeling and achieve state - of - the - art results in the Sift flow [ 7 1 ] , the Bercelona dataset [ 7 2 ] and the Standford background dataset [ 7 3 ] .", "ner": [["CNN", "Method"], ["scene labeling", "Task"], ["Sift flow", "Dataset"], ["Bercelona", "Dataset"], ["Standford background", "Dataset"]], "rel": [["CNN", "Used-For", "scene labeling"], ["Sift flow", "Benchmark-For", "scene labeling"], ["Bercelona", "Benchmark-For", "scene labeling"], ["Standford background", "Benchmark-For", "scene labeling"], ["CNN", "Evaluated-With", "Sift flow"], ["CNN", "Evaluated-With", "Bercelona"], ["CNN", "Evaluated-With", "Standford background"]], "rel_plus": [["CNN:Method", "Used-For", "scene labeling:Task"], ["Sift flow:Dataset", "Benchmark-For", "scene labeling:Task"], ["Bercelona:Dataset", "Benchmark-For", "scene labeling:Task"], ["Standford background:Dataset", "Benchmark-For", "scene labeling:Task"], ["CNN:Method", "Evaluated-With", "Sift flow:Dataset"], ["CNN:Method", "Evaluated-With", "Bercelona:Dataset"], ["CNN:Method", "Evaluated-With", "Standford background:Dataset"]]}
{"doc_id": "210164920", "sentence": "R - CNN [ 7 4 ] used selective search [ 7 5 ] algorithm to extract region proposals first and then applied CNN upon each proposal for PASCAL VOC semantic segmentation challenge [ 7 6 ] .", "ner": [["R - CNN", "Method"], ["selective search", "Method"], ["CNN", "Method"], ["PASCAL VOC semantic segmentation challenge", "Dataset"]], "rel": [["selective search", "Used-For", "R - CNN"], ["CNN", "Part-Of", "R - CNN"], ["R - CNN", "Used-For", "PASCAL VOC semantic segmentation challenge"]], "rel_plus": [["selective search:Method", "Used-For", "R - CNN:Method"], ["CNN:Method", "Part-Of", "R - CNN:Method"], ["R - CNN:Method", "Used-For", "PASCAL VOC semantic segmentation challenge:Dataset"]]}
{"doc_id": "210164920", "sentence": "R - CNN achieved record result over second order pooling ( O 2 P ) [ 7 7 ] which was a leading hand - engineered semantic segmentation system at that time .", "ner": [["R - CNN", "Method"], ["second order pooling", "Method"], ["O 2 P", "Method"], ["semantic segmentation", "Task"]], "rel": [["second order pooling", "Part-Of", "R - CNN"], ["O 2 P", "Synonym-Of", "second order pooling"], ["R - CNN", "Used-For", "semantic segmentation"]], "rel_plus": [["second order pooling:Method", "Part-Of", "R - CNN:Method"], ["O 2 P:Method", "Synonym-Of", "second order pooling:Method"], ["R - CNN:Method", "Used-For", "semantic segmentation:Task"]]}
{"doc_id": "210164920", "sentence": "At the same time , Gupta et al. [ 6 3 ] used CNN along with geocentric embedding on RGB - D images for semantic segmentation .", "ner": [["CNN", "Method"], ["semantic segmentation", "Task"]], "rel": [["CNN", "Used-For", "semantic segmentation"]], "rel_plus": [["CNN:Method", "Used-For", "semantic segmentation:Task"]]}
{"doc_id": "210164920", "sentence": "Among different CNN based semantic segmentation models , Fully Convolutional Network(FCN ) [ 7 8 ] , as discussed in section 3. 2 . 1 , gained the maximum attention and an FCN based semantic segmentation model trend has emerged .", "ner": [["CNN", "Method"], ["semantic segmentation", "Task"], ["Fully Convolutional Network(FCN", "Method"], ["FCN", "Method"], ["semantic segmentation", "Task"]], "rel": [["CNN", "Used-For", "semantic segmentation"], ["FCN", "Used-For", "semantic segmentation"]], "rel_plus": [["CNN:Method", "Used-For", "semantic segmentation:Task"], ["FCN:Method", "Used-For", "semantic segmentation:Task"]]}
{"doc_id": "210164920", "sentence": "Major changes in FCN which helped the model to achieve state of the art result are the base model VGG 1 6 , bipolar interpolation technique for up - sampling the final feature map and skip connection for combining low layer and high layer features in the final layer for fine - grained semantic segmentation .", "ner": [["FCN", "Method"], ["VGG 1 6", "Method"], ["bipolar interpolation", "Method"], ["skip connection", "Method"], ["fine - grained semantic segmentation", "Task"]], "rel": [["VGG 1 6", "Part-Of", "FCN"], ["bipolar interpolation", "Part-Of", "FCN"], ["skip connection", "Part-Of", "FCN"], ["FCN", "Used-For", "fine - grained semantic segmentation"]], "rel_plus": [["VGG 1 6:Method", "Part-Of", "FCN:Method"], ["bipolar interpolation:Method", "Part-Of", "FCN:Method"], ["skip connection:Method", "Part-Of", "FCN:Method"], ["FCN:Method", "Used-For", "fine - grained semantic segmentation:Task"]]}
{"doc_id": "210164920", "sentence": "FCN has used only local information for semantic segmentation but only local information makes semantic segmentation quite ambiguous .", "ner": [["FCN", "Method"], ["semantic segmentation", "Task"], ["semantic segmentation", "Task"]], "rel": [["FCN", "Used-For", "semantic segmentation"]], "rel_plus": [["FCN:Method", "Used-For", "semantic segmentation:Task"]]}
{"doc_id": "210164920", "sentence": "Chen et al aggregate ' atrous ' algorithm and conditional random ( CRF ) field in semantic segmentation and proposed DeepLab [ 8 2 ] as discussed in section 3. 2 . 2 .", "ner": [["atrous", "Method"], ["conditional random", "Method"], ["CRF", "Method"], ["semantic segmentation", "Task"], ["DeepLab", "Method"]], "rel": [["CRF", "Synonym-Of", "conditional random"], ["conditional random", "Used-For", "semantic segmentation"]], "rel_plus": [["CRF:Method", "Synonym-Of", "conditional random:Method"], ["conditional random:Method", "Used-For", "semantic segmentation:Task"]]}
{"doc_id": "210164920", "sentence": "Later the authors have incorporated ' Atrous Special Pyramid Pooling ( ASPP ) ' in DeepLabv 2 [ 8 3 ] .", "ner": [["Atrous Special Pyramid Pooling", "Method"], ["ASPP", "Method"], ["DeepLabv 2", "Method"]], "rel": [["ASPP", "Synonym-Of", "Atrous Special Pyramid Pooling"], ["Atrous Special Pyramid Pooling", "Part-Of", "DeepLabv 2"]], "rel_plus": [["ASPP:Method", "Synonym-Of", "Atrous Special Pyramid Pooling:Method"], ["Atrous Special Pyramid Pooling:Method", "Part-Of", "DeepLabv 2:Method"]]}
{"doc_id": "210164920", "sentence": "DeepLabv 3 [ 8 4 ] has gone further and used a cascaded deep ASPP module to incorporate multiple contexts .", "ner": [["DeepLabv 3", "Method"], ["ASPP module", "Method"]], "rel": [["ASPP module", "Part-Of", "DeepLabv 3"]], "rel_plus": [["ASPP module:Method", "Part-Of", "DeepLabv 3:Method"]]}
{"doc_id": "210164920", "sentence": "Deconvnet [ 8 5 ] used convolutional network followed by hierarchically opposite de - convolutional network for semantic segmentation as discussed in section 3. 2 . 3 .", "ner": [["Deconvnet", "Method"], ["convolutional network", "Method"], ["de - convolutional network", "Method"], ["semantic segmentation", "Task"]], "rel": [["convolutional network", "Part-Of", "Deconvnet"], ["de - convolutional network", "Part-Of", "Deconvnet"], ["Deconvnet", "Used-For", "semantic segmentation"]], "rel_plus": [["convolutional network:Method", "Part-Of", "Deconvnet:Method"], ["de - convolutional network:Method", "Part-Of", "Deconvnet:Method"], ["Deconvnet:Method", "Used-For", "semantic segmentation:Task"]]}
{"doc_id": "210164920", "sentence": "Ronneberger et al used a U - shaped network called U - Net [ 8 6 ] which has a contracting and an expansive pathway to approach semantic segmentation .", "ner": [["U - shaped network", "Method"], ["U - Net", "Method"], ["semantic segmentation", "Task"]], "rel": [["U - Net", "Synonym-Of", "U - shaped network"], ["U - Net", "Used-For", "semantic segmentation"]], "rel_plus": [["U - Net:Method", "Synonym-Of", "U - shaped network:Method"], ["U - Net:Method", "Used-For", "semantic segmentation:Task"]]}
{"doc_id": "210164920", "sentence": "In this way , U - Net incorporates both high - level feature and low - level spatial information together for more precise segmentation .", "ner": [["U - Net", "Method"], ["segmentation", "Task"]], "rel": [["U - Net", "Used-For", "segmentation"]], "rel_plus": [["U - Net:Method", "Used-For", "segmentation:Task"]]}
{"doc_id": "210164920", "sentence": "Recently , in [ 8 7 ] , the authors have used U - Net with multiRes block for multimodal biomedical image segmentation and got better result than using classical U - Net .", "ner": [["U - Net with multiRes block", "Method"], ["multimodal biomedical image segmentation", "Task"], ["U - Net", "Method"]], "rel": [["U - Net with multiRes block", "Used-For", "multimodal biomedical image segmentation"], ["U - Net with multiRes block", "Compare-With", "U - Net"]], "rel_plus": [["U - Net with multiRes block:Method", "Used-For", "multimodal biomedical image segmentation:Task"], ["U - Net with multiRes block:Method", "Compare-With", "U - Net:Method"]]}
{"doc_id": "210164920", "sentence": "SegNet [ 8 8 ] is a encoder - decoder network for semantic segmentation .", "ner": [["SegNet", "Method"], ["encoder - decoder network", "Method"], ["semantic segmentation", "Task"]], "rel": [["SegNet", "SubClass-Of", "encoder - decoder network"], ["SegNet", "Used-For", "semantic segmentation"]], "rel_plus": [["SegNet:Method", "SubClass-Of", "encoder - decoder network:Method"], ["SegNet:Method", "Used-For", "semantic segmentation:Task"]]}
{"doc_id": "210164920", "sentence": "The basic architectural intuition of U - Net , Deconvnet , and SegNet are similar except some individual modifications .", "ner": [["U - Net", "Method"], ["Deconvnet", "Method"], ["SegNet", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210164920", "sentence": "Liu et al mixed the essence of global average pooling and L 2 normalization layer in FCN [ 7 8 ] architecture , and proposed ParseNet [ 8 9 ] to achieve state of the art result in various datasets .", "ner": [["global average pooling", "Method"], ["L 2 normalization layer", "Method"], ["FCN", "Method"], ["ParseNet", "Method"]], "rel": [["L 2 normalization layer", "Part-Of", "FCN"]], "rel_plus": [["L 2 normalization layer:Method", "Part-Of", "FCN:Method"]]}
{"doc_id": "210164920", "sentence": "They have used Pyramid Pooling Module on top of the last extracted feature map to incorporate global contextual information for better segmentation .", "ner": [["Pyramid Pooling Module", "Method"], ["segmentation", "Task"]], "rel": [["Pyramid Pooling Module", "Used-For", "segmentation"]], "rel_plus": [["Pyramid Pooling Module:Method", "Used-For", "segmentation:Task"]]}
{"doc_id": "210164920", "sentence": "Pyramid Attention Network ( PAN ) [ 9 2 ] , ParseNet [ 8 9 ] , PSPNet [ 9 0 ] and GCN [ 9 1 ] have used global context information with local feature to have better segmentation .", "ner": [["Pyramid Attention Network", "Method"], ["PAN", "Method"], ["ParseNet", "Method"], ["PSPNet", "Method"], ["GCN", "Method"], ["segmentation", "Task"]], "rel": [["PAN", "Synonym-Of", "Pyramid Attention Network"], ["GCN", "Used-For", "segmentation"], ["PSPNet", "Used-For", "segmentation"], ["ParseNet", "Used-For", "segmentation"], ["Pyramid Attention Network", "Used-For", "segmentation"]], "rel_plus": [["PAN:Method", "Synonym-Of", "Pyramid Attention Network:Method"], ["GCN:Method", "Used-For", "segmentation:Task"], ["PSPNet:Method", "Used-For", "segmentation:Task"], ["ParseNet:Method", "Used-For", "segmentation:Task"], ["Pyramid Attention Network:Method", "Used-For", "segmentation:Task"]]}
{"doc_id": "210164920", "sentence": "Fully convolutional DenseNet [ 1 0 ] is used to address semantic segmentation in [ 9 3 , 9 4 ] .", "ner": [["Fully convolutional DenseNet", "Method"], ["semantic segmentation", "Task"]], "rel": [["Fully convolutional DenseNet", "Used-For", "semantic segmentation"]], "rel_plus": [["Fully convolutional DenseNet:Method", "Used-For", "semantic segmentation:Task"]]}
{"doc_id": "210164920", "sentence": "DeepUNet [ 9 5 ] , a ResNet based FCN , used to segment sea land .", "ner": [["DeepUNet", "Method"], ["ResNet", "Method"], ["FCN", "Method"]], "rel": [["FCN", "Part-Of", "DeepUNet"], ["ResNet", "Part-Of", "FCN"]], "rel_plus": [["FCN:Method", "Part-Of", "DeepUNet:Method"], ["ResNet:Method", "Part-Of", "FCN:Method"]]}
{"doc_id": "210164920", "sentence": "At the same time , ENet [ 9 6 ] , ICNet [ 9 7 ] are used as real - time semantic segmentation models for the autonomous vehicles .", "ner": [["ENet", "Method"], ["ICNet", "Method"], ["real - time semantic segmentation models", "Method"], ["autonomous vehicles", "Task"]], "rel": [["ICNet", "SubClass-Of", "real - time semantic segmentation models"], ["ENet", "SubClass-Of", "real - time semantic segmentation models"], ["ICNet", "Used-For", "autonomous vehicles"], ["ENet", "Used-For", "autonomous vehicles"]], "rel_plus": [["ICNet:Method", "SubClass-Of", "real - time semantic segmentation models:Method"], ["ENet:Method", "SubClass-Of", "real - time semantic segmentation models:Method"], ["ICNet:Method", "Used-For", "autonomous vehicles:Task"], ["ENet:Method", "Used-For", "autonomous vehicles:Task"]]}
{"doc_id": "210164920", "sentence": "Some recent works [ 9 8 , 9 9 , 1 0 0 ] have used combination of encoder - decoder architecture and dilated convolution for better segmentation .", "ner": [["encoder - decoder", "Method"], ["dilated convolution", "Method"], ["segmentation", "Task"]], "rel": [["dilated convolution", "Used-For", "segmentation"], ["encoder - decoder", "Used-For", "segmentation"]], "rel_plus": [["dilated convolution:Method", "Used-For", "segmentation:Task"], ["encoder - decoder:Method", "Used-For", "segmentation:Task"]]}
{"doc_id": "210164920", "sentence": "Kirillov et al. [ 1 0 1 ] used pointbased rendering in DeepLabV 3 [ 8 4 ] and in semanticFPN [ 1 0 2 ] and produce state - of - the - art semantic segmentation model .", "ner": [["DeepLabV 3", "Method"], ["semanticFPN", "Method"], ["semantic segmentation", "Task"]], "rel": [["semanticFPN", "Used-For", "semantic segmentation"], ["DeepLabV 3", "Used-For", "semantic segmentation"]], "rel_plus": [["semanticFPN:Method", "Used-For", "semantic segmentation:Task"], ["DeepLabV 3:Method", "Used-For", "semantic segmentation:Task"]]}
{"doc_id": "210164920", "sentence": "In this section , we are going to explore architectural details of some state of the art CNN based semantic segmentation models in detail .", "ner": [["CNN", "Method"], ["semantic segmentation", "Task"]], "rel": [["CNN", "Used-For", "semantic segmentation"]], "rel_plus": [["CNN:Method", "Used-For", "semantic segmentation:Task"]]}
{"doc_id": "210164920", "sentence": "Long et al. proposed the idea of Fully Convolutional Network(FCN ) [ 7 8 ] to address the semantic segmentation task .", "ner": [["Fully Convolutional Network(FCN )", "Method"], ["semantic segmentation", "Task"]], "rel": [["Fully Convolutional Network(FCN )", "Used-For", "semantic segmentation"]], "rel_plus": [["Fully Convolutional Network(FCN ):Method", "Used-For", "semantic segmentation:Task"]]}
{"doc_id": "210164920", "sentence": "They have used AlexNet [ 6 ] , VGGNet [ 8 ] and GoogleNet [ 9 ] , pre - trained on ILSVRC [ 1 0 3 ] data , as base model .", "ner": [["AlexNet", "Method"], ["VGGNet", "Method"], ["GoogleNet", "Method"], ["ILSVRC", "Dataset"]], "rel": [["AlexNet", "Trained-With", "ILSVRC"], ["VGGNet", "Trained-With", "ILSVRC"], ["GoogleNet", "Trained-With", "ILSVRC"]], "rel_plus": [["AlexNet:Method", "Trained-With", "ILSVRC:Dataset"], ["VGGNet:Method", "Trained-With", "ILSVRC:Dataset"], ["GoogleNet:Method", "Trained-With", "ILSVRC:Dataset"]]}
{"doc_id": "210164920", "sentence": "They transferred these models from classifiers to dense FCN by substituting fully connected layers with 1 \u00d7 1 convolutional layers and append a 1 \u00d7 1 convolution with channel dimension 2 1 to predict scores for each of the PASCAL VOC [ 1 0 4 ] class ( including background ) .", "ner": [["dense FCN", "Method"], ["fully connected layers", "Method"], ["1 \u00d7 1 convolutional layers", "Method"], ["1 \u00d7 1 convolution", "Method"], ["PASCAL VOC", "Dataset"]], "rel": [["fully connected layers", "Part-Of", "dense FCN"], ["1 \u00d7 1 convolution", "Part-Of", "dense FCN"], ["1 \u00d7 1 convolutional layers", "Part-Of", "fully connected layers"], ["dense FCN", "Used-For", "PASCAL VOC"]], "rel_plus": [["fully connected layers:Method", "Part-Of", "dense FCN:Method"], ["1 \u00d7 1 convolution:Method", "Part-Of", "dense FCN:Method"], ["1 \u00d7 1 convolutional layers:Method", "Part-Of", "fully connected layers:Method"], ["dense FCN:Method", "Used-For", "PASCAL VOC:Dataset"]]}
{"doc_id": "210164920", "sentence": "The authors have experienced that among FCN - AlexNet , FCN - VGG 1 6 and FCN - GoogLeNet , FCN - VGG 1 6 gave the highest mean IU( 5 6 . 0 % ) on PASCAL VOC 2 0 1 1 validation dataset .", "ner": [["FCN - AlexNet", "Method"], ["FCN - VGG 1 6", "Method"], ["FCN - GoogLeNet", "Method"], ["FCN - VGG 1 6", "Method"], ["PASCAL VOC 2 0 1 1", "Dataset"]], "rel": [["FCN - AlexNet", "Evaluated-With", "PASCAL VOC 2 0 1 1"], ["FCN - VGG 1 6", "Evaluated-With", "PASCAL VOC 2 0 1 1"], ["FCN - GoogLeNet", "Evaluated-With", "PASCAL VOC 2 0 1 1"], ["FCN - VGG 1 6", "Evaluated-With", "PASCAL VOC 2 0 1 1"]], "rel_plus": [["FCN - AlexNet:Method", "Evaluated-With", "PASCAL VOC 2 0 1 1:Dataset"], ["FCN - VGG 1 6:Method", "Evaluated-With", "PASCAL VOC 2 0 1 1:Dataset"], ["FCN - GoogLeNet:Method", "Evaluated-With", "PASCAL VOC 2 0 1 1:Dataset"], ["FCN - VGG 1 6:Method", "Evaluated-With", "PASCAL VOC 2 0 1 1:Dataset"]]}
{"doc_id": "210164920", "sentence": "So they choose the FCN - VGG 1 6 network for further experiments .", "ner": [["FCN - VGG 1 6", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210164920", "sentence": "Figure 4 shows different deep jet : FCN - 1 6 s and FCN - 8 s and FCN - 3 2 s. The authors have shown that FCN - 8 s gave the best result in PASCAL VOC 2 0 1 1 & 2 0 1 2 [ 1 0 4 ] test dataset and FCN - 1 6 s gave the best result on both NYUDv 2 [ 1 0 6 ] & SIFT Flow [ 7 1 ] datasets .", "ner": [["FCN - 1 6 s", "Method"], ["FCN - 8 s", "Method"], ["FCN - 3 2 s.", "Method"], ["FCN - 8 s", "Method"], ["PASCAL VOC 2 0 1 1 & 2 0 1 2", "Dataset"], ["FCN - 1 6 s", "Method"], ["NYUDv 2", "Dataset"], ["SIFT Flow", "Dataset"]], "rel": [["FCN - 8 s", "Evaluated-With", "PASCAL VOC 2 0 1 1 & 2 0 1 2"], ["FCN - 1 6 s", "Evaluated-With", "NYUDv 2"], ["FCN - 1 6 s", "Evaluated-With", "SIFT Flow"]], "rel_plus": [["FCN - 8 s:Method", "Evaluated-With", "PASCAL VOC 2 0 1 1 & 2 0 1 2:Dataset"], ["FCN - 1 6 s:Method", "Evaluated-With", "NYUDv 2:Dataset"], ["FCN - 1 6 s:Method", "Evaluated-With", "SIFT Flow:Dataset"]]}
{"doc_id": "210164920", "sentence": "Chen et al. has brought together methods from Deep Convolutional Neural Network(DCNN ) and probabilistic graphical model , and produced DeepLab [ 8 2 ] to address semantic segmentation .", "ner": [["Deep Convolutional Neural Network(DCNN )", "Method"], ["probabilistic graphical model", "Method"], ["DeepLab", "Method"], ["semantic segmentation", "Task"]], "rel": [["Deep Convolutional Neural Network(DCNN )", "Used-For", "semantic segmentation"], ["probabilistic graphical model", "Used-For", "semantic segmentation"], ["DeepLab", "Used-For", "semantic segmentation"]], "rel_plus": [["Deep Convolutional Neural Network(DCNN ):Method", "Used-For", "semantic segmentation:Task"], ["probabilistic graphical model:Method", "Used-For", "semantic segmentation:Task"], ["DeepLab:Method", "Used-For", "semantic segmentation:Task"]]}
{"doc_id": "210164920", "sentence": "DeepLab achieved 7 1 . 6 % IOU accuracy in the test set of the PASCAL VOC 2 0 1 2 semantic segmentation task .", "ner": [["DeepLab", "Method"], ["PASCAL VOC 2 0 1 2 semantic segmentation", "Dataset"]], "rel": [["DeepLab", "Evaluated-With", "PASCAL VOC 2 0 1 2 semantic segmentation"]], "rel_plus": [["DeepLab:Method", "Evaluated-With", "PASCAL VOC 2 0 1 2 semantic segmentation:Dataset"]]}
{"doc_id": "210164920", "sentence": "The authors have faced two technical difficulties in the application of DCNN to semantic segmentation : down sampling and spatial invariance .", "ner": [["DCNN", "Method"], ["semantic segmentation", "Task"]], "rel": [["DCNN", "Used-For", "semantic segmentation"]], "rel_plus": [["DCNN:Method", "Used-For", "semantic segmentation:Task"]]}
{"doc_id": "210164920", "sentence": "To handle the first problem , the authors have employed ' atrous ' ( with holes ) [ 1 0 7 ] algorithm for efficient dense computation of CNN .", "ner": [["atrous", "Method"], ["CNN", "Method"]], "rel": [["atrous", "Part-Of", "CNN"]], "rel_plus": [["atrous:Method", "Part-Of", "CNN:Method"]]}
{"doc_id": "210164920", "sentence": "Figure 5a and 5b shows atrous algorithm in 1 - D and in 2 - D. To handle the second problem , they have applied a fully connected pairwise conditional random field ( CRF ) to capture fine details .", "ner": [["atrous", "Method"], ["conditional random field", "Method"], ["CRF", "Method"]], "rel": [["CRF", "Synonym-Of", "conditional random field"]], "rel_plus": [["CRF:Method", "Synonym-Of", "conditional random field:Method"]]}
{"doc_id": "210164920", "sentence": "The authors again modified the DeepLab using Atrous Special Pooling Pyramid ( ASPP ) to aggregate multi - scale features Figure 5 : Illustration of atrous algorithm ( a ) in 1 - D , when kernel size= 3 , input - stride= 2 and output - stride= 1 [ 8 2 ] and ( b ) in 2 - D , when kernel size 3 \u00d7 3 , with rate 1 , 6 and 2 4 [ 8 4 ] for better localization and proposed DeepLabv 2 [ 8 3 ] .", "ner": [["DeepLab", "Method"], ["Atrous Special Pooling Pyramid", "Method"], ["ASPP", "Method"], ["atrous", "Method"], ["DeepLabv 2", "Method"]], "rel": [["Atrous Special Pooling Pyramid", "Part-Of", "DeepLab"], ["ASPP", "Synonym-Of", "Atrous Special Pooling Pyramid"]], "rel_plus": [["Atrous Special Pooling Pyramid:Method", "Part-Of", "DeepLab:Method"], ["ASPP:Method", "Synonym-Of", "Atrous Special Pooling Pyramid:Method"]]}
{"doc_id": "210164920", "sentence": "This architecture used both ResNet [ 1 0 ] and VGGNet [ 8 ] as base network .", "ner": [["ResNet", "Method"], ["VGGNet", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210164920", "sentence": "In DeepLabv 3 [ 8 4 ] , to incorporate multiple contexts in the network , the authors have used cascaded modules and have gone deeper especially with the ASPP module .   Deconvnet [ 8 5 ] proposed by Noh et al. , has a convolutional and deconvolutional network .", "ner": [["DeepLabv 3", "Method"], ["ASPP", "Method"], ["Deconvnet", "Method"], ["convolutional and deconvolutional network", "Method"]], "rel": [["ASPP", "Part-Of", "DeepLabv 3"], ["convolutional and deconvolutional network", "Part-Of", "Deconvnet"]], "rel_plus": [["ASPP:Method", "Part-Of", "DeepLabv 3:Method"], ["convolutional and deconvolutional network:Method", "Part-Of", "Deconvnet:Method"]]}
{"doc_id": "210164920", "sentence": "The convolutional network is topologically identical with the first 1 3 convolution layers and 2 fully connected layers of VGG 1 6 [ 8 ] except for the final classification layer .", "ner": [["convolutional network", "Method"], ["convolution", "Method"], ["fully connected layers", "Method"], ["VGG 1 6", "Method"]], "rel": [["fully connected layers", "Part-Of", "VGG 1 6"], ["convolution", "Part-Of", "VGG 1 6"]], "rel_plus": [["fully connected layers:Method", "Part-Of", "VGG 1 6:Method"], ["convolution:Method", "Part-Of", "VGG 1 6:Method"]]}
{"doc_id": "210164920", "sentence": "As in VGG 1 6 , pooling and rectification layers are also added after some of the convolutional layers .", "ner": [["VGG 1 6", "Method"], ["pooling", "Method"], ["rectification layers", "Method"], ["convolutional layers", "Method"]], "rel": [["rectification layers", "Part-Of", "VGG 1 6"], ["pooling", "Part-Of", "VGG 1 6"], ["convolutional layers", "Part-Of", "VGG 1 6"]], "rel_plus": [["rectification layers:Method", "Part-Of", "VGG 1 6:Method"], ["pooling:Method", "Part-Of", "VGG 1 6:Method"], ["convolutional layers:Method", "Part-Of", "VGG 1 6:Method"]]}
{"doc_id": "210164920", "sentence": "It also has multiple series of deconvolution , un - pooling and rectification layers .", "ner": [["deconvolution", "Method"], ["un - pooling", "Method"], ["rectification layers", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210164920", "sentence": "Following [ 7 ] [ 1 0 8 ] , unpooling is done using max - pooling indices which are stored at the time of convolution operation in the convolutional network .", "ner": [["unpooling", "Method"], ["max - pooling", "Method"], ["convolution operation", "Method"], ["convolutional network", "Method"]], "rel": [["max - pooling", "Used-For", "unpooling"], ["convolution operation", "Part-Of", "convolutional network"]], "rel_plus": [["max - pooling:Method", "Used-For", "unpooling:Method"], ["convolution operation:Method", "Part-Of", "convolutional network:Method"]]}
{"doc_id": "210164920", "sentence": "The architecture of Deconvnet is shown in figure 7 .   U - Net [ 8 6 ] is a U - shaped semantic segmentation which has a contracting path and an expansive path .", "ner": [["Deconvnet", "Method"], ["U - Net", "Method"], ["U - shaped semantic segmentation", "Method"]], "rel": [["U - Net", "SubClass-Of", "U - shaped semantic segmentation"]], "rel_plus": [["U - Net:Method", "SubClass-Of", "U - shaped semantic segmentation:Method"]]}
{"doc_id": "210164920", "sentence": "Every step of the contracting path consists of two consecutive 3 \u00d7 3 convolutions followed by ReLU nonlinearity and max - pooling using 2 \u00d7 2 window with stride 2 .", "ner": [["3 \u00d7 3 convolutions", "Method"], ["ReLU", "Method"], ["max - pooling", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210164920", "sentence": "Then two consecutive 3 \u00d7 3 convolution operations are applied followed by ReLU nonlinearity .", "ner": [["3 \u00d7 3 convolution", "Method"], ["ReLU", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210164920", "sentence": "The architecture of U - Net is shown in figure 8 .   Traditional CNN , used for classification tasks , loses resolution in its way and it is not suitable for dense prediction .", "ner": [["U - Net", "Method"], ["CNN", "Method"], ["classification", "Task"]], "rel": [["CNN", "Used-For", "classification"]], "rel_plus": [["CNN:Method", "Used-For", "classification:Task"]]}
{"doc_id": "210164920", "sentence": "Yu and Koltun have introduced a modified version of traditional CNN , called dialated convolution or Dialated - Net [ 1 1 0 ] , to accumulate multi - scale contextual information systematically for better segmentation without suffering the loss of resolution .", "ner": [["CNN", "Method"], ["dialated convolution", "Method"], ["Dialated - Net", "Method"], ["segmentation", "Task"]], "rel": [["dialated convolution", "SubClass-Of", "CNN"], ["Dialated - Net", "SubClass-Of", "CNN"], ["Dialated - Net", "Used-For", "segmentation"], ["dialated convolution", "Used-For", "segmentation"], ["CNN", "Used-For", "segmentation"]], "rel_plus": [["dialated convolution:Method", "SubClass-Of", "CNN:Method"], ["Dialated - Net:Method", "SubClass-Of", "CNN:Method"], ["Dialated - Net:Method", "Used-For", "segmentation:Task"], ["dialated convolution:Method", "Used-For", "segmentation:Task"], ["CNN:Method", "Used-For", "segmentation:Task"]]}
{"doc_id": "210164920", "sentence": "Dialated - Net is like a rectangular prism of convolutional layers , unlike conventional pyramidal CNN .", "ner": [["Dialated - Net", "Method"], ["rectangular prism of convolutional layers", "Method"], ["conventional pyramidal CNN", "Method"]], "rel": [["Dialated - Net", "Compare-With", "rectangular prism of convolutional layers"], ["Dialated - Net", "Compare-With", "conventional pyramidal CNN"]], "rel_plus": [["Dialated - Net:Method", "Compare-With", "rectangular prism of convolutional layers:Method"], ["Dialated - Net:Method", "Compare-With", "conventional pyramidal CNN:Method"]]}
{"doc_id": "210164920", "sentence": "Liu et al proposed an end - to - end architecture called ParseNet [ 8 9 ] which is an improvement of Fully Convolution Neural Network .", "ner": [["ParseNet", "Method"], ["Fully Convolution Neural Network", "Method"]], "rel": [["ParseNet", "SubClass-Of", "Fully Convolution Neural Network"]], "rel_plus": [["ParseNet:Method", "SubClass-Of", "Fully Convolution Neural Network:Method"]]}
{"doc_id": "210164920", "sentence": "Till convolutional feature map extraction , the ParseNet is the same as FCN [ 7 8 ] .", "ner": [["ParseNet", "Method"], ["FCN", "Method"]], "rel": [["ParseNet", "Compare-With", "FCN"]], "rel_plus": [["ParseNet:Method", "Compare-With", "FCN:Method"]]}
{"doc_id": "210164920", "sentence": "This network achieved state - of -the - art performance on ShiftFlow [ 7 1 ] , PASCAL - context [ 1 1 1 ] and near the state of the art on PASCAL VOC 2 0 1 2 dataset .   SegNet [ 8 8 ] has encoder - decoder architecture followed by a final pixel - wise classification layer .", "ner": [["ShiftFlow", "Dataset"], ["PASCAL - context", "Dataset"], ["PASCAL VOC 2 0 1 2", "Dataset"], ["SegNet", "Method"], ["encoder - decoder", "Method"]], "rel": [["encoder - decoder", "Part-Of", "SegNet"]], "rel_plus": [["encoder - decoder:Method", "Part-Of", "SegNet:Method"]]}
{"doc_id": "210164920", "sentence": "The authors did not use fully connected layers of VGG 1 6 to retain the resolution in the deepest layer and also to reduce the number of parameters from 1 3 4 M to 1 4 . 7 M .", "ner": [["fully connected layers", "Method"], ["VGG 1 6", "Method"]], "rel": [["fully connected layers", "Compare-With", "VGG 1 6"]], "rel_plus": [["fully connected layers:Method", "Compare-With", "VGG 1 6:Method"]]}
{"doc_id": "210164920", "sentence": "Then , to reduce internal covariate shift the authors have used batch normalization [ 1 1 2 ] [ 1 1 3 ] followed by ReLU [ 1 1 4 ] nonlinearity operation .", "ner": [["batch normalization", "Method"], ["ReLU", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210164920", "sentence": "A combination of max - pooling and sub - sampling operation achieves better classification accuracy but reduces the feature map size which leads to lossy image representation with blurred boundaries which is not ideal for segmentation purposes where boundary information is important .", "ner": [["max - pooling", "Method"], ["sub - sampling operation", "Method"], ["classification", "Task"], ["segmentation", "Task"]], "rel": [["sub - sampling operation", "Used-For", "classification"], ["max - pooling", "Used-For", "classification"]], "rel_plus": [["sub - sampling operation:Method", "Used-For", "classification:Task"], ["max - pooling:Method", "Used-For", "classification:Task"]]}
{"doc_id": "210164920", "sentence": "To retain boundary information in the encoder feature maps before sub - sampling , Seg - Net stores only the max - pooling indices for each encoder map .", "ner": [["sub - sampling", "Method"], ["Seg - Net", "Method"], ["max - pooling", "Method"]], "rel": [["max - pooling", "Part-Of", "Seg - Net"]], "rel_plus": [["max - pooling:Method", "Part-Of", "Seg - Net:Method"]]}
{"doc_id": "210164920", "sentence": "To achieve this , SegNet does up - sampling in its decoder using the stored max - pooling indices from the corresponding encoder feature map resulting high - resolution sparse feature map .", "ner": [["SegNet", "Method"], ["up - sampling", "Method"], ["decoder", "Method"], ["max - pooling", "Method"]], "rel": [["decoder", "Part-Of", "SegNet"], ["max - pooling", "Part-Of", "SegNet"], ["up - sampling", "Part-Of", "decoder"]], "rel_plus": [["decoder:Method", "Part-Of", "SegNet:Method"], ["max - pooling:Method", "Part-Of", "SegNet:Method"], ["up - sampling:Method", "Part-Of", "decoder:Method"]]}
{"doc_id": "210164920", "sentence": "The architecture of SegNet is shown in figure 1 1 .   Like ParseNet , Global Convolution Network [ 9 1 ] has also used global features along with local features to make the pixel - wise prediction more accurate .", "ner": [["SegNet", "Method"], ["ParseNet", "Method"], ["Global Convolution Network", "Method"], ["pixel - wise prediction", "Task"]], "rel": [["ParseNet", "Used-For", "pixel - wise prediction"], ["Global Convolution Network", "Used-For", "pixel - wise prediction"]], "rel_plus": [["ParseNet:Method", "Used-For", "pixel - wise prediction:Task"], ["Global Convolution Network:Method", "Used-For", "pixel - wise prediction:Task"]]}
{"doc_id": "210164920", "sentence": "In GCN , the authors did not use any fully connected layers or global pooling layers to retain spatial information .", "ner": [["GCN", "Method"], ["fully connected layers", "Method"], ["global pooling layers", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210164920", "sentence": "Then score maps of lower resolution are up - sampled with a deconvolution layer , and then added up with higher ones to generate new score maps for final segmentation .", "ner": [["deconvolution", "Method"], ["segmentation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "210164920", "sentence": "Pyramid Scene Parsing Network(PSPNet ) [ 9 0 ] , proposed by Zhao et al. , has also used global contextual information for better segmentation .", "ner": [["Pyramid Scene Parsing Network(PSPNet )", "Method"], ["segmentation", "Task"]], "rel": [["Pyramid Scene Parsing Network(PSPNet )", "Used-For", "segmentation"]], "rel_plus": [["Pyramid Scene Parsing Network(PSPNet ):Method", "Used-For", "segmentation:Task"]]}
{"doc_id": "210164920", "sentence": "In this model , the authors have used Pyramid Pooling Module on top of the last feature map extracted using dilated FCN .", "ner": [["Pyramid Pooling Module", "Method"], ["dilated FCN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210164920", "sentence": "To reduce dimension , the Figure 1 3 : PSPNet Model Design [ 9 0 ] pooled feature maps are convolved using 1 \u00d7 1 convolution layer .", "ner": [["PSPNet", "Method"], ["1 \u00d7 1 convolution", "Method"]], "rel": [["1 \u00d7 1 convolution", "Part-Of", "PSPNet"]], "rel_plus": [["1 \u00d7 1 convolution:Method", "Part-Of", "PSPNet:Method"]]}
{"doc_id": "210164920", "sentence": "DenseNet [ 1 0 ] is a CNN based classification network that contains only a down - sampling pathway for recognition .", "ner": [["DenseNet", "Method"], ["CNN based classification network", "Method"], ["recognition", "Task"]], "rel": [["DenseNet", "SubClass-Of", "CNN based classification network"], ["CNN based classification network", "Used-For", "recognition"], ["DenseNet", "Used-For", "recognition"]], "rel_plus": [["DenseNet:Method", "SubClass-Of", "CNN based classification network:Method"], ["CNN based classification network:Method", "Used-For", "recognition:Task"], ["DenseNet:Method", "Used-For", "recognition:Task"]]}
{"doc_id": "210164920", "sentence": "Gated - SCNN : Figure 1 5 : Architecture of Gated Shape CNN for semantic segmentation [ 9 4 ] Takikawa et al. proposed Gated -Shape CNN(GSCNN ) [ 9 4 ] for Semantic Segmentation .", "ner": [["Gated - SCNN", "Method"], ["Gated Shape CNN", "Method"], ["semantic segmentation", "Task"], ["Gated -Shape CNN(GSCNN )", "Method"], ["Semantic Segmentation", "Task"]], "rel": [["Gated Shape CNN", "Used-For", "semantic segmentation"], ["Gated -Shape CNN(GSCNN )", "Used-For", "Semantic Segmentation"]], "rel_plus": [["Gated Shape CNN:Method", "Used-For", "semantic segmentation:Task"], ["Gated -Shape CNN(GSCNN ):Method", "Used-For", "Semantic Segmentation:Task"]]}
{"doc_id": "210164920", "sentence": "Shape stream consists of multiple Gated Convolution Layer ( GCL ) which process boundary information of regions using low - level feature maps from the regular stream .", "ner": [["Gated Convolution Layer", "Method"], ["GCL", "Method"]], "rel": [["GCL", "Synonym-Of", "Gated Convolution Layer"]], "rel_plus": [["GCL:Method", "Synonym-Of", "Gated Convolution Layer:Method"]]}
{"doc_id": "210164920", "sentence": "From the year 2 0 1 2 , different CNN based semantic segmentation models have emerged in successive years to date .", "ner": [["CNN", "Method"], ["semantic segmentation", "Task"]], "rel": [["CNN", "Used-For", "semantic segmentation"]], "rel_plus": [["CNN:Method", "Used-For", "semantic segmentation:Task"]]}
{"doc_id": "210164920", "sentence": "Among different models , Fully Convolutional Network ( FCN ) has set a path for semantic segmentation .", "ner": [["Fully Convolutional Network", "Method"], ["FCN", "Method"], ["semantic segmentation", "Task"]], "rel": [["FCN", "Synonym-Of", "Fully Convolutional Network"], ["Fully Convolutional Network", "Used-For", "semantic segmentation"]], "rel_plus": [["FCN:Method", "Synonym-Of", "Fully Convolutional Network:Method"], ["Fully Convolutional Network:Method", "Used-For", "semantic segmentation:Task"]]}
{"doc_id": "210164920", "sentence": "DeepLab and its versions have used atrous algorithm in different ways .", "ner": [["DeepLab", "Method"], ["atrous", "Method"]], "rel": [["atrous", "Used-For", "DeepLab"]], "rel_plus": [["atrous:Method", "Used-For", "DeepLab:Method"]]}
{"doc_id": "210164920", "sentence": "SegNet , DeconvNet , U - Net have a similar architecture where the second part of those architectures is hierarchically opposite of the first half .", "ner": [["SegNet", "Method"], ["DeconvNet", "Method"], ["U - Net", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210164920", "sentence": "ParseNet , PSPNet , and GCN have addressed semantic segmentation with respect to contextual information .", "ner": [["ParseNet", "Method"], ["PSPNet", "Method"], ["GCN", "Method"], ["semantic segmentation", "Task"]], "rel": [["GCN", "Used-For", "semantic segmentation"], ["PSPNet", "Used-For", "semantic segmentation"], ["ParseNet", "Used-For", "semantic segmentation"]], "rel_plus": [["GCN:Method", "Used-For", "semantic segmentation:Task"], ["PSPNet:Method", "Used-For", "semantic segmentation:Task"], ["ParseNet:Method", "Used-For", "semantic segmentation:Task"]]}
{"doc_id": "210164920", "sentence": "So , the performance of a semantic segmentation model depends on the internal architecture of a network as well as other aspects such as the size of the data set , number of semantically annotated data , different training hyperparameters ( such as learning rate , momentum , weight decay ) , optimization algorithm , loss function , etc .", "ner": [["semantic segmentation", "Task"], ["momentum", "Method"], ["weight decay", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210164920", "sentence": "Table 2 presents base network ( pre - trained on ImageNet [ 1 1 6 ] dataset ) , data pre - processing technique ( basically data augmentation ) and different loss function used for different models .", "ner": [["ImageNet", "Dataset"], ["data augmentation", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210164920", "sentence": "Table 3 has briefly shown some important features of each model .   Like semantic segmentation , the applicability of CNN has been spread over instance segmentation too .", "ner": [["semantic segmentation", "Task"], ["CNN", "Method"], ["instance segmentation", "Task"]], "rel": [["CNN", "Used-For", "semantic segmentation"], ["CNN", "Used-For", "instance segmentation"]], "rel_plus": [["CNN:Method", "Used-For", "semantic segmentation:Task"], ["CNN:Method", "Used-For", "instance segmentation:Task"]]}
{"doc_id": "210164920", "sentence": "Unlike semantic segmentation , instance segmentation masks each instance of an object contained in an image independently [ 1 2 6 , 1 2 7 ] .", "ner": [["semantic segmentation", "Task"], ["instance segmentation", "Task"]], "rel": [["instance segmentation", "Compare-With", "semantic segmentation"]], "rel_plus": [["instance segmentation:Task", "Compare-With", "semantic segmentation:Task"]]}
{"doc_id": "210164920", "sentence": "The task of object detection and instance segmentation Data augmentation using extra annotated data of [ 1 1 8 ] Sum of cross - entropy loss Deconvnet [ 8 5 ] VGG 1 6 pre - trained on ILSVRC dataset Data augmentation using extra annotated data of [ 1 1 8 ] U - Net [ 8 6 ] FCN [ 7 8 ] Data augmentation by applying random elastic deformation to the available training images DialateNet [ 1 1 0 ] VGG 1 6 [ 8 ] Data augmentation using extra annotated data of [ 1 1 8 ] ParseNet [ 8 9 ] FCN [ 7 8 ] SegNet [ 8 8 ] VGG 1 6 [ 8 ] Local contrast normalization to RGB data Cross entropy loss GCN [ 9 1 ] ResNet 1 5 2 [ 1 0 ] as feature network and FCN - 4 [ 7 8 ] as segmentation network", "ner": [["object detection", "Task"], ["instance segmentation", "Task"], ["Data augmentation", "Method"], ["cross - entropy loss", "Method"], ["Deconvnet", "Method"], ["VGG 1 6", "Method"], ["ILSVRC", "Dataset"], ["Data augmentation", "Method"], ["U - Net", "Method"], ["FCN", "Method"], ["Data augmentation", "Method"], ["DialateNet", "Method"], ["VGG 1 6", "Method"], ["Data augmentation", "Method"], ["ParseNet", "Method"], ["FCN", "Method"], ["SegNet", "Method"], ["VGG 1 6", "Method"], ["Local contrast normalization", "Method"], ["Cross entropy loss", "Method"], ["GCN", "Method"], ["ResNet 1 5 2", "Method"], ["feature network", "Method"], ["FCN - 4", "Method"], ["segmentation network", "Method"]], "rel": [["VGG 1 6", "Trained-With", "ILSVRC"], ["Deconvnet", "Trained-With", "ILSVRC"], ["Cross entropy loss", "Part-Of", "GCN"], ["ResNet 1 5 2", "Used-For", "feature network"], ["FCN - 4", "Used-For", "segmentation network"]], "rel_plus": [["VGG 1 6:Method", "Trained-With", "ILSVRC:Dataset"], ["Deconvnet:Method", "Trained-With", "ILSVRC:Dataset"], ["Cross entropy loss:Method", "Part-Of", "GCN:Method"], ["ResNet 1 5 2:Method", "Used-For", "feature network:Method"], ["FCN - 4:Method", "Used-For", "segmentation network:Method"]]}
{"doc_id": "210164920", "sentence": "Semantic Boundaries Dataset [ 1 1 8 ] is used as auxiliary dataset PSPNet [ 9 0 ] Pretrained ResNet [ 1 0 ] Data augmentation : random mirror and random resize between 0. 5 and 2 , random rotation between - 1 0 and 1 0 degrees , random Gaussian blur Four losses : \u2022 Additional loss for initial result generation \u2022 Final loss for learning the residue later \u2022 Auxiliary loss for shallow layers \u2022 Master branch loss for final prediction FC - DenseNet [ 1 1 5 ] DensNet [ 1 0 ] Data augmentation using random cropping and vertical flipping Gated - SCNN [ 9 4 ] ResNet 1 0 1 [ 1 0 ] and WideResNet [ 1 1 9 ] \u2022 Segmentation loss for regular stream \u2022 Dual task loss for shape stream \u2022\u2022 Standard binary cross entropy loss for boundary refinement \u2022\u2022 Standard cross entropy for semantic segmentation are quite correlated .", "ner": [["Semantic Boundaries Dataset", "Dataset"], ["PSPNet", "Method"], ["ResNet", "Method"], ["Data augmentation", "Method"], ["random mirror", "Method"], ["random resize", "Method"], ["random rotation", "Method"], ["random Gaussian blur", "Method"], ["FC - DenseNet", "Method"], ["DensNet", "Method"], ["Data augmentation", "Method"], ["random cropping", "Method"], ["vertical flipping", "Method"], ["Gated - SCNN", "Method"], ["ResNet 1 0 1", "Method"], ["WideResNet", "Method"], ["binary cross entropy loss", "Method"], ["boundary refinement", "Task"], ["cross entropy", "Method"], ["semantic segmentation", "Task"]], "rel": [["Semantic Boundaries Dataset", "Used-For", "PSPNet"], ["random mirror", "SubClass-Of", "Data augmentation"], ["random resize", "SubClass-Of", "Data augmentation"], ["random rotation", "SubClass-Of", "Data augmentation"], ["random Gaussian blur", "SubClass-Of", "Data augmentation"], ["vertical flipping", "SubClass-Of", "Data augmentation"], ["random cropping", "SubClass-Of", "Data augmentation"], ["binary cross entropy loss", "Used-For", "boundary refinement"], ["cross entropy", "Used-For", "semantic segmentation"]], "rel_plus": [["Semantic Boundaries Dataset:Dataset", "Used-For", "PSPNet:Method"], ["random mirror:Method", "SubClass-Of", "Data augmentation:Method"], ["random resize:Method", "SubClass-Of", "Data augmentation:Method"], ["random rotation:Method", "SubClass-Of", "Data augmentation:Method"], ["random Gaussian blur:Method", "SubClass-Of", "Data augmentation:Method"], ["vertical flipping:Method", "SubClass-Of", "Data augmentation:Method"], ["random cropping:Method", "SubClass-Of", "Data augmentation:Method"], ["binary cross entropy loss:Method", "Used-For", "boundary refinement:Task"], ["cross entropy:Method", "Used-For", "semantic segmentation:Task"]]}
{"doc_id": "210164920", "sentence": "In object detection , researchers use the bounding box to detect each object instance of an image with a label for classification .", "ner": [["object detection", "Task"], ["classification", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "210164920", "sentence": "Instance segmentation put this task one step forward and put a segmentation mask for each instance .", "ner": [["Instance segmentation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "210164920", "sentence": "Concurrent to semantic segmentation research , instance segmentation research has also started to use the convolutional neural network(CNN ) for better segmentation accuracy .", "ner": [["semantic segmentation", "Task"], ["instance segmentation", "Task"], ["convolutional neural network(CNN )", "Method"], ["segmentation", "Task"]], "rel": [["convolutional neural network(CNN )", "Used-For", "semantic segmentation"], ["convolutional neural network(CNN )", "Used-For", "instance segmentation"], ["convolutional neural network(CNN )", "Used-For", "segmentation"]], "rel_plus": [["convolutional neural network(CNN ):Method", "Used-For", "semantic segmentation:Task"], ["convolutional neural network(CNN ):Method", "Used-For", "instance segmentation:Task"], ["convolutional neural network(CNN ):Method", "Used-For", "segmentation:Task"]]}
{"doc_id": "210164920", "sentence": "Herein , we are going to survey the evolution of CNN based instance segmentation models .", "ner": [["CNN", "Method"], ["instance segmentation", "Task"]], "rel": [["CNN", "Used-For", "instance segmentation"]], "rel_plus": [["CNN:Method", "Used-For", "instance segmentation:Task"]]}
{"doc_id": "210164920", "sentence": "In addition , we are going to bring up here an elaborate exploration of some state - of - the - art models for instance segmentation task .   CNN based instance segmentation has also started its journey along with semantic segmentation .", "ner": [["instance segmentation", "Task"], ["CNN", "Method"], ["instance segmentation", "Task"], ["semantic segmentation", "Task"]], "rel": [["CNN", "Used-For", "instance segmentation"], ["CNN", "Used-For", "semantic segmentation"]], "rel_plus": [["CNN:Method", "Used-For", "instance segmentation:Task"], ["CNN:Method", "Used-For", "semantic segmentation:Task"]]}
{"doc_id": "210164920", "sentence": "As we have mentioned in section 4 that instance segmentation task only adds a segmentation mask to the output of object detection task .", "ner": [["instance segmentation", "Task"], ["object detection", "Task"]], "rel": [["instance segmentation", "Used-For", "object detection"]], "rel_plus": [["instance segmentation:Task", "Used-For", "object detection:Task"]]}
{"doc_id": "210164920", "sentence": "That is why most of the CNN based instance segmentation models have used different CNN based object detection models to produce better segmentation accuracy and to reduce test time .", "ner": [["CNN based instance segmentation", "Method"], ["CNN based object detection", "Method"], ["segmentation", "Task"]], "rel": [["CNN based object detection", "Part-Of", "CNN based instance segmentation"], ["CNN based instance segmentation", "Used-For", "segmentation"]], "rel_plus": [["CNN based object detection:Method", "Part-Of", "CNN based instance segmentation:Method"], ["CNN based instance segmentation:Method", "Used-For", "segmentation:Task"]]}
{"doc_id": "210164920", "sentence": "Hariharan et al. have followed the architecture of R - CNN [ 7 4 ] object detector and proposed a novel architecture for instance segmentation called Simultaneous Detection and Segmentation(SDS ) [ 1 2 7 ] which is a 4 step instance segmentation model as described in section 4. 2 . 1 .", "ner": [["R - CNN", "Method"], ["instance segmentation", "Task"], ["Simultaneous Detection and Segmentation(SDS )", "Method"], ["instance segmentation", "Task"]], "rel": [["R - CNN", "Used-For", "instance segmentation"], ["Simultaneous Detection and Segmentation(SDS )", "Used-For", "instance segmentation"]], "rel_plus": [["R - CNN:Method", "Used-For", "instance segmentation:Task"], ["Simultaneous Detection and Segmentation(SDS ):Method", "Used-For", "instance segmentation:Task"]]}
{"doc_id": "210164920", "sentence": "Till this time CNN based models have only used the last layer feature map [ 1 7 ] , Fast R - CNN [ 1 8 ] have used two stages network for object detection .", "ner": [["CNN", "Method"], ["Fast R - CNN", "Method"], ["object detection", "Task"]], "rel": [["Fast R - CNN", "Used-For", "object detection"]], "rel_plus": [["Fast R - CNN:Method", "Used-For", "object detection:Task"]]}
{"doc_id": "210164920", "sentence": "The first stage detects object proposals using Selective Search [ 7 5 ] algorithm and second stage classify those proposals using different CNN based classifier .", "ner": [["Selective Search", "Method"], ["CNN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210164920", "sentence": "Multibox [ 1 2 9 , 1 3 0 ] , Deepbox [ 1 3 1 ] , Edgebox [ 1 3 2 ] have used CNN based proposal generation method for object detection .", "ner": [["Multibox", "Method"], ["Deepbox", "Method"], ["Edgebox", "Method"], ["CNN", "Method"], ["object detection", "Task"]], "rel": [["CNN", "Part-Of", "Multibox"], ["CNN", "Part-Of", "Deepbox"], ["CNN", "Part-Of", "Edgebox"], ["Edgebox", "Used-For", "object detection"], ["Deepbox", "Used-For", "object detection"], ["Multibox", "Used-For", "object detection"]], "rel_plus": [["CNN:Method", "Part-Of", "Multibox:Method"], ["CNN:Method", "Part-Of", "Deepbox:Method"], ["CNN:Method", "Part-Of", "Edgebox:Method"], ["Edgebox:Method", "Used-For", "object detection:Task"], ["Deepbox:Method", "Used-For", "object detection:Task"], ["Multibox:Method", "Used-For", "object detection:Task"]]}
{"doc_id": "210164920", "sentence": "Faster R - CNN [ 1 9 ] have used CNN based ' region proposal network ( RPN ) ' for generating box proposal .", "ner": [["Faster R - CNN", "Method"], ["CNN", "Method"], ["region proposal network", "Method"], ["RPN", "Method"]], "rel": [["region proposal network", "Part-Of", "Faster R - CNN"], ["RPN", "Synonym-Of", "region proposal network"], ["CNN", "Part-Of", "region proposal network"]], "rel_plus": [["region proposal network:Method", "Part-Of", "Faster R - CNN:Method"], ["RPN:Method", "Synonym-Of", "region proposal network:Method"], ["CNN:Method", "Part-Of", "region proposal network:Method"]]}
{"doc_id": "210164920", "sentence": "In parallel to this , instance segmentation algorithms such as SDS and Hyper column have used Multi - scale Combinatorial Grouping ( MCG ) [ 1 3 3 ] for region proposal generation .", "ner": [["instance segmentation", "Task"], ["SDS", "Method"], ["Hyper column", "Method"], ["Multi - scale Combinatorial Grouping", "Method"], ["MCG", "Method"], ["region proposal generation", "Task"]], "rel": [["SDS", "Used-For", "instance segmentation"], ["Hyper column", "Used-For", "instance segmentation"], ["Multi - scale Combinatorial Grouping", "Part-Of", "SDS"], ["Multi - scale Combinatorial Grouping", "Part-Of", "Hyper column"], ["MCG", "Synonym-Of", "Multi - scale Combinatorial Grouping"], ["Hyper column", "Used-For", "region proposal generation"]], "rel_plus": [["SDS:Method", "Used-For", "instance segmentation:Task"], ["Hyper column:Method", "Used-For", "instance segmentation:Task"], ["Multi - scale Combinatorial Grouping:Method", "Part-Of", "SDS:Method"], ["Multi - scale Combinatorial Grouping:Method", "Part-Of", "Hyper column:Method"], ["MCG:Method", "Synonym-Of", "Multi - scale Combinatorial Grouping:Method"], ["Hyper column:Method", "Used-For", "region proposal generation:Task"]]}
{"doc_id": "210164920", "sentence": "Deep - Mask [ 1 3 4 ] , as discussed in section 4. 2 . 2 , has also used CNN based RPN as Faster R - CNN to generate region proposals so that the model can be trained end to end .", "ner": [["Deep - Mask", "Method"], ["CNN based RPN", "Method"], ["Faster R - CNN", "Method"], ["generate region proposals", "Task"]], "rel": [["Faster R - CNN", "Part-Of", "Deep - Mask"], ["CNN based RPN", "Part-Of", "Faster R - CNN"], ["Deep - Mask", "Used-For", "generate region proposals"]], "rel_plus": [["Faster R - CNN:Method", "Part-Of", "Deep - Mask:Method"], ["CNN based RPN:Method", "Part-Of", "Faster R - CNN:Method"], ["Deep - Mask:Method", "Used-For", "generate region proposals:Task"]]}
{"doc_id": "210164920", "sentence": "Previous object detection and instance segmentation modules such as [ 7 4 ] , [ 1 7 ] , [ 1 8 ] , [ 1 9 ] [ 1 2 7 ] , [ 1 2 8 ] , [ 1 3 4 ] etc . have used computationally expensive external methods for generating object level or mask level proposals like Selective Search , MCG , CPMC [ 7 7 ] , RPN etc .", "ner": [["object detection", "Task"], ["instance segmentation", "Task"], ["Selective Search", "Method"], ["MCG", "Method"], ["CPMC", "Method"], ["RPN", "Method"]], "rel": [["Selective Search", "Used-For", "object detection"], ["MCG", "Used-For", "object detection"], ["CPMC", "Used-For", "object detection"], ["RPN", "Used-For", "object detection"], ["Selective Search", "Used-For", "instance segmentation"], ["MCG", "Used-For", "instance segmentation"], ["CPMC", "Used-For", "instance segmentation"], ["RPN", "Used-For", "instance segmentation"]], "rel_plus": [["Selective Search:Method", "Used-For", "object detection:Task"], ["MCG:Method", "Used-For", "object detection:Task"], ["CPMC:Method", "Used-For", "object detection:Task"], ["RPN:Method", "Used-For", "object detection:Task"], ["Selective Search:Method", "Used-For", "instance segmentation:Task"], ["MCG:Method", "Used-For", "instance segmentation:Task"], ["CPMC:Method", "Used-For", "instance segmentation:Task"], ["RPN:Method", "Used-For", "instance segmentation:Task"]]}
{"doc_id": "210164920", "sentence": "The authors have used a cascaded network for incorporating features from different CNN layers for instance segmentation .", "ner": [["CNN", "Method"], ["instance segmentation", "Task"]], "rel": [["CNN", "Used-For", "instance segmentation"]], "rel_plus": [["CNN:Method", "Used-For", "instance segmentation:Task"]]}
{"doc_id": "210164920", "sentence": "Also , the sharing of convolution features leads to faster segmentation models .", "ner": [["convolution", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210164920", "sentence": "In papers [ 1 3 6 ] , [ 1 3 7 ] , [ 8 0 ] , [ 1 7 ] , [ 1 3 8 ] , [ 1 2 8 ] , [ 7 8 ] , [ 1 3 9 ] researchers used contextual information and low level features into CNN in various ways for better segmentation .", "ner": [["CNN", "Method"], ["segmentation", "Task"]], "rel": [["CNN", "Used-For", "segmentation"]], "rel_plus": [["CNN:Method", "Used-For", "segmentation:Task"]]}
{"doc_id": "210164920", "sentence": "Zagoruko et al. [ 1 4 0 ] has also used those ideas by integrating skip connection , foveal structure and integral loss in Fast R - CNN [ 1 8 ] for better segmentation .", "ner": [["Fast R - CNN", "Method"], ["segmentation", "Task"]], "rel": [["Fast R - CNN", "Used-For", "segmentation"]], "rel_plus": [["Fast R - CNN:Method", "Used-For", "segmentation:Task"]]}
{"doc_id": "210164920", "sentence": "SDS , DeepMask , Hyper - columns have used feature maps from top layers of the network for object instance detection which leads to coarse object mask generation .", "ner": [["SDS", "Method"], ["DeepMask", "Method"], ["Hyper - columns", "Method"], ["object instance detection", "Task"]], "rel": [["Hyper - columns", "Used-For", "object instance detection"], ["DeepMask", "Used-For", "object instance detection"], ["SDS", "Used-For", "object instance detection"]], "rel_plus": [["Hyper - columns:Method", "Used-For", "object instance detection:Task"], ["DeepMask:Method", "Used-For", "object instance detection:Task"], ["SDS:Method", "Used-For", "object instance detection:Task"]]}
{"doc_id": "210164920", "sentence": "Introduction of skip connection in [ 1 4 1 , 1 4 2 , 1 4 3 , 1 4 0 ] reduces the coarseness of masks which is more helpful for semantic segmentation rather instance segmentation .", "ner": [["semantic segmentation", "Task"], ["instance segmentation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "210164920", "sentence": "Pinheiro et . al. [ 1 4 4 ] have used their model to generate a coarse feature map using CNN and then refined those models to get pixel - accurate instance segmentation masks using a refinement model as described in section 4. 2 . 5 .", "ner": [["CNN", "Method"], ["instance segmentation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "210164920", "sentence": "Traditional CNNs are translation invariant i.e images with the same properties but with different contextual information will score the same classification score .", "ner": [["CNNs", "Method"], ["classification", "Task"]], "rel": [["CNNs", "Used-For", "classification"]], "rel_plus": [["CNNs:Method", "Used-For", "classification:Task"]]}
{"doc_id": "210164920", "sentence": "Previous models , specially FCN , used a single score map for semantic segmentation .", "ner": [["FCN", "Method"], ["semantic segmentation", "Task"]], "rel": [["FCN", "Used-For", "semantic segmentation"]], "rel_plus": [["FCN:Method", "Used-For", "semantic segmentation:Task"]]}
{"doc_id": "210164920", "sentence": "But for instance segmentation , a model must be a translation variant so that the same image pixel of different instances having different contextual information can be segmented separately .", "ner": [["instance segmentation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "210164920", "sentence": "SDS , Hypercolumn , CFM [ 1 4 6 ] , MNC [ 1 3 5 ] , MultiPathNet [ 1 4 0 ] used two different subnetworks for object detection and segmentation which prevent the models to become an end to end trainable .", "ner": [["SDS", "Method"], ["Hypercolumn", "Method"], ["CFM", "Method"], ["MNC", "Method"], ["MultiPathNet", "Method"], ["object detection", "Task"], ["segmentation", "Task"]], "rel": [["MultiPathNet", "Used-For", "object detection"], ["MNC", "Used-For", "object detection"], ["CFM", "Used-For", "object detection"], ["Hypercolumn", "Used-For", "object detection"], ["SDS", "Used-For", "object detection"], ["MultiPathNet", "Used-For", "segmentation"], ["MNC", "Used-For", "segmentation"], ["CFM", "Used-For", "segmentation"], ["Hypercolumn", "Used-For", "segmentation"], ["SDS", "Used-For", "segmentation"]], "rel_plus": [["MultiPathNet:Method", "Used-For", "object detection:Task"], ["MNC:Method", "Used-For", "object detection:Task"], ["CFM:Method", "Used-For", "object detection:Task"], ["Hypercolumn:Method", "Used-For", "object detection:Task"], ["SDS:Method", "Used-For", "object detection:Task"], ["MultiPathNet:Method", "Used-For", "segmentation:Task"], ["MNC:Method", "Used-For", "segmentation:Task"], ["CFM:Method", "Used-For", "segmentation:Task"], ["Hypercolumn:Method", "Used-For", "segmentation:Task"], ["SDS:Method", "Used-For", "segmentation:Task"]]}
{"doc_id": "210164920", "sentence": "On the other hand [ 1 4 7 ] , [ 1 4 8 ] extends instance segmentation by grouping or clustering FCNs score map which involves a large amount of post - processing . [ 1 4 5 ] introduced a joint formulation of classification and segmentation masking subnets in an efficient way .", "ner": [["instance segmentation", "Task"], ["FCNs", "Method"], ["classification", "Task"], ["segmentation", "Task"]], "rel": [["FCNs", "Used-For", "instance segmentation"]], "rel_plus": [["FCNs:Method", "Used-For", "instance segmentation:Task"]]}
{"doc_id": "210164920", "sentence": "While [ 1 4 9 , 1 5 0 , 1 5 1 , 1 5 2 ] have used semantic segmentation models , Mask R - CNN [ 2 0 ] extends the object detection model Faster R - CNN by adding a binary mask prediction branch for instance segmentation .", "ner": [["semantic segmentation", "Task"], ["Mask R - CNN", "Method"], ["object detection", "Task"], ["Faster R - CNN", "Method"], ["binary mask prediction", "Task"], ["instance segmentation", "Task"]], "rel": [["Mask R - CNN", "Used-For", "semantic segmentation"], ["Faster R - CNN", "Used-For", "object detection"], ["binary mask prediction", "Used-For", "instance segmentation"]], "rel_plus": [["Mask R - CNN:Method", "Used-For", "semantic segmentation:Task"], ["Faster R - CNN:Method", "Used-For", "object detection:Task"], ["binary mask prediction:Task", "Used-For", "instance segmentation:Task"]]}
{"doc_id": "210164920", "sentence": "Object detection using the sliding window approach gave us quite successful work such as Faster R - CNN , Mask R - CNN , etc . with refinement step and SSD [ 2 3 ] , RetinaNet [ 2 6 ] without using refinement stage .", "ner": [["Object detection", "Task"], ["Faster R - CNN", "Method"], ["Mask R - CNN", "Method"], ["SSD", "Method"], ["RetinaNet", "Method"]], "rel": [["Faster R - CNN", "Used-For", "Object detection"], ["Mask R - CNN", "Used-For", "Object detection"], ["SSD", "Used-For", "Object detection"], ["RetinaNet", "Used-For", "Object detection"]], "rel_plus": [["Faster R - CNN:Method", "Used-For", "Object detection:Task"], ["Mask R - CNN:Method", "Used-For", "Object detection:Task"], ["SSD:Method", "Used-For", "Object detection:Task"], ["RetinaNet:Method", "Used-For", "Object detection:Task"]]}
{"doc_id": "210164920", "sentence": "Though sliding window approach is popular in object detection but it was missing in case of instance segmentation task .", "ner": [["object detection", "Task"], ["instance segmentation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "210164920", "sentence": "Chen et al. [ 1 5 8 ] have introduced dense instance segmentation to fill this gap and introduced TensorMask .", "ner": [["dense instance segmentation", "Task"], ["TensorMask", "Method"]], "rel": [["TensorMask", "Used-For", "dense instance segmentation"]], "rel_plus": [["TensorMask:Method", "Used-For", "dense instance segmentation:Task"]]}
{"doc_id": "210164920", "sentence": "Recently , Kirillov et al. [ 1 0 1 ] used point - based rendering in Mask R - CNN and produce state - of - the - art instance segmentation model .", "ner": [["Mask R - CNN", "Method"], ["instance segmentation", "Task"]], "rel": [["Mask R - CNN", "Used-For", "instance segmentation"]], "rel_plus": [["Mask R - CNN:Method", "Used-For", "instance segmentation:Task"]]}
{"doc_id": "210164920", "sentence": "In this section , we are going to elaborately discuss some state - of - the - art CNN based instance segmentation models .", "ner": [["CNN", "Method"], ["instance segmentation", "Task"]], "rel": [["CNN", "Used-For", "instance segmentation"]], "rel_plus": [["CNN:Method", "Used-For", "instance segmentation:Task"]]}
{"doc_id": "210164920", "sentence": "Simultaneous Detection and Segmentation ( SDS ) [ 1 2 7 ] model consists of 4 steps for instance segmentation .", "ner": [["Simultaneous Detection and Segmentation", "Method"], ["SDS", "Method"], ["instance segmentation", "Task"]], "rel": [["SDS", "Synonym-Of", "Simultaneous Detection and Segmentation"], ["Simultaneous Detection and Segmentation", "Used-For", "instance segmentation"]], "rel_plus": [["SDS:Method", "Synonym-Of", "Simultaneous Detection and Segmentation:Method"], ["Simultaneous Detection and Segmentation:Method", "Used-For", "instance segmentation:Task"]]}
{"doc_id": "210164920", "sentence": "The steps are proposal generation , feature extraction , region classification , and region refinement respectively .", "ner": [["proposal generation", "Task"], ["feature extraction", "Task"], ["region classification", "Task"], ["region refinement", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "210164920", "sentence": "On input image , the authors have used Multi - scale Combinatorial Grouping(MCG ) [ 1 3 3 ] algorithm for generating region proposals .", "ner": [["Multi - scale Combinatorial Grouping(MCG )", "Method"], ["generating region proposals", "Task"]], "rel": [["Multi - scale Combinatorial Grouping(MCG )", "Used-For", "generating region proposals"]], "rel_plus": [["Multi - scale Combinatorial Grouping(MCG ):Method", "Used-For", "generating region proposals:Task"]]}
{"doc_id": "210164920", "sentence": "As shown in figure 1 6 , the upper CNN generates a feature vector for bounding box of region proposals and the bottom CNN generates a feature vector for segmentation mask .", "ner": [["CNN", "Method"], ["CNN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210164920", "sentence": "Figure 1 6 : Architecture of SDS Network [ 1 2 7 ] Finally , to refine surviving candidates CNN feature maps are used for mask prediction .", "ner": [["SDS Network", "Method"], ["CNN", "Method"], ["mask prediction", "Task"]], "rel": [["CNN", "Used-For", "mask prediction"]], "rel_plus": [["CNN:Method", "Used-For", "mask prediction:Task"]]}
{"doc_id": "210164920", "sentence": "DeepMask [ 1 3 4 ] used CNN to generate segmentation proposals rather than less informative bounding box proposal algorithms such as Selective Search , MCG , etc .", "ner": [["DeepMask", "Method"], ["CNN", "Method"], ["generate segmentation proposals", "Task"], ["Selective Search", "Method"], ["MCG", "Method"]], "rel": [["CNN", "Part-Of", "DeepMask"], ["DeepMask", "Used-For", "generate segmentation proposals"]], "rel_plus": [["CNN:Method", "Part-Of", "DeepMask:Method"], ["DeepMask:Method", "Used-For", "generate segmentation proposals:Task"]]}
{"doc_id": "210164920", "sentence": "DeepMask used VGG - A[ 8 ] model ( discarding last max - pooling layer and all fully connected layers ) for feature extraction .", "ner": [["DeepMask", "Method"], ["VGG - A[", "Method"], ["max - pooling", "Method"], ["fully connected layers", "Method"], ["feature extraction", "Task"]], "rel": [["VGG - A[", "Part-Of", "DeepMask"], ["DeepMask", "Used-For", "feature extraction"]], "rel_plus": [["VGG - A[:Method", "Part-Of", "DeepMask:Method"], ["DeepMask:Method", "Used-For", "feature extraction:Task"]]}
{"doc_id": "210164920", "sentence": "The top branch which is the CNN based object proposal method of DeepMask predicts a class - agnostic segmentation mask and bottom branch assigns a score for estimating the likelihood of patch being centered on the full object .", "ner": [["CNN based object proposal method", "Method"], ["DeepMask", "Method"]], "rel": [["CNN based object proposal method", "Part-Of", "DeepMask"]], "rel_plus": [["CNN based object proposal method:Method", "Part-Of", "DeepMask:Method"]]}
{"doc_id": "210164920", "sentence": "Dai et al. [ 1 3 5 ] used a network with the cascaded structure to share convolutional features and also used region proposal network ( RPN ) for better instance segmentation .", "ner": [["region proposal network", "Method"], ["RPN", "Method"], ["instance segmentation", "Task"]], "rel": [["RPN", "Synonym-Of", "region proposal network"], ["region proposal network", "Used-For", "instance segmentation"]], "rel_plus": [["RPN:Method", "Synonym-Of", "region proposal network:Method"], ["region proposal network:Method", "Used-For", "instance segmentation:Task"]]}
{"doc_id": "210164920", "sentence": "The authors have decomposed the instance seg - mentation task into three sub tasks : instance differentiation ( class agnostic bounding box generation for each instance ) , mask estimation(estimated a pixel - level mask/instance ) and object categorization ( instances are labeled categorically ) .", "ner": [["instance seg - mentation", "Task"], ["instance differentiation", "Task"], ["mask estimation(estimated", "Task"], ["object categorization", "Task"]], "rel": [["mask estimation(estimated", "SubTask-Of", "instance seg - mentation"], ["instance differentiation", "SubTask-Of", "instance seg - mentation"], ["object categorization", "SubTask-Of", "instance seg - mentation"]], "rel_plus": [["mask estimation(estimated:Task", "SubTask-Of", "instance seg - mentation:Task"], ["instance differentiation:Task", "SubTask-Of", "instance seg - mentation:Task"], ["object categorization:Task", "SubTask-Of", "instance seg - mentation:Task"]]}
{"doc_id": "210164920", "sentence": "They proposed Multi - task Network Cascades ( MNC ) to address these sub - tasks in three different cascaded stages to share convolutional features .", "ner": [["Multi - task Network Cascades", "Method"], ["MNC", "Method"]], "rel": [["MNC", "Synonym-Of", "Multi - task Network Cascades"]], "rel_plus": [["MNC:Method", "Synonym-Of", "Multi - task Network Cascades:Method"]]}
{"doc_id": "210164920", "sentence": "As shown in figure 1 8 , MNC takes an arbitrary sized input which is a feature map extracted using VGG 1 6 network .", "ner": [["MNC", "Method"], ["VGG 1 6", "Method"]], "rel": [["VGG 1 6", "Part-Of", "MNC"]], "rel_plus": [["VGG 1 6:Method", "Part-Of", "MNC:Method"]]}
{"doc_id": "210164920", "sentence": "Again , shared convolutional features and output of the previous two stages are fed into the third stage for generating category score for each instance .   Zagoruko et al. integrate three modifications in the Fast R - CNN object detector and proposed Multipath Network [ 1 4 0 ] for both object detection and segmentation tasks .", "ner": [["Fast R - CNN", "Method"], ["Multipath Network", "Method"], ["object detection", "Task"], ["segmentation", "Task"]], "rel": [["Multipath Network", "Used-For", "object detection"], ["Fast R - CNN", "Used-For", "object detection"], ["Fast R - CNN", "Used-For", "segmentation"], ["Multipath Network", "Used-For", "segmentation"]], "rel_plus": [["Multipath Network:Method", "Used-For", "object detection:Task"], ["Fast R - CNN:Method", "Used-For", "object detection:Task"], ["Fast R - CNN:Method", "Used-For", "segmentation:Task"], ["Multipath Network:Method", "Used-For", "segmentation:Task"]]}
{"doc_id": "210164920", "sentence": "That is why , in [ 1 3 6 ] , [ 1 3 7 ] , [ 8 0 ] , [ 1 7 ] , [ 1 5 9 ] , the researcher used contextual information in various ways in CNN based model for better classification of objects .", "ner": [["CNN", "Method"], ["classification of objects", "Task"]], "rel": [["CNN", "Used-For", "classification of objects"]], "rel_plus": [["CNN:Method", "Used-For", "classification of objects:Task"]]}
{"doc_id": "210164920", "sentence": "In Multipath Network , the authors have connected third , fourth and fifth convolutional layers of VGG 1 6 to the four foveal regions to use multi - scale features for better object localization .", "ner": [["Multipath Network", "Method"], ["convolutional layers", "Method"], ["VGG 1 6", "Method"], ["object localization", "Task"]], "rel": [["VGG 1 6", "Part-Of", "Multipath Network"], ["convolutional layers", "Part-Of", "VGG 1 6"], ["Multipath Network", "Used-For", "object localization"]], "rel_plus": [["VGG 1 6:Method", "Part-Of", "Multipath Network:Method"], ["convolutional layers:Method", "Part-Of", "VGG 1 6:Method"], ["Multipath Network:Method", "Used-For", "object localization:Task"]]}
{"doc_id": "210164920", "sentence": "The use of the DeepMask segmentation proposal helped their model to be the 1st runner - up in MS COCO 2 0 1 5 [ 1 6 0 ] detection and segmentation challenges .   DeepMask generates accurate masks for object - level but the degree of alignment of the mask with the actual object boundary was not good .", "ner": [["DeepMask segmentation proposal", "Method"], ["MS COCO 2 0 1 5", "Dataset"], ["detection", "Task"], ["segmentation", "Task"], ["DeepMask", "Method"]], "rel": [["DeepMask segmentation proposal", "Evaluated-With", "MS COCO 2 0 1 5"], ["DeepMask segmentation proposal", "Used-For", "detection"], ["MS COCO 2 0 1 5", "Benchmark-For", "detection"], ["DeepMask segmentation proposal", "Used-For", "segmentation"], ["MS COCO 2 0 1 5", "Benchmark-For", "segmentation"]], "rel_plus": [["DeepMask segmentation proposal:Method", "Evaluated-With", "MS COCO 2 0 1 5:Dataset"], ["DeepMask segmentation proposal:Method", "Used-For", "detection:Task"], ["MS COCO 2 0 1 5:Dataset", "Benchmark-For", "detection:Task"], ["DeepMask segmentation proposal:Method", "Used-For", "segmentation:Task"], ["MS COCO 2 0 1 5:Dataset", "Benchmark-For", "segmentation:Task"]]}
{"doc_id": "210164920", "sentence": "Sharp - Mask [ 1 4 4 ] contains a bottom - up feed - forward network for producing coarse semantic segmentation mask and a top - down network to refine those masks using a refinement module .", "ner": [["Sharp - Mask", "Method"], ["bottom - up feed - forward network", "Method"], ["coarse semantic segmentation mask", "Task"], ["top - down network", "Method"]], "rel": [["bottom - up feed - forward network", "Part-Of", "Sharp - Mask"], ["top - down network", "Part-Of", "Sharp - Mask"], ["Sharp - Mask", "Used-For", "coarse semantic segmentation mask"]], "rel_plus": [["bottom - up feed - forward network:Method", "Part-Of", "Sharp - Mask:Method"], ["top - down network:Method", "Part-Of", "Sharp - Mask:Method"], ["Sharp - Mask:Method", "Used-For", "coarse semantic segmentation mask:Task"]]}
{"doc_id": "210164920", "sentence": "The authors have used feed - forward DeepMask segmentation proposal network with their refinement module and named it as SharpMask .", "ner": [["feed - forward DeepMask segmentation proposal network", "Method"], ["SharpMask", "Method"]], "rel": [["SharpMask", "Synonym-Of", "feed - forward DeepMask segmentation proposal network"]], "rel_plus": [["SharpMask:Method", "Synonym-Of", "feed - forward DeepMask segmentation proposal network:Method"]]}
{"doc_id": "210164920", "sentence": "This process continues until the reconstruction of the full resolution image and the final object mask .   The fully convolutional network is good for single instance segmentation of an object category .", "ner": [["fully convolutional network", "Method"], ["instance segmentation", "Task"]], "rel": [["fully convolutional network", "Used-For", "instance segmentation"]], "rel_plus": [["fully convolutional network:Method", "Used-For", "instance segmentation:Task"]]}
{"doc_id": "210164920", "sentence": "Dai et al have used the concept of relative position in FCN and proposed instance sensitive fully convolutional network ( InstanceFCN ) [ 2 1 ] for instance segmentation .", "ner": [["FCN", "Method"], ["instance sensitive fully convolutional network", "Method"], ["InstanceFCN", "Method"], ["instance segmentation", "Task"]], "rel": [["InstanceFCN", "Synonym-Of", "instance sensitive fully convolutional network"], ["instance sensitive fully convolutional network", "Used-For", "instance segmentation"], ["FCN", "Used-For", "instance segmentation"]], "rel_plus": [["InstanceFCN:Method", "Synonym-Of", "instance sensitive fully convolutional network:Method"], ["instance sensitive fully convolutional network:Method", "Used-For", "instance segmentation:Task"], ["FCN:Method", "Used-For", "instance segmentation:Task"]]}
{"doc_id": "210164920", "sentence": "InstanceFCN introduced position - sensitive score mapping to signify the relative position of an object instance but the authors have used two dif - Figure 2 1 : Architecture of Instance - sensitive fully convolutional network [ 2 1 ] . ferent sub networks for object segmentation and detection .", "ner": [["InstanceFCN", "Method"], ["Instance - sensitive fully convolutional network", "Method"], ["ferent sub networks", "Method"], ["object segmentation", "Task"], ["detection", "Task"]], "rel": [["ferent sub networks", "Used-For", "object segmentation"], ["ferent sub networks", "Used-For", "detection"]], "rel_plus": [["ferent sub networks:Method", "Used-For", "object segmentation:Task"], ["ferent sub networks:Method", "Used-For", "detection:Task"]]}
{"doc_id": "210164920", "sentence": "Li et al. [ 1 4 5 ] proposed the first end to end trainable fully convolutional network based model in which segmentation and detection are done jointly and concurrently in a single network by score map sharing as shown in figure 2 2 .", "ner": [["fully convolutional network", "Method"], ["segmentation", "Task"], ["detection", "Task"]], "rel": [["fully convolutional network", "Used-For", "segmentation"], ["fully convolutional network", "Used-For", "detection"]], "rel_plus": [["fully convolutional network:Method", "Used-For", "segmentation:Task"], ["fully convolutional network:Method", "Used-For", "detection:Task"]]}
{"doc_id": "210164920", "sentence": "These two score maps depend on detection score and segmentation score of a pixel in a given region of interests ( RoIs ) with respect to different relative position .", "ner": [["detection", "Task"], ["segmentation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "210164920", "sentence": "Mask R - CNN [ 2 0 ] contains three branches for predicting class , boundingbox and segmentation mask for instances within a region of interest ( RoI ) .", "ner": [["Mask R - CNN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210164920", "sentence": "As Faster R - CNN , Mask R - CNN contains two stages .", "ner": [["Faster R - CNN", "Method"], ["Mask R - CNN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210164920", "sentence": "Then to preserve the spatial location , the authors have used RoIAlign instead of RoIPool as in Faster R - CNN .", "ner": [["RoIAlign", "Method"], ["RoIPool", "Method"], ["Faster R - CNN", "Method"]], "rel": [["RoIAlign", "Part-Of", "Faster R - CNN"]], "rel_plus": [["RoIAlign:Method", "Part-Of", "Faster R - CNN:Method"]]}
{"doc_id": "210164920", "sentence": "In the second stage , it simultaneously predicts a class label , a bounding box offset and a binary mask for each individual RoI. In Mask R - CNN , the prediction of binary mask for each class was independent and it was not a multi - class prediction .   MaskLab [ 1 5 4 ] has utilized the merits of both semantic segmentation and object detection to handle instance segmentation .", "ner": [["Mask R - CNN", "Method"], ["MaskLab", "Method"], ["semantic segmentation", "Task"], ["object detection", "Task"], ["instance segmentation", "Task"]], "rel": [["MaskLab", "Used-For", "semantic segmentation"], ["MaskLab", "Used-For", "object detection"], ["MaskLab", "Used-For", "instance segmentation"]], "rel_plus": [["MaskLab:Method", "Used-For", "semantic segmentation:Task"], ["MaskLab:Method", "Used-For", "object detection:Task"], ["MaskLab:Method", "Used-For", "instance segmentation:Task"]]}
{"doc_id": "210164920", "sentence": "The authors have used Faster R - CNN [ 1 9 ] ( ResNet - 1 0 1 [ 1 0 ] based ) for predicting bounding boxes for object instances .", "ner": [["Faster R - CNN", "Method"], ["ResNet - 1 0 1", "Method"]], "rel": [["ResNet - 1 0 1", "Part-Of", "Faster R - CNN"]], "rel_plus": [["ResNet - 1 0 1:Method", "Part-Of", "Faster R - CNN:Method"]]}
{"doc_id": "210164920", "sentence": "The mask is then again concatenated with hyper - column features [ 1 2 8 ] extracted from low layers of ResNet - 1 0 1 and processed using a small CNN of three layers for further refinement .   The flow of information in the convolutional neural network is very important as the low - level feature maps are information - rich in terms of localization and the high - level feature maps are rich in semantic information .", "ner": [["ResNet - 1 0 1", "Method"], ["CNN", "Method"], ["convolutional neural network", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210164920", "sentence": "Based on Mask R - CNN and Feature Pyramid Network(FPN ) [ 1 6 2 ] , they have proposed a Path Aggregation Network ( PANet ) [ 1 5 7 ] for instance segmentation .", "ner": [["Mask R - CNN", "Method"], ["Feature Pyramid Network(FPN )", "Method"], ["Path Aggregation Network", "Method"], ["PANet", "Method"], ["instance segmentation", "Task"]], "rel": [["PANet", "Synonym-Of", "Path Aggregation Network"], ["Path Aggregation Network", "Used-For", "instance segmentation"]], "rel_plus": [["PANet:Method", "Synonym-Of", "Path Aggregation Network:Method"], ["Path Aggregation Network:Method", "Used-For", "instance segmentation:Task"]]}
{"doc_id": "210164920", "sentence": "PANet used FPN as its base network to extract features from different layers .", "ner": [["PANet", "Method"], ["FPN", "Method"]], "rel": [["FPN", "Part-Of", "PANet"]], "rel_plus": [["FPN:Method", "Part-Of", "PANet:Method"]]}
{"doc_id": "210164920", "sentence": "Previous instance segmentation models used methods in which the objects are detected using bounding box then segmentation is done .", "ner": [["instance segmentation", "Task"], ["segmentation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "210164920", "sentence": "Like feature pyramid network , TensorMask has also developed a pyramid structure , called tensorbipyramid over a scale - indexed list of 4D tensors to acquire the benefits of multi - scale .", "ner": [["feature pyramid network", "Method"], ["TensorMask", "Method"]], "rel": [["feature pyramid network", "Compare-With", "TensorMask"]], "rel_plus": [["feature pyramid network:Method", "Compare-With", "TensorMask:Method"]]}
{"doc_id": "210164920", "sentence": "Among different models , some of them are based on object detection models such as R - CNN , Fast R - CNN , Faster R - CNN , etc .", "ner": [["object detection models", "Method"], ["R - CNN", "Method"], ["Fast R - CNN", "Method"], ["Faster R - CNN", "Method"]], "rel": [["R - CNN", "SubClass-Of", "object detection models"], ["Fast R - CNN", "SubClass-Of", "object detection models"], ["Faster R - CNN", "SubClass-Of", "object detection models"]], "rel_plus": [["R - CNN:Method", "SubClass-Of", "object detection models:Method"], ["Fast R - CNN:Method", "SubClass-Of", "object detection models:Method"], ["Faster R - CNN:Method", "SubClass-Of", "object detection models:Method"]]}
{"doc_id": "210164920", "sentence": "Some models are based on semantic segmentation models such as FCN , U - Net , etc .", "ner": [["semantic segmentation models", "Method"], ["FCN", "Method"], ["U - Net", "Method"]], "rel": [["FCN", "SubClass-Of", "semantic segmentation models"], ["U - Net", "SubClass-Of", "semantic segmentation models"]], "rel_plus": [["FCN:Method", "SubClass-Of", "semantic segmentation models:Method"], ["U - Net:Method", "SubClass-Of", "semantic segmentation models:Method"]]}
{"doc_id": "210164920", "sentence": "SDS , DeepMask , SharpMask , InstanceFCN are based on proposal generation .", "ner": [["SDS", "Method"], ["DeepMask", "Method"], ["SharpMask", "Method"], ["InstanceFCN", "Method"], ["proposal generation", "Method"]], "rel": [["proposal generation", "Part-Of", "SDS"], ["proposal generation", "Part-Of", "DeepMask"], ["proposal generation", "Part-Of", "SharpMask"], ["proposal generation", "Part-Of", "InstanceFCN"]], "rel_plus": [["proposal generation:Method", "Part-Of", "SDS:Method"], ["proposal generation:Method", "Part-Of", "DeepMask:Method"], ["proposal generation:Method", "Part-Of", "SharpMask:Method"], ["proposal generation:Method", "Part-Of", "InstanceFCN:Method"]]}
{"doc_id": "210164920", "sentence": "InstanceFCN , FCIs , MaskLab calculate position - sensitive score maps for instance segmentation .", "ner": [["InstanceFCN", "Method"], ["FCIs", "Method"], ["MaskLab", "Method"], ["instance segmentation", "Task"]], "rel": [["MaskLab", "Used-For", "instance segmentation"], ["FCIs", "Used-For", "instance segmentation"], ["InstanceFCN", "Used-For", "instance segmentation"]], "rel_plus": [["MaskLab:Method", "Used-For", "instance segmentation:Task"], ["FCIs:Method", "Used-For", "instance segmentation:Task"], ["InstanceFCN:Method", "Used-For", "instance segmentation:Task"]]}
{"doc_id": "210164920", "sentence": "TensorMask used the sliding window approach for dense instance segmentation .", "ner": [["TensorMask", "Method"], ["instance segmentation", "Task"]], "rel": [["TensorMask", "Used-For", "instance segmentation"]], "rel_plus": [["TensorMask:Method", "Used-For", "instance segmentation:Task"]]}
{"doc_id": "210164920", "sentence": "Most of the state - of - the - art models used stochastic gradient descent(SGD ) [ 1 6 3 ] as an optimization algorithm with different initialization segmentation models on those datasets .", "ner": [["stochastic gradient descent(SGD )", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210164920", "sentence": "Panoptic segmentation ( PS ) [ 4 0 , 1 6 5 , 9 9 , 1 6 6 , 1 6 7 , 1 6 8 ] is the combination of semantic segmentation and instance segmentation .", "ner": [["Panoptic segmentation", "Task"], ["PS", "Task"], ["semantic segmentation", "Task"], ["instance segmentation", "Task"]], "rel": [["PS", "Synonym-Of", "Panoptic segmentation"], ["semantic segmentation", "SubTask-Of", "Panoptic segmentation"], ["instance segmentation", "SubTask-Of", "Panoptic segmentation"]], "rel_plus": [["PS:Task", "Synonym-Of", "Panoptic segmentation:Task"], ["semantic segmentation:Task", "SubTask-Of", "Panoptic segmentation:Task"], ["instance segmentation:Task", "SubTask-Of", "Panoptic segmentation:Task"]]}
{"doc_id": "210164920", "sentence": "The output of a panoptic segmentation model will contain two channels : one for pixel 's label ( semantic segmentation ) and another for predicting each pixel instance ( instance segmentation ) .   Convolutional neural network based image segmentation is a challenging work as it needs spatially variant features to preserve the context of a pixel for semantic labeling .", "ner": [["panoptic segmentation", "Task"], ["semantic segmentation", "Task"], ["instance segmentation", "Task"], ["Convolutional neural network", "Method"], ["image segmentation", "Task"], ["semantic labeling", "Task"]], "rel": [["Convolutional neural network", "Used-For", "image segmentation"]], "rel_plus": [["Convolutional neural network:Method", "Used-For", "image segmentation:Task"]]}
{"doc_id": "210164920", "sentence": "Semantic segmentation categorizes each pixel with a semantic label whereas instance segmentation segments individual instances of objects contained in an image .", "ner": [["Semantic segmentation", "Task"], ["instance segmentation", "Task"]], "rel": [["Semantic segmentation", "Compare-With", "instance segmentation"]], "rel_plus": [["Semantic segmentation:Task", "Compare-With", "instance segmentation:Task"]]}
{"doc_id": "210164920", "sentence": "In our article , we have presented the evolution of image segmentation models based on CNN .", "ner": [["image segmentation", "Task"], ["CNN", "Method"]], "rel": [["CNN", "Used-For", "image segmentation"]], "rel_plus": [["CNN:Method", "Used-For", "image segmentation:Task"]]}
{"doc_id": "51876625", "sentence": "Current state - of - the - art approaches for spatio - temporal action localization rely on detections at the frame level and model temporal context with 3D ConvNets .", "ner": [["spatio - temporal action localization", "Task"], ["3D ConvNets", "Method"]], "rel": [["3D ConvNets", "Used-For", "spatio - temporal action localization"]], "rel_plus": [["3D ConvNets:Method", "Used-For", "spatio - temporal action localization:Task"]]}
{"doc_id": "51876625", "sentence": "Our approach is weakly supervised and mines the relevant elements automatically with an actor - centric relational network ( ACRN ) .", "ner": [["actor - centric relational network", "Method"], ["ACRN", "Method"]], "rel": [["ACRN", "Synonym-Of", "actor - centric relational network"]], "rel_plus": [["ACRN:Method", "Synonym-Of", "actor - centric relational network:Method"]]}
{"doc_id": "51876625", "sentence": "ACRN computes and accumulates pair - wise relation information from actor and global scene features , and generates relation features for action classification .", "ner": [["ACRN", "Method"], ["action classification", "Task"]], "rel": [["ACRN", "Used-For", "action classification"]], "rel_plus": [["ACRN:Method", "Used-For", "action classification:Task"]]}
{"doc_id": "51876625", "sentence": "We show that ACRN outperforms alternative approaches which capture relation information , and that the proposed framework improves upon the state - of - the - art performance on JHMDB and AVA .", "ner": [["ACRN", "Method"], ["JHMDB", "Dataset"], ["AVA", "Dataset"]], "rel": [["ACRN", "Evaluated-With", "JHMDB"], ["ACRN", "Evaluated-With", "AVA"]], "rel_plus": [["ACRN:Method", "Evaluated-With", "JHMDB:Dataset"], ["ACRN:Method", "Evaluated-With", "AVA:Dataset"]]}
{"doc_id": "51876625", "sentence": "Now that we have large , diverse , and realistic datasets such as AVA [ 1 3 ] , SLAC [ 6 2 ] , and Charades [ 4 9 ] , why has action recognition performance not caught up ?", "ner": [["AVA", "Dataset"], ["SLAC", "Dataset"], ["Charades", "Dataset"], ["action recognition", "Task"]], "rel": [["AVA", "Benchmark-For", "action recognition"], ["SLAC", "Benchmark-For", "action recognition"], ["Charades", "Benchmark-For", "action recognition"]], "rel_plus": [["AVA:Dataset", "Benchmark-For", "action recognition:Task"], ["SLAC:Dataset", "Benchmark-For", "action recognition:Task"], ["Charades:Dataset", "Benchmark-For", "action recognition:Task"]]}
{"doc_id": "51876625", "sentence": "Models for spatio - temporal action localization from the last few years have been mainly based on architectures for recognizing objects [ 1 2 , 3 7 , 5 8 ] , building on the success of R - CNN style architectures [ 9 , 1 0 , 4 0 ] .", "ner": [["spatio - temporal action localization", "Task"], ["R - CNN", "Method"]], "rel": [["R - CNN", "Used-For", "spatio - temporal action localization"]], "rel_plus": [["R - CNN:Method", "Used-For", "spatio - temporal action localization:Task"]]}
{"doc_id": "51876625", "sentence": "Although the basic idea of exploiting context for action recognition is not new , earlier works [ 5 , 3 2 , 5 6 ] largely focused on the classification task ( label each trimmed clip with an action label ) .", "ner": [["action recognition", "Task"], ["classification", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "51876625", "sentence": "We refer to this approach as actor - centric relation network ( ACRN ) .", "ner": [["actor - centric relation network", "Method"], ["ACRN", "Method"]], "rel": [["ACRN", "Synonym-Of", "actor - centric relation network"]], "rel_plus": [["ACRN:Method", "Synonym-Of", "actor - centric relation network:Method"]]}
{"doc_id": "51876625", "sentence": "We evaluate our approach on JHMDB [ 2 2 ] and the recently released AVA dataset [ 1 3 ] .", "ner": [["JHMDB", "Dataset"], ["AVA", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "51876625", "sentence": "State - of - the - art methods rely either on two - stream 2D ConvNets [ 2 5 , 5 0 ] , 2D ConvNets with LSTMs [ 7 , 3 4 ] or 3D ConvNets [ 3 , 5 4 ] .", "ner": [["2D ConvNets", "Method"], ["2D ConvNets", "Method"], ["LSTMs", "Method"], ["3D ConvNets", "Method"]], "rel": [["LSTMs", "Part-Of", "2D ConvNets"]], "rel_plus": [["LSTMs:Method", "Part-Of", "2D ConvNets:Method"]]}
{"doc_id": "51876625", "sentence": "If we want to address long untrimmed videos , temporal localization is necessary in addition to action classification .", "ner": [["temporal localization", "Task"], ["action classification", "Task"]], "rel": [["temporal localization", "Used-For", "action classification"]], "rel_plus": [["temporal localization:Task", "Used-For", "action classification:Task"]]}
{"doc_id": "51876625", "sentence": "Many recent state - of - the - art methods [ 2 , 5 , 6 0 ] rely on temporal proposals and classification approaches similar in spirit to recent methods for object detection [ 4 0 ] .", "ner": [["object detection", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "51876625", "sentence": "The tubelet approach [ 2 4 ] concatenates SSD features [ 2 8 ] over spatio - temporal volumes and jointly estimates classification and regression over several frames .", "ner": [["SSD", "Method"], ["jointly estimates classification", "Task"]], "rel": [["SSD", "Used-For", "jointly estimates classification"]], "rel_plus": [["SSD:Method", "Used-For", "jointly estimates classification:Task"]]}
{"doc_id": "51876625", "sentence": "T - CNN [ 1 8 ] uses 3D convolutions to estimate short tubes , micro - tubes rely on two successive frames [ 4 2 ] and pose - guided 3D convolutions add pose to a two - stream approach [ 6 5 ] .", "ner": [["T - CNN", "Method"], ["3D convolutions", "Method"], ["3D convolutions", "Method"]], "rel": [["3D convolutions", "Part-Of", "T - CNN"]], "rel_plus": [["3D convolutions:Method", "Part-Of", "T - CNN:Method"]]}
{"doc_id": "51876625", "sentence": "Gu et al. [ 1 3 ] rely on inflated 3D ConvNet ( I 3 D ) convolutions [ 3 ] for Faster R - CNN [ 4 0 ] region proposals and show that the use of I 3 D over relatively long temporal windows [ 5 5 ] improves the performance .", "ner": [["inflated 3D ConvNet", "Method"], ["I 3 D", "Method"], ["convolutions", "Method"], ["Faster R - CNN", "Method"], ["region proposals", "Task"], ["I 3 D", "Method"]], "rel": [["I 3 D", "Synonym-Of", "inflated 3D ConvNet"], ["convolutions", "Part-Of", "inflated 3D ConvNet"], ["inflated 3D ConvNet", "Part-Of", "Faster R - CNN"], ["Faster R - CNN", "Used-For", "region proposals"]], "rel_plus": [["I 3 D:Method", "Synonym-Of", "inflated 3D ConvNet:Method"], ["convolutions:Method", "Part-Of", "inflated 3D ConvNet:Method"], ["inflated 3D ConvNet:Method", "Part-Of", "Faster R - CNN:Method"], ["Faster R - CNN:Method", "Used-For", "region proposals:Task"]]}
{"doc_id": "51876625", "sentence": "The spatio - temporal separable 3D ConvNet ( S 3 D ) [ 5 9 ] improves the I 3 D architecture by observing that the 3D convolutions can be replaced by separable spatial and temporal convolutions without loss in accuracy , and that using such convolutions in higher layers of the network results in faster and more accurate models .", "ner": [["spatio - temporal separable 3D ConvNet", "Method"], ["S 3 D", "Method"], ["I 3 D", "Method"], ["3D convolutions", "Method"], ["separable spatial and temporal convolutions", "Method"], ["convolutions", "Method"]], "rel": [["S 3 D", "Synonym-Of", "spatio - temporal separable 3D ConvNet"], ["separable spatial and temporal convolutions", "Part-Of", "spatio - temporal separable 3D ConvNet"], ["3D convolutions", "Part-Of", "spatio - temporal separable 3D ConvNet"], ["spatio - temporal separable 3D ConvNet", "Compare-With", "I 3 D"], ["separable spatial and temporal convolutions", "SubClass-Of", "convolutions"], ["3D convolutions", "SubClass-Of", "convolutions"]], "rel_plus": [["S 3 D:Method", "Synonym-Of", "spatio - temporal separable 3D ConvNet:Method"], ["separable spatial and temporal convolutions:Method", "Part-Of", "spatio - temporal separable 3D ConvNet:Method"], ["3D convolutions:Method", "Part-Of", "spatio - temporal separable 3D ConvNet:Method"], ["spatio - temporal separable 3D ConvNet:Method", "Compare-With", "I 3 D:Method"], ["separable spatial and temporal convolutions:Method", "SubClass-Of", "convolutions:Method"], ["3D convolutions:Method", "SubClass-Of", "convolutions:Method"]]}
{"doc_id": "51876625", "sentence": "The use of context information to improve visual recognition has been extensively studied in computer vision .", "ner": [["visual recognition", "Task"], ["computer vision", "Task"]], "rel": [["visual recognition", "SubTask-Of", "computer vision"]], "rel_plus": [["visual recognition:Task", "SubTask-Of", "computer vision:Task"]]}
{"doc_id": "51876625", "sentence": "Early work showed that context can help scene classification [ 3 5 ] , object detection [ 1 5 , 1 7 , 3 3 , 3 9 , 5 3 ] , and action recognition in images [ 6 1 ] .", "ner": [["scene classification", "Task"], ["object detection", "Task"], ["action recognition", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "51876625", "sentence": "Recent work [ 1 2 ] obtains state - of - the - art performance for human - action - object recognition on V - COCO [ 1 4 ] and HICO - DET [ 4 ] .", "ner": [["human - action - object recognition", "Task"], ["V - COCO", "Dataset"], ["HICO - DET", "Dataset"]], "rel": [["V - COCO", "Benchmark-For", "human - action - object recognition"], ["HICO - DET", "Benchmark-For", "human - action - object recognition"]], "rel_plus": [["V - COCO:Dataset", "Benchmark-For", "human - action - object recognition:Task"], ["HICO - DET:Dataset", "Benchmark-For", "human - action - object recognition:Task"]]}
{"doc_id": "51876625", "sentence": "In contrast to our approach , their model is only applied to static images and relies on full supervision of actor , action and objects as annotated in V - COCO [ 1 4 ] and HICO - DET [ 4 ] .", "ner": [["V - COCO", "Dataset"], ["HICO - DET", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "51876625", "sentence": "We note that the relational information between the actor of interest and other actors or objects are important to identify actions , but are typically ignored by recent action detection methods [ 3 7 , 2 4 ] ; such annotations could be time consuming to collect , and are not provided by many of the recent action recognition datasets [ 2 6 , 1 3 , 4 8 ] .", "ner": [["action detection", "Task"], ["action recognition", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "51876625", "sentence": "This action detection model was proposed in [ 1 3 ] , motivated by the success of applying end - to - end object detection algorithms to action detection [ 3 7 , 2 4 ] .", "ner": [["action detection", "Task"], ["object detection", "Task"], ["action detection", "Task"]], "rel": [["object detection", "Used-For", "action detection"]], "rel_plus": [["object detection:Task", "Used-For", "action detection:Task"]]}
{"doc_id": "51876625", "sentence": "The overall architecture of the base model largely resembles the Faster R - CNN detection algorithm .", "ner": [["Faster R - CNN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "51876625", "sentence": "For actor localization , our method uses the region proposal network ( RPN ) from Faster R - CNN to generate 2D actor proposals .", "ner": [["actor localization", "Task"], ["region proposal network", "Method"], ["RPN", "Method"], ["Faster R - CNN", "Method"]], "rel": [["region proposal network", "Used-For", "actor localization"], ["RPN", "Synonym-Of", "region proposal network"], ["region proposal network", "Part-Of", "Faster R - CNN"]], "rel_plus": [["region proposal network:Method", "Used-For", "actor localization:Task"], ["RPN:Method", "Synonym-Of", "region proposal network:Method"], ["region proposal network:Method", "Part-Of", "Faster R - CNN:Method"]]}
{"doc_id": "51876625", "sentence": "This allows more freedom to the choice of action classification features without adding much computation overhead , as classification feature computation is usually dominated by the neighboring frames .", "ner": [["action classification", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "51876625", "sentence": "We found that 3D ConvNets consistently outperform alternative approaches such as channel - wise stacking of frames at the input layer or average pooling at the output layer .", "ner": [["3D ConvNets", "Method"], ["average pooling", "Method"]], "rel": [["average pooling", "Part-Of", "3D ConvNets"]], "rel_plus": [["average pooling:Method", "Part-Of", "3D ConvNets:Method"]]}
{"doc_id": "51876625", "sentence": "The output feature map from 3D ConvNets has an extra time dimension , which is inconsistent with the 2D bounding box proposals generated by RPN .", "ner": [["3D ConvNets", "Method"], ["RPN", "Method"]], "rel": [["3D ConvNets", "Compare-With", "RPN"]], "rel_plus": [["3D ConvNets:Method", "Compare-With", "RPN:Method"]]}
{"doc_id": "51876625", "sentence": "The cropped actor features are then inflated back to 3D , to allow reusing the pre - trained 3D ConvNets weights for classification .", "ner": [["3D ConvNets", "Method"], ["classification", "Task"]], "rel": [["3D ConvNets", "Used-For", "classification"]], "rel_plus": [["3D ConvNets:Method", "Used-For", "classification:Task"]]}
{"doc_id": "51876625", "sentence": "For action classification , we use gated separable 3D network ( S 3 D - G ) [ 5 9 ] .", "ner": [["action classification", "Task"], ["gated separable 3D network", "Method"], ["S 3 D - G", "Method"]], "rel": [["gated separable 3D network", "Used-For", "action classification"], ["S 3 D - G", "Synonym-Of", "gated separable 3D network"]], "rel_plus": [["gated separable 3D network:Method", "Used-For", "action classification:Task"], ["S 3 D - G:Method", "Synonym-Of", "gated separable 3D network:Method"]]}
{"doc_id": "51876625", "sentence": "Compared with I 3 D [ 3 ] used in [ 1 3 ] , S 3 D - G replaces full 3D convolutions with separable spatial and temporal convolutions , and employs spatiotemporal feature gating layers .", "ner": [["I 3 D", "Method"], ["S 3 D - G", "Method"], ["3D convolutions", "Method"], ["separable spatial and temporal convolutions", "Method"], ["spatiotemporal feature gating layers", "Method"]], "rel": [["I 3 D", "Compare-With", "S 3 D - G"], ["separable spatial and temporal convolutions", "Part-Of", "S 3 D - G"], ["spatiotemporal feature gating layers", "Part-Of", "S 3 D - G"]], "rel_plus": [["I 3 D:Method", "Compare-With", "S 3 D - G:Method"], ["separable spatial and temporal convolutions:Method", "Part-Of", "S 3 D - G:Method"], ["spatiotemporal feature gating layers:Method", "Part-Of", "S 3 D - G:Method"]]}
{"doc_id": "51876625", "sentence": "Overall , S 3 D - G is faster , has fewer parameters , provides higher accuracy compared to other 3D ConvNet models , and has a flexible design which makes it ideal for the large - scale action detection setup .", "ner": [["S 3 D - G", "Method"], ["3D ConvNet", "Method"], ["action detection", "Task"]], "rel": [["S 3 D - G", "Compare-With", "3D ConvNet"], ["S 3 D - G", "Used-For", "action detection"]], "rel_plus": [["S 3 D - G:Method", "Compare-With", "3D ConvNet:Method"], ["S 3 D - G:Method", "Used-For", "action detection:Task"]]}
{"doc_id": "51876625", "sentence": "Following the recommendation from [ 5 9 ] , we use the top - heavy configuration and use 2D convolutions without gating until the Mixed 4b block ( we follow the same naming conventions as the Inception networks [ 5 2 ] ) , and switch to separable 3D convolutions with gating onwards .", "ner": [["2D convolutions", "Method"], ["Mixed 4b block", "Method"], ["Inception networks", "Method"], ["separable 3D convolutions", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "51876625", "sentence": "Regions in Mixed 4f corresponding to actor RPN proposals are temporally flattened and used as the input for the action classification network .", "ner": [["RPN", "Method"], ["action classification", "Task"]], "rel": [["RPN", "Used-For", "action classification"]], "rel_plus": [["RPN:Method", "Used-For", "action classification:Task"]]}
{"doc_id": "51876625", "sentence": "Thus , we refer to our approach as an actor - centric relation network ( ACRN ) .", "ner": [["actor - centric relation network", "Method"], ["ACRN", "Method"]], "rel": [["ACRN", "Synonym-Of", "actor - centric relation network"]], "rel_plus": [["ACRN:Method", "Synonym-Of", "actor - centric relation network:Method"]]}
{"doc_id": "51876625", "sentence": "This simplification avoids the need of generating object proposals , and has been shown to be effective for video classification [ 5 6 ] and question answering [ 4 4 ] tasks .", "ner": [["video classification", "Task"], ["question answering", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "51876625", "sentence": "Action detection with actor - centric relation network ( ACRN ) .", "ner": [["Action detection", "Task"], ["actor - centric relation network", "Method"], ["ACRN", "Method"]], "rel": [["actor - centric relation network", "Used-For", "Action detection"], ["ACRN", "Synonym-Of", "actor - centric relation network"]], "rel_plus": [["actor - centric relation network:Method", "Used-For", "Action detection:Task"], ["ACRN:Method", "Synonym-Of", "actor - centric relation network:Method"]]}
{"doc_id": "51876625", "sentence": "We now discuss how to incorporate ACRN into our action detection framework .", "ner": [["ACRN", "Method"], ["action detection", "Task"]], "rel": [["ACRN", "Used-For", "action detection"]], "rel_plus": [["ACRN:Method", "Used-For", "action detection:Task"]]}
{"doc_id": "51876625", "sentence": "In practice , we use Mixed 5b and Mixed 5c blocks of the S 3 D - G network and an average pooling layer ( similar to the action classifier network ) to output a 1 \u00d7 1 \u00d7 1 0 2 4 feature ( f to get a 1 \u00d7 1 \u00d7 2 0 4 8 representation which is used for action classification and bounding - box regression for a given actor box b i ( see Figure 2 ) .", "ner": [["Mixed 5b", "Method"], ["Mixed 5c blocks", "Method"], ["S 3 D - G", "Method"], ["average pooling", "Method"], ["action classification", "Task"], ["bounding - box regression", "Task"]], "rel": [["Mixed 5c blocks", "Part-Of", "S 3 D - G"], ["Mixed 5b", "Part-Of", "S 3 D - G"], ["average pooling", "Part-Of", "S 3 D - G"], ["S 3 D - G", "Used-For", "action classification"], ["S 3 D - G", "Used-For", "bounding - box regression"]], "rel_plus": [["Mixed 5c blocks:Method", "Part-Of", "S 3 D - G:Method"], ["Mixed 5b:Method", "Part-Of", "S 3 D - G:Method"], ["average pooling:Method", "Part-Of", "S 3 D - G:Method"], ["S 3 D - G:Method", "Used-For", "action classification:Task"], ["S 3 D - G:Method", "Used-For", "bounding - box regression:Task"]]}
{"doc_id": "51876625", "sentence": "We report results on the JHMDB [ 2 2 ] and AVA [ 1 3 ] action detection benchmarks .", "ner": [["JHMDB", "Dataset"], ["AVA", "Dataset"], ["action detection", "Task"]], "rel": [["AVA", "Benchmark-For", "action detection"], ["JHMDB", "Benchmark-For", "action detection"]], "rel_plus": [["AVA:Dataset", "Benchmark-For", "action detection:Task"], ["JHMDB:Dataset", "Benchmark-For", "action detection:Task"]]}
{"doc_id": "51876625", "sentence": "For the Base - Model , we use the ResNet - 5 0 [ 1 6 ] RGB model for actor localization and the S 3 D - G [ 5 9 ] two - stream model for action classification .", "ner": [["ResNet - 5 0", "Method"], ["actor localization", "Task"], ["S 3 D - G", "Method"], ["action classification", "Task"]], "rel": [["ResNet - 5 0", "Used-For", "actor localization"], ["S 3 D - G", "Used-For", "action classification"]], "rel_plus": [["ResNet - 5 0:Method", "Used-For", "actor localization:Task"], ["S 3 D - G:Method", "Used-For", "action classification:Task"]]}
{"doc_id": "51876625", "sentence": "As is standard practice , the ResNet - 5 0 model is pre - trained on ImageNet and S 3 D - G RGB+Flow streams are pre - trained on Kinetics .", "ner": [["ResNet - 5 0", "Method"], ["ImageNet", "Dataset"], ["S 3 D - G RGB+Flow streams", "Method"], ["Kinetics", "Dataset"]], "rel": [["ResNet - 5 0", "Trained-With", "ImageNet"], ["S 3 D - G RGB+Flow streams", "Trained-With", "Kinetics"]], "rel_plus": [["ResNet - 5 0:Method", "Trained-With", "ImageNet:Dataset"], ["S 3 D - G RGB+Flow streams:Method", "Trained-With", "Kinetics:Dataset"]]}
{"doc_id": "51876625", "sentence": "The classification head ( Mixed 5b , Mixed 5c ) are initialized from RGB stream pre - trained on Kinetics for both RN and actor classification , but they are updated separately ( weights are not shared ) .", "ner": [["Mixed 5b", "Method"], ["Mixed 5c", "Method"], ["Kinetics", "Dataset"], ["RN", "Task"], ["actor classification", "Task"]], "rel": [["Mixed 5b", "Trained-With", "Kinetics"], ["Mixed 5c", "Trained-With", "Kinetics"], ["Mixed 5b", "Used-For", "RN"], ["Mixed 5c", "Used-For", "RN"], ["Mixed 5b", "Used-For", "actor classification"], ["Mixed 5c", "Used-For", "actor classification"]], "rel_plus": [["Mixed 5b:Method", "Trained-With", "Kinetics:Dataset"], ["Mixed 5c:Method", "Trained-With", "Kinetics:Dataset"], ["Mixed 5b:Method", "Used-For", "RN:Task"], ["Mixed 5c:Method", "Used-For", "RN:Task"], ["Mixed 5b:Method", "Used-For", "actor classification:Task"], ["Mixed 5c:Method", "Used-For", "actor classification:Task"]]}
{"doc_id": "51876625", "sentence": "We train the model for 2 0 0 K and 1. 2 M steps for JHMDB and AVA respectively , and use start asynchronous SGD with a batch - size of 1 per GPU ( 1 1 GPUs in total ) , mini - batch size of 2 5 6 for actor RPN and 6 4 for action classifier ( following [ 1 3 ] ) .", "ner": [["JHMDB", "Dataset"], ["AVA", "Dataset"], ["asynchronous SGD", "Method"], ["RPN", "Method"], ["action classifier", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "51876625", "sentence": "To stabilize training , the batch - norm updates are disabled during training and we apply a gradient multiplier of 0.0 1 to gradients from RN to the feature map .   We perform a number of ablation experiments to better understand the properties of our actor - centric relation network and its impact on the action detection performance .", "ner": [["RN", "Method"], ["actor - centric relation network", "Method"], ["action detection", "Task"]], "rel": [["actor - centric relation network", "Used-For", "action detection"]], "rel_plus": [["actor - centric relation network:Method", "Used-For", "action detection:Task"]]}
{"doc_id": "51876625", "sentence": "ACRN : global and actor feature maps are used to compute relation feature maps by ACRN , which are fed into the classification head .", "ner": [["ACRN", "Method"], ["ACRN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "51876625", "sentence": "We compare our best models with the state - of - the - art methods on JHMDB and AVA .", "ner": [["JHMDB", "Dataset"], ["AVA", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "51876625", "sentence": "We fix the number of input frames to 2 0 for I 3 D , Base - Model and ACRN .", "ner": [["I 3 D", "Method"], ["Base - Model", "Method"], ["ACRN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "51876625", "sentence": "We also look into the per - class performance breakdown : on the JHMDB dataset , ACRN outperforms the Base - Model significantly for catch ( 1 2 % ) , jump ( 6% ) , shoot gun ( 5% ) and wave ( 1 0 % ) .", "ner": [["JHMDB", "Dataset"], ["ACRN", "Method"]], "rel": [["ACRN", "Evaluated-With", "JHMDB"]], "rel_plus": [["ACRN:Method", "Evaluated-With", "JHMDB:Dataset"]]}
{"doc_id": "51876625", "sentence": "Model frame - AP video - AP Peng et al. [ 3 7 ] 5 8 . 5 7 3 . 1 ACT [ 2 4 ] 6 5 . 7 7 3 . 7 I 3 D [ 1 3 ] 7 3   To qualitatively verify what relations are learned by ACRN , we apply the class activation map ( CAM ) [ 6 3 ] method to visualize the per - category relation heatmaps based on ACRN outputs .", "ner": [["ACRN", "Method"], ["class activation map", "Method"], ["CAM", "Method"], ["ACRN", "Method"]], "rel": [["CAM", "Synonym-Of", "class activation map"]], "rel_plus": [["CAM:Method", "Synonym-Of", "class activation map:Method"]]}
{"doc_id": "51876625", "sentence": "We modify the inference network by removing the average pooling operation after the ACRN branch , and apply the final action classifier as 1 \u00d7 1 convolutions on the relation feature map .", "ner": [["average pooling", "Method"], ["ACRN", "Method"], ["1 \u00d7 1 convolutions", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "51876625", "sentence": "Figure 5 and Figure 6 show the visualizations of the top - 1 and top - 2 highest scoring detections on JHMDB and AVA respectively .", "ner": [["JHMDB", "Dataset"], ["AVA", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "51876625", "sentence": "Finally , Figure 7 illustrates examples for which the false alarms of the Base - Model are removed by ACRN ( top row ) and the missing detections are captured by ACRN ( bottom row ) .", "ner": [["Base - Model", "Method"], ["ACRN", "Method"], ["ACRN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "195347056", "sentence": "This paper proposes a series of new approaches to improve Generative Adversarial Network ( GAN ) for conditional image synthesis and we name the proposed model as ArtGAN .", "ner": [["Generative Adversarial Network", "Method"], ["GAN", "Method"], ["conditional image synthesis", "Task"], ["ArtGAN", "Method"]], "rel": [["GAN", "Synonym-Of", "Generative Adversarial Network"], ["ArtGAN", "SubClass-Of", "Generative Adversarial Network"], ["Generative Adversarial Network", "Used-For", "conditional image synthesis"], ["ArtGAN", "Used-For", "conditional image synthesis"]], "rel_plus": [["GAN:Method", "Synonym-Of", "Generative Adversarial Network:Method"], ["ArtGAN:Method", "SubClass-Of", "Generative Adversarial Network:Method"], ["Generative Adversarial Network:Method", "Used-For", "conditional image synthesis:Task"], ["ArtGAN:Method", "Used-For", "conditional image synthesis:Task"]]}
{"doc_id": "195347056", "sentence": "In the experiments , we evaluate ArtGAN on CIFAR - 1 0 and STL - 1 0 via ablation studies .", "ner": [["ArtGAN", "Method"], ["CIFAR - 1 0", "Dataset"], ["STL - 1 0", "Dataset"]], "rel": [["ArtGAN", "Evaluated-With", "CIFAR - 1 0"], ["ArtGAN", "Evaluated-With", "STL - 1 0"]], "rel_plus": [["ArtGAN:Method", "Evaluated-With", "CIFAR - 1 0:Dataset"], ["ArtGAN:Method", "Evaluated-With", "STL - 1 0:Dataset"]]}
{"doc_id": "195347056", "sentence": "Qualitatively , we demonstrate that ArtGAN is able to generate plausible - looking images on Oxford - 1 0 2 and CUB - 2 0 0 , as well as able to draw realistic artworks based on style , artist , and genre .", "ner": [["ArtGAN", "Method"], ["Oxford - 1 0 2", "Dataset"], ["CUB - 2 0 0", "Dataset"]], "rel": [["ArtGAN", "Evaluated-With", "Oxford - 1 0 2"], ["ArtGAN", "Evaluated-With", "CUB - 2 0 0"]], "rel_plus": [["ArtGAN:Method", "Evaluated-With", "Oxford - 1 0 2:Dataset"], ["ArtGAN:Method", "Evaluated-With", "CUB - 2 0 0:Dataset"]]}
{"doc_id": "195347056", "sentence": "The source code and models are available at : https://github.com/cs - chan/ArtGAN Recently , Goodfellow et al. [ 1 ] introduced an interesting features learning method via adversarial training a deep generative model , called the Generative Adversarial Networks ( GAN ) .", "ner": [["chan/ArtGAN", "Method"], ["Generative Adversarial Networks", "Method"], ["GAN", "Method"]], "rel": [["GAN", "Synonym-Of", "Generative Adversarial Networks"]], "rel_plus": [["GAN:Method", "Synonym-Of", "Generative Adversarial Networks:Method"]]}
{"doc_id": "195347056", "sentence": "Unlike the traditional deep discriminative models [ 2 ] - [ 4 ] , the representations learnt by GAN can be visualized through the trained generator ( of the GAN ) in the form of synthetic images .", "ner": [["deep discriminative models", "Method"], ["GAN", "Method"], ["GAN", "Method"]], "rel": [["GAN", "Compare-With", "deep discriminative models"]], "rel_plus": [["GAN:Method", "Compare-With", "deep discriminative models:Method"]]}
{"doc_id": "195347056", "sentence": "Since then , many extensions of GAN [ 5 ] - [ 1 1 ] have been introduced and shown significant promise in synthetically generating structural images using the MNIST [ 1 2 ] , CIFAR - 1 0 [ 1 3 ] and ImageNet [ 1 4 ] datasets .", "ner": [["GAN", "Method"], ["synthetically generating structural images", "Task"], ["MNIST", "Dataset"], ["CIFAR - 1 0", "Dataset"], ["ImageNet", "Dataset"]], "rel": [["MNIST", "Benchmark-For", "synthetically generating structural images"], ["GAN", "Used-For", "synthetically generating structural images"], ["CIFAR - 1 0", "Benchmark-For", "synthetically generating structural images"], ["ImageNet", "Benchmark-For", "synthetically generating structural images"], ["GAN", "Evaluated-With", "MNIST"], ["GAN", "Evaluated-With", "CIFAR - 1 0"], ["GAN", "Evaluated-With", "ImageNet"]], "rel_plus": [["MNIST:Dataset", "Benchmark-For", "synthetically generating structural images:Task"], ["GAN:Method", "Used-For", "synthetically generating structural images:Task"], ["CIFAR - 1 0:Dataset", "Benchmark-For", "synthetically generating structural images:Task"], ["ImageNet:Dataset", "Benchmark-For", "synthetically generating structural images:Task"], ["GAN:Method", "Evaluated-With", "MNIST:Dataset"], ["GAN:Method", "Evaluated-With", "CIFAR - 1 0:Dataset"], ["GAN:Method", "Evaluated-With", "ImageNet:Dataset"]]}
{"doc_id": "195347056", "sentence": "To this end , we propose a novel Generative Adversarial Network , namely as the ArtGAN 1 .", "ner": [["Generative Adversarial Network", "Method"], ["ArtGAN", "Method"]], "rel": [["ArtGAN", "SubClass-Of", "Generative Adversarial Network"]], "rel_plus": [["ArtGAN:Method", "SubClass-Of", "Generative Adversarial Network:Method"]]}
{"doc_id": "195347056", "sentence": "In summary , our key contributions are 1 ) we propose a novel conditional GAN variant , namely as the ArtGAN to emulate the concept of effective learning to generate very challenging images ( i.e. artwork ) .", "ner": [["GAN", "Method"], ["ArtGAN", "Method"]], "rel": [["ArtGAN", "SubClass-Of", "GAN"]], "rel_plus": [["ArtGAN:Method", "SubClass-Of", "GAN:Method"]]}
{"doc_id": "195347056", "sentence": "To the best of our knowledge , no existing empirical research has addressed the implementation of a generative model on a large scale paintings dataset . 2 ) we propose a novel magnified learning to synthesize better quality images . 3 ) Empirically , we show that our model is capable of generating high quality artwork that exhibit similar visual representations within genre , artist , or style . 4 ) Our model is also able to generate high resolution images on CIFAR - 1 0 [ 1 3 ] , STL - 1 0 [ 1 9 ] , Oxford - 1 0 2 [ 2 0 ] , and CUB - 2 0 0 [ 2 1 ] that look natural and contain clear object structures in them .", "ner": [["CIFAR - 1 0", "Dataset"], ["STL - 1 0", "Dataset"], ["Oxford - 1 0 2", "Dataset"], ["CUB - 2 0 0", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "195347056", "sentence": "For instance , we extend the original qualitative experiments from Wikiart [ 2 3 ] to CIFAR - 1 0 [ 1 3 ] , STL - 1 0 [ 1 9 ] , Oxford - 1 0 2 [ 2 0 ] , and CUB - 2 0 0 [ 2 1 ] datasets .", "ner": [["Wikiart", "Dataset"], ["CIFAR - 1 0", "Dataset"], ["STL - 1 0", "Dataset"], ["Oxford - 1 0 2", "Dataset"], ["CUB - 2 0 0", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "195347056", "sentence": "In addition , we included the Inception score [ 2 4 ] as a quantitative metric and ArtGAN obtains state - of - the - art result on CIFAR - 1 0 dataset .", "ner": [["ArtGAN", "Method"], ["CIFAR - 1 0", "Dataset"]], "rel": [["ArtGAN", "Evaluated-With", "CIFAR - 1 0"]], "rel_plus": [["ArtGAN:Method", "Evaluated-With", "CIFAR - 1 0:Dataset"]]}
{"doc_id": "195347056", "sentence": "Recently , advances in deep models nourish a series of deep generative models [ 2 8 ] , [ 2 9 ] for image generation through the Bayesian inference , typically trained by maximizing the log - likelihood .", "ner": [["deep generative models", "Method"], ["image generation", "Task"], ["Bayesian inference", "Method"], ["log - likelihood", "Method"]], "rel": [["Bayesian inference", "Used-For", "deep generative models"], ["log - likelihood", "Used-For", "deep generative models"], ["deep generative models", "Used-For", "image generation"]], "rel_plus": [["Bayesian inference:Method", "Used-For", "deep generative models:Method"], ["log - likelihood:Method", "Used-For", "deep generative models:Method"], ["deep generative models:Method", "Used-For", "image generation:Task"]]}
{"doc_id": "195347056", "sentence": "The denoising autoencoders ( DAE ) [ 3 0 ] was introduced to overcome the intractable problem , but the reconstructed images are generally blur .", "ner": [["denoising autoencoders", "Method"], ["DAE", "Method"]], "rel": [["DAE", "Synonym-Of", "denoising autoencoders"]], "rel_plus": [["DAE:Method", "Synonym-Of", "denoising autoencoders:Method"]]}
{"doc_id": "195347056", "sentence": "PixelRNN [ 3 2 ] is another autoregressive approach for image generation which has received much attentions recently .", "ner": [["PixelRNN", "Method"], ["image generation", "Task"]], "rel": [["PixelRNN", "Used-For", "image generation"]], "rel_plus": [["PixelRNN:Method", "Used-For", "image generation:Task"]]}
{"doc_id": "195347056", "sentence": "Its extensions ( PixelCNN [ 3 3 ] and PixelCNN++ [ 3 4 ] ) are able to synthesize decent images but are computationally expensive to train 3 .", "ner": [["PixelCNN", "Method"], ["PixelCNN++", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "195347056", "sentence": "A more significant breakthrough framework , Generative Adversarial Network ( GAN ) was introduced by Goodfellow et al. [ 1 ] .", "ner": [["Generative Adversarial Network", "Method"], ["GAN", "Method"]], "rel": [["GAN", "Synonym-Of", "Generative Adversarial Network"]], "rel_plus": [["GAN:Method", "Synonym-Of", "Generative Adversarial Network:Method"]]}
{"doc_id": "195347056", "sentence": "Wasserstein GAN ( WGAN ) uses the Earth - Mover ( EM ) distance to address the vanishing gradient and saturated Jensen - Shannon distance problems while enforcing Lipschitz constraint .", "ner": [["Wasserstein GAN", "Method"], ["WGAN", "Method"], ["Earth - Mover", "Method"], ["EM", "Method"], ["Jensen - Shannon distance problems", "Task"]], "rel": [["WGAN", "Synonym-Of", "Wasserstein GAN"], ["Earth - Mover", "Part-Of", "Wasserstein GAN"], ["EM", "Synonym-Of", "Earth - Mover"], ["Earth - Mover", "Used-For", "Jensen - Shannon distance problems"]], "rel_plus": [["WGAN:Method", "Synonym-Of", "Wasserstein GAN:Method"], ["Earth - Mover:Method", "Part-Of", "Wasserstein GAN:Method"], ["EM:Method", "Synonym-Of", "Earth - Mover:Method"], ["Earth - Mover:Method", "Used-For", "Jensen - Shannon distance problems:Task"]]}
{"doc_id": "195347056", "sentence": "Although they argued that the performance is more stable at convergence , WGAN is still outperformed by DCGAN [ 6 ] in terms of convergent speed and Inception score .", "ner": [["WGAN", "Method"], ["DCGAN", "Method"]], "rel": [["WGAN", "Compare-With", "DCGAN"]], "rel_plus": [["WGAN:Method", "Compare-With", "DCGAN:Method"]]}
{"doc_id": "195347056", "sentence": "A similar solution was introduced in Loss - Sensitive GAN ( LS - GAN ) [ 3 7 ] with theoretical analysis on Lipschitz densities .", "ner": [["Loss - Sensitive GAN", "Method"], ["LS - GAN", "Method"]], "rel": [["LS - GAN", "Synonym-Of", "Loss - Sensitive GAN"]], "rel_plus": [["LS - GAN:Method", "Synonym-Of", "Loss - Sensitive GAN:Method"]]}
{"doc_id": "195347056", "sentence": "Recently , another subfamily of GAN was introduced by employing an autoencoder in the discriminator .", "ner": [["GAN", "Method"], ["autoencoder", "Method"], ["discriminator", "Method"]], "rel": [["discriminator", "Part-Of", "GAN"], ["autoencoder", "Part-Of", "discriminator"]], "rel_plus": [["discriminator:Method", "Part-Of", "GAN:Method"], ["autoencoder:Method", "Part-Of", "discriminator:Method"]]}
{"doc_id": "195347056", "sentence": "The Energy - based GAN ( EBGAN ) [ 1 5 ] is trained by replacing the discriminator with an autoencoder and it has demonstrated decent quality synthetic images up to 2 5 6 \u00d7 2 5 6 pixels .", "ner": [["Energy - based GAN", "Method"], ["EBGAN", "Method"], ["discriminator", "Method"], ["autoencoder", "Method"]], "rel": [["EBGAN", "Synonym-Of", "Energy - based GAN"], ["autoencoder", "Part-Of", "Energy - based GAN"]], "rel_plus": [["EBGAN:Method", "Synonym-Of", "Energy - based GAN:Method"], ["autoencoder:Method", "Part-Of", "Energy - based GAN:Method"]]}
{"doc_id": "195347056", "sentence": "Denoising Feature Matching ( DFM ) [ 3 8 ] keeps the traditional GAN adversarial loss , while an additional complementary information to the generator is computed using a denoising autoencoder in the feature space learnt by the discriminator .", "ner": [["Denoising Feature Matching", "Method"], ["DFM", "Method"], ["GAN adversarial loss", "Method"], ["generator", "Method"], ["denoising autoencoder", "Method"], ["discriminator", "Method"]], "rel": [["DFM", "Synonym-Of", "Denoising Feature Matching"], ["GAN adversarial loss", "Part-Of", "Denoising Feature Matching"], ["generator", "Part-Of", "Denoising Feature Matching"], ["discriminator", "Part-Of", "Denoising Feature Matching"], ["denoising autoencoder", "Part-Of", "generator"]], "rel_plus": [["DFM:Method", "Synonym-Of", "Denoising Feature Matching:Method"], ["GAN adversarial loss:Method", "Part-Of", "Denoising Feature Matching:Method"], ["generator:Method", "Part-Of", "Denoising Feature Matching:Method"], ["discriminator:Method", "Part-Of", "Denoising Feature Matching:Method"], ["denoising autoencoder:Method", "Part-Of", "generator:Method"]]}
{"doc_id": "195347056", "sentence": "DFM achieved stateof - the - art Inception score on CIFAR - 1 0 in the unsupervised settings .", "ner": [["DFM", "Method"], ["CIFAR - 1 0", "Dataset"]], "rel": [["DFM", "Evaluated-With", "CIFAR - 1 0"]], "rel_plus": [["DFM:Method", "Evaluated-With", "CIFAR - 1 0:Dataset"]]}
{"doc_id": "195347056", "sentence": "A closely related work , Boundary Equilibrium GAN ( BEGAN ) [ 1 6 ] was proposed with a new equilibrium enforcing method .", "ner": [["Boundary Equilibrium GAN", "Method"], ["BEGAN", "Method"]], "rel": [["BEGAN", "Synonym-Of", "Boundary Equilibrium GAN"]], "rel_plus": [["BEGAN:Method", "Synonym-Of", "Boundary Equilibrium GAN:Method"]]}
{"doc_id": "195347056", "sentence": "Surprisingly , it demonstrated realistic face generation but is significantly outperformed by DFM on CIFAR - 1 0 .", "ner": [["face generation", "Task"], ["DFM", "Method"], ["CIFAR - 1 0", "Dataset"]], "rel": [["DFM", "Used-For", "face generation"], ["CIFAR - 1 0", "Benchmark-For", "face generation"], ["DFM", "Evaluated-With", "CIFAR - 1 0"]], "rel_plus": [["DFM:Method", "Used-For", "face generation:Task"], ["CIFAR - 1 0:Dataset", "Benchmark-For", "face generation:Task"], ["DFM:Method", "Evaluated-With", "CIFAR - 1 0:Dataset"]]}
{"doc_id": "195347056", "sentence": "This prior information has many forms , for instance a distorted image for inpainting [ 3 2 ] , [ 3 9 ] ; natural image for super - resolution [ 8 ] or style transfer [ 4 0 ] - [ 4 2 ] ; text codes for text to image translation [ 1 0 ] , [ 1 1 ] .", "ner": [["inpainting", "Task"], ["super - resolution", "Task"], ["style transfer", "Task"], ["text to image translation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "195347056", "sentence": "An earlier work that employed conditional setting in GAN was the Conditional GAN ( CondGAN ) [ 5 ] by feeding the labels or modes to the generator and discriminator .", "ner": [["GAN", "Method"], ["Conditional GAN", "Method"], ["CondGAN", "Method"], ["generator", "Method"], ["discriminator", "Method"]], "rel": [["Conditional GAN", "SubClass-Of", "GAN"], ["generator", "Part-Of", "GAN"], ["discriminator", "Part-Of", "GAN"], ["CondGAN", "Synonym-Of", "Conditional GAN"], ["generator", "Part-Of", "Conditional GAN"], ["discriminator", "Part-Of", "Conditional GAN"]], "rel_plus": [["Conditional GAN:Method", "SubClass-Of", "GAN:Method"], ["generator:Method", "Part-Of", "GAN:Method"], ["discriminator:Method", "Part-Of", "GAN:Method"], ["CondGAN:Method", "Synonym-Of", "Conditional GAN:Method"], ["generator:Method", "Part-Of", "Conditional GAN:Method"], ["discriminator:Method", "Part-Of", "Conditional GAN:Method"]]}
{"doc_id": "195347056", "sentence": "While this website 4 unofficially demonstrated generating images on CIFAR - 1 0 using CondGAN , the objects in the generated images are hardly recognizable .", "ner": [["CIFAR - 1 0", "Dataset"], ["CondGAN", "Method"]], "rel": [["CondGAN", "Evaluated-With", "CIFAR - 1 0"]], "rel_plus": [["CondGAN:Method", "Evaluated-With", "CIFAR - 1 0:Dataset"]]}
{"doc_id": "195347056", "sentence": "A similar work was introduced in InfoGAN [ 4 4 ] with the discriminator in InfoGAN replaced by a multiclass classifier .", "ner": [["InfoGAN", "Method"], ["discriminator", "Method"], ["InfoGAN", "Method"]], "rel": [["discriminator", "Part-Of", "InfoGAN"]], "rel_plus": [["discriminator:Method", "Part-Of", "InfoGAN:Method"]]}
{"doc_id": "195347056", "sentence": "In addition to the GAN variants , PixelCNN [ 3 3 ] , [ 3 4 ] also demonstrated decent results on conditional image generation but is computationally expensive for sampling .", "ner": [["GAN", "Method"], ["PixelCNN", "Method"], ["conditional image generation", "Task"]], "rel": [["PixelCNN", "Used-For", "conditional image generation"], ["GAN", "Used-For", "conditional image generation"]], "rel_plus": [["PixelCNN:Method", "Used-For", "conditional image generation:Task"], ["GAN:Method", "Used-For", "conditional image generation:Task"]]}
{"doc_id": "195347056", "sentence": "Built on the Deep Generator Network ( DGN ) [ 4 5 ] , Plug and Play Generative Networks ( PPGN ) [ 4 6 ] is able to produce high quality images at high resolution .", "ner": [["Deep Generator Network", "Method"], ["DGN", "Method"], ["Plug and Play Generative Networks", "Method"], ["PPGN", "Method"]], "rel": [["DGN", "Synonym-Of", "Deep Generator Network"], ["PPGN", "Synonym-Of", "Plug and Play Generative Networks"]], "rel_plus": [["DGN:Method", "Synonym-Of", "Deep Generator Network:Method"], ["PPGN:Method", "Synonym-Of", "Plug and Play Generative Networks:Method"]]}
{"doc_id": "195347056", "sentence": "However , PPGN differs to the other generative models discussed , where images are generated in one - shot from the latent codes in the traditional generative models .", "ner": [["PPGN", "Method"], ["generative models", "Method"], ["generative models", "Method"]], "rel": [["PPGN", "Compare-With", "generative models"]], "rel_plus": [["PPGN:Method", "Compare-With", "generative models:Method"]]}
{"doc_id": "195347056", "sentence": "Generative Adversarial Networks ( GAN ) [ 1 ] contains two networks that are trained by competing with each other .", "ner": [["Generative Adversarial Networks", "Method"], ["GAN", "Method"]], "rel": [["GAN", "Synonym-Of", "Generative Adversarial Networks"]], "rel_plus": [["GAN:Method", "Synonym-Of", "Generative Adversarial Networks:Method"]]}
{"doc_id": "195347056", "sentence": "Overall , the training procedure is a two - player min - max game with the following objective function , ( 1 ) The basic structure of ArtGAN is similar to GAN , such that it consists of a discriminator and a generator that are simultaneously trained using the minmax formulation of GAN , as described in Eq. 1 .", "ner": [["ArtGAN", "Method"], ["GAN", "Method"], ["GAN", "Method"]], "rel": [["ArtGAN", "Compare-With", "GAN"]], "rel_plus": [["ArtGAN:Method", "Compare-With", "GAN:Method"]]}
{"doc_id": "195347056", "sentence": "The generator loss function L G to be minimized for training G is defined as , Inspired by recent works [ 1 5 ] , [ 1 6 ] , [ 3 8 ] , we incorporates an autoencoder into the categorical discriminator in ArtGAN for additional complementary information .", "ner": [["generator loss function", "Method"], ["L G", "Method"], ["autoencoder", "Method"], ["discriminator", "Method"], ["ArtGAN", "Method"]], "rel": [["L G", "Synonym-Of", "generator loss function"], ["autoencoder", "Part-Of", "discriminator"], ["discriminator", "Part-Of", "ArtGAN"]], "rel_plus": [["L G:Method", "Synonym-Of", "generator loss function:Method"], ["autoencoder:Method", "Part-Of", "discriminator:Method"], ["discriminator:Method", "Part-Of", "ArtGAN:Method"]]}
{"doc_id": "195347056", "sentence": "The first two types , namely as the ArtGAN - EB and the ArtGAN - AE are implemented using the pixel - level autoencoder , similar to the EBGAN [ 1 5 ] .", "ner": [["ArtGAN - EB", "Method"], ["ArtGAN - AE", "Method"], ["pixel - level autoencoder", "Method"], ["EBGAN", "Method"]], "rel": [["pixel - level autoencoder", "Part-Of", "ArtGAN - EB"], ["pixel - level autoencoder", "Part-Of", "ArtGAN - AE"], ["ArtGAN - AE", "Compare-With", "EBGAN"], ["ArtGAN - EB", "Compare-With", "EBGAN"]], "rel_plus": [["pixel - level autoencoder:Method", "Part-Of", "ArtGAN - EB:Method"], ["pixel - level autoencoder:Method", "Part-Of", "ArtGAN - AE:Method"], ["ArtGAN - AE:Method", "Compare-With", "EBGAN:Method"], ["ArtGAN - EB:Method", "Compare-With", "EBGAN:Method"]]}
{"doc_id": "195347056", "sentence": "The third type , namely the ArtGAN - DFM is an extension of the Denoising Feature Matching ( DFM ) [ 3 8 ] to a conditional setup , forming a Conditional DFM .", "ner": [["ArtGAN - DFM", "Method"], ["Denoising Feature Matching", "Method"], ["DFM", "Method"], ["DFM", "Method"]], "rel": [["DFM", "Synonym-Of", "Denoising Feature Matching"], ["ArtGAN - DFM", "SubClass-Of", "Denoising Feature Matching"]], "rel_plus": [["DFM:Method", "Synonym-Of", "Denoising Feature Matching:Method"], ["ArtGAN - DFM:Method", "SubClass-Of", "Denoising Feature Matching:Method"]]}
{"doc_id": "195347056", "sentence": "ArtGAN - EB : EBGAN [ 1 5 ] is formulated according to the energy - based models by replacing the discriminator with an autoencoder , such that D AE ( \u00b7 ) = Dec(Enc ( \u00b7 ) ) , where Dec and Enc are the decoder and encoder , respectively .", "ner": [["ArtGAN - EB", "Method"], ["EBGAN", "Method"], ["discriminator", "Method"], ["autoencoder", "Method"], ["decoder", "Method"], ["encoder", "Method"]], "rel": [["autoencoder", "Part-Of", "EBGAN"], ["encoder", "Part-Of", "autoencoder"], ["decoder", "Part-Of", "autoencoder"]], "rel_plus": [["autoencoder:Method", "Part-Of", "EBGAN:Method"], ["encoder:Method", "Part-Of", "autoencoder:Method"], ["decoder:Method", "Part-Of", "autoencoder:Method"]]}
{"doc_id": "195347056", "sentence": "While the generator loss L Geb is formulated as , In order to formulate a conditional energy - based loss function , ArtGAN - EB propose a novel discriminator loss function L Debc as , While , the new generator loss L Gae is defined as , ArtGAN - AE : The discriminator loss is similar to L Debc , except that we do not use the generated images as adversarial samples to update the decoder .", "ner": [["generator loss", "Method"], ["L Geb", "Method"], ["energy - based loss function", "Method"], ["ArtGAN - EB", "Method"], ["discriminator loss function", "Method"], ["L Debc", "Method"], ["generator loss", "Method"], ["L Gae", "Method"], ["ArtGAN - AE", "Method"], ["discriminator loss", "Method"], ["L Debc", "Method"], ["decoder", "Method"]], "rel": [["L Geb", "Synonym-Of", "generator loss"], ["discriminator loss function", "Part-Of", "ArtGAN - EB"], ["generator loss", "Part-Of", "ArtGAN - EB"], ["L Debc", "Synonym-Of", "discriminator loss function"], ["L Gae", "Synonym-Of", "generator loss"]], "rel_plus": [["L Geb:Method", "Synonym-Of", "generator loss:Method"], ["discriminator loss function:Method", "Part-Of", "ArtGAN - EB:Method"], ["generator loss:Method", "Part-Of", "ArtGAN - EB:Method"], ["L Debc:Method", "Synonym-Of", "discriminator loss function:Method"], ["L Gae:Method", "Synonym-Of", "generator loss:Method"]]}
{"doc_id": "195347056", "sentence": "This was inspired by DFM [ 3 8 ] to use the autoencoder as a source of complementary information when updating the generator , instead of using the autoencoder as an adversarial function ( as in [ 1 5 ] ) .", "ner": [["DFM", "Method"], ["autoencoder", "Method"], ["generator", "Method"], ["autoencoder", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "195347056", "sentence": "Hence , the discriminator loss L Dae of ArtGAN - AE is formulated as , Meanwhile , ArtGAN - AE shares the same generator loss as the ArtGAN - EB .", "ner": [["discriminator loss", "Method"], ["L Dae", "Method"], ["ArtGAN - AE", "Method"], ["ArtGAN - AE", "Method"], ["generator loss", "Method"], ["ArtGAN - EB", "Method"]], "rel": [["L Dae", "Synonym-Of", "discriminator loss"], ["discriminator loss", "Part-Of", "ArtGAN - AE"], ["generator loss", "Part-Of", "ArtGAN - AE"], ["generator loss", "Part-Of", "ArtGAN - EB"]], "rel_plus": [["L Dae:Method", "Synonym-Of", "discriminator loss:Method"], ["discriminator loss:Method", "Part-Of", "ArtGAN - AE:Method"], ["generator loss:Method", "Part-Of", "ArtGAN - AE:Method"], ["generator loss:Method", "Part-Of", "ArtGAN - EB:Method"]]}
{"doc_id": "195347056", "sentence": "In specific , suppose a generator in the traditional GAN trained on CIFAR - 1 0 usually generates 3 2 \u00d7 3 2 pixels images , G : z \u2192 R 3 2 \u00d7 3 2 \u00d7 C .", "ner": [["generator", "Method"], ["GAN", "Method"], ["CIFAR - 1 0", "Dataset"]], "rel": [["generator", "Part-Of", "GAN"], ["GAN", "Trained-With", "CIFAR - 1 0"]], "rel_plus": [["generator:Method", "Part-Of", "GAN:Method"], ["GAN:Method", "Trained-With", "CIFAR - 1 0:Dataset"]]}
{"doc_id": "195347056", "sentence": "Unlike PPGN [ 4 6 ] which requires a computationally expensive recurrent sampling procedure to generate one image , ArtGAN with magnified learning , namely as the ArtGAN - M is able to generate high quality images at high resolution in one - shot .", "ner": [["PPGN", "Method"], ["ArtGAN with magnified learning", "Method"], ["ArtGAN - M", "Method"]], "rel": [["ArtGAN with magnified learning", "Compare-With", "PPGN"], ["ArtGAN - M", "Synonym-Of", "ArtGAN with magnified learning"]], "rel_plus": [["ArtGAN with magnified learning:Method", "Compare-With", "PPGN:Method"], ["ArtGAN - M:Method", "Synonym-Of", "ArtGAN with magnified learning:Method"]]}
{"doc_id": "195347056", "sentence": "Hence , the comparative studies are first conducted using the objectness metric from Inception score on CIFAR - 1 0 [ 1 3 ] and STL - 1 0 [ 1 9 ] datasets .", "ner": [["CIFAR - 1 0", "Dataset"], ["STL - 1 0", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "195347056", "sentence": "In addition , we also trained the model on Oxford - 1 0 2 [ 2 0 ] and CUB - 2 0 0 [ 2 1 ] for additional performance assessments .", "ner": [["Oxford - 1 0 2", "Dataset"], ["CUB - 2 0 0", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "195347056", "sentence": "We used similar design to BEGAN [ 1 6 ] by employing nearest neighbour upsampling instead of strided deconvolution layer in the generator as suggested by Odena et al. [ 5 2 ] in order to avoid checkerboard artifacts .", "ner": [["BEGAN", "Method"], ["nearest neighbour upsampling", "Method"], ["strided deconvolution", "Method"], ["generator", "Method"]], "rel": [["nearest neighbour upsampling", "Part-Of", "BEGAN"], ["generator", "Part-Of", "BEGAN"], ["nearest neighbour upsampling", "Part-Of", "generator"]], "rel_plus": [["nearest neighbour upsampling:Method", "Part-Of", "BEGAN:Method"], ["generator:Method", "Part-Of", "BEGAN:Method"], ["nearest neighbour upsampling:Method", "Part-Of", "generator:Method"]]}
{"doc_id": "195347056", "sentence": "Batch normalization and leaky ReLU are used for both the discriminator and generator .", "ner": [["Batch normalization", "Method"], ["leaky ReLU", "Method"], ["discriminator", "Method"], ["generator", "Method"]], "rel": [["Batch normalization", "Part-Of", "discriminator"], ["leaky ReLU", "Part-Of", "discriminator"], ["Batch normalization", "Part-Of", "generator"], ["leaky ReLU", "Part-Of", "generator"]], "rel_plus": [["Batch normalization:Method", "Part-Of", "discriminator:Method"], ["leaky ReLU:Method", "Part-Of", "discriminator:Method"], ["Batch normalization:Method", "Part-Of", "generator:Method"], ["leaky ReLU:Method", "Part-Of", "generator:Method"]]}
{"doc_id": "195347056", "sentence": "The list of models to be evaluated are as follows : 1 ) ArtGAN -Baseline model . 2 ) ArtGAN - EB -The first type of categorical autoencoderbased discriminator . 3 ) ArtGAN - AE -The second type of categorical autoencoder - based discriminator . 4 ) ArtGAN - DFM -The third type of categorical autoencoder - based discriminator . 5 ) ArtGAN - M -ArtGAN with magnified learning . 6 ) ArtGAN - D -ArtGAN with deeper architecture ( more layers and number of parameters ) .", "ner": [["ArtGAN", "Method"], ["ArtGAN - EB", "Method"], ["autoencoderbased discriminator", "Method"], ["ArtGAN - AE", "Method"], ["autoencoder - based discriminator", "Method"], ["ArtGAN - DFM", "Method"], ["autoencoder - based discriminator", "Method"], ["ArtGAN - M", "Method"], ["-ArtGAN with magnified learning", "Method"], ["ArtGAN - D", "Method"], ["-ArtGAN with deeper architecture", "Method"]], "rel": [["autoencoderbased discriminator", "Part-Of", "ArtGAN - EB"], ["autoencoder - based discriminator", "Part-Of", "ArtGAN - AE"], ["autoencoder - based discriminator", "Part-Of", "ArtGAN - DFM"], ["-ArtGAN with magnified learning", "Synonym-Of", "ArtGAN - M"], ["-ArtGAN with deeper architecture", "Synonym-Of", "ArtGAN - D"]], "rel_plus": [["autoencoderbased discriminator:Method", "Part-Of", "ArtGAN - EB:Method"], ["autoencoder - based discriminator:Method", "Part-Of", "ArtGAN - AE:Method"], ["autoencoder - based discriminator:Method", "Part-Of", "ArtGAN - DFM:Method"], ["-ArtGAN with magnified learning:Method", "Synonym-Of", "ArtGAN - M:Method"], ["-ArtGAN with deeper architecture:Method", "Synonym-Of", "ArtGAN - D:Method"]]}
{"doc_id": "195347056", "sentence": "This is implemented to verify that network size is not the main factor that contributes to the improvements observed in the experiments when using magnified learning . 7 ) ArtGAN - AEM -ArtGAN - AE with magnified learning . 8) ArtGAN - AEMT -Huang et al. [ 5 3 ] employed a trick by updating more steps for the generator per each discriminator update step .", "ner": [["magnified learning", "Method"], ["ArtGAN - AEM", "Method"], ["-ArtGAN - AE with magnified learning", "Method"], ["ArtGAN - AEMT", "Method"], ["generator", "Method"], ["discriminator", "Method"]], "rel": [["-ArtGAN - AE with magnified learning", "Synonym-Of", "ArtGAN - AEM"], ["generator", "Part-Of", "ArtGAN - AEMT"], ["discriminator", "Part-Of", "ArtGAN - AEMT"]], "rel_plus": [["-ArtGAN - AE with magnified learning:Method", "Synonym-Of", "ArtGAN - AEM:Method"], ["generator:Method", "Part-Of", "ArtGAN - AEMT:Method"], ["discriminator:Method", "Part-Of", "ArtGAN - AEMT:Method"]]}
{"doc_id": "195347056", "sentence": "One of this case can be seen in the experiments when we compare ArtGAN ( baseline ) and ArtGAN - EB in Table I .", "ner": [["ArtGAN", "Method"], ["ArtGAN - EB", "Method"]], "rel": [["ArtGAN", "Compare-With", "ArtGAN - EB"]], "rel_plus": [["ArtGAN:Method", "Compare-With", "ArtGAN - EB:Method"]]}
{"doc_id": "195347056", "sentence": "Although ArtGAN - EB performed better than ArtGAN with higher Inception score ( 8. 2 6 by ArtGAN - EB compared to 8. 2 1 by ArtGAN - DFM ) , it has worse objectness score ( 3 3 . 5 1 by ArtGAN - EB compared to 3 3 . 2 4 by ArtGAN ) .", "ner": [["ArtGAN - EB", "Method"], ["ArtGAN", "Method"], ["ArtGAN - EB", "Method"], ["ArtGAN - DFM", "Method"], ["ArtGAN - EB", "Method"], ["ArtGAN", "Method"]], "rel": [["ArtGAN - EB", "Compare-With", "ArtGAN"], ["ArtGAN - EB", "Compare-With", "ArtGAN - DFM"], ["ArtGAN - EB", "Compare-With", "ArtGAN"]], "rel_plus": [["ArtGAN - EB:Method", "Compare-With", "ArtGAN:Method"], ["ArtGAN - EB:Method", "Compare-With", "ArtGAN - DFM:Method"], ["ArtGAN - EB:Method", "Compare-With", "ArtGAN:Method"]]}
{"doc_id": "195347056", "sentence": "Furthermore , latent space interpolation on the proposed ArtGAN - AEM is performed to \" probe \" the structure of the latent space z. The smooth transitions between samples when the latent space is interpolated usually indicates how well the generative models understand the structure of the images .", "ner": [["ArtGAN - AEM", "Method"], ["generative models", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "195347056", "sentence": "It can be noticed that comparison to other stateof - the - art generative models , ArtGAN - AEMT obtains stateof - the - art result with a score of 8. 8 1 \u00b1 0. 1 4 , outperforming two lastest methods -SGAN [ 5 3 ] ( 8. 5 9 \u00b1 0. 1 2 ) and AC - GAN [ 9 ] ( 8. 2 5 \u00b1 0.0 7 ) .", "ner": [["generative models", "Method"], ["ArtGAN - AEMT", "Method"], ["-SGAN", "Method"], ["AC - GAN", "Method"]], "rel": [["-SGAN", "SubClass-Of", "generative models"], ["AC - GAN", "SubClass-Of", "generative models"], ["ArtGAN - AEMT", "Compare-With", "generative models"], ["ArtGAN - AEMT", "Compare-With", "-SGAN"], ["ArtGAN - AEMT", "Compare-With", "AC - GAN"]], "rel_plus": [["-SGAN:Method", "SubClass-Of", "generative models:Method"], ["AC - GAN:Method", "SubClass-Of", "generative models:Method"], ["ArtGAN - AEMT:Method", "Compare-With", "generative models:Method"], ["ArtGAN - AEMT:Method", "Compare-With", "-SGAN:Method"], ["ArtGAN - AEMT:Method", "Compare-With", "AC - GAN:Method"]]}
{"doc_id": "195347056", "sentence": "Scores Unlabelled Infusion training [ 5 5 ] 4. 6 2 \u00b1 0.0 6 ALI [ 5 6 ] ( as reported in [ 3 8 ] ) 5. 3 4 \u00b1 0.0 5 BEGAN [ 1 6 ] 5. 6 2 GMAN [ 5 7 ] 6. 0 0 \u00b1 0. 1 9 EGAN - Ent - VI [ 5 8 ] 7. 0 7 \u00b1 0. 1 0 LR - GAN [ 5 9 ] 7. 1 7 \u00b1 0.0 7 Denoising feature matching [ 3 8 ] 7 6. 5 8 Improved GAN [ 2 4 ] 8. 0 9 \u00b1 0.0 7 AC - GAN [ 9 ] 8. 2 5 \u00b1 0.0 7 SGAN [ 5 3 ] 8. 5 9 \u00b1 0. 1 2 Proposed methods 8 Although STL - 1 0 is primarily used to develop unsupervised features learning , we stay focus on our goal in this paper and use STL - 1 0 for conditional image synthesis in a supervised fashion .", "ner": [["ALI", "Method"], ["BEGAN", "Method"], ["GMAN", "Method"], ["EGAN - Ent - VI", "Method"], ["GAN", "Method"], ["Denoising feature matching", "Method"], ["Improved GAN", "Method"], ["AC - GAN", "Method"], ["SGAN", "Method"], ["STL - 1 0", "Dataset"], ["STL - 1 0", "Dataset"], ["conditional image synthesis", "Task"]], "rel": [["STL - 1 0", "Used-For", "conditional image synthesis"]], "rel_plus": [["STL - 1 0:Dataset", "Used-For", "conditional image synthesis:Task"]]}
{"doc_id": "195347056", "sentence": "As such , it makes STL - 1 0 a more challenging dataset than CIFAR - 1 0 .", "ner": [["STL - 1 0", "Dataset"], ["CIFAR - 1 0", "Dataset"]], "rel": [["STL - 1 0", "Compare-With", "CIFAR - 1 0"]], "rel_plus": [["STL - 1 0:Dataset", "Compare-With", "CIFAR - 1 0:Dataset"]]}
{"doc_id": "195347056", "sentence": "No mode collapse is observed in this part of the experiments .   In order to further understand the effect of different model components , we conduct extensive ablation studies by comparing the performances of the proposed models on CIFAR - 1 0 ( Table I ) and STL - 1 0 ( Table II ) datasets .", "ner": [["CIFAR - 1 0", "Dataset"], ["STL - 1 0", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "195347056", "sentence": "First , the performance of magnified learning can be assessed by comparing the ArtGAN - M and the ArtGAN - D. Note that the ArtGAN - D is not overfit since its performance is similar to the baseline ( ArtGAN ) .", "ner": [["magnified learning", "Method"], ["ArtGAN - M", "Method"], ["ArtGAN - D.", "Method"], ["ArtGAN - D", "Method"], ["ArtGAN", "Method"]], "rel": [["ArtGAN - M", "Used-For", "magnified learning"], ["ArtGAN - D.", "Used-For", "magnified learning"], ["ArtGAN - M", "Compare-With", "ArtGAN - D."], ["ArtGAN - D", "Compare-With", "ArtGAN"]], "rel_plus": [["ArtGAN - M:Method", "Used-For", "magnified learning:Method"], ["ArtGAN - D.:Method", "Used-For", "magnified learning:Method"], ["ArtGAN - M:Method", "Compare-With", "ArtGAN - D.:Method"], ["ArtGAN - D:Method", "Compare-With", "ArtGAN:Method"]]}
{"doc_id": "195347056", "sentence": "In addition , the ArtGAN - D has more number of parameters than the ArtGAN - M. Hence , it shows that the additional parameter numbers are not the main factor that contribute to the improvement in the ArtGAN - M. These results also show that magnified learning improves the generated image quality with significantly better objectness and Inception score when compared to the ArtGAN baseline .", "ner": [["ArtGAN - D", "Method"], ["ArtGAN - M.", "Method"], ["ArtGAN - M.", "Method"], ["magnified learning", "Method"], ["ArtGAN", "Method"]], "rel": [["ArtGAN - D", "Compare-With", "ArtGAN - M."], ["magnified learning", "Compare-With", "ArtGAN"]], "rel_plus": [["ArtGAN - D:Method", "Compare-With", "ArtGAN - M.:Method"], ["magnified learning:Method", "Compare-With", "ArtGAN:Method"]]}
{"doc_id": "195347056", "sentence": "Finally , ArtGAN - AEM , which is the ArtGAN - AE with magnified learning , achieved the best results with consistent and significant improvements .", "ner": [["ArtGAN - AEM", "Method"], ["ArtGAN - AE with magnified learning", "Method"]], "rel": [["ArtGAN - AE with magnified learning", "Synonym-Of", "ArtGAN - AEM"]], "rel_plus": [["ArtGAN - AE with magnified learning:Method", "Synonym-Of", "ArtGAN - AEM:Method"]]}
{"doc_id": "195347056", "sentence": "Caltech - UCSD Birds - 2 0 0 - 2 0 1 1 ( CUB - 2 0 0 ) [ 2 1 ] is an image dataset with 2 0 0 bird species and a total of 1 1 , 7 8 8 images .", "ner": [["Caltech - UCSD Birds - 2 0 0 - 2 0 1 1", "Dataset"], ["CUB - 2 0 0", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "195347056", "sentence": "Similar to the Oxford - 1 0 2 dataset , the discriminator has a poor performance on bird species classification ( \u223c 2 0 % accuracy ) .", "ner": [["Oxford - 1 0 2", "Dataset"], ["discriminator", "Method"], ["bird species classification", "Task"]], "rel": [["discriminator", "Used-For", "bird species classification"]], "rel_plus": [["discriminator:Method", "Used-For", "bird species classification:Task"]]}
{"doc_id": "195347056", "sentence": "Currently , Wikiart is the largest public dataset available that contains around 8 0 , 0 0 0 annotated paintings for genres , artists and styles classification tasks .", "ner": [["Wikiart", "Dataset"], ["genres , artists and styles classification", "Task"]], "rel": [["Wikiart", "Benchmark-For", "genres , artists and styles classification"]], "rel_plus": [["Wikiart:Dataset", "Benchmark-For", "genres , artists and styles classification:Task"]]}
{"doc_id": "195347056", "sentence": "Figure   Fig   In this work , we proposed a novel GAN variant called ArtGAN which leverage the labels information for better representation learning and image quality .", "ner": [["GAN", "Method"], ["ArtGAN", "Method"], ["representation learning", "Method"]], "rel": [["ArtGAN", "SubClass-Of", "GAN"]], "rel_plus": [["ArtGAN:Method", "SubClass-Of", "GAN:Method"]]}
{"doc_id": "195347056", "sentence": "Empirical experiments showed that the proposed ArtGAN - AEM achieved the state - of - the - art results on CIFAR - 1 0 and STL - 1 0 .", "ner": [["ArtGAN - AEM", "Method"], ["CIFAR - 1 0", "Dataset"], ["STL - 1 0", "Dataset"]], "rel": [["ArtGAN - AEM", "Evaluated-With", "CIFAR - 1 0"], ["ArtGAN - AEM", "Evaluated-With", "STL - 1 0"]], "rel_plus": [["ArtGAN - AEM:Method", "Evaluated-With", "CIFAR - 1 0:Dataset"], ["ArtGAN - AEM:Method", "Evaluated-With", "STL - 1 0:Dataset"]]}
{"doc_id": "195347056", "sentence": "More importantly , the generated paintings showed that ArtGAN - AEM is able to learn artistic representations from the Wikiart paintings that are usually non - figurative and structured abstract .", "ner": [["ArtGAN - AEM", "Method"], ["Wikiart", "Dataset"]], "rel": [["ArtGAN - AEM", "Trained-With", "Wikiart"]], "rel_plus": [["ArtGAN - AEM:Method", "Trained-With", "Wikiart:Dataset"]]}
{"doc_id": "195347056", "sentence": "Finally , we show more qualitative results for experiments on Wikiart , CIFAR - 1 0 , STL - 1 0 , CUB - 2 0 0 , and Oxford - 1 0 2 datasets .", "ner": [["Wikiart", "Dataset"], ["CIFAR - 1 0", "Dataset"], ["STL - 1 0", "Dataset"], ["CUB - 2 0 0", "Dataset"], ["Oxford - 1 0 2", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "195347056", "sentence": "Table IV shows the architectures for the generators . 2 ) STL - 1 0 : Table V and Table VI show the network architectures of the discriminator and generator used on STL - 1 0 . 3 ) Wikiart : All tasks in Wikiart ( i.e. genres , styles , and artists ) used the same architectures described in Table VII   and Table VIII . 4 ) Oxford - 1 0 2 flowers and CUB - 2 0 0 birds : Oxford - 1 0 2 and CUB - 2 0 0 datasets share the same network architectures as described in Table IX and Table X .", "ner": [["STL - 1 0", "Dataset"], ["discriminator", "Method"], ["generator", "Method"], ["STL - 1 0", "Dataset"], ["Wikiart", "Dataset"], ["Wikiart", "Dataset"], ["Oxford - 1 0 2 flowers", "Dataset"], ["CUB - 2 0 0 birds", "Dataset"], ["Oxford - 1 0 2", "Dataset"], ["CUB - 2 0 0", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "195347056", "sentence": "C. More generated samples 1 ) Wikiart : More generated fine - art paintings are visualized in Figure 1 2 , Figure 1 3 , and Figure 1 4 at high resolution ( 1 2 8 \u00d7 1 2 8 pixels ) . 2 ) CIFAR - 1 0 : Figure 1 5 shows generated images at 6 4 \u00d7 6 4 resolution trained on CIFAR - 1 0 . 3 ) STL - 1 0 : Figure 1 6 shows generated images at resolution of 1 2 8 \u00d7 1 2 8 pixels trained on STL - 1 0 . 4 ) CUB - 2 0 0 birds : Figure 1 7 shows more generated CUB - 2 0 0 images .", "ner": [["Wikiart", "Dataset"], ["CIFAR - 1 0", "Dataset"], ["CIFAR - 1 0", "Dataset"], ["STL - 1 0", "Dataset"], ["STL - 1 0", "Dataset"], ["CUB - 2 0 0 birds", "Dataset"], ["CUB - 2 0 0", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "195347056", "sentence": "Each sample represents one of the 2 0 0 bird species . 5 ) Oxford - 1 0 2 flowers : Figure 1 8 shows more generated flower images on Oxford - 1 0 2 at high resolution ( 1 2 8 \u00d7 1 2 8 pixels ) .", "ner": [["Oxford - 1 0 2 flowers", "Dataset"], ["Oxford - 1 0 2", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "211020570", "sentence": "First , a spatial transformer - generative adversarial network which consists of convolutional layers and residual units is utilized to solve the initialization issues caused by face detectors , such as rotation and scale variations , to obtain improved face bounding boxes for face alignment .", "ner": [["spatial transformer - generative adversarial network", "Method"], ["convolutional layers", "Method"], ["residual units", "Method"], ["face alignment", "Task"]], "rel": [["convolutional layers", "Part-Of", "spatial transformer - generative adversarial network"], ["residual units", "Part-Of", "spatial transformer - generative adversarial network"], ["spatial transformer - generative adversarial network", "Used-For", "face alignment"]], "rel_plus": [["convolutional layers:Method", "Part-Of", "spatial transformer - generative adversarial network:Method"], ["residual units:Method", "Part-Of", "spatial transformer - generative adversarial network:Method"], ["spatial transformer - generative adversarial network:Method", "Used-For", "face alignment:Task"]]}
{"doc_id": "211020570", "sentence": "F ACE alignment ( or facial landmark detection ) aims to locate a set of predefined human facial landmarks , such as the corners of the eyes , the eyebrows , and the tip of the nose for high - level vision tasks , such as face recognition [ 1 ] , expression recognition [ 2 ] , facial animation [ 3 ] and 3D face modelling [ 4 ] .", "ner": [["F ACE alignment", "Task"], ["facial landmark detection", "Task"], ["high - level vision tasks", "Task"], ["face recognition", "Task"], ["expression recognition", "Task"], ["facial animation", "Task"], ["3D face modelling", "Task"]], "rel": [["facial landmark detection", "Synonym-Of", "F ACE alignment"], ["face recognition", "SubTask-Of", "high - level vision tasks"], ["expression recognition", "SubTask-Of", "high - level vision tasks"], ["facial animation", "SubTask-Of", "high - level vision tasks"], ["3D face modelling", "SubTask-Of", "high - level vision tasks"]], "rel_plus": [["facial landmark detection:Task", "Synonym-Of", "F ACE alignment:Task"], ["face recognition:Task", "SubTask-Of", "high - level vision tasks:Task"], ["expression recognition:Task", "SubTask-Of", "high - level vision tasks:Task"], ["facial animation:Task", "SubTask-Of", "high - level vision tasks:Task"], ["3D face modelling:Task", "SubTask-Of", "high - level vision tasks:Task"]]}
{"doc_id": "211020570", "sentence": "Recently , progresses have been made by convolutional neural networks ( CNNs ) in semantic segmentation [ 5 ] and in human pose estimation and face alignment based on heatmap regression [ 6 ] .", "ner": [["convolutional neural networks", "Method"], ["CNNs", "Method"], ["semantic segmentation", "Task"], ["human pose estimation", "Task"], ["face alignment", "Task"], ["heatmap regression", "Method"]], "rel": [["CNNs", "Synonym-Of", "convolutional neural networks"], ["convolutional neural networks", "Used-For", "semantic segmentation"], ["convolutional neural networks", "Used-For", "human pose estimation"], ["convolutional neural networks", "Used-For", "face alignment"], ["heatmap regression", "Used-For", "face alignment"]], "rel_plus": [["CNNs:Method", "Synonym-Of", "convolutional neural networks:Method"], ["convolutional neural networks:Method", "Used-For", "semantic segmentation:Task"], ["convolutional neural networks:Method", "Used-For", "human pose estimation:Task"], ["convolutional neural networks:Method", "Used-For", "face alignment:Task"], ["heatmap regression:Method", "Used-For", "face alignment:Task"]]}
{"doc_id": "211020570", "sentence": "The hourglass network [ 6 ] offers a method for human pose estimation .", "ner": [["hourglass network", "Method"], ["human pose estimation", "Task"]], "rel": [["hourglass network", "Used-For", "human pose estimation"]], "rel_plus": [["hourglass network:Method", "Used-For", "human pose estimation:Task"]]}
{"doc_id": "211020570", "sentence": "The hourglass network has been introduced to face alignment task and achieved efficient performance .", "ner": [["hourglass network", "Method"], ["face alignment", "Task"]], "rel": [["hourglass network", "Used-For", "face alignment"]], "rel_plus": [["hourglass network:Method", "Used-For", "face alignment:Task"]]}
{"doc_id": "211020570", "sentence": "Robust cascaded pose regression ( RCPR ) [ 7 ] is the first method that simultaneously detects landmarks and estimates occlusions .", "ner": [["Robust cascaded pose regression", "Method"], ["RCPR", "Method"]], "rel": [["RCPR", "Synonym-Of", "Robust cascaded pose regression"]], "rel_plus": [["RCPR:Method", "Synonym-Of", "Robust cascaded pose regression:Method"]]}
{"doc_id": "211020570", "sentence": "First , spatial transformer -generative adversarial network ( ST - GAN ) normalizes a face to a canonical state .", "ner": [["spatial transformer -generative adversarial network", "Method"], ["ST - GAN", "Method"]], "rel": [["ST - GAN", "Synonym-Of", "spatial transformer -generative adversarial network"]], "rel_plus": [["ST - GAN:Method", "Synonym-Of", "spatial transformer -generative adversarial network:Method"]]}
{"doc_id": "211020570", "sentence": "The work of [ 1 2 ] proposed a deep regression framework with two - stage reinitialization to address the problems of face image initialization and landmark detection .", "ner": [["deep regression framework", "Method"], ["two - stage reinitialization", "Method"], ["face image initialization", "Task"], ["landmark detection", "Task"]], "rel": [["two - stage reinitialization", "Part-Of", "deep regression framework"], ["deep regression framework", "Used-For", "face image initialization"], ["deep regression framework", "Used-For", "landmark detection"]], "rel_plus": [["two - stage reinitialization:Method", "Part-Of", "deep regression framework:Method"], ["deep regression framework:Method", "Used-For", "face image initialization:Task"], ["deep regression framework:Method", "Used-For", "landmark detection:Task"]]}
{"doc_id": "211020570", "sentence": "In this model , the spatial transformer networks ( STNs ) is embedded as subnets at each stage .", "ner": [["spatial transformer networks", "Method"], ["STNs", "Method"]], "rel": [["STNs", "Synonym-Of", "spatial transformer networks"]], "rel_plus": [["STNs:Method", "Synonym-Of", "spatial transformer networks:Method"]]}
{"doc_id": "211020570", "sentence": "In this work 1 , a multistage model ( MSM ) is proposed to address the problem of face image initialization and to facilitate the robustness of face alignment under occlusion .", "ner": [["multistage model", "Method"], ["MSM", "Method"], ["face image initialization", "Task"], ["face alignment", "Task"]], "rel": [["MSM", "Synonym-Of", "multistage model"], ["multistage model", "Used-For", "face image initialization"], ["multistage model", "Used-For", "face alignment"]], "rel_plus": [["MSM:Method", "Synonym-Of", "multistage model:Method"], ["multistage model:Method", "Used-For", "face image initialization:Task"], ["multistage model:Method", "Used-For", "face alignment:Task"]]}
{"doc_id": "211020570", "sentence": "The MSM consists of three parts : a spatial transformer -generative adversarial network ( ST - GAN ) , a two - stage hourglass network and an exemplar - based shape dictionary .", "ner": [["MSM", "Method"], ["spatial transformer -generative adversarial network", "Method"], ["ST - GAN", "Method"], ["hourglass network", "Method"]], "rel": [["spatial transformer -generative adversarial network", "Part-Of", "MSM"], ["hourglass network", "Part-Of", "MSM"], ["ST - GAN", "Synonym-Of", "spatial transformer -generative adversarial network"]], "rel_plus": [["spatial transformer -generative adversarial network:Method", "Part-Of", "MSM:Method"], ["hourglass network:Method", "Part-Of", "MSM:Method"], ["ST - GAN:Method", "Synonym-Of", "spatial transformer -generative adversarial network:Method"]]}
{"doc_id": "211020570", "sentence": "First , ST - GAN produces better initial facial images by removing rigid transformations from translation , scale and rotation .", "ner": [["ST - GAN", "Method"], ["translation", "Task"], ["scale", "Task"], ["rotation", "Task"]], "rel": [["ST - GAN", "Used-For", "translation"], ["ST - GAN", "Used-For", "scale"], ["ST - GAN", "Used-For", "rotation"]], "rel_plus": [["ST - GAN:Method", "Used-For", "translation:Task"], ["ST - GAN:Method", "Used-For", "scale:Task"], ["ST - GAN:Method", "Used-For", "rotation:Task"]]}
{"doc_id": "211020570", "sentence": "In summary , we make the following contributions to the face alignment task : 1 ) A spatial transformer -generative adversarial network is proposed to produce promising initial face images for face alignment . 2 ) Based on the intensity of the heatmaps obtained by a two stage hourglass network , a scoring scheme is designed to measure the quality of predicted landmarks locations , which can estimate the occlusion level of each landmark and distinguish the aligned landmarks from misaligned landmarks . 3 ) An exemplar - based shape dictionary is employed to impose geometric constraints .", "ner": [["face alignment", "Task"], ["spatial transformer -generative adversarial network", "Method"], ["face alignment", "Task"], ["hourglass network", "Method"]], "rel": [["spatial transformer -generative adversarial network", "Used-For", "face alignment"]], "rel_plus": [["spatial transformer -generative adversarial network:Method", "Used-For", "face alignment:Task"]]}
{"doc_id": "211020570", "sentence": "The landmarks with high scores are used to search similar shapes from dictionary , and the landmarks with low scores are refined by shape reconstruction using similar shapes . 4 ) Experiment results on several benchmark datasets ( 3 0 0 - W , COFW and WFLW ) show that the proposed multistage model outperforms most recent face alignment methods , especially for faces with difficult scenarios such as large pose , lighting and occlusion , etc .", "ner": [["shape reconstruction", "Method"], ["3 0 0 - W", "Dataset"], ["COFW", "Dataset"], ["WFLW", "Dataset"], ["face alignment", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "211020570", "sentence": "In this section , we first review the development of face alignment , and then briefly review STNs .", "ner": [["face alignment", "Task"], ["STNs", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "211020570", "sentence": "According to the difference in facial representations , these methods can be divided into two categories : one is the holistic - based representation , such as active appearance model ( AAM ) [ 1 7 ] , the other is part - based representation , such as active shape model ( ASM ) [ 1 8 ] , constrained local model ( CLM ) [ 1 9 ] , Gauss - Newton deformable part model ( GN - DPM ) [ 2 0 ] .", "ner": [["holistic - based representation", "Method"], ["active appearance model", "Method"], ["AAM", "Method"], ["part - based representation", "Method"], ["active shape model", "Method"], ["ASM", "Method"], ["constrained local model", "Method"], ["CLM", "Method"], ["Gauss - Newton deformable part model", "Method"], ["GN - DPM", "Method"]], "rel": [["active appearance model", "SubClass-Of", "holistic - based representation"], ["AAM", "Synonym-Of", "active appearance model"], ["active shape model", "SubClass-Of", "part - based representation"], ["constrained local model", "SubClass-Of", "part - based representation"], ["Gauss - Newton deformable part model", "SubClass-Of", "part - based representation"], ["ASM", "Synonym-Of", "active shape model"], ["CLM", "Synonym-Of", "constrained local model"]], "rel_plus": [["active appearance model:Method", "SubClass-Of", "holistic - based representation:Method"], ["AAM:Method", "Synonym-Of", "active appearance model:Method"], ["active shape model:Method", "SubClass-Of", "part - based representation:Method"], ["constrained local model:Method", "SubClass-Of", "part - based representation:Method"], ["Gauss - Newton deformable part model:Method", "SubClass-Of", "part - based representation:Method"], ["ASM:Method", "Synonym-Of", "active shape model:Method"], ["CLM:Method", "Synonym-Of", "constrained local model:Method"]]}
{"doc_id": "211020570", "sentence": "The supervised descent method ( SDM ) [ 2 2 ] focuses on solving the optimization problem of the least squares method .", "ner": [["supervised descent method", "Method"], ["SDM", "Method"]], "rel": [["SDM", "Synonym-Of", "supervised descent method"]], "rel_plus": [["SDM:Method", "Synonym-Of", "supervised descent method:Method"]]}
{"doc_id": "211020570", "sentence": "The regression - based approach mentioned above employs the handcrafted feature descriptors ( e.g. , SIFT [ 2 2 ] , HoG [ 2 4 ] , or random forest/fern descriptors [ 1 1 ] ) to extract facial texture information .", "ner": [["handcrafted feature descriptors", "Method"], ["SIFT", "Method"], ["HoG", "Method"], ["random", "Method"]], "rel": [["SIFT", "SubClass-Of", "handcrafted feature descriptors"], ["HoG", "SubClass-Of", "handcrafted feature descriptors"], ["random", "SubClass-Of", "handcrafted feature descriptors"]], "rel_plus": [["SIFT:Method", "SubClass-Of", "handcrafted feature descriptors:Method"], ["HoG:Method", "SubClass-Of", "handcrafted feature descriptors:Method"], ["random:Method", "SubClass-Of", "handcrafted feature descriptors:Method"]]}
{"doc_id": "211020570", "sentence": "Recently , CNNs have made a series of breakthroughs in many visual analysis tasks such as image classification [ 2 6 ] , semantic segmentation [ 5 ] , and human pose estimation [ 6 ] .", "ner": [["CNNs", "Method"], ["visual analysis tasks", "Task"], ["image classification", "Task"], ["semantic segmentation", "Task"], ["human pose estimation", "Task"]], "rel": [["image classification", "SubTask-Of", "visual analysis tasks"], ["semantic segmentation", "SubTask-Of", "visual analysis tasks"], ["human pose estimation", "SubTask-Of", "visual analysis tasks"], ["CNNs", "Used-For", "visual analysis tasks"], ["CNNs", "Used-For", "image classification"], ["CNNs", "Used-For", "semantic segmentation"], ["CNNs", "Used-For", "human pose estimation"]], "rel_plus": [["image classification:Task", "SubTask-Of", "visual analysis tasks:Task"], ["semantic segmentation:Task", "SubTask-Of", "visual analysis tasks:Task"], ["human pose estimation:Task", "SubTask-Of", "visual analysis tasks:Task"], ["CNNs:Method", "Used-For", "visual analysis tasks:Task"], ["CNNs:Method", "Used-For", "image classification:Task"], ["CNNs:Method", "Used-For", "semantic segmentation:Task"], ["CNNs:Method", "Used-For", "human pose estimation:Task"]]}
{"doc_id": "211020570", "sentence": "The application of CNNs greatly boosts the performance of face alignment .", "ner": [["CNNs", "Method"], ["face alignment", "Task"]], "rel": [["CNNs", "Used-For", "face alignment"]], "rel_plus": [["CNNs:Method", "Used-For", "face alignment:Task"]]}
{"doc_id": "211020570", "sentence": "CNN - based methods can be generally classified into two categories : coordinate regression methods [ 2 7 ] - [ 2 9 ] and heatmap regression methods [ 3 0 ] - [ 3 5 ] .", "ner": [["CNN - based methods", "Method"], ["coordinate regression", "Method"], ["heatmap regression", "Method"]], "rel": [["coordinate regression", "SubClass-Of", "CNN - based methods"], ["heatmap regression", "SubClass-Of", "CNN - based methods"]], "rel_plus": [["coordinate regression:Method", "SubClass-Of", "CNN - based methods:Method"], ["heatmap regression:Method", "SubClass-Of", "CNN - based methods:Method"]]}
{"doc_id": "211020570", "sentence": "Sun et al. [ 2 7 ] first introduced CNNs to the face alignment field , and cascaded three CNNs to detect facial landmarks in a multistage manner .", "ner": [["CNNs", "Method"], ["face alignment", "Task"], ["CNNs", "Method"]], "rel": [["CNNs", "Used-For", "face alignment"]], "rel_plus": [["CNNs:Method", "Used-For", "face alignment:Task"]]}
{"doc_id": "211020570", "sentence": "Xiao et al. [ 2 9 ] proposed a framework that leverages the advantages of CNNs and recurrent neural networks ( RNNs ) .", "ner": [["CNNs", "Method"], ["recurrent neural networks", "Method"], ["RNNs", "Method"]], "rel": [["RNNs", "Synonym-Of", "recurrent neural networks"]], "rel_plus": [["RNNs:Method", "Synonym-Of", "recurrent neural networks:Method"]]}
{"doc_id": "211020570", "sentence": "The feature extraction stage is replaced with a CNN , and the fitting stage is replaced with an RNN .", "ner": [["feature extraction", "Method"], ["CNN", "Method"], ["fitting stage", "Task"], ["RNN", "Method"]], "rel": [["RNN", "Used-For", "fitting stage"]], "rel_plus": [["RNN:Method", "Used-For", "fitting stage:Task"]]}
{"doc_id": "211020570", "sentence": "Valle et al. [ 3 5 ] combined a CNN and ensemble of regression trees ( ERT ) to enhance computational efficiency .", "ner": [["CNN", "Method"], ["ensemble of regression trees", "Method"], ["ERT", "Method"]], "rel": [["ERT", "Synonym-Of", "ensemble of regression trees"]], "rel_plus": [["ERT:Method", "Synonym-Of", "ensemble of regression trees:Method"]]}
{"doc_id": "211020570", "sentence": "Jaderberg et al. [ 1 5 ] first presented STN that explicitly learns invariance to translation , scale and rotation .", "ner": [["STN", "Method"], ["translation", "Task"], ["scale", "Task"], ["rotation", "Task"]], "rel": [["STN", "Used-For", "translation"], ["STN", "Used-For", "scale"], ["STN", "Used-For", "rotation"]], "rel_plus": [["STN:Method", "Used-For", "translation:Task"], ["STN:Method", "Used-For", "scale:Task"], ["STN:Method", "Used-For", "rotation:Task"]]}
{"doc_id": "211020570", "sentence": "Benefiting from STN , they achieved stateof - the - art performance in several image classification tasks , such as MNIST [ 3 7 ] digit classification .", "ner": [["STN", "Method"], ["image classification", "Task"], ["MNIST", "Dataset"], ["digit classification", "Task"]], "rel": [["STN", "Used-For", "image classification"], ["MNIST", "Benchmark-For", "digit classification"], ["STN", "Used-For", "digit classification"]], "rel_plus": [["STN:Method", "Used-For", "image classification:Task"], ["MNIST:Dataset", "Benchmark-For", "digit classification:Task"], ["STN:Method", "Used-For", "digit classification:Task"]]}
{"doc_id": "211020570", "sentence": "In [ 3 8 ] , an STN was embedded in cascaded CNNs , to jointly learn spatial transformation and landmark localization for face detection .", "ner": [["STN", "Method"], ["cascaded CNNs", "Method"], ["spatial transformation", "Task"], ["landmark localization", "Task"], ["face detection", "Task"]], "rel": [["STN", "Part-Of", "cascaded CNNs"], ["cascaded CNNs", "Used-For", "spatial transformation"], ["cascaded CNNs", "Used-For", "landmark localization"], ["cascaded CNNs", "Used-For", "face detection"]], "rel_plus": [["STN:Method", "Part-Of", "cascaded CNNs:Method"], ["cascaded CNNs:Method", "Used-For", "spatial transformation:Task"], ["cascaded CNNs:Method", "Used-For", "landmark localization:Task"], ["cascaded CNNs:Method", "Used-For", "face detection:Task"]]}
{"doc_id": "211020570", "sentence": "In [ 3 9 ] , STN is applied to the task of image composition , and an STN is embedded in the generator of the generative adversarial network ( GAN ) for warping a specific object of a given image and placing it in the scene image .", "ner": [["STN", "Method"], ["image composition", "Task"], ["STN", "Method"], ["generator", "Method"], ["generative adversarial network", "Method"], ["GAN", "Method"]], "rel": [["STN", "Used-For", "image composition"], ["STN", "Part-Of", "generator"], ["GAN", "Synonym-Of", "generative adversarial network"], ["generator", "Part-Of", "generative adversarial network"]], "rel_plus": [["STN:Method", "Used-For", "image composition:Task"], ["STN:Method", "Part-Of", "generator:Method"], ["GAN:Method", "Synonym-Of", "generative adversarial network:Method"], ["generator:Method", "Part-Of", "generative adversarial network:Method"]]}
{"doc_id": "211020570", "sentence": "As illustrated in Fig. 1 , MSM consists of three pivotal steps : GAN - based spatial transformation , CNN - based landmark detection and exemplar - based shape reconstruction .", "ner": [["MSM", "Method"], ["GAN - based spatial transformation", "Method"], ["CNN - based landmark detection", "Method"], ["exemplar - based shape reconstruction", "Method"]], "rel": [["GAN - based spatial transformation", "Part-Of", "MSM"], ["CNN - based landmark detection", "Part-Of", "MSM"], ["exemplar - based shape reconstruction", "Part-Of", "MSM"]], "rel_plus": [["GAN - based spatial transformation:Method", "Part-Of", "MSM:Method"], ["CNN - based landmark detection:Method", "Part-Of", "MSM:Method"], ["exemplar - based shape reconstruction:Method", "Part-Of", "MSM:Method"]]}
{"doc_id": "211020570", "sentence": "There are two typical methods for facial image pre - processing : one is based on affine transformation , and the other is based on STNs .", "ner": [["affine transformation", "Task"], ["STNs", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "211020570", "sentence": "Affine transformation methods first detect several fiducial key points and then calculate the parameters of affine transformation by Procrustes analysis based on located key points and the key points of the mean face shape .", "ner": [["Affine transformation", "Task"], ["affine transformation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "211020570", "sentence": "It is obvious that affine transformation methods have the same limitations as the conventional face alignment algorithm , regarding sensitivity to occlusion and blur .", "ner": [["affine transformation", "Task"], ["face alignment", "Task"]], "rel": [["affine transformation", "Compare-With", "face alignment"]], "rel_plus": [["affine transformation:Task", "Compare-With", "face alignment:Task"]]}
{"doc_id": "211020570", "sentence": "STN - based methods explicitly learn image warping without key point detection , which is more flexible and robust than the affine transformation approach .", "ner": [["STN - based methods", "Method"], ["key point detection", "Task"], ["affine transformation approach", "Method"]], "rel": [["STN - based methods", "Compare-With", "affine transformation approach"]], "rel_plus": [["STN - based methods:Method", "Compare-With", "affine transformation approach:Method"]]}
{"doc_id": "211020570", "sentence": "As shown in Fig. 2 , the proposed spatial transformer -generative adversarial network ( ST - GAN ) consists of two parts : a generative deep neural network ( GDNN ) and a discriminative deep neural network ( DDNN ) .", "ner": [["spatial transformer -generative adversarial network", "Method"], ["ST - GAN", "Method"], ["generative deep neural network", "Method"], ["GDNN", "Method"], ["discriminative deep neural network", "Method"], ["DDNN", "Method"]], "rel": [["ST - GAN", "Synonym-Of", "spatial transformer -generative adversarial network"], ["GDNN", "Synonym-Of", "generative deep neural network"], ["DDNN", "Synonym-Of", "discriminative deep neural network"]], "rel_plus": [["ST - GAN:Method", "Synonym-Of", "spatial transformer -generative adversarial network:Method"], ["GDNN:Method", "Synonym-Of", "generative deep neural network:Method"], ["DDNN:Method", "Synonym-Of", "discriminative deep neural network:Method"]]}
{"doc_id": "211020570", "sentence": "Similar to original STN [ 1 5 ] , the generative deep neural network consists of three main components : a localization network , a grid generator and a sampler .", "ner": [["STN", "Method"], ["generative deep neural network", "Method"], ["localization network", "Method"], ["grid generator", "Method"], ["sampler", "Method"]], "rel": [["STN", "Compare-With", "generative deep neural network"], ["localization network", "Part-Of", "generative deep neural network"], ["grid generator", "Part-Of", "generative deep neural network"], ["sampler", "Part-Of", "generative deep neural network"]], "rel_plus": [["STN:Method", "Compare-With", "generative deep neural network:Method"], ["localization network:Method", "Part-Of", "generative deep neural network:Method"], ["grid generator:Method", "Part-Of", "generative deep neural network:Method"], ["sampler:Method", "Part-Of", "generative deep neural network:Method"]]}
{"doc_id": "211020570", "sentence": "The overall configuration of the proposed GDNN and DDNN are listed in Table I and Table II , respectively .", "ner": [["GDNN", "Method"], ["DDNN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "211020570", "sentence": "Each of the first 9 convolutional layers of the GDNN is of size 3 \u00d7 3 with different strides .", "ner": [["convolutional layers", "Method"], ["GDNN", "Method"]], "rel": [["convolutional layers", "Part-Of", "GDNN"]], "rel_plus": [["convolutional layers:Method", "Part-Of", "GDNN:Method"]]}
{"doc_id": "211020570", "sentence": "At the end , a 4 \u00d7 4 global average pooling layer and a 1 \u00d7 1 convolutional layer are utilized to regress the transformation matrix \u03b8 .", "ner": [["4 \u00d7 4 global average pooling layer", "Method"], ["1 \u00d7 1 convolutional layer", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "211020570", "sentence": "Each of the first 6 convolutional layers is of size 4 \u00d7 4 with stride 2 , the convolutional layer 7 is of size 2 \u00d7 2 with stride 1 .", "ner": [["convolutional layers", "Method"], ["convolutional layer", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "211020570", "sentence": "The loss function of discriminator DDNN is defined as follows ( for simplicity , GDNN is denoted as G , DDNN is denoted as D ): where I real refers to real sample which is the ground truth image without rotation , scale and unnecessary background .", "ner": [["discriminator DDNN", "Method"], ["GDNN", "Method"], ["G", "Method"], ["DDNN", "Method"], ["D", "Method"]], "rel": [["G", "Synonym-Of", "GDNN"], ["D", "Synonym-Of", "DDNN"]], "rel_plus": [["G:Method", "Synonym-Of", "GDNN:Method"], ["D:Method", "Synonym-Of", "DDNN:Method"]]}
{"doc_id": "211020570", "sentence": "Thus , GDNN is optimized to fool discriminator DDNN by regressing more accurate parameter that will improve the learning of the spatial transformation .", "ner": [["GDNN", "Method"], ["discriminator DDNN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "211020570", "sentence": "Stacked hourglass network [ 6 ] , which is a repeated encoder and decoder architecture , has proven to have some distinct advantages : 1 ) It is a simple , minimally designed network with the capability of capturing information at different scales ; 2 ) In a symmetrical topology , two feature maps with the same resolution are connected by skip connections to better maintain low - level information ; 3 ) There is a Recently , many work adopted four or eight hourglass modules as network backbone , but such strategy are computationally expensive for real - time applications .", "ner": [["Stacked hourglass network", "Method"], ["skip connections", "Method"], ["hourglass modules", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "211020570", "sentence": "A residual block can be expressed as follows : Where x n+ 1 and x n are the output and input feature maps of the n - th block , W n denotes the weights of convolutional layers .", "ner": [["residual block", "Method"], ["convolutional layers", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "211020570", "sentence": "F consists of batch normalization , ReLU is used for non linearity function , two 1 \u00d7 1 convolutional layers and a 3 \u00d7 3 convolutional layer , with an 1 \u00d7 1 skip convolutional layer are used to match different channels of input and output feature maps .", "ner": [["batch normalization", "Method"], ["ReLU", "Method"], ["1 \u00d7 1 convolutional layers", "Method"], ["3 \u00d7 3 convolutional layer", "Method"], ["1 \u00d7 1 skip convolutional layer", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "211020570", "sentence": "As shown in Fig. 3 , the input of the network is a face image normalized by the previous ST - GAN with a spatial resolution of 1 2 8 \u00d7 1 2 8 , followed by two 3 \u00d7 3 convolutional layers to increase the number of feature channels and a max pooling layer to decrease the resolution from 1 2 8 to 6 4 , through a 3 \u00d7 3 convolutional layer and a residual unit , the number of channels is increased to 2 5 6 . then the feature maps with 2 5 6 channels and 6 4 \u00d7 6 4 resolution are fed to the hourglass module .", "ner": [["ST - GAN", "Method"], ["\u00d7 3 convolutional layers", "Method"], ["max pooling layer", "Method"], ["3 \u00d7 3 convolutional layer", "Method"], ["hourglass module", "Method"]], "rel": [["\u00d7 3 convolutional layers", "Part-Of", "ST - GAN"], ["max pooling layer", "Part-Of", "ST - GAN"], ["3 \u00d7 3 convolutional layer", "Part-Of", "ST - GAN"]], "rel_plus": [["\u00d7 3 convolutional layers:Method", "Part-Of", "ST - GAN:Method"], ["max pooling layer:Method", "Part-Of", "ST - GAN:Method"], ["3 \u00d7 3 convolutional layer:Method", "Part-Of", "ST - GAN:Method"]]}
{"doc_id": "211020570", "sentence": "The hourglass module consists of a four - layer recursive structure , and each level consists of a downsampling layer , residual units , a skip connection layer and a deconvolutional layer . which uses upsampling layer to recover the size of the feature maps , deconvolution [ 4 0 ] is introduced to replace upsampling layers to better maintain spatial semantic information .", "ner": [["hourglass module", "Method"], ["downsampling layer", "Method"], ["residual units", "Method"], ["skip connection layer", "Method"], ["deconvolutional layer", "Method"], ["upsampling layer", "Method"], ["deconvolution", "Method"], ["upsampling layers", "Method"]], "rel": [["downsampling layer", "Part-Of", "hourglass module"], ["residual units", "Part-Of", "hourglass module"], ["skip connection layer", "Part-Of", "hourglass module"], ["deconvolutional layer", "Part-Of", "hourglass module"], ["upsampling layer", "Part-Of", "deconvolutional layer"], ["deconvolution", "Part-Of", "deconvolutional layer"]], "rel_plus": [["downsampling layer:Method", "Part-Of", "hourglass module:Method"], ["residual units:Method", "Part-Of", "hourglass module:Method"], ["skip connection layer:Method", "Part-Of", "hourglass module:Method"], ["deconvolutional layer:Method", "Part-Of", "hourglass module:Method"], ["upsampling layer:Method", "Part-Of", "deconvolutional layer:Method"], ["deconvolution:Method", "Part-Of", "deconvolutional layer:Method"]]}
{"doc_id": "211020570", "sentence": "Batch normalization is performed before all convolutional layers to accelerate convergence except for the first convolutional layer with 3 \u00d7 3 kernels .", "ner": [["Batch normalization", "Method"], ["convolutional layers", "Method"], ["convolutional layer", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "211020570", "sentence": "The location of each predicted landmark is decoded from corresponding heatmap by taking the location with the maximum value as follows : where l is the index of the landmark and the corresponding heatmap . c(l ) gives the coordinate of the l - th landmark .", "ner": [], "rel": [], "rel_plus": []}
{"doc_id": "211020570", "sentence": "Require : Face image I , face rectangle R , shape dictionary D S , threshold T . 1 : Crop I according to R , get facial part image I R . 2 : Feed I R to ST - GAN , get normalize face image I N and transform parameter \u03b8 . 3 : Feed I N to stacked hourglass network , get preliminary face shape S. 4 : Calculate weight w i by Eq. 9 for each landmark . 5 : for i = 1 to L do 6 :   In this section , we conduct extensive experiments and analysis to show the effectiveness of the proposed method .", "ner": [["ST - GAN", "Method"], ["stacked hourglass network", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "211020570", "sentence": "Our method is evaluated on several challenging datasets including 3 0 0 - W , COFW and WFLW . 1 ) 3 0 0 - W [ 2 5 ] : 3 0 0 - W is currently the most widely used dataset .", "ner": [["3 0 0 - W", "Dataset"], ["COFW", "Dataset"], ["WFLW", "Dataset"], ["3 0 0 - W", "Dataset"], ["3 0 0 - W", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "211020570", "sentence": "It was created from four datasets including the AFW [ 4 2 ] , LFPW [ 4 3 ] , HELEN [ 4 4 ] and IBUG [ 2 5 ] dataset , each face image is annotated with 6 8 landmarks .", "ner": [["AFW", "Dataset"], ["LFPW", "Dataset"], ["HELEN", "Dataset"], ["IBUG", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "211020570", "sentence": "The training set consists of the AFW , LFPW training set and HELEN training set , resulting in a total of 3 1 4 8 images .", "ner": [["AFW", "Dataset"], ["LFPW", "Dataset"], ["HELEN", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "211020570", "sentence": "The common set consists of the LPFW test set and HELEN test set , resulting in a total of 5 5 4 images .", "ner": [["LPFW", "Dataset"], ["HELEN", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "211020570", "sentence": "The full set consists of a common set and challenge set containing 6 8 9 images . 2 ) 3 0 0 - W private test set [ 4 5 ] : The 3 0 0 - W private test set was introduced after the 3 0 0 - W dataset and was used for the 3 0 0 - W Challenge benchmark .", "ner": [["3 0 0 - W", "Dataset"], ["3 0 0 - W", "Dataset"], ["3 0 0 - W", "Dataset"], ["3 0 0 - W", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "211020570", "sentence": "It consists of 3 0 0 indoor images and 3 0 0 outdoor images , each image was annotated 6 8 landmarks using the same annotation scheme as the one of 3 0 0 - W. 3 ) COFW [ 7 ] : The COFW dataset focuses on occlusion in nature .", "ner": [["3 0 0 - W.", "Dataset"], ["COFW", "Dataset"], ["COFW", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "211020570", "sentence": "In our experiment we use reannotated version [ 4 6 ] of the 6 8 landmarks for comparison to other approaches . 4 ) WFLW [ 3 4 ] : WFLW is considered the most challenging dataset .", "ner": [["WFLW", "Dataset"], ["WFLW", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "211020570", "sentence": "For the 3 0 0 - W , 3 0 0 - W test set and COFW dataset , image with an NRMSE ( inter - ocular ) of 0.0 8 or greater is considered a failure .", "ner": [["3 0 0 - W", "Dataset"], ["3 0 0 - W", "Dataset"], ["COFW", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "211020570", "sentence": "We independently trained three models : ST - GAN , stacked hourglass network and face shape dictionary .", "ner": [["ST - GAN", "Method"], ["stacked hourglass network", "Method"], ["face shape dictionary", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "211020570", "sentence": "In the face shape dictionary training procedure , the 3 0 0 - W training set and semifrontal face of the Menpo [ 4 7 ] dataset are used to train 6 8 - point face shape dictionaries .", "ner": [["face shape dictionary", "Method"], ["3 0 0 - W", "Dataset"], ["Menpo", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "211020570", "sentence": "Training the ST - GAN and stacked hourglass network took around 8 hours and 6 hours respectively .", "ner": [["ST - GAN", "Method"], ["stacked hourglass network", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "211020570", "sentence": "The Python implementation process images at 1 4 FPS on average , the CNN part ( the ST - GAN and stacked hourglass network ) took around 5 0 ms and the shape reconstruction took around 2 0 ms per image .", "ner": [["CNN part", "Method"], ["ST - GAN", "Method"], ["stacked hourglass network", "Method"], ["shape reconstruction", "Method"]], "rel": [["ST - GAN", "Part-Of", "CNN part"], ["stacked hourglass network", "Part-Of", "CNN part"]], "rel_plus": [["ST - GAN:Method", "Part-Of", "CNN part:Method"], ["stacked hourglass network:Method", "Part-Of", "CNN part:Method"]]}
{"doc_id": "211020570", "sentence": "In Table III , we compare our results with LBF [ 1 1 ] , TCDCN [ 4 8 ] , CFSS [ 4 9 ] , MDM [ 5 0 ] , RAR [ 2 9 ] , DAN [ 3 0 ] , TSR [ 1 2 ] , SHN [ 1 3 ] , LAB [ 3 4 ] , DCFE [ 3 5 ] , 3DDE [ 5 1 ] , PCD - CNN [ 5 2 ] , SAN [ 5 3 ] , DeCaFA [ 5 5 ] , AGCFN [ 5 6 ] and ODN [ 5 4 ] are also used in Table III .", "ner": [["LBF", "Method"], ["TCDCN", "Method"], ["CFSS", "Method"], ["MDM", "Method"], ["RAR", "Method"], ["DAN", "Method"], ["TSR", "Method"], ["SHN", "Method"], ["LAB", "Method"], ["DCFE", "Method"], ["3DDE", "Method"], ["PCD - CNN", "Method"], ["SAN", "Method"], ["DeCaFA", "Method"], ["AGCFN", "Method"], ["ODN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "211020570", "sentence": "First , we report the NRMSE results on 3 0 0 - W dataset of the proposed MSM method and those of other methods in Table III .", "ner": [["3 0 0 - W", "Dataset"], ["MSM", "Method"]], "rel": [["MSM", "Evaluated-With", "3 0 0 - W"]], "rel_plus": [["MSM:Method", "Evaluated-With", "3 0 0 - W:Dataset"]]}
{"doc_id": "211020570", "sentence": "For the Common Subset and Fullset of 3 0 0 - W , the inter - pupils NRMSE values of LAB is slightly better than those of the MSM .", "ner": [["3 0 0 - W", "Dataset"], ["LAB", "Method"], ["MSM", "Method"]], "rel": [["MSM", "Evaluated-With", "3 0 0 - W"], ["LAB", "Evaluated-With", "3 0 0 - W"], ["LAB", "Compare-With", "MSM"]], "rel_plus": [["MSM:Method", "Evaluated-With", "3 0 0 - W:Dataset"], ["LAB:Method", "Evaluated-With", "3 0 0 - W:Dataset"], ["LAB:Method", "Compare-With", "MSM:Method"]]}
{"doc_id": "211020570", "sentence": "However , the LAB is much more computational expensive due to a network architecture using eight stacked hourglass modules versus two stacked hourglass modules in the MSM .", "ner": [["LAB", "Method"], ["hourglass modules", "Method"], ["hourglass modules", "Method"], ["MSM", "Method"]], "rel": [["hourglass modules", "Part-Of", "LAB"], ["hourglass modules", "Part-Of", "MSM"], ["LAB", "Compare-With", "MSM"]], "rel_plus": [["hourglass modules:Method", "Part-Of", "LAB:Method"], ["hourglass modules:Method", "Part-Of", "MSM:Method"], ["LAB:Method", "Compare-With", "MSM:Method"]]}
{"doc_id": "211020570", "sentence": "For the Common Subset and Fullset of 3 0 0 - W , comparable inter - ocular NRMSE values are obtained by the 3DDE using a UNet - based network and MSM using two stacked hourglass modules in which MSM obtained slightly higher and slightly lower NRMSE values respectively in the [ 5 9 ] .", "ner": [["3 0 0 - W", "Dataset"], ["3DDE", "Method"], ["UNet - based", "Method"], ["MSM", "Method"], ["hourglass modules", "Method"], ["MSM", "Method"]], "rel": [["3DDE", "Evaluated-With", "3 0 0 - W"], ["MSM", "Evaluated-With", "3 0 0 - W"], ["UNet - based", "Part-Of", "3DDE"], ["hourglass modules", "Part-Of", "MSM"]], "rel_plus": [["3DDE:Method", "Evaluated-With", "3 0 0 - W:Dataset"], ["MSM:Method", "Evaluated-With", "3 0 0 - W:Dataset"], ["UNet - based:Method", "Part-Of", "3DDE:Method"], ["hourglass modules:Method", "Part-Of", "MSM:Method"]]}
{"doc_id": "211020570", "sentence": "Although 3 0 0 - W is the most widely used face alignment dataset , its small sample size and relatively simple face images limit its scope to be used for comprehensive evaluation on the performance of an algorithm under a broad range of conditions .   To evaluate the robustness to occlusion of the MSM method subject to various occluded face images , the COFW dataset is used which is regarded as a challenging dataset for existing state - of - the - art face alignment methods .", "ner": [["3 0 0 - W", "Dataset"], ["face alignment", "Task"], ["MSM", "Method"], ["COFW", "Dataset"], ["face alignment", "Task"]], "rel": [["3 0 0 - W", "Benchmark-For", "face alignment"], ["MSM", "Evaluated-With", "COFW"], ["MSM", "Used-For", "face alignment"], ["COFW", "Benchmark-For", "face alignment"]], "rel_plus": [["3 0 0 - W:Dataset", "Benchmark-For", "face alignment:Task"], ["MSM:Method", "Evaluated-With", "COFW:Dataset"], ["MSM:Method", "Used-For", "face alignment:Task"], ["COFW:Dataset", "Benchmark-For", "face alignment:Task"]]}
{"doc_id": "211020570", "sentence": "In Table V , various methods including RCPR , TCDCN , HPM [ 4 6 ] , CFSS , SHN , JMFA [ 3 3 ] , AGCFN and LAB are compared .", "ner": [["RCPR", "Method"], ["TCDCN", "Method"], ["HPM", "Method"], ["CFSS", "Method"], ["SHN", "Method"], ["JMFA", "Method"], ["AGCFN", "Method"], ["LAB", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "211020570", "sentence": "The MSM was trained on the 3 0 0 - W dataset with a total of 3 1 4 8 face training images .", "ner": [["MSM", "Method"], ["3 0 0 - W", "Dataset"]], "rel": [["MSM", "Trained-With", "3 0 0 - W"]], "rel_plus": [["MSM:Method", "Trained-With", "3 0 0 - W:Dataset"]]}
{"doc_id": "211020570", "sentence": "The NRMSE values for SHN and JMFA are slightly higher than those of the MSM method .", "ner": [["SHN", "Method"], ["JMFA", "Method"], ["MSM", "Method"]], "rel": [["SHN", "Compare-With", "MSM"], ["JMFA", "Compare-With", "MSM"]], "rel_plus": [["SHN:Method", "Compare-With", "MSM:Method"], ["JMFA:Method", "Compare-With", "MSM:Method"]]}
{"doc_id": "211020570", "sentence": "It should be noted that the training sets of both the SHN and the JMFA are much larger than that of the MSM in which the SHN and the JMFA include the 3 0 0 - W and Menpo [ 4 7 ] training sets , for a total of 9 3 6 0 face images , which is almost three times more images than that of the MSM .", "ner": [["SHN", "Method"], ["JMFA", "Method"], ["MSM", "Method"], ["SHN", "Method"], ["JMFA", "Method"], ["3 0 0 - W", "Dataset"], ["Menpo", "Dataset"], ["MSM", "Method"]], "rel": [["SHN", "Compare-With", "MSM"], ["JMFA", "Compare-With", "MSM"], ["SHN", "Trained-With", "3 0 0 - W"], ["JMFA", "Trained-With", "3 0 0 - W"], ["SHN", "Trained-With", "Menpo"], ["JMFA", "Trained-With", "Menpo"]], "rel_plus": [["SHN:Method", "Compare-With", "MSM:Method"], ["JMFA:Method", "Compare-With", "MSM:Method"], ["SHN:Method", "Trained-With", "3 0 0 - W:Dataset"], ["JMFA:Method", "Trained-With", "3 0 0 - W:Dataset"], ["SHN:Method", "Trained-With", "Menpo:Dataset"], ["JMFA:Method", "Trained-With", "Menpo:Dataset"]]}
{"doc_id": "211020570", "sentence": "Fig. 1 1 shows the CED curves which indicate the MSM outperforms other methods ( including SAPM [ 6 0 ] ) by a large margin on the COFW dataset .", "ner": [["MSM", "Method"], ["SAPM", "Method"], ["COFW", "Dataset"]], "rel": [["MSM", "Compare-With", "SAPM"], ["MSM", "Evaluated-With", "COFW"], ["SAPM", "Evaluated-With", "COFW"]], "rel_plus": [["MSM:Method", "Compare-With", "SAPM:Method"], ["MSM:Method", "Evaluated-With", "COFW:Dataset"], ["SAPM:Method", "Evaluated-With", "COFW:Dataset"]]}
{"doc_id": "211020570", "sentence": "The framework consists of several pivotal components including ST - GAN , stacked hourglass network and examplar - based face shape reconstruction .", "ner": [["ST - GAN", "Method"], ["stacked hourglass network", "Method"], ["examplar - based face shape reconstruction", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "211020570", "sentence": "Their effectiveness are validated within the framework based on the COFW and WFLW datasets .", "ner": [["COFW", "Dataset"], ["WFLW", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "211020570", "sentence": "To further evaluate the robustness of ST - GAN , a 5 0 - layer residual network ( Res - 5 0 ) is introduced to verify whether the ST - GAN is effective to coordinate regression - based method .", "ner": [["ST - GAN", "Method"], ["5 0 - layer residual network", "Method"], ["Res - 5 0", "Method"], ["ST - GAN", "Method"], ["coordinate regression - based method", "Method"]], "rel": [["5 0 - layer residual network", "Part-Of", "ST - GAN"], ["Res - 5 0", "Synonym-Of", "5 0 - layer residual network"], ["ST - GAN", "Compare-With", "coordinate regression - based method"]], "rel_plus": [["5 0 - layer residual network:Method", "Part-Of", "ST - GAN:Method"], ["Res - 5 0:Method", "Synonym-Of", "5 0 - layer residual network:Method"], ["ST - GAN:Method", "Compare-With", "coordinate regression - based method:Method"]]}
{"doc_id": "211020570", "sentence": "Since Res - 5 0 requires input images size of 2 2 4 \u00d7 2 2 4 , the size of the average pooling kernel in Res - 5 0 is resized from 7 to 4 , and the size of the network input is 1 2 8 \u00d7 1 2 8 .", "ner": [["Res - 5 0", "Method"], ["average pooling", "Method"], ["Res - 5 0", "Method"]], "rel": [["average pooling", "Part-Of", "Res - 5 0"]], "rel_plus": [["average pooling:Method", "Part-Of", "Res - 5 0:Method"]]}
{"doc_id": "211020570", "sentence": "Each proposed component was analyzed , i.e. , with ST - GAN ( labeled as ST - GAN ) , hourglass network ( labeled as HG ) , and shape reconstruction ( labeled as SR ) , by comparing their NRMSE and failure rates .", "ner": [["ST - GAN", "Method"], ["ST - GAN", "Method"], ["hourglass network", "Method"], ["HG", "Method"], ["shape reconstruction", "Method"], ["SR", "Method"]], "rel": [["HG", "Synonym-Of", "hourglass network"], ["ST - GAN", "Compare-With", "hourglass network"], ["SR", "Synonym-Of", "shape reconstruction"], ["ST - GAN", "Compare-With", "shape reconstruction"], ["hourglass network", "Compare-With", "shape reconstruction"]], "rel_plus": [["HG:Method", "Synonym-Of", "hourglass network:Method"], ["ST - GAN:Method", "Compare-With", "hourglass network:Method"], ["SR:Method", "Synonym-Of", "shape reconstruction:Method"], ["ST - GAN:Method", "Compare-With", "shape reconstruction:Method"], ["hourglass network:Method", "Compare-With", "shape reconstruction:Method"]]}
{"doc_id": "211020570", "sentence": "Note that our baseline is HG , and ST - GAN+HG+SR represents the full MSM method .", "ner": [["HG", "Method"], ["ST - GAN+HG+SR", "Method"], ["full MSM", "Method"]], "rel": [["full MSM", "Synonym-Of", "ST - GAN+HG+SR"]], "rel_plus": [["full MSM:Method", "Synonym-Of", "ST - GAN+HG+SR:Method"]]}
{"doc_id": "211020570", "sentence": "Table VII and Table VIII show the NRMSE values and failure rates obtained by different configurations of our framework evaluated on the COFW and WFLW datasets .", "ner": [["COFW", "Dataset"], ["WFLW", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "211020570", "sentence": "When combined with the ST - GAN , the Res - 5 0 network reduces the NRMSE from 4. 7 6 % to 4. 2 3 % , and the hourglass network decrease the NRMSE from 4. 6 4 % to 4. 3 4 % .", "ner": [["ST - GAN", "Method"], ["Res - 5 0", "Method"], ["hourglass network", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "211020570", "sentence": "This result demonstrates that the proposed ST - GAN method improved the performance of the face alignment task because STN can remove the translation , scale and rotation variation in each face , which can further reduce the variance in the regression target .", "ner": [["ST - GAN", "Method"], ["face alignment", "Task"], ["STN", "Method"], ["translation", "Task"], ["scale", "Task"], ["rotation", "Task"]], "rel": [["STN", "Part-Of", "ST - GAN"], ["ST - GAN", "Used-For", "face alignment"], ["STN", "Used-For", "translation"], ["STN", "Used-For", "scale"], ["STN", "Used-For", "rotation"]], "rel_plus": [["STN:Method", "Part-Of", "ST - GAN:Method"], ["ST - GAN:Method", "Used-For", "face alignment:Task"], ["STN:Method", "Used-For", "translation:Task"], ["STN:Method", "Used-For", "scale:Task"], ["STN:Method", "Used-For", "rotation:Task"]]}
{"doc_id": "211020570", "sentence": "Compared with the baseline ( HG ) of our work , the innovations introduced in this paper exhibit a certain improvement for each subset of the WFLW dataset .", "ner": [["HG", "Method"], ["WFLW", "Dataset"]], "rel": [["HG", "Evaluated-With", "WFLW"]], "rel_plus": [["HG:Method", "Evaluated-With", "WFLW:Dataset"]]}
{"doc_id": "211020570", "sentence": "In Fig. 1 3 , the CED curves show that ST - GAN+HG+SR which representing the full MSM method outperforms the other two configurations .", "ner": [["ST - GAN+HG+SR", "Method"], ["full MSM", "Method"]], "rel": [["full MSM", "Synonym-Of", "ST - GAN+HG+SR"]], "rel_plus": [["full MSM:Method", "Synonym-Of", "ST - GAN+HG+SR:Method"]]}
{"doc_id": "211020570", "sentence": "Examples of the outputs obtained by the proposed ST - GAN on the WFLW dataset are shown in Fig. 1 4 .", "ner": [["ST", "Method"], ["WFLW", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "211020570", "sentence": "Our method leverages the best advantages of STNs , CNNs and exemplar - based shape constraints .", "ner": [["STNs", "Method"], ["CNNs", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "211020570", "sentence": "Extensive experiments and ablation study have been conducted using challenging datasets ( 3 0 0 - W , COFW and WFLW ) , the experimental results and analysis have demonstrated the effectiveness of the proposed multistage model as compared to other state - of - the - art methods .", "ner": [["3 0 0 - W", "Dataset"], ["COFW", "Dataset"], ["WFLW", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "52180375", "sentence": "Unlike previous works that capture contexts by multi - scale features fusion , we propose a Dual Attention Networks ( DANet ) to adaptively integrate local features with their global dependencies .", "ner": [["Dual Attention Networks", "Method"], ["DANet", "Method"]], "rel": [["DANet", "Synonym-Of", "Dual Attention Networks"]], "rel_plus": [["DANet:Method", "Synonym-Of", "Dual Attention Networks:Method"]]}
{"doc_id": "52180375", "sentence": "We achieve new state - of - the - art segmentation performance on three challenging scene segmentation datasets , i.e. , Cityscapes , PASCAL Context and COCO Stuff dataset .", "ner": [["scene segmentation", "Task"], ["Cityscapes", "Dataset"], ["PASCAL Context", "Dataset"], ["COCO Stuff", "Dataset"]], "rel": [["Cityscapes", "Benchmark-For", "scene segmentation"], ["PASCAL Context", "Benchmark-For", "scene segmentation"], ["COCO Stuff", "Benchmark-For", "scene segmentation"]], "rel_plus": [["Cityscapes:Dataset", "Benchmark-For", "scene segmentation:Task"], ["PASCAL Context:Dataset", "Benchmark-For", "scene segmentation:Task"], ["COCO Stuff:Dataset", "Benchmark-For", "scene segmentation:Task"]]}
{"doc_id": "52180375", "sentence": "We make the code and trained model publicly available at https://github.com/junfu 1 1 1 5 /DANet Scene segmentation is a fundamental and challenging problem , whose goal is to segment and parse a scene image into different image regions associated with semantic categories including stuff ( e.g. sky , road , grass ) and discrete objects ( e.g. person , car , bicycle ) .", "ner": [["Scene segmentation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "52180375", "sentence": "Recently , state - of - the - art methods based on Fully Convolutional Networks ( FCNs ) [ 1 3 ] have been proposed to address the above issues .", "ner": [["Fully Convolutional Networks", "Method"], ["FCNs", "Method"]], "rel": [["FCNs", "Synonym-Of", "Fully Convolutional Networks"]], "rel_plus": [["FCNs:Method", "Synonym-Of", "Fully Convolutional Networks:Method"]]}
{"doc_id": "52180375", "sentence": "Another type of methods employs recurrent neural networks to exploit long - range dependencies , thus improving scene segmentation accuracy .", "ner": [["recurrent neural networks", "Method"], ["scene segmentation", "Task"]], "rel": [["recurrent neural networks", "Used-For", "scene segmentation"]], "rel_plus": [["recurrent neural networks:Method", "Used-For", "scene segmentation:Task"]]}
{"doc_id": "52180375", "sentence": "To address above problems , we propose a novel framework , called as Dual Attention Network ( DANet ) , for natural scene image segmentation , which is illustrated in Figure . 2 .", "ner": [["Dual Attention Network", "Method"], ["DANet", "Method"], ["image segmentation", "Task"]], "rel": [["DANet", "Synonym-Of", "Dual Attention Network"], ["Dual Attention Network", "Used-For", "image segmentation"]], "rel_plus": [["DANet:Method", "Synonym-Of", "Dual Attention Network:Method"], ["Dual Attention Network:Method", "Used-For", "image segmentation:Task"]]}
{"doc_id": "52180375", "sentence": "Our main contributions can be summarized as follows : \u2022 We propose a novel Dual Attention Network ( DANet ) with self - attention mechanism to enhance the discriminant ability of feature representations for scene segmentation . \u2022 A position attention module is proposed to learn the spatial interdependencies of features and a channel attention module is designed to model channel interdependencies .", "ner": [["Dual Attention Network", "Method"], ["DANet", "Method"], ["self - attention mechanism", "Method"], ["scene segmentation", "Task"], ["channel attention module", "Method"]], "rel": [["DANet", "Synonym-Of", "Dual Attention Network"], ["self - attention mechanism", "Part-Of", "Dual Attention Network"], ["Dual Attention Network", "Used-For", "scene segmentation"]], "rel_plus": [["DANet:Method", "Synonym-Of", "Dual Attention Network:Method"], ["self - attention mechanism:Method", "Part-Of", "Dual Attention Network:Method"], ["Dual Attention Network:Method", "Used-For", "scene segmentation:Task"]]}
{"doc_id": "52180375", "sentence": "It significantly improves the segmentation results by modeling rich contextual dependencies over local features . \u2022 We achieve new state - of - the - art results on three popular benchmarks including Cityscapes dataset [ 5 ] , PAS - CAL Context dataset [ 1 4 ] and COCO Stuff dataset [ 2 ] .", "ner": [["Cityscapes", "Dataset"], ["PAS - CAL Context", "Dataset"], ["COCO Stuff", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "52180375", "sentence": "Fully Convolutional Networks ( FCNs ) based methods have made great progress in semantic segmentation .", "ner": [["Fully Convolutional Networks", "Method"], ["FCNs", "Method"], ["semantic segmentation", "Task"]], "rel": [["FCNs", "Synonym-Of", "Fully Convolutional Networks"], ["Fully Convolutional Networks", "Used-For", "semantic segmentation"]], "rel_plus": [["FCNs:Method", "Synonym-Of", "Fully Convolutional Networks:Method"], ["Fully Convolutional Networks:Method", "Used-For", "semantic segmentation:Task"]]}
{"doc_id": "52180375", "sentence": "First , Deeplabv 2 [ 3 ] and Deeplabv 3 [ 4 ] adopt atrous spatial pyramid pooling to embed contextual information , which consist of parallel dilated convolutions with different dilated rates .", "ner": [["Deeplabv 2", "Method"], ["Deeplabv 3", "Method"], ["atrous spatial pyramid pooling", "Method"], ["dilated convolutions", "Method"]], "rel": [["atrous spatial pyramid pooling", "Part-Of", "Deeplabv 2"], ["atrous spatial pyramid pooling", "Part-Of", "Deeplabv 3"], ["dilated convolutions", "Part-Of", "atrous spatial pyramid pooling"]], "rel_plus": [["atrous spatial pyramid pooling:Method", "Part-Of", "Deeplabv 2:Method"], ["atrous spatial pyramid pooling:Method", "Part-Of", "Deeplabv 3:Method"], ["dilated convolutions:Method", "Part-Of", "atrous spatial pyramid pooling:Method"]]}
{"doc_id": "52180375", "sentence": "PSP - Net [ 2 9 ] designs a pyramid pooling module to collect the effective contextual prior , containing information of different scales .", "ner": [["PSP - Net", "Method"], ["pyramid pooling module", "Method"]], "rel": [["pyramid pooling module", "Part-Of", "PSP - Net"]], "rel_plus": [["pyramid pooling module:Method", "Part-Of", "PSP - Net:Method"]]}
{"doc_id": "52180375", "sentence": "PSANet [ 3 0 ] captures pixel - wise relation by a convolution layer and relative position information in spatial dimension .", "ner": [["PSANet", "Method"], ["convolution layer", "Method"]], "rel": [["convolution layer", "Part-Of", "PSANet"]], "rel_plus": [["convolution layer:Method", "Part-Of", "PSANet:Method"]]}
{"doc_id": "52180375", "sentence": "In addition , EncNet [ 2 7 ] introduces a channel attention mechanism to capture global context .", "ner": [["EncNet", "Method"], ["channel attention mechanism", "Method"]], "rel": [["channel attention mechanism", "Part-Of", "EncNet"]], "rel_plus": [["channel attention mechanism:Method", "Part-Of", "EncNet:Method"]]}
{"doc_id": "52180375", "sentence": "Noted that we remove the downsampling operations and employ dilated convolutions in the last two ResNet blocks , thus enlarging the size of the final feature map size to 1/ 8 of the input image .", "ner": [["dilated convolutions", "Method"], ["ResNet", "Method"]], "rel": [["dilated convolutions", "Part-Of", "ResNet"]], "rel_plus": [["dilated convolutions:Method", "Part-Of", "ResNet:Method"]]}
{"doc_id": "52180375", "sentence": "Take the spatial attention modules in the upper part of the Figure . 2 as an example , we first apply a convolution layer to obtain the features of dimension reduction .", "ner": [["spatial attention", "Method"], ["convolution layer", "Method"]], "rel": [["convolution layer", "Part-Of", "spatial attention"]], "rel_plus": [["convolution layer:Method", "Part-Of", "spatial attention:Method"]]}
{"doc_id": "52180375", "sentence": "After that we perform a matrix multiplication between the transpose of C and B , and apply a softmax layer to calculate the spatial attention map S \u2208 R N \u00d7N : where s ji measures the i th position 's impact on j th position .", "ner": [["softmax", "Method"], ["spatial attention", "Method"]], "rel": [["softmax", "Used-For", "spatial attention"]], "rel_plus": [["softmax:Method", "Used-For", "spatial attention:Method"]]}
{"doc_id": "52180375", "sentence": "Specifically , we reshape A to R C \u00d7 N , and then perform a matrix multiplication between A and the transpose of A. Finally , we apply a softmax layer to obtain the channel attention map X \u2208 R C \u00d7 C : where x ji measures the i th channel 's impact on the j th channel .", "ner": [["softmax", "Method"], ["channel attention", "Method"]], "rel": [["softmax", "Used-For", "channel attention"]], "rel_plus": [["softmax:Method", "Used-For", "channel attention:Method"]]}
{"doc_id": "52180375", "sentence": "To evaluate the proposed method , we carry out comprehensive experiments on Cityscapes dataset [ 5 ] , PAS - CAL VOC 2 0 1 2 [ 7 ] , PASCAL Context dataset [ 1 4 ] and COCO Stuff dataset [ 2 ] .", "ner": [["Cityscapes", "Dataset"], ["PAS - CAL VOC 2 0 1 2", "Dataset"], ["PASCAL Context", "Dataset"], ["COCO Stuff", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "52180375", "sentence": "Finally , we report our results on PASCAL VOC 2 0 1 2 , PASCAL Context and COCO Stuff .", "ner": [["PASCAL VOC 2 0 1 2", "Dataset"], ["PASCAL Context", "Dataset"], ["COCO Stuff", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "52180375", "sentence": "For data augmentation , we apply random cropping ( cropsize 7 6 8) and random left - right flipping during training in the ablation study for Cityscapes datasets .   We employ the dual attention modules on top of the dilation network to capture long - range dependencies for better scene understanding .", "ner": [["data augmentation", "Method"], ["random cropping", "Method"], ["Cityscapes", "Dataset"], ["dual attention modules", "Method"], ["scene understanding", "Task"]], "rel": [["random cropping", "SubClass-Of", "data augmentation"], ["dual attention modules", "Used-For", "scene understanding"]], "rel_plus": [["random cropping:Method", "SubClass-Of", "data augmentation:Method"], ["dual attention modules:Method", "Used-For", "scene understanding:Task"]]}
{"doc_id": "52180375", "sentence": "Furthermore , when we adopt a deeper pre - trained network ( ResNet - 1 0 1 ) , the network with two attention modules significantly improves the segmentation performance over the baseline model by 5. 0 3 % .", "ner": [["deeper pre - trained network", "Method"], ["ResNet - 1 0 1", "Method"]], "rel": [["ResNet - 1 0 1", "Part-Of", "deeper pre - trained network"]], "rel_plus": [["ResNet - 1 0 1:Method", "Part-Of", "deeper pre - trained network:Method"]]}
{"doc_id": "52180375", "sentence": "Following [ 4 ] , we adopt the same strategies to improve performance further . ( 1 ) DA : Data augmentation with random scaling . ( 2 ) Multi - Grid : we apply employ a hierarchy of grids of different sizes ( 4 , 8 , 1 6 ) in the last ResNet block . ( 3 ) MS : We average the segmentation probability maps from 8 image scales{ 0 . 5 0. 7 5 1 1. 2 5 1. 5 1. 7 5 2 2. 2 } for inference .", "ner": [["DA", "Method"], ["Data augmentation with random scaling", "Method"], ["Multi - Grid", "Method"], ["ResNet", "Method"], ["MS", "Method"]], "rel": [["DA", "Synonym-Of", "Data augmentation with random scaling"], ["Multi - Grid", "Used-For", "ResNet"]], "rel_plus": [["DA:Method", "Synonym-Of", "Data augmentation with random scaling:Method"], ["Multi - Grid:Method", "Used-For", "ResNet:Method"]]}
{"doc_id": "52180375", "sentence": "Data augmentation with random scaling improves the performance by almost 1. 2 6 % , which shows that network benefits from enriching scale diversity of training data .", "ner": [["Data augmentation with random scaling", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "52180375", "sentence": "Finally , segmentation map fusion further improves the performance to 8 1 . 5 0 % , which outperforms well - known method Deeplabv 3 [ 4 ] ( 7 9 . 3 0 % on Cityscape val set ) by 2. 2 0 % .    For position attention , the overall self - attention map is in size of ( H \u00d7 W ) \u00d7 ( H \u00d7 W ) , which means that for each specific point in the image , there is an corresponding subattention map whose size is ( H \u00d7 W ) .", "ner": [["segmentation map fusion", "Method"], ["Deeplabv 3", "Method"], ["Cityscape", "Dataset"], ["position attention", "Method"], ["self - attention map", "Method"]], "rel": [["Deeplabv 3", "Evaluated-With", "Cityscape"], ["self - attention map", "Part-Of", "position attention"]], "rel_plus": [["Deeplabv 3:Method", "Evaluated-With", "Cityscape:Dataset"], ["self - attention map:Method", "Part-Of", "position attention:Method"]]}
{"doc_id": "52180375", "sentence": "In short , these visualizations further demonstrate the necessity of capturing long - range dependencies for improving feature representation in scene segmentation . [ 1 5 ] 7 6 . 9 - [ 2 9 ] 7 8   We further compare our method with existing methods on the Cityscapes testing set .", "ner": [["scene segmentation", "Task"], ["Cityscapes", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "52180375", "sentence": "In particular , our model outperforms PSANet [ 3 0 ] by a large margin with the same backbone ResNet - 1 0 1 .", "ner": [["PSANet", "Method"], ["ResNet - 1 0 1", "Method"]], "rel": [["ResNet - 1 0 1", "Part-Of", "PSANet"]], "rel_plus": [["ResNet - 1 0 1:Method", "Part-Of", "PSANet:Method"]]}
{"doc_id": "52180375", "sentence": "Mean IoU% FCN [ 1 3 ] 6 2 . 2 DeepLab - v 2 (Res 1 0 1 - COCO ) [ 3 ] 7 1 . 6 Piecewise [ 1 1 ] 7 5 . 3 ResNet 3 8 [ 1 0 ] 8 2 . 5 PSPNet(Res 1 0 1 ) [ 2 9 ] 8 2 . 6 EncNet ( Res 1 0 1 ) [ 2 7 ] 8 2 . 9 DANet(Res 1 0 1 ) 8 2 . 6   We carry out experiments on the PASCAL VOC 2 0 1 2 dataset to further evaluate the effectiveness of our method .", "ner": [["FCN", "Method"], ["DeepLab - v 2", "Method"], ["(Res 1 0 1 - COCO", "Method"], ["Piecewise", "Method"], ["ResNet 3 8", "Method"], ["PSPNet(Res 1 0 1 )", "Method"], ["EncNet", "Method"], ["Res 1 0 1", "Method"], ["DANet(Res 1 0 1 )", "Method"], ["PASCAL VOC 2 0 1 2", "Dataset"]], "rel": [["(Res 1 0 1 - COCO", "Part-Of", "DeepLab - v 2"], ["Res 1 0 1", "Part-Of", "EncNet"]], "rel_plus": [["(Res 1 0 1 - COCO:Method", "Part-Of", "DeepLab - v 2:Method"], ["Res 1 0 1:Method", "Part-Of", "EncNet:Method"]]}
{"doc_id": "52180375", "sentence": "Our attention modules improves perfor - Mean IoU% FCN - 8 s [ 1 3 ] 3 7 . 8 Piecewise [ 1 1 ] 4 3 . 3 DeepLab - v 2 ( Res 1 0 1 - COCO ) [ 3 ] 4 5 . 7 RefineNet ( Res 1 5 2 ) [ 1 0 ] 4 7 . 3 PSPNet ( Res 1 0 1 ) [ 2 9 ] 4 7 . 8   In this subsection , we carry out experiments on the PAS - CAL Context dataset to further evaluate the effectiveness of our method .", "ner": [["FCN - 8 s", "Method"], ["Piecewise", "Method"], ["DeepLab - v 2", "Method"], ["Res 1 0 1 - COCO", "Method"], ["RefineNet", "Method"], ["Res 1 5 2", "Method"], ["PSPNet", "Method"], ["Res 1 0 1", "Method"], ["PAS - CAL Context", "Dataset"]], "rel": [["Res 1 0 1 - COCO", "Part-Of", "DeepLab - v 2"], ["Res 1 5 2", "Part-Of", "RefineNet"], ["Res 1 0 1", "Part-Of", "PSPNet"]], "rel_plus": [["Res 1 0 1 - COCO:Method", "Part-Of", "DeepLab - v 2:Method"], ["Res 1 5 2:Method", "Part-Of", "RefineNet:Method"], ["Res 1 0 1:Method", "Part-Of", "PSPNet:Method"]]}
{"doc_id": "52180375", "sentence": "Among previous works , Deeplab - v 2 and RefineNet adopt multi - scale feature fusion by different atrous convolution or different stage of encoder .", "ner": [["Deeplab - v 2", "Method"], ["RefineNet", "Method"], ["multi - scale feature fusion", "Method"], ["atrous convolution", "Method"]], "rel": [["multi - scale feature fusion", "Part-Of", "Deeplab - v 2"], ["atrous convolution", "Part-Of", "Deeplab - v 2"], ["multi - scale feature fusion", "Part-Of", "RefineNet"], ["atrous convolution", "Part-Of", "RefineNet"], ["atrous convolution", "Used-For", "multi - scale feature fusion"]], "rel_plus": [["multi - scale feature fusion:Method", "Part-Of", "Deeplab - v 2:Method"], ["atrous convolution:Method", "Part-Of", "Deeplab - v 2:Method"], ["multi - scale feature fusion:Method", "Part-Of", "RefineNet:Method"], ["atrous convolution:Method", "Part-Of", "RefineNet:Method"], ["atrous convolution:Method", "Used-For", "multi - scale feature fusion:Method"]]}
{"doc_id": "52180375", "sentence": "In addition , they trained their model with extra COCO data or adopt a deeper model ( ResNet 1 5 2 ) to improve their segmentation results .", "ner": [["COCO", "Dataset"], ["ResNet 1 5 2", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "52180375", "sentence": "Among the compared methods , DAG - RNN [ 1 8 ] utilizes chain - RNNs for 2D images to model rich spatial dependencies , and Ding et al. [ 6 ] adopts a gating mechanism in the decoder stage for improving inconspicuous objects Mean IoU% FCN - 8 s [ 1 3 ] 2 2 . 7 DeepLab - v 2 (Res 1 0 1 ) [ 3 ] 2 6 . 9 DAG - RNN [ 1 8 ] 3 1 . 2 RefineNet ( Res 1 0 1 ) [ 1 0 ] 3 3 . 6 Ding et al. ( Res 1 0 1 ) [ 6 ] 3 5 . 7 Dilated FCN ( Res 5 0 ) 3 1 . 9 DANet ( Res 5 0 ) 3 7 . 2 DANet ( Res 1 0 1 ) 3 9 . 7 and background stuff segmentation . our method could capture long - range contextual information more effectively and learn better feature representation in scene segmentation .", "ner": [["DAG - RNN", "Method"], ["RNNs", "Method"], ["FCN - 8 s", "Method"], ["DeepLab - v 2", "Method"], ["(Res 1 0 1", "Method"], ["DAG - RNN", "Method"], ["RefineNet", "Method"], ["Res 1 0 1", "Method"], ["Ding et al.", "Method"], ["Res 1 0 1", "Method"], ["Dilated FCN", "Method"], ["Res 5 0", "Method"], ["DANet", "Method"], ["Res 5 0", "Method"], ["DANet", "Method"], ["Res 1 0 1", "Method"], ["segmentation", "Task"], ["scene segmentation", "Task"]], "rel": [["(Res 1 0 1", "Part-Of", "DeepLab - v 2"], ["Res 1 0 1", "Part-Of", "RefineNet"], ["Res 1 0 1", "Part-Of", "Ding et al."], ["Res 5 0", "Part-Of", "Dilated FCN"], ["Res 5 0", "Part-Of", "DANet"], ["Res 1 0 1", "Part-Of", "DANet"]], "rel_plus": [["(Res 1 0 1:Method", "Part-Of", "DeepLab - v 2:Method"], ["Res 1 0 1:Method", "Part-Of", "RefineNet:Method"], ["Res 1 0 1:Method", "Part-Of", "Ding et al.:Method"], ["Res 5 0:Method", "Part-Of", "Dilated FCN:Method"], ["Res 5 0:Method", "Part-Of", "DANet:Method"], ["Res 1 0 1:Method", "Part-Of", "DANet:Method"]]}
{"doc_id": "52180375", "sentence": "In this paper , we have presented a Dual Attention Network ( DANet ) for scene segmentation , which adaptively integrates local semantic features using the self - attention mechanism .", "ner": [["Dual Attention Network", "Method"], ["DANet", "Method"], ["scene segmentation", "Task"]], "rel": [["DANet", "Synonym-Of", "Dual Attention Network"], ["Dual Attention Network", "Used-For", "scene segmentation"]], "rel_plus": [["DANet:Method", "Synonym-Of", "Dual Attention Network:Method"], ["Dual Attention Network:Method", "Used-For", "scene segmentation:Task"]]}
{"doc_id": "52180375", "sentence": "Our attention network achieves outstanding performance consistently on four scene segmentation datasets , i.e. Cityscapes , Pascal VOC 2 0 1 2 , Pascal Context , and COCO Stuff .", "ner": [["scene segmentation", "Task"], ["Cityscapes", "Dataset"], ["Pascal VOC 2 0 1 2", "Dataset"], ["Pascal Context", "Dataset"], ["COCO Stuff", "Dataset"]], "rel": [["COCO Stuff", "Benchmark-For", "scene segmentation"], ["Pascal Context", "Benchmark-For", "scene segmentation"], ["Pascal VOC 2 0 1 2", "Benchmark-For", "scene segmentation"], ["Cityscapes", "Benchmark-For", "scene segmentation"]], "rel_plus": [["COCO Stuff:Dataset", "Benchmark-For", "scene segmentation:Task"], ["Pascal Context:Dataset", "Benchmark-For", "scene segmentation:Task"], ["Pascal VOC 2 0 1 2:Dataset", "Benchmark-For", "scene segmentation:Task"], ["Cityscapes:Dataset", "Benchmark-For", "scene segmentation:Task"]]}
{"doc_id": "198231883", "sentence": "We address Unsupervised Video Object Segmentation ( UVOS ) , the task of automatically generating accurate pixel masks for salient objects in a video sequence and of tracking these objects consistently through time , without any input about which objects should be tracked .", "ner": [["Unsupervised Video Object Segmentation", "Task"], ["UVOS", "Task"]], "rel": [["UVOS", "Synonym-Of", "Unsupervised Video Object Segmentation"]], "rel_plus": [["UVOS:Task", "Synonym-Of", "Unsupervised Video Object Segmentation:Task"]]}
{"doc_id": "198231883", "sentence": "Towards solving this task , we present UnOVOST ( Unsupervised Offline Video Object Segmentation and Tracking ) as a simple and generic algorithm which is able to track and segment a large variety of objects .", "ner": [["UnOVOST", "Method"], ["Unsupervised Offline Video Object Segmentation and Tracking", "Method"]], "rel": [["UnOVOST", "Synonym-Of", "Unsupervised Offline Video Object Segmentation and Tracking"]], "rel_plus": [["UnOVOST:Method", "Synonym-Of", "Unsupervised Offline Video Object Segmentation and Tracking:Method"]]}
{"doc_id": "198231883", "sentence": "When evaluating our approach on the DAVIS 2 0 1 7 Unsupervised dataset we obtain state - of - the - art performance with a mean J & F score of 6 7 . 9 % on the val , 5 8 % on the test - dev and 5 6 . 4 % on the test - challenge benchmarks , obtaining first place in the DAVIS 2 0 1 9 Unsupervised Video Object Segmentation Challenge .", "ner": [["DAVIS 2 0 1 7", "Dataset"], ["DAVIS 2 0 1 9 Unsupervised Video Object Segmentation", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "198231883", "sentence": "UnOVOST even performs competitively with many semi - supervised video object segmentation algorithms even though it is not given any input as to which objects should be tracked and segmented .", "ner": [["UnOVOST", "Method"], ["semi - supervised video object segmentation", "Task"]], "rel": [["UnOVOST", "Used-For", "semi - supervised video object segmentation"]], "rel_plus": [["UnOVOST:Method", "Used-For", "semi - supervised video object segmentation:Task"]]}
{"doc_id": "198231883", "sentence": "Video Object Segmentation ( VOS ) aims at automatically generating accurate pixel masks for objects in each frame of a video , then associating those proposed object pixel masks in the successive frames to obtain temporally consistent tracks .", "ner": [["Video Object Segmentation", "Task"], ["VOS", "Task"]], "rel": [["VOS", "Synonym-Of", "Video Object Segmentation"]], "rel_plus": [["VOS:Task", "Synonym-Of", "Video Object Segmentation:Task"]]}
{"doc_id": "198231883", "sentence": "Example results of UnOVOST on three sequences from the DAVIS Unsupervised Dataset .", "ner": [["UnOVOST", "Method"], ["DAVIS Unsupervised", "Dataset"]], "rel": [["UnOVOST", "Evaluated-With", "DAVIS Unsupervised"]], "rel_plus": [["UnOVOST:Method", "Evaluated-With", "DAVIS Unsupervised:Dataset"]]}
{"doc_id": "198231883", "sentence": "One such example is in robotics and autonomous vehicles where it is of crucial importance to be able to understand the precise location and motion of a huge variety of objects , from far more categories than present in any labeled dataset .", "ner": [["robotics", "Task"], ["autonomous vehicles", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "198231883", "sentence": "To solve this UVOS task , we present the UnOVOST ( Unsupervised Offline Video Object Segmentation and Tracking ) algorithm .", "ner": [["UVOS", "Task"], ["UnOVOST", "Method"], ["Unsupervised Offline Video Object Segmentation and Tracking", "Method"]], "rel": [["UnOVOST", "Used-For", "UVOS"], ["UnOVOST", "Synonym-Of", "Unsupervised Offline Video Object Segmentation and Tracking"]], "rel_plus": [["UnOVOST:Method", "Used-For", "UVOS:Task"], ["UnOVOST:Method", "Synonym-Of", "Unsupervised Offline Video Object Segmentation and Tracking:Method"]]}
{"doc_id": "198231883", "sentence": "For the second - stage , we propose a novel Forest Path Cutting ( FPC ) algorithm .", "ner": [["Forest Path Cutting", "Method"], ["FPC", "Method"]], "rel": [["FPC", "Synonym-Of", "Forest Path Cutting"]], "rel_plus": [["FPC:Method", "Synonym-Of", "Forest Path Cutting:Method"]]}
{"doc_id": "198231883", "sentence": "When evaluating UnOVOST on the unsupervised DAVIS benchmark dataset [ 5 ] we achieve state - of - the - art results compared to all previous methods , as well as results competitive with semi - supervised methods using the given firstframe mask as guidance for which objects to track and segment .", "ner": [["UnOVOST", "Method"], ["unsupervised DAVIS benchmark", "Dataset"]], "rel": [["UnOVOST", "Evaluated-With", "unsupervised DAVIS benchmark"]], "rel_plus": [["UnOVOST:Method", "Evaluated-With", "unsupervised DAVIS benchmark:Dataset"]]}
{"doc_id": "198231883", "sentence": "Our method also achieves the first place in the DAVIS 2 0 1 9 Unsupervised Video Object Segmentation Challenge .", "ner": [["DAVIS 2 0 1 9 Unsupervised Video Object Segmentation", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "198231883", "sentence": "When extending our method to the task of Video Instance Segmentation ( VIS ) by adding classifying our object tracks , we also obtain state - of - the - art results on the YouTube - VIS benchmark and won the 2 0 1 9 YouTube - VIS challenge .", "ner": [["Video Instance Segmentation", "Task"], ["VIS", "Task"], ["YouTube - VIS", "Dataset"], ["2 0 1 9 YouTube - VIS challenge", "Dataset"]], "rel": [["VIS", "Synonym-Of", "Video Instance Segmentation"]], "rel_plus": [["VIS:Task", "Synonym-Of", "Video Instance Segmentation:Task"]]}
{"doc_id": "198231883", "sentence": "The UVOS task ( also known as zero - shot VOS ) is quite recent , and there are few methods that tackle this task .", "ner": [["UVOS", "Task"], ["zero - shot VOS", "Task"]], "rel": [["UVOS", "Synonym-Of", "zero - shot VOS"]], "rel_plus": [["UVOS:Task", "Synonym-Of", "zero - shot VOS:Task"]]}
{"doc_id": "198231883", "sentence": "UVOS [ 5 ] was proposed as a challenge task for the 2 0 1 9 DAVIS Challenge on Video Object Segmentation . [ 5 ] evaluate the RVOS ( Recurrent Video Object Segmentation ) [ 3 2 ] method for the UVOS task .", "ner": [["UVOS", "Task"], ["2 0 1 9 DAVIS Challenge", "Dataset"], ["Video Object Segmentation", "Task"], ["RVOS", "Method"], ["Recurrent Video Object Segmentation", "Method"], ["UVOS", "Task"]], "rel": [["2 0 1 9 DAVIS Challenge", "Benchmark-For", "UVOS"], ["UVOS", "SubTask-Of", "Video Object Segmentation"], ["2 0 1 9 DAVIS Challenge", "Benchmark-For", "Video Object Segmentation"], ["RVOS", "Synonym-Of", "Recurrent Video Object Segmentation"], ["RVOS", "Used-For", "UVOS"]], "rel_plus": [["2 0 1 9 DAVIS Challenge:Dataset", "Benchmark-For", "UVOS:Task"], ["UVOS:Task", "SubTask-Of", "Video Object Segmentation:Task"], ["2 0 1 9 DAVIS Challenge:Dataset", "Benchmark-For", "Video Object Segmentation:Task"], ["RVOS:Method", "Synonym-Of", "Recurrent Video Object Segmentation:Method"], ["RVOS:Method", "Used-For", "UVOS:Task"]]}
{"doc_id": "198231883", "sentence": "Our method , UnOVOST , outperforms RVOS by more than 2 5 percentage points on the J & F metric on all benchmarks .", "ner": [["UnOVOST", "Method"], ["RVOS", "Method"]], "rel": [["UnOVOST", "Compare-With", "RVOS"]], "rel_plus": [["UnOVOST:Method", "Compare-With", "RVOS:Method"]]}
{"doc_id": "198231883", "sentence": "The second [ 4 5 ] and third [ 7 ] place methods presented very different approaches to the UVOS task . [ 4 5 ] propose to run a detector on each frame , as well as a series of single object trackers used to merge the detections into tracks . [ 7 ] adapts [ 2 1 ] from semi - supervised VOS to UVOS task , while adding a proposal pruning step after a number of initial frames , and then tracking these objects as though this was a semisupervised task .", "ner": [["UVOS", "Task"], ["semi - supervised VOS", "Task"], ["UVOS", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "198231883", "sentence": "There has been a number of papers tackling singleobject unsupervised video object segmentation ( SOUVOS ) [ 1 7 , 1 8 , 1 3 , 3 0 , 3 1 ] .", "ner": [["singleobject unsupervised video object segmentation", "Task"], ["SOUVOS", "Task"]], "rel": [["SOUVOS", "Synonym-Of", "singleobject unsupervised video object segmentation"]], "rel_plus": [["SOUVOS:Task", "Synonym-Of", "singleobject unsupervised video object segmentation:Task"]]}
{"doc_id": "198231883", "sentence": "SOUVOS is closer related to foreground/background segmentation as it requires only one foreground area to be segmented which often is a grouping of multiple objects into one foreground object .", "ner": [["SOUVOS", "Task"], ["foreground/background segmentation", "Task"]], "rel": [["SOUVOS", "Compare-With", "foreground/background segmentation"]], "rel_plus": [["SOUVOS:Task", "Compare-With", "foreground/background segmentation:Task"]]}
{"doc_id": "198231883", "sentence": "This task differs from video object segmentation in that it only requires the segmentation of objects that are moving [ 2 ] , whereas UVOS requires the segmentation of all objects whether they are moving or not .", "ner": [["video object segmentation", "Task"], ["UVOS", "Task"]], "rel": [["video object segmentation", "Compare-With", "UVOS"]], "rel_plus": [["video object segmentation:Task", "Compare-With", "UVOS:Task"]]}
{"doc_id": "198231883", "sentence": "Motion segmentation methods [ 3 , 4 0 , 8 ] are often based on low - level vision features such as the optical - flow . [ 8 ] adapts Mask R - CNN [ 1 0 ] to operate on both image and optical - flow input . [ 4 0 ] extract features from the combination of the image and the optical - flow and clusters these . [ 3 ] develops a twostage model that estimates piece - wise rigid motions , which are then merged into objects .", "ner": [["Motion segmentation", "Task"], ["Mask R - CNN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "198231883", "sentence": "This is evaluated as either a multi - object task , or a foreground/background estimation task often using the FSMB [ 2 4 ] dataset .", "ner": [["foreground/background estimation task", "Task"], ["FSMB", "Dataset"]], "rel": [["FSMB", "Benchmark-For", "foreground/background estimation task"]], "rel_plus": [["FSMB:Dataset", "Benchmark-For", "foreground/background estimation task:Task"]]}
{"doc_id": "198231883", "sentence": "These tracklets are then merged into long - term consistent object tracks using the tracklets ' visual similarity and our novel Forest Path Cutting ( FPC ) data association algorithm . ( SSVOS ) is where the objects that need to be tracked are given as segmentation masks in the first frame .", "ner": [["Forest Path Cutting", "Method"], ["FPC", "Method"], ["SSVOS", "Task"]], "rel": [["FPC", "Synonym-Of", "Forest Path Cutting"]], "rel_plus": [["FPC:Method", "Synonym-Of", "Forest Path Cutting:Method"]]}
{"doc_id": "198231883", "sentence": "These methods are not able to be easily adapted to UVOS as they rely heavily on the first - frame mask . [ 2 1 ] is the closest related SSVOS to our method , as it also produces generic object segmentation proposals and links these in time with spatio - temporal and visual similarity cues .", "ner": [["UVOS", "Task"], ["SSVOS", "Task"], ["object segmentation", "Task"]], "rel": [["SSVOS", "SubTask-Of", "object segmentation"]], "rel_plus": [["SSVOS:Task", "SubTask-Of", "object segmentation:Task"]]}
{"doc_id": "198231883", "sentence": "SSVOS is often evaluated on the DAVIS 2 0 1 7 semi - supervised dataset .", "ner": [["SSVOS", "Task"], ["DAVIS 2 0 1 7 semi - supervised dataset", "Dataset"]], "rel": [["DAVIS 2 0 1 7 semi - supervised dataset", "Benchmark-For", "SSVOS"]], "rel_plus": [["DAVIS 2 0 1 7 semi - supervised dataset:Dataset", "Benchmark-For", "SSVOS:Task"]]}
{"doc_id": "198231883", "sentence": "Video Instance Segmentation and Multi - Object Tracking and Segmentation .", "ner": [["Video Instance Segmentation", "Task"], ["Multi - Object Tracking and Segmentation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "198231883", "sentence": "Recently , the related tasks of Video Instance Segmentation ( VIS ) [ 4 3 ] and Multi - Object Tracking and Segmentation ( MOTS ) [ 3 4 ] has been proposed .", "ner": [["Video Instance Segmentation", "Task"], ["VIS", "Task"], ["Multi - Object Tracking and Segmentation", "Task"], ["MOTS", "Task"]], "rel": [["VIS", "Synonym-Of", "Video Instance Segmentation"], ["MOTS", "Synonym-Of", "Multi - Object Tracking and Segmentation"]], "rel_plus": [["VIS:Task", "Synonym-Of", "Video Instance Segmentation:Task"], ["MOTS:Task", "Synonym-Of", "Multi - Object Tracking and Segmentation:Task"]]}
{"doc_id": "198231883", "sentence": "MOTS differs from VIS in that in MOTS sequences are much longer and many more instances are present with objects disappearing and reappearing much more often .", "ner": [["MOTS", "Task"], ["VIS", "Task"], ["MOTS", "Task"]], "rel": [["MOTS", "Compare-With", "VIS"]], "rel_plus": [["MOTS:Task", "Compare-With", "VIS:Task"]]}
{"doc_id": "198231883", "sentence": "MOTS is evaluated on the KITTI and MOTChallenge datasets [ 3 4 ] .", "ner": [["MOTS", "Task"], ["KITTI", "Dataset"], ["MOTChallenge datasets", "Dataset"]], "rel": [["KITTI", "Benchmark-For", "MOTS"], ["MOTChallenge datasets", "Benchmark-For", "MOTS"]], "rel_plus": [["KITTI:Dataset", "Benchmark-For", "MOTS:Task"], ["MOTChallenge datasets:Dataset", "Benchmark-For", "MOTS:Task"]]}
{"doc_id": "198231883", "sentence": "VIS on the YouTubeVIS benchmark [ 4 3 ] .", "ner": [["VIS", "Task"], ["YouTubeVIS", "Dataset"]], "rel": [["YouTubeVIS", "Benchmark-For", "VIS"]], "rel_plus": [["YouTubeVIS:Dataset", "Benchmark-For", "VIS:Task"]]}
{"doc_id": "198231883", "sentence": "We extend UnOVOST from the UVOS task to the VIS task by classifying our resulting tracks , and also achieve state - of - the - art performance on this task .", "ner": [["UnOVOST", "Method"], ["UVOS", "Task"], ["VIS", "Task"]], "rel": [["UnOVOST", "Used-For", "UVOS"], ["UnOVOST", "Used-For", "VIS"]], "rel_plus": [["UnOVOST:Method", "Used-For", "UVOS:Task"], ["UnOVOST:Method", "Used-For", "VIS:Task"]]}
{"doc_id": "198231883", "sentence": "The task of multi - object tracking ( MOT ) has a long research history [ 1 6 ] .", "ner": [["multi - object tracking", "Task"], ["MOT", "Task"]], "rel": [["MOT", "Synonym-Of", "multi - object tracking"]], "rel_plus": [["MOT:Task", "Synonym-Of", "multi - object tracking:Task"]]}
{"doc_id": "198231883", "sentence": "The leading paradigm for MOT has become trackingby - detection , where a set of object detections are proposed , and tracking is reduced to a data - association problem .", "ner": [["MOT", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "198231883", "sentence": "In this section we detail the specifics of our novel Un - OVOST algorithm for tackling the UVOS task .", "ner": [["Un - OVOST", "Method"], ["UVOS", "Task"]], "rel": [["Un - OVOST", "Used-For", "UVOS"]], "rel_plus": [["Un - OVOST:Method", "Used-For", "UVOS:Task"]]}
{"doc_id": "198231883", "sentence": "Specifically we use a Mask R - CNN [ 1 0 ] implementation by [ 3 7 ] with a ResNet 1 0 1 [ 1 1 ] backbone trained on COCO [ 2 0 ] .", "ner": [["Mask R - CNN", "Method"], ["ResNet 1 0 1", "Method"], ["COCO", "Dataset"]], "rel": [["ResNet 1 0 1", "Part-Of", "Mask R - CNN"], ["ResNet 1 0 1", "Trained-With", "COCO"]], "rel_plus": [["ResNet 1 0 1:Method", "Part-Of", "Mask R - CNN:Method"], ["ResNet 1 0 1:Method", "Trained-With", "COCO:Dataset"]]}
{"doc_id": "198231883", "sentence": "The proposal projection is the segmentation mask generated by warping a proposal by its corresponding optical flow vectors calculated using PWC - Net [ 2 9 ] .", "ner": [["optical flow", "Method"], ["PWC - Net", "Method"]], "rel": [["optical flow", "Part-Of", "PWC - Net"]], "rel_plus": [["optical flow:Method", "Part-Of", "PWC - Net:Method"]]}
{"doc_id": "198231883", "sentence": "To do this we introduce a novel Forest Path Cutting ( FPC ) algorithm .", "ner": [["Forest Path Cutting", "Method"], ["FPC", "Method"]], "rel": [["FPC", "Synonym-Of", "Forest Path Cutting"]], "rel_plus": [["FPC:Method", "Synonym-Of", "Forest Path Cutting:Method"]]}
{"doc_id": "198231883", "sentence": "This is a wide ResNet [ 3 8 ] trained with a batch - hard soft - margin version of the triplet loss .", "ner": [["wide ResNet", "Method"], ["triplet loss", "Method"]], "rel": [["triplet loss", "Part-Of", "wide ResNet"]], "rel_plus": [["triplet loss:Method", "Part-Of", "wide ResNet:Method"]]}
{"doc_id": "198231883", "sentence": "This is pretrained to distinguish classes on COCO [ 2 0 ] , before being trained to distinguish instances on YouTube - VOS [ 4 2 ] .", "ner": [["COCO", "Dataset"], ["YouTube - VOS", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "198231883", "sentence": "For this task we introduce our Forest Path Cutting ( FPC ) algorithm as can be seen in Algorithm 1 and Figure 4 .", "ner": [["Forest Path Cutting", "Method"], ["FPC", "Method"]], "rel": [["FPC", "Synonym-Of", "Forest Path Cutting"]], "rel_plus": [["FPC:Method", "Synonym-Of", "Forest Path Cutting:Method"]]}
{"doc_id": "198231883", "sentence": "This penalizes large temporal gaps between tracklets , making it more likely that objects undergoing short occlusion are correctly tracked , and ensures that the most salient objects are grouped consistently throughout the video , as objects to be Ours VSD [ 4 5 ] KIS [ 7 ] RVOS [ 3 2 ] U 1 7 tracked in UVOS are present in mostly all frames .", "ner": [["VSD", "Method"], ["KIS", "Method"], ["RVOS", "Method"], ["UVOS", "Task"]], "rel": [["VSD", "Used-For", "UVOS"], ["KIS", "Used-For", "UVOS"], ["RVOS", "Used-For", "UVOS"]], "rel_plus": [["VSD:Method", "Used-For", "UVOS:Task"], ["KIS:Method", "Used-For", "UVOS:Task"], ["RVOS:Method", "Used-For", "UVOS:Task"]]}
{"doc_id": "198231883", "sentence": "This video saliency metric prefers tracks Ours PReMVOS [ 2 1 ] DyeNet [ 1 9 ] FEELVOS [ 3 3 ] OSVOS - S [ 2 3 ] CINM [ 1 ] RGMP [ 3 9 ] OnAVOS [ 3 5 ] VideoMatch [ 1 2 ] OSVOS [ 4 ] FAVOS [ 6 ] SiamMask [ 3 6 ] OSMN [ 4 4 ] Figure 6 .", "ner": [["PReMVOS", "Method"], ["DyeNet", "Method"], ["FEELVOS", "Method"], ["OSVOS - S", "Method"], ["CINM", "Method"], ["RGMP", "Method"], ["OnAVOS", "Method"], ["VideoMatch", "Method"], ["OSVOS", "Method"], ["FAVOS", "Method"], ["SiamMask", "Method"], ["OSMN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "198231883", "sentence": "Quality versus timing plot comparing UnOVOST to state - of - the - art semi - supervised methods on DAVIS 1 7 val .", "ner": [["UnOVOST", "Method"], ["DAVIS 1 7 val", "Dataset"]], "rel": [["UnOVOST", "Evaluated-With", "DAVIS 1 7 val"]], "rel_plus": [["UnOVOST:Method", "Evaluated-With", "DAVIS 1 7 val:Dataset"]]}
{"doc_id": "198231883", "sentence": "We evaluate UnOVOST on the DAVIS 2 0 1 7 Unsupervised dataset [ 5 ] .", "ner": [["UnOVOST", "Method"], ["DAVIS 2 0 1 7 Unsupervised dataset", "Dataset"]], "rel": [["UnOVOST", "Evaluated-With", "DAVIS 2 0 1 7 Unsupervised dataset"]], "rel_plus": [["UnOVOST:Method", "Evaluated-With", "DAVIS 2 0 1 7 Unsupervised dataset:Dataset"]]}
{"doc_id": "198231883", "sentence": "The train and val sets contain the same videos as the DAVIS 2 0 1 7 semisupervised dataset , however they have been re - annotated according to the definition of the UVOS task .", "ner": [["DAVIS 2 0 1 7 semisupervised dataset", "Dataset"], ["UVOS", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "198231883", "sentence": "Table 1 shows our results on these three UVOS benchmarks , and compares our method to three other methods that have presented UVOS results .", "ner": [["UVOS", "Task"], ["UVOS", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "198231883", "sentence": "UnOVOST outperforms all other previous UVOS algorithms over all datasets , often by a large margin .", "ner": [["UnOVOST", "Method"], ["UVOS", "Task"]], "rel": [["UnOVOST", "Used-For", "UVOS"]], "rel_plus": [["UnOVOST:Method", "Used-For", "UVOS:Task"]]}
{"doc_id": "198231883", "sentence": "The test - dev set is significantly more difficult , and yet the UnOVOST algorithm can still perform extremely well , especially when compared to the performance of RVOS [ 3 2 ] .", "ner": [["UnOVOST", "Method"], ["RVOS", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "198231883", "sentence": "As well as comparing to other UVOS methods , we also compare our results on the DAVIS 2 0 1 7 val set to the current stateof - the - art semi - supervised VOS methods .", "ner": [["UVOS", "Task"], ["DAVIS 2 0 1 7 val set", "Dataset"], ["semi - supervised VOS", "Task"]], "rel": [["DAVIS 2 0 1 7 val set", "Benchmark-For", "UVOS"], ["DAVIS 2 0 1 7 val set", "Benchmark-For", "semi - supervised VOS"]], "rel_plus": [["DAVIS 2 0 1 7 val set:Dataset", "Benchmark-For", "UVOS:Task"], ["DAVIS 2 0 1 7 val set:Dataset", "Benchmark-For", "semi - supervised VOS:Task"]]}
{"doc_id": "198231883", "sentence": "We ablate the use of different similarity features for comparing tracklets , as well as the ReID vectors we compare to using last layer activations of pretrained ResNet 1 0 1 [ 1 1 ] and VGG [ 2 8 ] models trained on ImageNet .", "ner": [["ResNet 1 0 1", "Method"], ["VGG", "Method"], ["ImageNet", "Dataset"]], "rel": [["ResNet 1 0 1", "Trained-With", "ImageNet"], ["VGG", "Trained-With", "ImageNet"]], "rel_plus": [["ResNet 1 0 1:Method", "Trained-With", "ImageNet:Dataset"], ["VGG:Method", "Trained-With", "ImageNet:Dataset"]]}
{"doc_id": "198231883", "sentence": "The task of Video Instance Segmentation ( VIS ) is very similar to UVOS , however in VIS the objects to be tracked must be classified into a set of predefined classes rather than just being salient throughout a video .", "ner": [["Video Instance Segmentation", "Task"], ["VIS", "Task"], ["UVOS", "Task"], ["VIS", "Task"]], "rel": [["VIS", "Synonym-Of", "Video Instance Segmentation"], ["Video Instance Segmentation", "Compare-With", "UVOS"]], "rel_plus": [["VIS:Task", "Synonym-Of", "Video Instance Segmentation:Task"], ["Video Instance Segmentation:Task", "Compare-With", "UVOS:Task"]]}
{"doc_id": "198231883", "sentence": "To investigate the generalization of UnOVOST beyond the UVOS domain we run our algorithm on the YouTube - VIS dataset [ 4 3 ] after training our detector and segmentor on the set of 4 0 classes in this dataset and adding another classification network to im - prove classification results .", "ner": [["UnOVOST", "Method"], ["UVOS", "Task"], ["YouTube - VIS", "Dataset"]], "rel": [["UnOVOST", "Evaluated-With", "YouTube - VIS"]], "rel_plus": [["UnOVOST:Method", "Evaluated-With", "YouTube - VIS:Dataset"]]}
{"doc_id": "198231883", "sentence": "Apart from that we run Un - OVOST with exactly the same parameters as for the unsupervised DAVIS task .", "ner": [["Un - OVOST", "Method"], ["unsupervised DAVIS", "Dataset"]], "rel": [["Un - OVOST", "Evaluated-With", "unsupervised DAVIS"]], "rel_plus": [["Un - OVOST:Method", "Evaluated-With", "unsupervised DAVIS:Dataset"]]}
{"doc_id": "198231883", "sentence": "The previous state - of - the - art VIS method is MaskTrack R - CNN [ 4 3 ] , which achieves a mAP scores of 3 0 . 3 and 3 2 . 2 on the YouTube - VIS validation and test set respectively .", "ner": [["VIS", "Task"], ["MaskTrack R - CNN", "Method"], ["YouTube - VIS validation", "Dataset"]], "rel": [["MaskTrack R - CNN", "Used-For", "VIS"], ["YouTube - VIS validation", "Benchmark-For", "VIS"], ["YouTube - VIS validation", "Evaluated-With", "MaskTrack R - CNN"]], "rel_plus": [["MaskTrack R - CNN:Method", "Used-For", "VIS:Task"], ["YouTube - VIS validation:Dataset", "Benchmark-For", "VIS:Task"], ["YouTube - VIS validation:Dataset", "Evaluated-With", "MaskTrack R - CNN:Method"]]}
{"doc_id": "198231883", "sentence": "With these scores UnOVOST also won the 2 0 1 9 YouTube - VIS Challenge on Video Instance Segmentation , outperforming 1 8 other methods .", "ner": [["UnOVOST", "Method"], ["2 0 1 9 YouTube - VIS Challenge", "Dataset"], ["Video Instance Segmentation", "Task"]], "rel": [["UnOVOST", "Evaluated-With", "2 0 1 9 YouTube - VIS Challenge"], ["UnOVOST", "Used-For", "Video Instance Segmentation"], ["2 0 1 9 YouTube - VIS Challenge", "Benchmark-For", "Video Instance Segmentation"]], "rel_plus": [["UnOVOST:Method", "Evaluated-With", "2 0 1 9 YouTube - VIS Challenge:Dataset"], ["UnOVOST:Method", "Used-For", "Video Instance Segmentation:Task"], ["2 0 1 9 YouTube - VIS Challenge:Dataset", "Benchmark-For", "Video Instance Segmentation:Task"]]}
{"doc_id": "198231883", "sentence": "In this paper , we present the novel UnOVOST ( Unsupervised Offline Video Object Segmentation and Tracking ) algorithm for tackling the unsupervised video object segmentation task .", "ner": [["UnOVOST", "Method"], ["Unsupervised Offline Video Object Segmentation and Tracking", "Method"], ["unsupervised video object segmentation", "Task"]], "rel": [["UnOVOST", "Synonym-Of", "Unsupervised Offline Video Object Segmentation and Tracking"], ["UnOVOST", "Used-For", "unsupervised video object segmentation"]], "rel_plus": [["UnOVOST:Method", "Synonym-Of", "Unsupervised Offline Video Object Segmentation and Tracking:Method"], ["UnOVOST:Method", "Used-For", "unsupervised video object segmentation:Task"]]}
{"doc_id": "198231883", "sentence": "UnOVOST outperforms all previous UVOS methods , while even performing competitively with many semi - supervised video object segmentation algorithms without requiring any human input as to which objects should be tracked and segmented .   We adapt UnOVOST to the Video Instance Segmentation ( VIS ) domain is the following way .", "ner": [["UnOVOST", "Method"], ["UVOS", "Task"], ["semi - supervised video object segmentation", "Task"], ["UnOVOST", "Method"], ["Video Instance Segmentation", "Task"], ["VIS", "Task"]], "rel": [["UnOVOST", "Used-For", "UVOS"], ["UnOVOST", "Used-For", "semi - supervised video object segmentation"], ["VIS", "Synonym-Of", "Video Instance Segmentation"], ["UnOVOST", "Used-For", "Video Instance Segmentation"]], "rel_plus": [["UnOVOST:Method", "Used-For", "UVOS:Task"], ["UnOVOST:Method", "Used-For", "semi - supervised video object segmentation:Task"], ["VIS:Task", "Synonym-Of", "Video Instance Segmentation:Task"], ["UnOVOST:Method", "Used-For", "Video Instance Segmentation:Task"]]}
{"doc_id": "198231883", "sentence": "For detection we adapt the Mask R - CNN [ 1 0 ] detector to the YT - VIS benchmark to detect the 4 0 object classes .", "ner": [["Mask R - CNN", "Method"], ["YT - VIS", "Dataset"]], "rel": [["YT - VIS", "Evaluated-With", "Mask R - CNN"]], "rel_plus": [["YT - VIS:Dataset", "Evaluated-With", "Mask R - CNN:Method"]]}
{"doc_id": "198231883", "sentence": "To adapt this network to VIS , we created a training set by combining the YT - VIS [ 4 3 ] , COCO [ 2 0 ] and OpenImages [ 1 5 ] datasets .", "ner": [["VIS", "Task"], ["YT - VIS", "Dataset"], ["COCO", "Dataset"], ["OpenImages", "Dataset"]], "rel": [["OpenImages", "Benchmark-For", "VIS"], ["YT - VIS", "Benchmark-For", "VIS"], ["COCO", "Benchmark-For", "VIS"]], "rel_plus": [["OpenImages:Dataset", "Benchmark-For", "VIS:Task"], ["YT - VIS:Dataset", "Benchmark-For", "VIS:Task"], ["COCO:Dataset", "Benchmark-For", "VIS:Task"]]}
{"doc_id": "198231883", "sentence": "This is because OpenImages only has a class which is a mix , and because in the YT - VIS training set it is unclear exactly what the difference between these two classes should be ( e.g. baboons are labeled as both ape and monkey , some gorillas mislabeled as monkeys ) .", "ner": [["OpenImages", "Dataset"], ["YT - VIS training set", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "198231883", "sentence": "For COCO we use the 1 9 classes which overlap with the YT - VIS classes .", "ner": [["COCO", "Dataset"], ["YT - VIS", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "198231883", "sentence": "We map the OpenImages classes to YouTube - VIS classes , with all of our 3 9 classes being mapped to by at least one OpenImages class .", "ner": [["OpenImages", "Dataset"], ["YouTube - VIS", "Dataset"], ["OpenImages", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "198231883", "sentence": "We set all of the background of OpenImages images to be ignore regions and we do n't sample negatives from this dataset ( as OpenImages is not densely annotated ) .", "ner": [["OpenImages", "Dataset"], ["OpenImages", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "198231883", "sentence": "Also images form the YT - VIS dataset are sampled three times more often than those in COCO and OpenImages .", "ner": [["YT - VIS", "Dataset"], ["COCO", "Dataset"], ["OpenImages", "Dataset"]], "rel": [["YT - VIS", "Compare-With", "COCO"], ["YT - VIS", "Compare-With", "OpenImages"]], "rel_plus": [["YT - VIS:Dataset", "Compare-With", "COCO:Dataset"], ["YT - VIS:Dataset", "Compare-With", "OpenImages:Dataset"]]}
{"doc_id": "198231883", "sentence": "The classification branch our Mask R - CNN detector works reasonably well , but still often misclassifies examples .", "ner": [["Mask R - CNN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "198231883", "sentence": "To improve this , we use a ResNeXt - 1 0 1 3 2 x 4 8 d classifier [ 4 1 ] pretrained on 9 4 0 million Instagram images [ 2 2 ] , before being trained on ImageNet [ 9 ] .", "ner": [["ResNeXt - 1 0 1 3 2 x 4 8 d", "Method"], ["ImageNet", "Dataset"]], "rel": [["ResNeXt - 1 0 1 3 2 x 4 8 d", "Trained-With", "ImageNet"]], "rel_plus": [["ResNeXt - 1 0 1 3 2 x 4 8 d:Method", "Trained-With", "ImageNet:Dataset"]]}
{"doc_id": "198231883", "sentence": "We then defined a mapping of ImageNet ( INet ) classes to YT - VIS classes .", "ner": [["ImageNet", "Dataset"], ["INet", "Dataset"], ["YT - VIS", "Dataset"]], "rel": [["INet", "Synonym-Of", "ImageNet"]], "rel_plus": [["INet:Dataset", "Synonym-Of", "ImageNet:Dataset"]]}
{"doc_id": "198231883", "sentence": "This mapping results in 3 1 0 of the 1 0 0 0 INet classes being mapped to our 4 0 YT - VIS classes , with 1 2 3 INet classes being mapped to dog and 2 0 to truck .", "ner": [["INet", "Dataset"], ["YT - VIS", "Dataset"], ["INet", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "198231883", "sentence": "Some INet classes are mapped to multiple YT - VIS classes , e.g. \" Amphibious vehicle \" being mapped to both boat and truck .", "ner": [["INet", "Dataset"], ["YT - VIS", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "198231883", "sentence": "There are 1 1 INet classes mapped to just monkey , 2 to just ape and 7 to both due to the ambiguity in YT - VIS as to what is a ape and what is a monkey .", "ner": [["INet", "Dataset"], ["YT - VIS", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "198231883", "sentence": "The final INet classification score for each YT - VIS class is then the sum of the classification scores for all of the contributing INet classes .", "ner": [["INet", "Dataset"], ["YT - VIS", "Dataset"], ["INet", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "198231883", "sentence": "The final classification scores were then a weighted combination of the scores from our Mask R - CNN detector and our INet trained classifier .", "ner": [["Mask R - CNN", "Method"], ["INet", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "198231883", "sentence": "We finetune the segmentation head of Mask R - CNN on the YT - VIS dataset separately for the 4 0 classes .", "ner": [["Mask R - CNN", "Method"], ["YT - VIS", "Dataset"]], "rel": [["Mask R - CNN", "Trained-With", "YT - VIS"]], "rel_plus": [["Mask R - CNN:Method", "Trained-With", "YT - VIS:Dataset"]]}
{"doc_id": "198231883", "sentence": "We use UnOVOST exactly as in the main paper for unsupervised VOS with exactly the same parameters .", "ner": [["UnOVOST", "Method"], ["unsupervised VOS", "Task"]], "rel": [["UnOVOST", "Used-For", "unsupervised VOS"]], "rel_plus": [["UnOVOST:Method", "Used-For", "unsupervised VOS:Task"]]}
{"doc_id": "198231883", "sentence": "We do this for both detection scores and INet scores .", "ner": [["INet", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "201070697", "sentence": "In this paper , we introduce the attention mechanism directly to the generative adversarial network ( GAN ) architecture and propose a novel spatial attention GAN model ( SPA - GAN ) for image - to - image translation tasks .", "ner": [["generative adversarial network", "Method"], ["GAN", "Method"], ["spatial attention GAN model", "Method"], ["SPA - GAN", "Method"], ["image - to - image translation", "Task"]], "rel": [["GAN", "Synonym-Of", "generative adversarial network"], ["SPA - GAN", "Synonym-Of", "spatial attention GAN model"], ["generative adversarial network", "Used-For", "image - to - image translation"], ["spatial attention GAN model", "Used-For", "image - to - image translation"]], "rel_plus": [["GAN:Method", "Synonym-Of", "generative adversarial network:Method"], ["SPA - GAN:Method", "Synonym-Of", "spatial attention GAN model:Method"], ["generative adversarial network:Method", "Used-For", "image - to - image translation:Task"], ["spatial attention GAN model:Method", "Used-For", "image - to - image translation:Task"]]}
{"doc_id": "201070697", "sentence": "We also find it helpful to introduce an additional feature map loss in SPA - GAN training to preserve domain specific features during translation .", "ner": [["feature map loss", "Method"], ["SPA - GAN", "Method"]], "rel": [["feature map loss", "Part-Of", "SPA - GAN"]], "rel_plus": [["feature map loss:Method", "Part-Of", "SPA - GAN:Method"]]}
{"doc_id": "201070697", "sentence": "Compared with existing attention - guided GAN models , SPA - GAN is a lightweight model that does not need additional attention networks or supervision .", "ner": [["attention - guided GAN models", "Method"], ["SPA - GAN", "Method"]], "rel": [["SPA - GAN", "Compare-With", "attention - guided GAN models"]], "rel_plus": [["SPA - GAN:Method", "Compare-With", "attention - guided GAN models:Method"]]}
{"doc_id": "201070697", "sentence": "Image - to - image translation is to learn a mapping between images from a source domain and images from a target domain and has many applications including image colorization , generating semantic labels from images [ 1 ] , image super resolution [ 2 ] , [ 3 ] and domain adaptation [ 4 ] .", "ner": [["Image - to - image translation", "Task"], ["image colorization", "Task"], ["generating semantic labels from images", "Task"], ["image super resolution", "Task"], ["domain adaptation", "Task"]], "rel": [["Image - to - image translation", "Used-For", "image colorization"], ["Image - to - image translation", "Used-For", "generating semantic labels from images"], ["Image - to - image translation", "Used-For", "image super resolution"]], "rel_plus": [["Image - to - image translation:Task", "Used-For", "image colorization:Task"], ["Image - to - image translation:Task", "Used-For", "generating semantic labels from images:Task"], ["Image - to - image translation:Task", "Used-For", "image super resolution:Task"]]}
{"doc_id": "201070697", "sentence": "The task of locating areas of interest is more important in applications of image - to - image translation where the translation should be applied only to a particular type of object rather than the whole image .", "ner": [["image - to - image translation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "201070697", "sentence": "In this paper , we introduce the attention mechanism directly to the generative adversarial network ( GAN ) architecture and propose a novel spatial attention GAN model ( SPA - GAN ) for image - to - image translation .", "ner": [["generative adversarial network", "Method"], ["GAN", "Method"], ["spatial attention GAN model", "Method"], ["SPA - GAN", "Method"], ["image - to - image translation", "Task"]], "rel": [["GAN", "Synonym-Of", "generative adversarial network"], ["SPA - GAN", "Synonym-Of", "spatial attention GAN model"]], "rel_plus": [["GAN:Method", "Synonym-Of", "generative adversarial network:Method"], ["SPA - GAN:Method", "Synonym-Of", "spatial attention GAN model:Method"]]}
{"doc_id": "201070697", "sentence": "Based on the proposed attention mechanism , we used a modified cycle consistency loss during SPA - GAN training and also introduced a generator feature map loss to preserve domain specific features . \u2022 Earlier approaches on attention - guided image - to - image translation [ 7 ] , [ 8 ] require loading generators , discriminators and additional attention networks into the GPU memory all at once , which may cause computational and memory limitations .", "ner": [["attention mechanism", "Method"], ["cycle consistency loss", "Method"], ["SPA - GAN", "Method"], ["generator feature map loss", "Method"], ["image - to - image translation", "Task"], ["generators", "Method"], ["discriminators", "Method"]], "rel": [["cycle consistency loss", "Part-Of", "SPA - GAN"], ["generator feature map loss", "Part-Of", "SPA - GAN"]], "rel_plus": [["cycle consistency loss:Method", "Part-Of", "SPA - GAN:Method"], ["generator feature map loss:Method", "Part-Of", "SPA - GAN:Method"]]}
{"doc_id": "201070697", "sentence": "In comparison , SPA - GAN is a lightweight model that does not need additional attention networks or supervision during training . \u2022 SPA - GAN demonstrates the effectiveness of directly incorporating the attention mechanism into GAN models .", "ner": [["SPA - GAN", "Method"], ["SPA - GAN", "Method"], ["attention mechanism", "Method"], ["GAN", "Method"]], "rel": [["attention mechanism", "Part-Of", "GAN"]], "rel_plus": [["attention mechanism:Method", "Part-Of", "GAN:Method"]]}
{"doc_id": "201070697", "sentence": "Through extensive experiments , we show that , both qualitatively and quantitatively , SPA - GAN significantly outperforms other state - of - the - art image - to - image translation methods .", "ner": [["SPA - GAN", "Method"], ["image - to - image translation", "Task"]], "rel": [["SPA - GAN", "Used-For", "image - to - image translation"]], "rel_plus": [["SPA - GAN:Method", "Used-For", "image - to - image translation:Task"]]}
{"doc_id": "201070697", "sentence": "Finally , we conclude in Section V.   Recently , GAN - based methods have been widely used in image - to - image translation and produced appealing results .", "ner": [["GAN - based methods", "Method"], ["image - to - image translation", "Task"]], "rel": [["GAN - based methods", "Used-For", "image - to - image translation"]], "rel_plus": [["GAN - based methods:Method", "Used-For", "image - to - image translation:Task"]]}
{"doc_id": "201070697", "sentence": "In pix 2 pix [ 1 ] , conditional GAN ( cGAN ) was used to learn a mapping from an input image to an output image ; cGAN learns a conditional generative model using paired images from source and target domains .", "ner": [["pix 2 pix", "Task"], ["conditional GAN", "Method"], ["cGAN", "Method"], ["cGAN", "Method"], ["conditional generative model", "Method"]], "rel": [["conditional GAN", "Used-For", "pix 2 pix"], ["cGAN", "Synonym-Of", "conditional GAN"], ["conditional generative model", "Part-Of", "cGAN"]], "rel_plus": [["conditional GAN:Method", "Used-For", "pix 2 pix:Task"], ["cGAN:Method", "Synonym-Of", "conditional GAN:Method"], ["conditional generative model:Method", "Part-Of", "cGAN:Method"]]}
{"doc_id": "201070697", "sentence": "CycleGAN was proposed by Zhu et al. [ 5 ] for image - to - image translation tasks in the absence of paired examples .", "ner": [["CycleGAN", "Method"], ["image - to - image translation", "Task"]], "rel": [["CycleGAN", "Used-For", "image - to - image translation"]], "rel_plus": [["CycleGAN:Method", "Used-For", "image - to - image translation:Task"]]}
{"doc_id": "201070697", "sentence": "Similarly , DiscoGAN [ 1 1 ] and DualGAN [ 6 ] use an unsupervised learning approach for image - to - image translation based on unpaired data , but with different loss functions .", "ner": [["DiscoGAN", "Method"], ["DualGAN", "Method"], ["unsupervised learning", "Method"], ["image - to - image translation", "Task"]], "rel": [["unsupervised learning", "Used-For", "DiscoGAN"], ["unsupervised learning", "Used-For", "DualGAN"], ["DiscoGAN", "Used-For", "image - to - image translation"], ["DualGAN", "Used-For", "image - to - image translation"]], "rel_plus": [["unsupervised learning:Method", "Used-For", "DiscoGAN:Method"], ["unsupervised learning:Method", "Used-For", "DualGAN:Method"], ["DiscoGAN:Method", "Used-For", "image - to - image translation:Task"], ["DualGAN:Method", "Used-For", "image - to - image translation:Task"]]}
{"doc_id": "201070697", "sentence": "Liu et al. [ 1 2 ] proposed unsupervised image - to - image translation network ( UNIT ) based on Coupled GANs [ 1 0 ] and a shared - latent space assumption , which assumes a pair of corresponding images from different domains can be mapped to the same latent representation in a shared - latent space .", "ner": [["unsupervised image - to - image translation network", "Method"], ["UNIT", "Method"], ["Coupled GANs", "Method"]], "rel": [["UNIT", "Synonym-Of", "unsupervised image - to - image translation network"], ["Coupled GANs", "Part-Of", "unsupervised image - to - image translation network"]], "rel_plus": [["UNIT:Method", "Synonym-Of", "unsupervised image - to - image translation network:Method"], ["Coupled GANs:Method", "Part-Of", "unsupervised image - to - image translation network:Method"]]}
{"doc_id": "201070697", "sentence": "Huang et al. [ 1 3 ] proposed multimodal unsupervised image - to - image translation framework ( MUNIT ) that assumes two latent representations for style and content .", "ner": [["multimodal unsupervised image - to - image translation framework", "Method"], ["MUNIT", "Method"]], "rel": [["MUNIT", "Synonym-Of", "multimodal unsupervised image - to - image translation framework"]], "rel_plus": [["MUNIT:Method", "Synonym-Of", "multimodal unsupervised image - to - image translation framework:Method"]]}
{"doc_id": "201070697", "sentence": "Similarly , Lee et al. [ 1 7 ] introduced diverse image - to - image translation ( DRIT ) based on disentangled representation on unpaired data that decomposes the latent space into two space : a domain - invariant content space capturing shared information and a domain - specific attribute space to produce diverse outputs given the same content .", "ner": [["diverse image - to - image translation", "Method"], ["DRIT", "Method"]], "rel": [["DRIT", "Synonym-Of", "diverse image - to - image translation"]], "rel_plus": [["DRIT:Method", "Synonym-Of", "diverse image - to - image translation:Method"]]}
{"doc_id": "201070697", "sentence": "Recently , HarmonicGAN proposed by Zhang et al. [ 1 5 ] for unpaired image - to - image translation , introduces spatial smoothing to enforce consistent mappings during translation .", "ner": [["HarmonicGAN", "Method"], ["image - to - image translation", "Task"]], "rel": [["HarmonicGAN", "Used-For", "image - to - image translation"]], "rel_plus": [["HarmonicGAN:Method", "Used-For", "image - to - image translation:Task"]]}
{"doc_id": "201070697", "sentence": "InstaGAN [ 1 6 ] utilizes the object segmentation masks as extra supervision to perform multiinstance domain - to - domain image translation .", "ner": [["InstaGAN", "Method"], ["object segmentation", "Task"], ["multiinstance domain - to - domain image translation", "Task"]], "rel": [["object segmentation", "Used-For", "InstaGAN"], ["InstaGAN", "Used-For", "multiinstance domain - to - domain image translation"]], "rel_plus": [["object segmentation:Task", "Used-For", "InstaGAN:Method"], ["InstaGAN:Method", "Used-For", "multiinstance domain - to - domain image translation:Task"]]}
{"doc_id": "201070697", "sentence": "Inspired from human attention mechanism [ 1 8 ] , attentionbased models have gained popularity in a variety of computer vision and machine learning tasks including neural machine translation [ 1 9 ] , image classification [ 2 0 ] , [ 2 1 ] , image segmentation [ 2 2 ] , image and video captioning [ 2 3 ] , [ 2 4 ] and visual question answering [ 2 5 ] .", "ner": [["attention mechanism", "Method"], ["attentionbased models", "Method"], ["computer vision", "Task"], ["neural machine translation", "Task"], ["image classification", "Task"], ["image segmentation", "Task"], ["image and video captioning", "Task"], ["visual question answering", "Task"]], "rel": [["attentionbased models", "Used-For", "computer vision"], ["attentionbased models", "Used-For", "neural machine translation"], ["attentionbased models", "Used-For", "image classification"], ["attentionbased models", "Used-For", "image segmentation"], ["attentionbased models", "Used-For", "image and video captioning"], ["attentionbased models", "Used-For", "visual question answering"]], "rel_plus": [["attentionbased models:Method", "Used-For", "computer vision:Task"], ["attentionbased models:Method", "Used-For", "neural machine translation:Task"], ["attentionbased models:Method", "Used-For", "image classification:Task"], ["attentionbased models:Method", "Used-For", "image segmentation:Task"], ["attentionbased models:Method", "Used-For", "image and video captioning:Task"], ["attentionbased models:Method", "Used-For", "visual question answering:Task"]]}
{"doc_id": "201070697", "sentence": "Zhou et al. [ 2 6 ] produce attention maps for each class by removing top average - pooling layer and improving object localization accuracy .", "ner": [["average - pooling", "Method"], ["object localization", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "201070697", "sentence": "Zagoruyko et al. [ 9 ] improve the performance of a student convolutional neural network ( CNN ) by transferring the attention from a teacher CNN .", "ner": [["convolutional neural network", "Method"], ["CNN", "Method"], ["CNN", "Method"]], "rel": [["CNN", "Synonym-Of", "convolutional neural network"]], "rel_plus": [["CNN:Method", "Synonym-Of", "convolutional neural network:Method"]]}
{"doc_id": "201070697", "sentence": "Wang et . al. [ 2 1 ] propose a residual attention network for image classification with a trunk - and - mask attention mechanism .", "ner": [["residual attention network", "Method"], ["image classification", "Task"], ["trunk - and - mask attention mechanism", "Method"]], "rel": [["trunk - and - mask attention mechanism", "Part-Of", "residual attention network"], ["residual attention network", "Used-For", "image classification"]], "rel_plus": [["trunk - and - mask attention mechanism:Method", "Part-Of", "residual attention network:Method"], ["residual attention network:Method", "Used-For", "image classification:Task"]]}
{"doc_id": "201070697", "sentence": "Recent studies show that incorporation of attention learning in GAN models leads to more realistic images in both image generation and image - to - image translation tasks .", "ner": [["GAN", "Method"], ["image generation", "Task"], ["image - to - image translation", "Task"]], "rel": [["GAN", "Used-For", "image generation"], ["GAN", "Used-For", "image - to - image translation"]], "rel_plus": [["GAN:Method", "Used-For", "image generation:Task"], ["GAN:Method", "Used-For", "image - to - image translation:Task"]]}
{"doc_id": "201070697", "sentence": "For example , Zhang et al. [ 2 8 ] propose self - attention GAN that uses a self - attention mechanism for image generation .", "ner": [["self - attention GAN", "Method"], ["self - attention mechanism", "Method"], ["image generation", "Task"]], "rel": [["self - attention mechanism", "Part-Of", "self - attention GAN"], ["self - attention GAN", "Used-For", "image generation"]], "rel_plus": [["self - attention mechanism:Method", "Part-Of", "self - attention GAN:Method"], ["self - attention GAN:Method", "Used-For", "image generation:Task"]]}
{"doc_id": "201070697", "sentence": "Fig. 1 shows the main components of SPA - GAN and compares it to the CycleGAN model with no feedback attention .", "ner": [["SPA - GAN", "Method"], ["CycleGAN", "Method"]], "rel": [["SPA - GAN", "Compare-With", "CycleGAN"]], "rel_plus": [["SPA - GAN:Method", "Compare-With", "CycleGAN:Method"]]}
{"doc_id": "201070697", "sentence": "While CycleGAN is trained using the adversarial and cycle consistency losses , SPA - GAN integrates the adversarial , modified cycle consistency and feature map losses to generate more realistic outputs .", "ner": [["CycleGAN", "Method"], ["SPA - GAN", "Method"]], "rel": [["CycleGAN", "Compare-With", "SPA - GAN"]], "rel_plus": [["CycleGAN:Method", "Compare-With", "SPA - GAN:Method"]]}
{"doc_id": "201070697", "sentence": "To overcome this , Cycle consistency loss is proposed in CycleGAN [ 5 ] to measure the discrepancy between the input image x and the image F(G(x ) ) generated by the inverse mapping that translates the input image back to the original domain space .", "ner": [["Cycle consistency loss", "Method"], ["CycleGAN", "Method"]], "rel": [["Cycle consistency loss", "Part-Of", "CycleGAN"]], "rel_plus": [["Cycle consistency loss:Method", "Part-Of", "CycleGAN:Method"]]}
{"doc_id": "201070697", "sentence": "Similar to CycleGAN , we take advantage of cycle consistency loss to achieve one - to - one correspondence mapping .", "ner": [["CycleGAN", "Method"], ["cycle consistency loss", "Method"]], "rel": [["cycle consistency loss", "Part-Of", "CycleGAN"]], "rel_plus": [["cycle consistency loss:Method", "Part-Of", "CycleGAN:Method"]]}
{"doc_id": "201070697", "sentence": "Finally , by combining the adversarial loss , modified cycle consistency loss and the generator feature map loss , the full objective function of SPA - GAN is expressed as : where \u03bb cyc and \u03bb f m control the importance of different terms , and we aim to solve the following min - max problem : IV .", "ner": [["cycle consistency loss", "Method"], ["SPA - GAN", "Method"]], "rel": [["cycle consistency loss", "Part-Of", "SPA - GAN"]], "rel_plus": [["cycle consistency loss:Method", "Part-Of", "SPA - GAN:Method"]]}
{"doc_id": "201070697", "sentence": "We evaluate SPA - GAN on the Horse\u2194 Zebra , Apple \u2194 Orange datasets provided in [ 5 ] and the Lion \u2194 Tiger dataset downloaded from ImageNet [ 3 0 ] , which consists 2 , 0 8 6 images for tigers and 1 , 7 9 5 images for lions .", "ner": [["SPA - GAN", "Method"], ["ImageNet", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "201070697", "sentence": "We also evaluate SPA - GAN on image - to - image translation tasks that require to translate the whole image , e.g. , Winter\u2194Summer in [ 5 ] and gender conversion on the Facescrub [ 3 1 ] dataset .", "ner": [["SPA - GAN", "Method"], ["image - to - image translation", "Task"], ["Facescrub", "Dataset"]], "rel": [["SPA - GAN", "Used-For", "image - to - image translation"]], "rel_plus": [["SPA - GAN:Method", "Used-For", "image - to - image translation:Task"]]}
{"doc_id": "201070697", "sentence": "Different from [ 7 ] , [ 8 ] that add additional attention networks to the CycleGAN framework , SPA - GAN does not include any additional attention network or supervision , and its training time is similar to CycleGAN .", "ner": [["CycleGAN", "Method"], ["SPA - GAN", "Method"], ["attention network", "Method"], ["CycleGAN", "Method"]], "rel": [["CycleGAN", "Compare-With", "SPA - GAN"]], "rel_plus": [["CycleGAN:Method", "Compare-With", "SPA - GAN:Method"]]}
{"doc_id": "201070697", "sentence": "CycleGAN adopts GAN with cycle consistency loss for unpaired image - to - image translation task [ 5 ] .", "ner": [["CycleGAN", "Method"], ["GAN", "Method"], ["cycle consistency loss", "Method"], ["image - to - image translation", "Task"]], "rel": [["GAN", "Part-Of", "CycleGAN"], ["cycle consistency loss", "Part-Of", "GAN"], ["CycleGAN", "Used-For", "image - to - image translation"]], "rel_plus": [["GAN:Method", "Part-Of", "CycleGAN:Method"], ["cycle consistency loss:Method", "Part-Of", "GAN:Method"], ["CycleGAN:Method", "Used-For", "image - to - image translation:Task"]]}
{"doc_id": "201070697", "sentence": "An unsupervised dual learning framework for image to image translation on unlabeled images from two domains that uses Wasserstein GAN loss rather than the sigmoid cross - entropy loss [ 6 ] .", "ner": [["unsupervised dual learning framework", "Method"], ["image to image translation", "Task"], ["Wasserstein GAN loss", "Method"], ["sigmoid cross - entropy loss", "Method"]], "rel": [["Wasserstein GAN loss", "Part-Of", "unsupervised dual learning framework"], ["unsupervised dual learning framework", "Used-For", "image to image translation"]], "rel_plus": [["Wasserstein GAN loss:Method", "Part-Of", "unsupervised dual learning framework:Method"], ["unsupervised dual learning framework:Method", "Used-For", "image to image translation:Task"]]}
{"doc_id": "201070697", "sentence": "The number of output style is set to 1 in our experiments for both MUNIT and DRIT [ 1 7 ] .", "ner": [["MUNIT", "Method"], ["DRIT", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "201070697", "sentence": "AGGAN [ 7 ] and Attention - GAN . [ 8 ] Similar unsupervised image - to - image translation methods with added attention networks .", "ner": [["AGGAN", "Method"], ["Attention - GAN", "Method"], ["unsupervised image - to - image translation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "201070697", "sentence": "Since the code of Attention - GAN [ 8 ] is not released , and it was outperformed by AGGAN [ 7 ] , we did the comparison with AGGAN only .", "ner": [["Attention - GAN", "Method"], ["AGGAN", "Method"], ["AGGAN", "Method"]], "rel": [["Attention - GAN", "Compare-With", "AGGAN"]], "rel_plus": [["Attention - GAN:Method", "Compare-With", "AGGAN:Method"]]}
{"doc_id": "201070697", "sentence": "It has been recently used for performance evaluation of imageto - image translation and image generation models [ 7 ] , [ 3 4 ] .", "ner": [["imageto - image translation", "Task"], ["image generation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "201070697", "sentence": "In our experiment , we fine - tuned the inception network [ 3 8 ] pretrained on ImageNet [ 3 0 ] for each translation and report the top - 1 classification performance on the images generated by each method .", "ner": [["inception network", "Method"], ["ImageNet", "Dataset"], ["classification", "Task"]], "rel": [["inception network", "Trained-With", "ImageNet"], ["inception network", "Used-For", "classification"]], "rel_plus": [["inception network:Method", "Trained-With", "ImageNet:Dataset"], ["inception network:Method", "Used-For", "classification:Task"]]}
{"doc_id": "201070697", "sentence": "In this case , our model is reduced to the CycleGAN architecture ( CycleGAN ) .", "ner": [["CycleGAN", "Method"], ["CycleGAN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "201070697", "sentence": "Next , we feed the spatial attention from the discriminator to the generator in CycleGAN but without the generator feature map loss ( SPA - GAN - wo - L f m ) .", "ner": [["spatial attention", "Method"], ["discriminator", "Method"], ["generator", "Method"], ["CycleGAN", "Method"], ["generator feature map loss", "Method"], ["SPA - GAN - wo - L f m", "Method"]], "rel": [["spatial attention", "Part-Of", "discriminator"], ["discriminator", "Part-Of", "CycleGAN"], ["generator", "Part-Of", "CycleGAN"]], "rel_plus": [["spatial attention:Method", "Part-Of", "discriminator:Method"], ["discriminator:Method", "Part-Of", "CycleGAN:Method"], ["generator:Method", "Part-Of", "CycleGAN:Method"]]}
{"doc_id": "201070697", "sentence": "Comparison between the attention maps generated by the attention network in AGGAN [ 7 ] and the generated attention maps computed in the discriminator of our SPA - GAN model ( third row ) on different datasets .", "ner": [["attention network", "Method"], ["AGGAN", "Method"], ["discriminator", "Method"], ["SPA - GAN", "Method"]], "rel": [["attention network", "Part-Of", "AGGAN"], ["discriminator", "Part-Of", "SPA - GAN"]], "rel_plus": [["attention network:Method", "Part-Of", "AGGAN:Method"], ["discriminator:Method", "Part-Of", "SPA - GAN:Method"]]}
{"doc_id": "201070697", "sentence": "AGGAN generates the disconnected attention map for zebra while SPA - GAN attends on the discriminative regions of zebra ( the first column ) .", "ner": [["AGGAN", "Method"], ["SPA - GAN", "Method"]], "rel": [["AGGAN", "Compare-With", "SPA - GAN"]], "rel_plus": [["AGGAN:Method", "Compare-With", "SPA - GAN:Method"]]}
{"doc_id": "201070697", "sentence": "In Column 4 , AGGAN attends on the whole oranges while SPA - GAN generates the attention map with higher values around the boundaries and the top of the oranges . we also compared maximum - based attention ( SPA - GAN - A max ) with sum - based attention adopted in SPA - GAN .", "ner": [["AGGAN", "Method"], ["SPA - GAN", "Method"], ["maximum - based attention", "Method"], ["SPA - GAN - A max", "Method"], ["sum - based attention", "Method"], ["SPA - GAN", "Method"]], "rel": [["AGGAN", "Compare-With", "SPA - GAN"], ["maximum - based attention", "Part-Of", "SPA - GAN - A max"], ["sum - based attention", "Part-Of", "SPA - GAN"]], "rel_plus": [["AGGAN:Method", "Compare-With", "SPA - GAN:Method"], ["maximum - based attention:Method", "Part-Of", "SPA - GAN - A max:Method"], ["sum - based attention:Method", "Part-Of", "SPA - GAN:Method"]]}
{"doc_id": "201070697", "sentence": "Each row from top to bottom are the input images , attention maps computed in the discriminator of our SPA - GAN model and the attention maps generated by the attention network in AGGAN [ 7 ] , respectively .", "ner": [["discriminator", "Method"], ["SPA - GAN", "Method"], ["attention network", "Method"], ["AGGAN", "Method"]], "rel": [["discriminator", "Part-Of", "SPA - GAN"], ["attention network", "Part-Of", "AGGAN"]], "rel_plus": [["discriminator:Method", "Part-Of", "SPA - GAN:Method"], ["attention network:Method", "Part-Of", "AGGAN:Method"]]}
{"doc_id": "201070697", "sentence": "In the orange \u2192 apple translation , the SPA - GAN attention map computed in the discriminator focuses on both the shape and texture of the generated and real apple images in order to correctly classify the input image .", "ner": [["orange \u2192 apple translation", "Task"], ["SPA - GAN", "Method"]], "rel": [["SPA - GAN", "Used-For", "orange \u2192 apple translation"]], "rel_plus": [["SPA - GAN:Method", "Used-For", "orange \u2192 apple translation:Task"]]}
{"doc_id": "201070697", "sentence": "In this example , the SPA - GAN spatial attention map has higher values around the boundaries and on the top part of the oranges while AGGAN attends on the whole oranges .", "ner": [["SPA - GAN", "Method"], ["AGGAN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "201070697", "sentence": "In all rows of Fig. 3 , DRIT , CycleGAN and AGGAN only changed the color of the objects and do n't succeed in translating shape differences between apple and orange domains .", "ner": [["DRIT", "Method"], ["CycleGAN", "Method"], ["AGGAN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "201070697", "sentence": "As shown in Fig. 4 , DualGAN , UNIT , MUNIT and DRIT altered the background of the input image .", "ner": [["DualGAN", "Method"], ["UNIT", "Method"], ["MUNIT", "Method"], ["DRIT", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "201070697", "sentence": "CycleGAN and AGGAN generate visually better results and preserve the input background .", "ner": [["CycleGAN", "Method"], ["AGGAN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "201070697", "sentence": "For example , CycleGAN does n't succeed to translate the head of zebra in row 1 and 4 while AGGAN misses the body or the head of the animal for row 1 , 3 and 4 .", "ner": [["CycleGAN", "Method"], ["AGGAN", "Method"]], "rel": [["CycleGAN", "Compare-With", "AGGAN"]], "rel_plus": [["CycleGAN:Method", "Compare-With", "AGGAN:Method"]]}
{"doc_id": "201070697", "sentence": "The generated objects by CycleGAN and AGGAN are mixed with parts from the target as well as the source domain .", "ner": [["CycleGAN", "Method"], ["AGGAN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "201070697", "sentence": "In the tiger \u2192 lion translation , all other methods kept some tiger patterns after translation .", "ner": [["tiger \u2192 lion translation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "201070697", "sentence": "SPA - GAN is more successful in generating tiger pattern in lion \u2192 tiger translation ( row 8 and 9 ) compared to other methods .", "ner": [["SPA - GAN", "Method"], ["lion \u2192 tiger translation", "Task"]], "rel": [["SPA - GAN", "Used-For", "lion \u2192 tiger translation"]], "rel_plus": [["SPA - GAN:Method", "Used-For", "lion \u2192 tiger translation:Task"]]}
{"doc_id": "201070697", "sentence": "This clearly shows that the apples generated by AGGAN still maintain a higher level of feature similarity to real oranges when compared to SPA - GAN .", "ner": [["AGGAN", "Method"], ["SPA - GAN", "Method"]], "rel": [["AGGAN", "Compare-With", "SPA - GAN"]], "rel_plus": [["AGGAN:Method", "Compare-With", "SPA - GAN:Method"]]}
{"doc_id": "201070697", "sentence": "Finally , we evaluated SPA - GAN on image - to - image translation datasets that require to translate the whole image .", "ner": [["SPA - GAN", "Method"], ["image - to - image translation", "Task"]], "rel": [["SPA - GAN", "Used-For", "image - to - image translation"]], "rel_plus": [["SPA - GAN:Method", "Used-For", "image - to - image translation:Task"]]}
{"doc_id": "201070697", "sentence": "The spatial attention maps obtained from the discriminator in these two datasets clearly demonstrate the effectiveness of SPA - GAN in a variety of image - to - image translation tasks .", "ner": [["SPA - GAN", "Method"], ["image - to - image translation", "Task"]], "rel": [["SPA - GAN", "Used-For", "image - to - image translation"]], "rel_plus": [["SPA - GAN:Method", "Used-For", "image - to - image translation:Task"]]}
{"doc_id": "201070697", "sentence": "Please see the supplementary material for more visual examples on Facescrub and GTA [ 3 9 ] \u2194 Cityscapes [ 4 0 ] datasets .", "ner": [["Facescrub", "Dataset"], ["GTA", "Dataset"], ["Cityscapes", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "201070697", "sentence": "In this paper , we proposed SPA - GAN for image - to - image translation in unsupervised settings .", "ner": [["SPA - GAN", "Method"], ["image - to - image translation", "Task"]], "rel": [["SPA - GAN", "Used-For", "image - to - image translation"]], "rel_plus": [["SPA - GAN:Method", "Used-For", "image - to - image translation:Task"]]}
{"doc_id": "201070697", "sentence": "DRIT , CycleGAN and AAGAN only changed the color of the objects and do not succeed in translating shape differences between apple and orange domains .", "ner": [["DRIT", "Method"], ["CycleGAN", "Method"], ["AAGAN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "201070697", "sentence": "DRIT , CycleGAN and AAGAN only changed the color of the objects and do not succeed in translating shape differences between apple and orange domains .", "ner": [["DRIT", "Method"], ["CycleGAN", "Method"], ["AAGAN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "201070697", "sentence": "CycleGAN and AGGAN miss certain parts of the object in the translation .", "ner": [["CycleGAN", "Method"], ["AGGAN", "Method"], ["translation", "Task"]], "rel": [["AGGAN", "Used-For", "translation"], ["CycleGAN", "Used-For", "translation"]], "rel_plus": [["AGGAN:Method", "Used-For", "translation:Task"], ["CycleGAN:Method", "Used-For", "translation:Task"]]}
{"doc_id": "201070697", "sentence": "In row 3 , AGGAN attention network and CycleGAN fail to detect the zebra as foreground , and so change the background image content ( fence ) while SPA - GAN detects the zebra and translates it to the target domain .", "ner": [["AGGAN attention network", "Method"], ["CycleGAN", "Method"], ["SPA - GAN", "Method"]], "rel": [["CycleGAN", "Compare-With", "SPA - GAN"], ["AGGAN attention network", "Compare-With", "SPA - GAN"]], "rel_plus": [["CycleGAN:Method", "Compare-With", "SPA - GAN:Method"], ["AGGAN attention network:Method", "Compare-With", "SPA - GAN:Method"]]}
{"doc_id": "201070697", "sentence": "Translation results on the gender conversion ( Facescrub dataset ) requiring holistic translation for the input image with no specific type of object .", "ner": [["Translation", "Task"], ["Facescrub", "Dataset"]], "rel": [["Facescrub", "Benchmark-For", "Translation"]], "rel_plus": [["Facescrub:Dataset", "Benchmark-For", "Translation:Task"]]}
{"doc_id": "202577400", "sentence": "It has been widely proven that modelling long - range dependencies in fully convolutional networks ( FCNs ) via global aggregation modules is critical for complex scene understanding tasks such as semantic segmentation and object detection .", "ner": [["fully convolutional networks", "Method"], ["FCNs", "Method"], ["scene understanding", "Task"], ["semantic segmentation", "Task"], ["object detection", "Task"]], "rel": [["FCNs", "Synonym-Of", "fully convolutional networks"], ["semantic segmentation", "SubTask-Of", "scene understanding"], ["object detection", "SubTask-Of", "scene understanding"], ["fully convolutional networks", "Used-For", "scene understanding"], ["fully convolutional networks", "Used-For", "semantic segmentation"], ["fully convolutional networks", "Used-For", "object detection"]], "rel_plus": [["FCNs:Method", "Synonym-Of", "fully convolutional networks:Method"], ["semantic segmentation:Task", "SubTask-Of", "scene understanding:Task"], ["object detection:Task", "SubTask-Of", "scene understanding:Task"], ["fully convolutional networks:Method", "Used-For", "scene understanding:Task"], ["fully convolutional networks:Method", "Used-For", "semantic segmentation:Task"], ["fully convolutional networks:Method", "Used-For", "object detection:Task"]]}
{"doc_id": "202577400", "sentence": "To resolve this problem , we propose to first use \\emph{Global Aggregation } and then \\emph{Local Distribution } , which is called GALD , where long - range dependencies are more confidently used inside large pattern regions and vice versa .", "ner": [["\\emph{Global Aggregation", "Method"], ["\\emph{Local Distribution", "Method"], ["GALD", "Method"]], "rel": [["\\emph{Global Aggregation", "Part-Of", "GALD"], ["\\emph{Local Distribution", "Part-Of", "GALD"]], "rel_plus": [["\\emph{Global Aggregation:Method", "Part-Of", "GALD:Method"], ["\\emph{Local Distribution:Method", "Part-Of", "GALD:Method"]]}
{"doc_id": "202577400", "sentence": "GALD is end - to - end trainable and can be easily plugged into existing FCNs with various global aggregation modules for a wide range of vision tasks , and consistently improves the performance of state - of - the - art object detection and instance segmentation approaches .", "ner": [["GALD", "Method"], ["FCNs", "Method"], ["object detection", "Task"], ["instance segmentation", "Task"]], "rel": [["GALD", "Part-Of", "FCNs"], ["FCNs", "Used-For", "object detection"], ["FCNs", "Used-For", "instance segmentation"]], "rel_plus": [["GALD:Method", "Part-Of", "FCNs:Method"], ["FCNs:Method", "Used-For", "object detection:Task"], ["FCNs:Method", "Used-For", "instance segmentation:Task"]]}
{"doc_id": "202577400", "sentence": "In particular , GALD used in semantic segmentation achieves new state - of - the - art performance on Cityscapes test set with mIoU 8 3 . 3 \\% .", "ner": [["GALD", "Method"], ["semantic segmentation", "Task"], ["Cityscapes", "Dataset"]], "rel": [["GALD", "Used-For", "semantic segmentation"], ["Cityscapes", "Benchmark-For", "semantic segmentation"], ["GALD", "Evaluated-With", "Cityscapes"]], "rel_plus": [["GALD:Method", "Used-For", "semantic segmentation:Task"], ["Cityscapes:Dataset", "Benchmark-For", "semantic segmentation:Task"], ["GALD:Method", "Evaluated-With", "Cityscapes:Dataset"]]}
{"doc_id": "202577400", "sentence": "Code is available at : \\url{https://github.com/lxtGH/GALD - Net } Detection and segmentation tasks have made steady progress with more powerful representations learned from Fully Convolutional Networks ( FCNs ) .", "ner": [["Detection", "Task"], ["segmentation", "Task"], ["Fully Convolutional Networks", "Method"], ["FCNs", "Method"]], "rel": [["Fully Convolutional Networks", "Used-For", "Detection"], ["Fully Convolutional Networks", "Used-For", "segmentation"], ["FCNs", "Synonym-Of", "Fully Convolutional Networks"]], "rel_plus": [["Fully Convolutional Networks:Method", "Used-For", "Detection:Task"], ["Fully Convolutional Networks:Method", "Used-For", "segmentation:Task"], ["FCNs:Method", "Synonym-Of", "Fully Convolutional Networks:Method"]]}
{"doc_id": "202577400", "sentence": "Since stacking more convolutional layers is not an effective way to achieve large receptive fields for long - range dependency modeling [ 2 4 , 4 4 ] , several Global Aggregation ( GA ) modules have been proposed to resolve this problem .", "ner": [["Global Aggregation", "Method"], ["GA", "Method"]], "rel": [["GA", "Synonym-Of", "Global Aggregation"]], "rel_plus": [["GA:Method", "Synonym-Of", "Global Aggregation:Method"]]}
{"doc_id": "202577400", "sentence": "In contrast to a standard convolutional layer which aggregates features in a small local window , GA modules use long - range operators such as averaging pooling [ 3 , 4 2 ] and spatialwise feature propagation over the whole image [ 1 4 , 2 8 , 3 5 ] .", "ner": [["convolutional layer", "Method"], ["GA", "Method"]], "rel": [["convolutional layer", "Compare-With", "GA"]], "rel_plus": [["convolutional layer:Method", "Compare-With", "GA:Method"]]}
{"doc_id": "202577400", "sentence": "FCNs coupled with GA modules have consistently improved basic FCNs especially for large objects .", "ner": [["FCNs", "Method"], ["GA", "Method"], ["FCNs", "Method"]], "rel": [["GA", "Part-Of", "FCNs"]], "rel_plus": [["GA:Method", "Part-Of", "FCNs:Method"]]}
{"doc_id": "202577400", "sentence": "Unfortunately , the advantage of GA modules for large objects is a disadvantage for small patterns such as object boundaries and small objects , where features from GA modules tends to oversmooth the predictions for these small patterns .", "ner": [["GA", "Method"], ["GA", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202577400", "sentence": "Accordingly , we propose a Local Distribution ( LD ) module after a GA module ( together as GALD for short ) to adaptively distribute GA features at each position as illustrated in Fig. 1 .", "ner": [["Local Distribution", "Method"], ["LD", "Method"], ["GA", "Method"], ["GALD", "Method"], ["GA", "Method"]], "rel": [["LD", "Synonym-Of", "Local Distribution"]], "rel_plus": [["LD:Method", "Synonym-Of", "Local Distribution:Method"]]}
{"doc_id": "202577400", "sentence": "LD is a simple and universal module , and can be combined with existing GA modules to form different GALD modules for various detection and segmentation tasks .", "ner": [["LD", "Method"], ["GA", "Method"], ["GALD", "Method"], ["detection", "Task"], ["segmentation", "Task"]], "rel": [["GA", "Part-Of", "GALD"], ["LD", "Part-Of", "GALD"], ["GALD", "Used-For", "detection"], ["GALD", "Used-For", "segmentation"]], "rel_plus": [["GA:Method", "Part-Of", "GALD:Method"], ["LD:Method", "Part-Of", "GALD:Method"], ["GALD:Method", "Used-For", "detection:Task"], ["GALD:Method", "Used-For", "segmentation:Task"]]}
{"doc_id": "202577400", "sentence": "In our experiment , LD is verified on GA modules such as PSP [ 4 2 ] , ASPP [ 5 ] , Non - Local [ 2 8 ] and CGNL [ 3 6 ] , and achieves consistent performance improvement .", "ner": [["LD", "Method"], ["GA", "Method"], ["PSP", "Method"], ["ASPP", "Method"], ["Non - Local", "Method"], ["CGNL", "Method"]], "rel": [["PSP", "SubClass-Of", "GA"], ["ASPP", "SubClass-Of", "GA"], ["Non - Local", "SubClass-Of", "GA"], ["CGNL", "SubClass-Of", "GA"]], "rel_plus": [["PSP:Method", "SubClass-Of", "GA:Method"], ["ASPP:Method", "SubClass-Of", "GA:Method"], ["Non - Local:Method", "SubClass-Of", "GA:Method"], ["CGNL:Method", "SubClass-Of", "GA:Method"]]}
{"doc_id": "202577400", "sentence": "We also extensively verify GALD on three vision benchmarks , including Cityscapes for semantic segmentation , Pascal VOC 2 0 0 7 for object detection , and MS COCO for both object detection and instance segmentation , and all achieve notable improvement .", "ner": [["GALD", "Method"], ["Cityscapes", "Dataset"], ["semantic segmentation", "Task"], ["Pascal VOC 2 0 0 7", "Dataset"], ["object detection", "Task"], ["MS COCO", "Dataset"], ["object detection", "Task"], ["instance segmentation", "Task"]], "rel": [["GALD", "Evaluated-With", "Cityscapes"], ["Cityscapes", "Benchmark-For", "semantic segmentation"], ["GALD", "Used-For", "semantic segmentation"], ["GALD", "Evaluated-With", "Pascal VOC 2 0 0 7"], ["Pascal VOC 2 0 0 7", "Benchmark-For", "object detection"], ["GALD", "Used-For", "object detection"], ["GALD", "Evaluated-With", "MS COCO"], ["MS COCO", "Benchmark-For", "object detection"], ["GALD", "Used-For", "object detection"], ["MS COCO", "Benchmark-For", "instance segmentation"], ["GALD", "Used-For", "instance segmentation"]], "rel_plus": [["GALD:Method", "Evaluated-With", "Cityscapes:Dataset"], ["Cityscapes:Dataset", "Benchmark-For", "semantic segmentation:Task"], ["GALD:Method", "Used-For", "semantic segmentation:Task"], ["GALD:Method", "Evaluated-With", "Pascal VOC 2 0 0 7:Dataset"], ["Pascal VOC 2 0 0 7:Dataset", "Benchmark-For", "object detection:Task"], ["GALD:Method", "Used-For", "object detection:Task"], ["GALD:Method", "Evaluated-With", "MS COCO:Dataset"], ["MS COCO:Dataset", "Benchmark-For", "object detection:Task"], ["GALD:Method", "Used-For", "object detection:Task"], ["MS COCO:Dataset", "Benchmark-For", "instance segmentation:Task"], ["GALD:Method", "Used-For", "instance segmentation:Task"]]}
{"doc_id": "202577400", "sentence": "In particular , for semantic segmentation evaluated on Cityscapes test set , GALD achieves mIoU of 8 3 . 3 % with single model and ResNet 1 0 1 as our backbone network , which surpasses all previously best published singlemodel results using ResNet 1 0 1 as backbone network .", "ner": [["semantic segmentation", "Task"], ["Cityscapes", "Dataset"], ["GALD", "Method"], ["ResNet 1 0 1", "Method"], ["ResNet 1 0 1", "Method"]], "rel": [["Cityscapes", "Benchmark-For", "semantic segmentation"], ["GALD", "Evaluated-With", "Cityscapes"], ["ResNet 1 0 1", "Part-Of", "GALD"]], "rel_plus": [["Cityscapes:Dataset", "Benchmark-For", "semantic segmentation:Task"], ["GALD:Method", "Evaluated-With", "Cityscapes:Dataset"], ["ResNet 1 0 1:Method", "Part-Of", "GALD:Method"]]}
{"doc_id": "202577400", "sentence": "To keep spatial information required by detection and segmentation tasks , convolutional networks designed for image classification are modified to FCNs by removing global information aggregation layers such as global average pooling layer and fully - connected layers [ 2 3 ] .", "ner": [["detection", "Task"], ["segmentation", "Task"], ["convolutional networks", "Method"], ["image classification", "Task"], ["FCNs", "Method"], ["global average pooling layer", "Method"], ["fully - connected layers", "Method"]], "rel": [["FCNs", "SubClass-Of", "convolutional networks"], ["convolutional networks", "Used-For", "image classification"], ["global average pooling layer", "Part-Of", "FCNs"], ["fully - connected layers", "Part-Of", "FCNs"]], "rel_plus": [["FCNs:Method", "SubClass-Of", "convolutional networks:Method"], ["convolutional networks:Method", "Used-For", "image classification:Task"], ["global average pooling layer:Method", "Part-Of", "FCNs:Method"], ["fully - connected layers:Method", "Part-Of", "FCNs:Method"]]}
{"doc_id": "202577400", "sentence": "In PSPnet [ 4 2 ] , average pooled features of multiple window sizes including global average pooling are upsampled to the same size and concatenated together to enrich global information .", "ner": [["PSPnet", "Method"], ["global average pooling", "Method"]], "rel": [["global average pooling", "Part-Of", "PSPnet"]], "rel_plus": [["global average pooling:Method", "Part-Of", "PSPnet:Method"]]}
{"doc_id": "202577400", "sentence": "The DeepLab series of papers [ 2 , 3 , 5 ] propose atrous or dilated convolutions and atrous spatial pyramid pooling ( ASPP ) to increase the effective receptive field .", "ner": [["DeepLab", "Method"], ["atrous", "Method"], ["dilated convolutions", "Method"], ["atrous spatial pyramid pooling", "Method"], ["ASPP", "Method"]], "rel": [["ASPP", "Synonym-Of", "atrous spatial pyramid pooling"]], "rel_plus": [["ASPP:Method", "Synonym-Of", "atrous spatial pyramid pooling:Method"]]}
{"doc_id": "202577400", "sentence": "In addition to concatenating global information into feature maps , multiplying global information into feature maps also shows better performance [ 2 9 , 3 4 , 3 6 , 3 7 ] .In particular , EncNet [ 3 7 ] and DFN [ 3 4 ] use attention along the channel dimension of the convolutional feature map to account for global context such as the co - occurrences of different classes in the scene .", "ner": [["EncNet", "Method"], ["DFN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202577400", "sentence": "Using non - local operator , impressive results are achieved in OCNet [ 3 5 ] , CoCurNet [ 3 8 ] , DANet [ 1 4 ] , A 2 Net [ 7 ] , CCnet [ 1 5 ] and Compact Generalized Non - Local Net [ 3 6 ] .", "ner": [["OCNet", "Method"], ["CoCurNet", "Method"], ["DANet", "Method"], ["A 2 Net", "Method"], ["CCnet", "Method"], ["Compact Generalized Non - Local Net", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202577400", "sentence": "OCNet [ 3 5 ] uses non - local bolocks to learn pixel - wise relationship while CoCurNet [ 3 8 ] adds extra global average pooling path to learn whole scene statistic .", "ner": [["OCNet", "Method"], ["CoCurNet", "Method"], ["global average pooling", "Method"]], "rel": [["OCNet", "Compare-With", "CoCurNet"], ["global average pooling", "Part-Of", "CoCurNet"]], "rel_plus": [["OCNet:Method", "Compare-With", "CoCurNet:Method"], ["global average pooling:Method", "Part-Of", "CoCurNet:Method"]]}
{"doc_id": "202577400", "sentence": "BeyondGrids [ 1 8 ] learns to cluster different graph nodes and does graph convolution in parallel .", "ner": [["BeyondGrids", "Method"], ["graph convolution", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202577400", "sentence": "DGCNet [ 3 9 ] proposes to use graph convolution network in both channel and spatial space to harvest different global context information .", "ner": [["DGCNet", "Method"], ["graph convolution network", "Method"]], "rel": [["graph convolution network", "Part-Of", "DGCNet"]], "rel_plus": [["graph convolution network:Method", "Part-Of", "DGCNet:Method"]]}
{"doc_id": "202577400", "sentence": "All previous work focus on global context modeling , our work also utilizes global information modeling but takes a further step to better distribute the global information to each position , and further improves GA modules on both detection and segmentation tasks .   Our method , Global Aggregation ( GA ) then Local Distribution ( LD ) , dubbed GALD , exploits the long - range contextual information of the feature F \u2208 R H \u00d7 W \u00d7C from a fully - convolution network ( FCN ) , and then adaptively distributed the global context to each spatial and channel position of the output feature , F GALD \u2208 R H \u00d7 W \u00d7C .", "ner": [["GA", "Method"], ["detection", "Task"], ["segmentation", "Task"], ["Global Aggregation", "Method"], ["GA", "Method"], ["Local Distribution", "Method"], ["LD", "Method"], ["GALD", "Method"], ["fully - convolution network", "Method"], ["FCN", "Method"], ["F GALD", "Method"]], "rel": [["GA", "Used-For", "detection"], ["GA", "Used-For", "segmentation"], ["GA", "Synonym-Of", "Global Aggregation"], ["LD", "Synonym-Of", "Local Distribution"], ["FCN", "Synonym-Of", "fully - convolution network"]], "rel_plus": [["GA:Method", "Used-For", "detection:Task"], ["GA:Method", "Used-For", "segmentation:Task"], ["GA:Method", "Synonym-Of", "Global Aggregation:Method"], ["LD:Method", "Synonym-Of", "Local Distribution:Method"], ["FCN:Method", "Synonym-Of", "fully - convolution network:Method"]]}
{"doc_id": "202577400", "sentence": "To calculate a feature vector for each position , GA module takes feature vectors of F in a large window even the whole feature map depending on different GA designs .", "ner": [["GA", "Method"], ["GA", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202577400", "sentence": "Take the Compact Generalized Non - Local ( CGNL ) [ 3 6 ] as an example , similar to non - local [ 2 8 ] , it aggregates contextual information from all spatial and channel positions in the same group .", "ner": [["Compact Generalized Non - Local", "Method"], ["CGNL", "Method"]], "rel": [["CGNL", "Synonym-Of", "Compact Generalized Non - Local"]], "rel_plus": [["CGNL:Method", "Synonym-Of", "Compact Generalized Non - Local:Method"]]}
{"doc_id": "202577400", "sentence": "LD is proposed to adaptively use F GA considering patterns on each position .", "ner": [["LD", "Method"], ["F GA", "Method"]], "rel": [["F GA", "Part-Of", "LD"]], "rel_plus": [["F GA:Method", "Part-Of", "LD:Method"]]}
{"doc_id": "202577400", "sentence": "The spatial operators for each pattern/channel is modeled as a set of depth - wise convolutional layers with F GA as input , i.e. , where M \u2208 [ 0 , 1 ] H \u00d7 W \u00d7C contains the mask maps for each pattern and describes the recalculated spatial extents of each pattern , \u03c3 ( \u00b7 ) is the sigmoid function , W d is the weights of those depth - wised convolutional filters with d as the downsampling rate by stride convolution .", "ner": [["depth - wise convolutional layers", "Method"], ["F GA", "Method"], ["sigmoid", "Method"], ["depth - wised convolutional filters", "Method"]], "rel": [["F GA", "Part-Of", "depth - wise convolutional layers"]], "rel_plus": [["F GA:Method", "Part-Of", "depth - wise convolutional layers:Method"]]}
{"doc_id": "202577400", "sentence": "With the mask maps M , F GA is refined into F GALD by where the element - wise multiplication , and elements in F GA are weighted according the estimated spatial extent of each pattern at each position .", "ner": [["F GA", "Method"], ["F GALD", "Method"], ["F GA", "Method"]], "rel": [["F GA", "Part-Of", "F GALD"]], "rel_plus": [["F GA:Method", "Part-Of", "F GALD:Method"]]}
{"doc_id": "202577400", "sentence": "In summary , LD predicts local weights M for each position of GA features and avoids issues of coarse feature representation .", "ner": [["LD", "Method"], ["GA", "Method"]], "rel": [["LD", "Part-Of", "GA"]], "rel_plus": [["LD:Method", "Part-Of", "GA:Method"]]}
{"doc_id": "202577400", "sentence": "As a common practice [ 4 2 ] , original feature F and global aggregated feature F GA are concatenated together for final task - specific head , i.e. , where M adds point - wise trade - off between global information F GA and local detailed information F. Note that since the lack of details in GA , LD module only changes the proportion and distribution of coarse features in GA and leads to a fine - grained feature representation output F o .", "ner": [["F GA", "Method"], ["F GA", "Method"], ["GA", "Method"], ["LD", "Method"], ["GA", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202577400", "sentence": "For semantic segmentation , GALD is added right after a FCN , features from Eq. 3 are used for final prediction .", "ner": [["semantic segmentation", "Task"], ["GALD", "Method"], ["FCN", "Method"]], "rel": [["FCN", "Used-For", "semantic segmentation"], ["GALD", "Part-Of", "FCN"]], "rel_plus": [["FCN:Method", "Used-For", "semantic segmentation:Task"], ["GALD:Method", "Part-Of", "FCN:Method"]]}
{"doc_id": "202577400", "sentence": "To further boost the performance , Online Hard Example Mining ( OHEM ) loss [ 3 0 ] is used for training , where only top - K ranked pixels according their losses are used during back - propagation .", "ner": [["Online Hard Example Mining", "Method"], ["OHEM", "Method"]], "rel": [["OHEM", "Synonym-Of", "Online Hard Example Mining"]], "rel_plus": [["OHEM:Method", "Synonym-Of", "Online Hard Example Mining:Method"]]}
{"doc_id": "202577400", "sentence": "For object detection and instance segmentation task , GALD is added at the end of stage 4 of a ResNet backbone , FPN [ 2 0 ] is used to build a strong baseline with a feature pyramid for multi - scale object detection .", "ner": [["object detection", "Task"], ["instance segmentation", "Task"], ["GALD", "Method"], ["ResNet", "Method"], ["FPN", "Method"], ["feature pyramid", "Method"], ["multi - scale object detection", "Task"]], "rel": [["ResNet", "Used-For", "object detection"], ["GALD", "Used-For", "object detection"], ["ResNet", "Used-For", "instance segmentation"], ["GALD", "Used-For", "instance segmentation"], ["GALD", "Part-Of", "ResNet"], ["feature pyramid", "Part-Of", "FPN"], ["FPN", "Used-For", "multi - scale object detection"], ["feature pyramid", "Used-For", "multi - scale object detection"]], "rel_plus": [["ResNet:Method", "Used-For", "object detection:Task"], ["GALD:Method", "Used-For", "object detection:Task"], ["ResNet:Method", "Used-For", "instance segmentation:Task"], ["GALD:Method", "Used-For", "instance segmentation:Task"], ["GALD:Method", "Part-Of", "ResNet:Method"], ["feature pyramid:Method", "Part-Of", "FPN:Method"], ["FPN:Method", "Used-For", "multi - scale object detection:Task"], ["feature pyramid:Method", "Used-For", "multi - scale object detection:Task"]]}
{"doc_id": "202577400", "sentence": "F GALD sits on top of FPN and passes information from the top - down pathway .", "ner": [["F GALD", "Method"], ["FPN", "Method"]], "rel": [["F GALD", "Part-Of", "FPN"]], "rel_plus": [["F GALD:Method", "Part-Of", "FPN:Method"]]}
{"doc_id": "202577400", "sentence": "In this section , we verify GALD on three scene understanding tasks including semantic segmentation , object detection and instance segmentation .", "ner": [["GALD", "Method"], ["scene understanding", "Task"], ["semantic segmentation", "Task"], ["object detection", "Task"], ["instance segmentation", "Task"]], "rel": [["GALD", "Used-For", "scene understanding"], ["GALD", "Used-For", "semantic segmentation"], ["GALD", "Used-For", "object detection"], ["GALD", "Used-For", "instance segmentation"]], "rel_plus": [["GALD:Method", "Used-For", "scene understanding:Task"], ["GALD:Method", "Used-For", "semantic segmentation:Task"], ["GALD:Method", "Used-For", "object detection:Task"], ["GALD:Method", "Used-For", "instance segmentation:Task"]]}
{"doc_id": "202577400", "sentence": "MS COCO : MS COCO [ 1 9 ] is built for detecting and segmenting objects found in everyday life in their natural environment .", "ner": [["MS COCO", "Dataset"], ["MS COCO", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "202577400", "sentence": "Pascal VOC : Pascal VOC [ 1 1 ] is a widely used public benchmark for semantic segmentation and object detection covering 2 0 object categories including the background .", "ner": [["Pascal VOC", "Dataset"], ["Pascal VOC", "Dataset"], ["semantic segmentation", "Task"], ["object detection", "Task"]], "rel": [["Pascal VOC", "Benchmark-For", "semantic segmentation"], ["Pascal VOC", "Benchmark-For", "object detection"]], "rel_plus": [["Pascal VOC:Dataset", "Benchmark-For", "semantic segmentation:Task"], ["Pascal VOC:Dataset", "Benchmark-For", "object detection:Task"]]}
{"doc_id": "202577400", "sentence": "We use VOC 2 0 0 7 and VOC 2 0 1 2 trainval set as training set and report results on VOC 2 0 0 7 test set .   Semantic Segmentation We employ Fully Convolutional Networks ( FCNs ) as baseline , where ResNet pretrained on ImageNet is chosen as the backbone following the same setting as PSPNet [ 4 2 ] , the proposed GALD is appended to the backbone with random initialization .", "ner": [["VOC 2 0 0 7", "Dataset"], ["VOC 2 0 1 2 trainval set", "Dataset"], ["VOC 2 0 0 7 test set", "Dataset"], ["Semantic Segmentation", "Task"], ["Fully Convolutional Networks", "Method"], ["FCNs", "Method"], ["ResNet", "Method"], ["ImageNet", "Dataset"], ["PSPNet", "Method"], ["GALD", "Method"]], "rel": [["FCNs", "Synonym-Of", "Fully Convolutional Networks"], ["ResNet", "Part-Of", "Fully Convolutional Networks"], ["GALD", "Part-Of", "ResNet"], ["ResNet", "Trained-With", "ImageNet"], ["ResNet", "Part-Of", "PSPNet"]], "rel_plus": [["FCNs:Method", "Synonym-Of", "Fully Convolutional Networks:Method"], ["ResNet:Method", "Part-Of", "Fully Convolutional Networks:Method"], ["GALD:Method", "Part-Of", "ResNet:Method"], ["ResNet:Method", "Trained-With", "ImageNet:Dataset"], ["ResNet:Method", "Part-Of", "PSPNet:Method"]]}
{"doc_id": "202577400", "sentence": "For optimization , we also keep the same setting as PSPNet , where mini - batch SGD with momentum 0. 9 and initial learning rate 0.0 1 is used to train all models with 5 0 K iterations , using mini - batch size of 8 and crop size of 7 6 9 .", "ner": [["PSPNet", "Method"], ["SGD with momentum", "Method"]], "rel": [["SGD with momentum", "Part-Of", "PSPNet"]], "rel_plus": [["SGD with momentum:Method", "Part-Of", "PSPNet:Method"]]}
{"doc_id": "202577400", "sentence": "Object Detection and Instance Segmentation For object detection and instance segmentation , mmdetection [ 1 ] is used as our baseline implementation for fair comparison .", "ner": [["Object Detection", "Task"], ["Instance Segmentation", "Task"], ["object detection", "Task"], ["instance segmentation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "202577400", "sentence": "GALD is evaluated for object detection on Pascal VOC based on Faster R - CNN , and for both object detection and instance segmentation on MS COCO based on Mask R - CNN .", "ner": [["GALD", "Method"], ["object detection", "Task"], ["Pascal VOC", "Dataset"], ["Faster R - CNN", "Method"], ["object detection", "Task"], ["instance segmentation", "Task"], ["MS COCO", "Dataset"], ["Mask R - CNN", "Method"]], "rel": [["Pascal VOC", "Benchmark-For", "object detection"], ["Faster R - CNN", "Used-For", "object detection"], ["GALD", "Used-For", "object detection"], ["Faster R - CNN", "Evaluated-With", "Pascal VOC"], ["GALD", "Evaluated-With", "Pascal VOC"], ["GALD", "Part-Of", "Faster R - CNN"], ["MS COCO", "Benchmark-For", "object detection"], ["Mask R - CNN", "Used-For", "object detection"], ["GALD", "Used-For", "object detection"], ["MS COCO", "Benchmark-For", "instance segmentation"], ["Mask R - CNN", "Used-For", "instance segmentation"], ["GALD", "Used-For", "instance segmentation"], ["Mask R - CNN", "Evaluated-With", "MS COCO"], ["GALD", "Evaluated-With", "MS COCO"], ["GALD", "Part-Of", "Mask R - CNN"]], "rel_plus": [["Pascal VOC:Dataset", "Benchmark-For", "object detection:Task"], ["Faster R - CNN:Method", "Used-For", "object detection:Task"], ["GALD:Method", "Used-For", "object detection:Task"], ["Faster R - CNN:Method", "Evaluated-With", "Pascal VOC:Dataset"], ["GALD:Method", "Evaluated-With", "Pascal VOC:Dataset"], ["GALD:Method", "Part-Of", "Faster R - CNN:Method"], ["MS COCO:Dataset", "Benchmark-For", "object detection:Task"], ["Mask R - CNN:Method", "Used-For", "object detection:Task"], ["GALD:Method", "Used-For", "object detection:Task"], ["MS COCO:Dataset", "Benchmark-For", "instance segmentation:Task"], ["Mask R - CNN:Method", "Used-For", "instance segmentation:Task"], ["GALD:Method", "Used-For", "instance segmentation:Task"], ["Mask R - CNN:Method", "Evaluated-With", "MS COCO:Dataset"], ["GALD:Method", "Evaluated-With", "MS COCO:Dataset"], ["GALD:Method", "Part-Of", "Mask R - CNN:Method"]]}
{"doc_id": "202577400", "sentence": "Two groups of experiments are conducted on Cityscapes , the first group of experiments verifies the effectiveness of our GALD framework by ablation studies .", "ner": [["Cityscapes", "Dataset"], ["GALD", "Method"]], "rel": [["GALD", "Evaluated-With", "Cityscapes"]], "rel_plus": [["GALD:Method", "Evaluated-With", "Cityscapes:Dataset"]]}
{"doc_id": "202577400", "sentence": "Comparison with baseline We explore our LD module with four different GA modules as illustrated in Fig. 3 (a) - (d ) .", "ner": [["LD", "Method"], ["GA", "Method"]], "rel": [["GA", "Part-Of", "LD"]], "rel_plus": [["GA:Method", "Part-Of", "LD:Method"]]}
{"doc_id": "202577400", "sentence": "Table 1 ( a ) first reports the performances of adding four GA modules to the baseline FCN , where all methods are using the same backbone ResNet 5 0 for fair comparison .", "ner": [["GA", "Method"], ["FCN", "Method"], ["ResNet 5 0", "Method"]], "rel": [["GA", "Part-Of", "FCN"], ["ResNet 5 0", "Part-Of", "FCN"]], "rel_plus": [["GA:Method", "Part-Of", "FCN:Method"], ["ResNet 5 0:Method", "Part-Of", "FCN:Method"]]}
{"doc_id": "202577400", "sentence": "Obviously , all GA modules significantly improves the baseline FCN on semantic segmentation task , where CGNL performs better than other three GA modules .", "ner": [["GA", "Method"], ["FCN", "Method"], ["semantic segmentation", "Task"], ["CGNL", "Method"], ["GA", "Method"]], "rel": [["GA", "Part-Of", "FCN"], ["FCN", "Used-For", "semantic segmentation"], ["CGNL", "Compare-With", "GA"]], "rel_plus": [["GA:Method", "Part-Of", "FCN:Method"], ["FCN:Method", "Used-For", "semantic segmentation:Task"], ["CGNL:Method", "Compare-With", "GA:Method"]]}
{"doc_id": "202577400", "sentence": "Arrangements of LD and GA Considering LD module can also improve the baseline , we further study different arrangements of LD and GA as illustrated in Fig. 3 (e) - (g ) . ( f ) and ( g ) represent LDGA and Parallel in Table . 1(c ) respectively .", "ner": [["LD", "Method"], ["GA", "Method"], ["LD", "Method"], ["LD", "Method"], ["GA", "Method"], ["LDGA", "Method"]], "rel": [["LD", "Compare-With", "GA"]], "rel_plus": [["LD:Method", "Compare-With", "GA:Method"]]}
{"doc_id": "202577400", "sentence": "LDGA means first doing LD then doing GA while Parallel concatenates the output of LD and GA .", "ner": [["LDGA", "Method"], ["LD", "Method"], ["GA", "Method"], ["LD", "Method"], ["GA", "Method"]], "rel": [["LD", "Part-Of", "LDGA"], ["GA", "Part-Of", "LDGA"]], "rel_plus": [["LD:Method", "Part-Of", "LDGA:Method"], ["GA:Method", "Part-Of", "LDGA:Method"]]}
{"doc_id": "202577400", "sentence": "Fig. 4 shows the mask maps learned in LDGA and GALD , where mask maps learned by GALD are more focused on regions inside large objects then weight global features more in these regions , while mask maps from LDGA have no obvious focus on large objects since the LD module has not accessed to global feature yet .", "ner": [["LDGA", "Method"], ["GALD", "Method"], ["GALD", "Method"], ["LDGA", "Method"], ["LD", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202577400", "sentence": "Compared with stronger backbone To further prove the effectiveness of our method , we compare GALD using ResNet 5 0 as backbone with a stronger backbone ResNet 1 0 1 in Table 1(d ) .", "ner": [["GALD", "Method"], ["ResNet 5 0", "Method"], ["ResNet 1 0 1", "Method"]], "rel": [["GALD", "Part-Of", "ResNet 5 0"], ["GALD", "Part-Of", "ResNet 1 0 1"]], "rel_plus": [["GALD:Method", "Part-Of", "ResNet 5 0:Method"], ["GALD:Method", "Part-Of", "ResNet 1 0 1:Method"]]}
{"doc_id": "202577400", "sentence": "Our method achieves similar performance improvement comparing GA modules with stronger backbone which further prove the effectiveness of LD module .", "ner": [["GA", "Method"], ["LD", "Method"]], "rel": [["GA", "Compare-With", "LD"]], "rel_plus": [["GA:Method", "Compare-With", "LD:Method"]]}
{"doc_id": "202577400", "sentence": "Comparison with different downsampling strategies We also explore three different downsampling strategies for LD , including average pooling , bilinear interpolation and depth - wise stride convolution .", "ner": [["downsampling strategies", "Method"], ["LD", "Method"], ["average pooling", "Method"], ["bilinear interpolation", "Method"], ["depth - wise stride convolution", "Method"]], "rel": [["average pooling", "SubClass-Of", "downsampling strategies"], ["bilinear interpolation", "SubClass-Of", "downsampling strategies"], ["depth - wise stride convolution", "SubClass-Of", "downsampling strategies"], ["average pooling", "Part-Of", "LD"], ["bilinear interpolation", "Part-Of", "LD"], ["depth - wise stride convolution", "Part-Of", "LD"]], "rel_plus": [["average pooling:Method", "SubClass-Of", "downsampling strategies:Method"], ["bilinear interpolation:Method", "SubClass-Of", "downsampling strategies:Method"], ["depth - wise stride convolution:Method", "SubClass-Of", "downsampling strategies:Method"], ["average pooling:Method", "Part-Of", "LD:Method"], ["bilinear interpolation:Method", "Part-Of", "LD:Method"], ["depth - wise stride convolution:Method", "Part-Of", "LD:Method"]]}
{"doc_id": "202577400", "sentence": "Table 1 ( e ) reports the comparison results , depth - wise stride convolution achieves the best result , while average pooling and bilinear interpolation even slightly degrades the performance , which shows that the learnable filters for each channel is important to refine the features from the GA module .", "ner": [["depth - wise stride convolution", "Method"], ["average pooling", "Method"], ["bilinear interpolation", "Method"], ["GA", "Method"]], "rel": [["depth - wise stride convolution", "Compare-With", "average pooling"], ["depth - wise stride convolution", "Compare-With", "bilinear interpolation"]], "rel_plus": [["depth - wise stride convolution:Method", "Compare-With", "average pooling:Method"], ["depth - wise stride convolution:Method", "Compare-With", "bilinear interpolation:Method"]]}
{"doc_id": "202577400", "sentence": "Visualization of GALD To further study the features at different stages , we add another two segmentation heads on features outputted from FCN and GA respectively , the model is fine   We further compare our results with other state - of - the - art methods in this section .", "ner": [["GALD", "Method"], ["FCN", "Method"], ["GA", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202577400", "sentence": "We choose dilated ResNet 5 0 and ResNet 1 0 1 as backbone models .", "ner": [["ResNet 5 0", "Method"], ["ResNet 1 0 1", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202577400", "sentence": "By further adding Mapillary [ 2 5 ] as training data , the proposed method achieves 8 3 . 3 % mIoU based on ResNet 1 0 1 .", "ner": [["Mapillary", "Dataset"], ["ResNet 1 0 1", "Method"]], "rel": [["ResNet 1 0 1", "Trained-With", "Mapillary"]], "rel_plus": [["ResNet 1 0 1:Method", "Trained-With", "Mapillary:Dataset"]]}
{"doc_id": "202577400", "sentence": "To the best of our knowledge , this is the first single model using ResNet 1 0 1 as backbone that surpasses 8 3 % mIoU on Cityscapes test server .", "ner": [["ResNet 1 0 1", "Method"], ["Cityscapes", "Dataset"]], "rel": [["ResNet 1 0 1", "Evaluated-With", "Cityscapes"]], "rel_plus": [["ResNet 1 0 1:Method", "Evaluated-With", "Cityscapes:Dataset"]]}
{"doc_id": "202577400", "sentence": "Pascal VOC : We perform experiments on the PASCAL VOC 2 0 0 7 data set to evaluate the effect of GALD for object detection .", "ner": [["Pascal VOC", "Dataset"], ["PASCAL VOC 2 0 0 7", "Dataset"], ["GALD", "Method"], ["object detection", "Task"]], "rel": [["GALD", "Evaluated-With", "PASCAL VOC 2 0 0 7"], ["GALD", "Used-For", "object detection"], ["PASCAL VOC 2 0 0 7", "Benchmark-For", "object detection"]], "rel_plus": [["GALD:Method", "Evaluated-With", "PASCAL VOC 2 0 0 7:Dataset"], ["GALD:Method", "Used-For", "object detection:Task"], ["PASCAL VOC 2 0 0 7:Dataset", "Benchmark-For", "object detection:Task"]]}
{"doc_id": "202577400", "sentence": "We train all the models on the union set of VOC 2 0 0 7 trainval and VOC 2 0 1 2 trainval ( 0 7 + 1 2 ) for 1 4 epochs with weight decay of 0.0 0 0 1 and momentum of 0. 9 .", "ner": [["VOC 2 0 0 7 trainval", "Dataset"], ["VOC 2 0 1 2 trainval ( 0 7 + 1 2 )", "Dataset"], ["weight decay", "Method"], ["momentum", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202577400", "sentence": "As results listed in Table 3 ( a ) , GALD consistently improves detection accuracy over the strong baseline Faster - RCNN using both ResNet 5 0 and ResNet 1 0 1 as backbone , which demonstrates the effectiveness of GALD for object detection .", "ner": [["GALD", "Method"], ["detection", "Task"], ["Faster - RCNN", "Method"], ["ResNet 5 0", "Method"], ["ResNet 1 0 1", "Method"], ["GALD", "Method"], ["object detection", "Task"]], "rel": [["GALD", "Used-For", "detection"], ["GALD", "Part-Of", "Faster - RCNN"], ["ResNet 5 0", "Part-Of", "Faster - RCNN"], ["ResNet 1 0 1", "Part-Of", "Faster - RCNN"], ["GALD", "Used-For", "object detection"]], "rel_plus": [["GALD:Method", "Used-For", "detection:Task"], ["GALD:Method", "Part-Of", "Faster - RCNN:Method"], ["ResNet 5 0:Method", "Part-Of", "Faster - RCNN:Method"], ["ResNet 1 0 1:Method", "Part-Of", "Faster - RCNN:Method"], ["GALD:Method", "Used-For", "object detection:Task"]]}
{"doc_id": "202577400", "sentence": "COCO : To further verify the generality of GALD , we conduct the experiments on instance segmentation task on MS COCO based on the state - of - the - art method Mask R - CNN .", "ner": [["COCO", "Dataset"], ["GALD", "Method"], ["instance segmentation", "Task"], ["MS COCO", "Dataset"], ["Mask R - CNN", "Method"]], "rel": [["MS COCO", "Benchmark-For", "instance segmentation"], ["Mask R - CNN", "Evaluated-With", "MS COCO"], ["GALD", "Part-Of", "Mask R - CNN"]], "rel_plus": [["MS COCO:Dataset", "Benchmark-For", "instance segmentation:Task"], ["Mask R - CNN:Method", "Evaluated-With", "MS COCO:Dataset"], ["GALD:Method", "Part-Of", "Mask R - CNN:Method"]]}
{"doc_id": "202577400", "sentence": "Figure 6 ( b ) compares the object detection and instance segmentation results of our method with baseline .", "ner": [["object detection", "Task"], ["instance segmentation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "202577400", "sentence": "With GALD , Mask R - CNN can find objects that are missed in baseline ( e.g. , the \" light \" in the third column ) , resolve ambiguity in region classification ( e.g. , the \" bed \" in the first column ) and help to better estimate the spatial contents for objects ( e.g. , \" bear \" in last column ) .", "ner": [["GALD", "Method"], ["Mask R - CNN", "Method"], ["region classification", "Task"]], "rel": [["GALD", "Part-Of", "Mask R - CNN"], ["Mask R - CNN", "Used-For", "region classification"], ["GALD", "Used-For", "region classification"]], "rel_plus": [["GALD:Method", "Part-Of", "Mask R - CNN:Method"], ["Mask R - CNN:Method", "Used-For", "region classification:Task"], ["GALD:Method", "Used-For", "region classification:Task"]]}
{"doc_id": "202577400", "sentence": "Table 3 : Results on Pascal VOC dataset ( a ) and MS COCO dataset ( b ) .", "ner": [["Pascal VOC", "Dataset"], ["MS COCO", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "202577400", "sentence": "Mask - RCNN + GALD   In this paper , we propose GALD to adaptively distribute global information to each position for scene understanding tasks .", "ner": [["Mask - RCNN + GALD", "Method"], ["GALD", "Method"], ["scene understanding", "Task"]], "rel": [["GALD", "Used-For", "scene understanding"]], "rel_plus": [["GALD:Method", "Used-For", "scene understanding:Task"]]}
{"doc_id": "202577400", "sentence": "GALD benefits from both the GA module for ambiguity resolving and LD module for detail refinement .", "ner": [["GALD", "Method"], ["GA", "Method"], ["LD", "Method"]], "rel": [["GA", "Part-Of", "GALD"], ["LD", "Part-Of", "GALD"]], "rel_plus": [["GA:Method", "Part-Of", "GALD:Method"], ["LD:Method", "Part-Of", "GALD:Method"]]}
{"doc_id": "202577400", "sentence": "Extensive experiments verify the universality of GALD in improving the performance of semantic segmentation , object detection and instance segmentation .", "ner": [["GALD", "Method"], ["semantic segmentation", "Task"], ["object detection", "Task"], ["instance segmentation", "Task"]], "rel": [["GALD", "Used-For", "semantic segmentation"], ["GALD", "Used-For", "object detection"], ["GALD", "Used-For", "instance segmentation"]], "rel_plus": [["GALD:Method", "Used-For", "semantic segmentation:Task"], ["GALD:Method", "Used-For", "object detection:Task"], ["GALD:Method", "Used-For", "instance segmentation:Task"]]}
{"doc_id": "202577400", "sentence": "In the future , we will study the effectiveness of GALD for more vision tasks where both global and local information are important such as depth estimation .", "ner": [["GALD", "Method"], ["depth estimation", "Task"]], "rel": [["GALD", "Used-For", "depth estimation"]], "rel_plus": [["GALD:Method", "Used-For", "depth estimation:Task"]]}
{"doc_id": "202577400", "sentence": "Method road swalk build wall fence pole tlight sign veg . terrain sky person rider car truck bus train mbike bike mIoU ResNet 3 8 [ 3 1 ] Ctiyscapes provides about 2 0 0 0 0 coarse labeled images for training .", "ner": [["ResNet 3 8", "Method"], ["Ctiyscapes", "Dataset"]], "rel": [["ResNet 3 8", "Trained-With", "Ctiyscapes"]], "rel_plus": [["ResNet 3 8:Method", "Trained-With", "Ctiyscapes:Dataset"]]}
{"doc_id": "202577400", "sentence": "Again we following the same steps in previous part , we get more accurate model and our single model with ResNet 1 0 1 as backbone can achieve 8 3 . 3 % mIoU ranked 3 - rd in cityscapes leaderboard by the time of paper publication .", "ner": [["ResNet 1 0 1", "Method"], ["cityscapes", "Dataset"]], "rel": [["ResNet 1 0 1", "Evaluated-With", "cityscapes"]], "rel_plus": [["ResNet 1 0 1:Method", "Evaluated-With", "cityscapes:Dataset"]]}
{"doc_id": "202577400", "sentence": "Models trained with extra data sets including COCO and Mapillary data sets are shown in Table 5 .   Here we give the detailed detection results on VOC 2 0 0 7 shown in and compared the results with previous detection methods , ours model achieves considerable results .", "ner": [["COCO", "Dataset"], ["Mapillary", "Dataset"], ["detection", "Task"], ["VOC 2 0 0 7", "Dataset"]], "rel": [["VOC 2 0 0 7", "Benchmark-For", "detection"]], "rel_plus": [["VOC 2 0 0 7:Dataset", "Benchmark-For", "detection:Task"]]}
{"doc_id": "202577400", "sentence": "Here we show more results on Cityscapes and COCO dataset .", "ner": [["Cityscapes", "Dataset"], ["COCO", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "7507210", "sentence": "As a result , we are able to derive a new , shallower , architecture of residual networks which significantly outperforms much deeper models such as ResNet - 2 0 0 on the ImageNet classification dataset .", "ner": [["ResNet - 2 0 0", "Method"], ["ImageNet classification", "Dataset"]], "rel": [["ResNet - 2 0 0", "Evaluated-With", "ImageNet classification"]], "rel_plus": [["ResNet - 2 0 0:Method", "Evaluated-With", "ImageNet classification:Dataset"]]}
{"doc_id": "7507210", "sentence": "We also show that this performance is transferable to other problem domains by developing a semantic segmentation approach which outperforms the state - of - the - art by a remarkable margin on datasets including PASCAL VOC , PASCAL Context , and Cityscapes .", "ner": [["semantic segmentation", "Task"], ["PASCAL VOC", "Dataset"], ["PASCAL Context", "Dataset"], ["Cityscapes", "Dataset"]], "rel": [["PASCAL VOC", "Benchmark-For", "semantic segmentation"], ["PASCAL Context", "Benchmark-For", "semantic segmentation"], ["Cityscapes", "Benchmark-For", "semantic segmentation"]], "rel_plus": [["PASCAL VOC:Dataset", "Benchmark-For", "semantic segmentation:Task"], ["PASCAL Context:Dataset", "Benchmark-For", "semantic segmentation:Task"], ["Cityscapes:Dataset", "Benchmark-For", "semantic segmentation:Task"]]}
{"doc_id": "7507210", "sentence": "The code and models are available at https://github.com/itijyou/ademxapp The convolutional networks used by the computer vision community have been growing deeper and deeper each year since Krizhevsky et al. [ 1 6 ] proposed AlexNet in 2 0 1 2 .", "ner": [["computer vision", "Task"], ["AlexNet", "Method"]], "rel": [["AlexNet", "Used-For", "computer vision"]], "rel_plus": [["AlexNet:Method", "Used-For", "computer vision:Task"]]}
{"doc_id": "7507210", "sentence": "The deepest network [ 1 2 ] in the literature is a residual network ( ResNet ) with 1, 2 0 2 trainable layers , which was trained using the tiny images in the CIFAR - 1 0 dataset [ 1 5 ] .", "ner": [["residual network", "Method"], ["ResNet", "Method"], ["CIFAR - 1 0", "Dataset"]], "rel": [["ResNet", "Synonym-Of", "residual network"], ["residual network", "Trained-With", "CIFAR - 1 0"]], "rel_plus": [["ResNet:Method", "Synonym-Of", "residual network:Method"], ["residual network:Method", "Trained-With", "CIFAR - 1 0:Dataset"]]}
{"doc_id": "7507210", "sentence": "Most networks operating on more practically interesting image sizes tend to have the order of one , to two , hundred layers , e.g. the 2 0 0 - layer ResNet [ 1 3 ] and 9 6 - layer InceptionResNet [ 3 0 ] .", "ner": [["ResNet", "Method"], ["InceptionResNet", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "7507210", "sentence": "These networks were trained using the ImageNet classification dataset [ 2 7 ] , where the images are of much higher resolution .", "ner": [["ImageNet classification", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "7507210", "sentence": "On the other hand , Zagoruyko and Komodakis showed that it is possible to train much shallower but wider networks on CIFAR - 1 0 , which outperform a ResNet [ 1 2 ] with its more than one thousand layers .", "ner": [["CIFAR - 1 0", "Dataset"], ["ResNet", "Method"]], "rel": [["ResNet", "Trained-With", "CIFAR - 1 0"]], "rel_plus": [["ResNet:Method", "Trained-With", "CIFAR - 1 0:Dataset"]]}
{"doc_id": "7507210", "sentence": "Some of them achieve the state - of - the - art results on the ImageNet classification dataset [ 2 7 ] . \u2022 We evaluate the impact of using different networks on the performance of semantic image segmentation , and show these networks , as pre - trained features , can boost existing algorithms a lot .", "ner": [["ImageNet classification", "Dataset"], ["semantic image segmentation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "7507210", "sentence": "We achieve the best results on PASCAL VOC [ 8 ] , PASCAL Context [ 2 4 ] , and Cityscapes [ 5 ] .", "ner": [["PASCAL VOC", "Dataset"], ["PASCAL Context", "Dataset"], ["Cityscapes", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "7507210", "sentence": "Our work here is closely related to two topics , residual network ( ResNet ) based image classification and semantic image segmentation using fully convolutional networks .", "ner": [["residual network", "Method"], ["ResNet", "Method"], ["image classification", "Task"], ["fully convolutional networks", "Method"]], "rel": [["ResNet", "Synonym-Of", "residual network"], ["residual network", "Used-For", "image classification"], ["fully convolutional networks", "Used-For", "image classification"]], "rel_plus": [["ResNet:Method", "Synonym-Of", "residual network:Method"], ["residual network:Method", "Used-For", "image classification:Task"], ["fully convolutional networks:Method", "Used-For", "image classification:Task"]]}
{"doc_id": "7507210", "sentence": "ResNets have outperformed previous models at a variety of tasks , such as object detection [ 7 ] and semantic image segmentation [ 3 ] .", "ner": [["ResNets", "Method"], ["object detection", "Task"]], "rel": [["ResNets", "Used-For", "object detection"]], "rel_plus": [["ResNets:Method", "Used-For", "object detection:Task"]]}
{"doc_id": "7507210", "sentence": "They are gradually replacing VGGNets [ 2 8 ] in the computer vision community , as the standard feature extractors .", "ner": [["VGGNets", "Method"], ["computer vision", "Task"]], "rel": [["VGGNets", "Used-For", "computer vision"]], "rel_plus": [["VGGNets:Method", "Used-For", "computer vision:Task"]]}
{"doc_id": "7507210", "sentence": "On the other hand , Zagoruyko and Komodakis [ 3 6 ] found that a wide sixteen - layer ResNet outperformed the original thin thousand - layer ResNet [ 1 3 ] on datasets composed of tiny images such as CIFAR - 1 0 [ 1 5 ] .", "ner": [["sixteen - layer ResNet", "Method"], ["thousand - layer ResNet", "Method"], ["CIFAR - 1 0", "Dataset"]], "rel": [["sixteen - layer ResNet", "Compare-With", "thousand - layer ResNet"], ["thousand - layer ResNet", "Evaluated-With", "CIFAR - 1 0"], ["sixteen - layer ResNet", "Evaluated-With", "CIFAR - 1 0"]], "rel_plus": [["sixteen - layer ResNet:Method", "Compare-With", "thousand - layer ResNet:Method"], ["thousand - layer ResNet:Method", "Evaluated-With", "CIFAR - 1 0:Dataset"], ["sixteen - layer ResNet:Method", "Evaluated-With", "CIFAR - 1 0:Dataset"]]}
{"doc_id": "7507210", "sentence": "Analysing the interactions between residual units at a given effective depth , here labelled l , illuminates the paths taken by gradients during training . search of configuration space is impractical on large scale datasets such as the ImageNet classification dataset [ 2 7 ] .", "ner": [["ImageNet classification", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "7507210", "sentence": "Long et al. [ 2 2 ] proposed the fully convolutional networks ( FCN ) to this end .", "ner": [["fully convolutional networks", "Method"], ["FCN", "Method"]], "rel": [["FCN", "Synonym-Of", "fully convolutional networks"]], "rel_plus": [["FCN:Method", "Synonym-Of", "fully convolutional networks:Method"]]}
{"doc_id": "7507210", "sentence": "We thus here base our semantic image segmentation approach on fully convolutional networks , and will show the impact of different pre - trained features on final segmentation results .", "ner": [["semantic image segmentation", "Task"], ["fully convolutional networks", "Method"], ["segmentation", "Task"]], "rel": [["fully convolutional networks", "Used-For", "semantic image segmentation"], ["fully convolutional networks", "Used-For", "segmentation"]], "rel_plus": [["fully convolutional networks:Method", "Used-For", "semantic image segmentation:Task"], ["fully convolutional networks:Method", "Used-For", "segmentation:Task"]]}
{"doc_id": "7507210", "sentence": "In the full preactivation version , the components of a stage are in turn a batch normalization [ 1 4 ] , a rectified linear unit [ 2 5 ] ( ReLU ) non - linearity , and a convolution layer .", "ner": [["batch normalization", "Method"], ["rectified linear unit", "Method"], ["ReLU", "Method"], ["convolution layer", "Method"]], "rel": [["ReLU", "Synonym-Of", "rectified linear unit"]], "rel_plus": [["ReLU:Method", "Synonym-Of", "rectified linear unit:Method"]]}
{"doc_id": "7507210", "sentence": "For the twounit ResNet as illustrated in Fig. 1 , there are three , e.g. , M a , M b , and M 2 e , sub - networks respectively corresponding to the three terms in Eqn.( 3 ) , i.e. , y 0 , f 1 ( y 0 , w 1 ) , and Veit et al. in [ 3 1 ] showed that the paths which gradients take through a ResNet are typically far shorter than the total depth of that network .", "ner": [["ResNet", "Method"], ["ResNet", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "7507210", "sentence": "We can see a ResNet as an ensemble of different sub - networks , i.e. , We show an unravelled three - unit ResNet with different effective depths in Fig. 2 .", "ner": [["ResNet", "Method"], ["ResNet", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "7507210", "sentence": "There might be differences in the details [ 3 2 ] between fusion by concatenation ( Inception - v 4 ) and fusion by summation ( ResNets ) .", "ner": [["Inception - v 4", "Method"], ["ResNets", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "7507210", "sentence": "We have learned the strength of depth from the previous plain deep networks without any shortcuts , e.g. , the AlexNet [ 1 6 ] and VGGNets [ 2 8 ] .", "ner": [["AlexNet", "Method"], ["VGGNets", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "7507210", "sentence": "Szegedy et al. [ 3 0 ] came to a similar conclusion , according to the comparison between their proposed Inception networks . \u2022 Veit et al. [ 3 1 ] found that there is a relatively small effective depth for a very deep ResNet , e.g. , seventeen residual units for a 1 1 0 - layer ResNet . most of the current state - of - the - art models on the ImageNet classification dataset [ 2 7 ] seem over - deepened , e.g. , the 2 0 0 - layer ResNet [ 1 3 ] and 9 6 - layer Inception - ResNet [ 3 0 ] .", "ner": [["Inception networks", "Method"], ["1 1 0 - layer ResNet", "Method"], ["ImageNet classification", "Dataset"], ["2 0 0 - layer ResNet", "Method"], ["9 6 - layer Inception - ResNet", "Method"]], "rel": [["9 6 - layer Inception - ResNet", "Evaluated-With", "ImageNet classification"], ["2 0 0 - layer ResNet", "Evaluated-With", "ImageNet classification"]], "rel_plus": [["9 6 - layer Inception - ResNet:Method", "Evaluated-With", "ImageNet classification:Dataset"], ["2 0 0 - layer ResNet:Method", "Evaluated-With", "ImageNet classification:Dataset"]]}
{"doc_id": "7507210", "sentence": "However , even if a rather shallow network ( eight - unit , or twenty - layer ) can outperform ResNet - 1 5 2 on the ImageNet classification dataset , we will not go that shallow , because an appropriate depth is vital to train good features .", "ner": [["ResNet - 1 5 2", "Method"], ["ImageNet classification", "Dataset"]], "rel": [["ResNet - 1 5 2", "Evaluated-With", "ImageNet classification"]], "rel_plus": [["ResNet - 1 5 2:Method", "Evaluated-With", "ImageNet classification:Dataset"]]}
{"doc_id": "7507210", "sentence": "Dashed blue rectangles to denote convolution stages , which are respectively composed of a batch normalization , an ReLU nonlinearity and a convolution layer , following the second version of ResNets [ 1 3 ] .", "ner": [["convolution", "Method"], ["batch normalization", "Method"], ["ReLU", "Method"], ["convolution layer", "Method"], ["ResNets", "Method"]], "rel": [["convolution layer", "Part-Of", "ResNets"], ["ReLU", "Part-Of", "ResNets"], ["batch normalization", "Part-Of", "ResNets"]], "rel_plus": [["convolution layer:Method", "Part-Of", "ResNets:Method"], ["ReLU:Method", "Part-Of", "ResNets:Method"], ["batch normalization:Method", "Part-Of", "ResNets:Method"]]}
{"doc_id": "7507210", "sentence": "We average the top - most feature maps into 4, 0 9 6 - dimensional final features , which matches the cases of AlexNet [ 1 6 ] and VGGNets [ 2 8 ] .", "ner": [["AlexNet", "Method"], ["VGGNets", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "7507210", "sentence": "Our approach is similar to the fully convolutional networks ( FCN ) [ 2 2 ] implemented in the first version of DeepLab [ 2 ] .", "ner": [["fully convolutional networks", "Method"], ["FCN", "Method"], ["DeepLab", "Method"]], "rel": [["FCN", "Synonym-Of", "fully convolutional networks"]], "rel_plus": [["FCN:Method", "Synonym-Of", "fully convolutional networks:Method"]]}
{"doc_id": "7507210", "sentence": "Besides , we do not apply any multi - scale testing , model averaging or CRF based post - processing , except for the test set of ADE 2 0 K [ 4 0 ] .", "ner": [["CRF", "Method"], ["ADE 2 0 K", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "7507210", "sentence": "We set stride of the first convolution layer in B 5 to one , and increase the dilation rate from one to two for the following layers ; We do the same thing to the first convolution layer in B 6 too , and increase the dilation rate from two to four for the following layers .", "ner": [["convolution layer", "Method"], ["convolution layer", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "7507210", "sentence": "We remove the top - most three pooling layers ( before B 4 , B 5 and B 6 ) , increase the strides of according convolution layers up to two , and tune it for 4 5 k iterations using the ImageNet dataset [ 2 7 ] , starting from a learning rate of 0.0 1 . 2 ) Classifier .", "ner": [["pooling layers", "Method"], ["convolution layers", "Method"], ["ImageNet", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "7507210", "sentence": "The dropout rate is 0. 3 for those with 2, 0 4 8 channels , e.g. , the last three units in ResNets and the second last units ( B 6 ) in our networks ; while 0. 5 for those with 4, 0 9 6 channels , e.g. , the top - most units ( B 7 ) in our networks .", "ner": [["dropout", "Method"], ["ResNets", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "7507210", "sentence": "During training , we first resize an image by a ratio randomly sampled from [ 0. 7 , 1. 3 ] , and then generate a sample by cropping one 5 0 0 \u00d7 5 0 0 subwindow at a randomly selected location .   We evaluate our proposed networks 1 on the ILSVRC 2 0 1 2 classification dataset [ 2 7 ] , with 1. 2 8 million images for training , respectively belonging to 1, 0 0 0 categories .", "ner": [["ILSVRC 2 0 1 2 classification", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "7507210", "sentence": "However , we list the ten - crop result for VGG 1 6 [ 2 8 ] since it is not inherently a fully convolutional network .", "ner": [["VGG 1 6", "Method"], ["fully convolutional network", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "7507210", "sentence": "Notable points about the results are as follows . 1 ) We can not make statistically significant improvement by using ResNet - 1 5 2 instead of ResNet - 1 0 1 .", "ner": [["ResNet - 1 5 2", "Method"], ["ResNet - 1 0 1", "Method"]], "rel": [["ResNet - 1 5 2", "Compare-With", "ResNet - 1 0 1"]], "rel_plus": [["ResNet - 1 5 2:Method", "Compare-With", "ResNet - 1 0 1:Method"]]}
{"doc_id": "7507210", "sentence": "With our settings , it even can not be tuned using many mainstream GPUs with only 1 2 GB memories . 3 ) Model B performs worse than ResNet - 1 0 1 , even if it performs better on the classification task as shown in Table 1 .", "ner": [["ResNet - 1 0 1", "Method"], ["classification", "Task"]], "rel": [["ResNet - 1 0 1", "Used-For", "classification"]], "rel_plus": [["ResNet - 1 0 1:Method", "Used-For", "classification:Task"]]}
{"doc_id": "7507210", "sentence": "This is reasonable since there are only object categories in this dataset , while Places 3 6 5 is for scene classification tasks .", "ner": [["Places 3 6 5", "Dataset"], ["scene classification", "Task"]], "rel": [["Places 3 6 5", "Benchmark-For", "scene classification"]], "rel_plus": [["Places 3 6 5:Dataset", "Benchmark-For", "scene classification:Task"]]}
{"doc_id": "7507210", "sentence": "Comparison by semantic image segmentation scores ( % ) and GPU memory usages ( GB/device ) during tuning on the PAS - CAL VOC val set [ 8 ] with 1, 4 4 9 images .", "ner": [["semantic image segmentation", "Task"], ["PAS - CAL VOC", "Dataset"]], "rel": [["PAS - CAL VOC", "Benchmark-For", "semantic image segmentation"]], "rel_plus": [["PAS - CAL VOC:Dataset", "Benchmark-For", "semantic image segmentation:Task"]]}
{"doc_id": "7507210", "sentence": "This is a significant margin , considering that the gap between ResNet - based and VGGNet - based methods is 3. 8 % .", "ner": [["ResNet", "Method"], ["VGGNet", "Method"]], "rel": [["ResNet", "Compare-With", "VGGNet"]], "rel_plus": [["ResNet:Method", "Compare-With", "VGGNet:Method"]]}
{"doc_id": "7507210", "sentence": "Nevertheless , our method outperforms theirs by 2. 8 % , which further shows the effectiveness of our features pretrained only using the ImageNet classification data [ 2 7 ] .", "ner": [["ImageNet classification", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "7507210", "sentence": "On this dataset , ResNet - 1 5 2 again shows no advantage against ResNet - 1 0 1 .", "ner": [["ResNet - 1 5 2", "Method"], ["ResNet - 1 0 1", "Method"]], "rel": [["ResNet - 1 5 2", "Compare-With", "ResNet - 1 0 1"]], "rel_plus": [["ResNet - 1 5 2:Method", "Compare-With", "ResNet - 1 0 1:Method"]]}
{"doc_id": "7507210", "sentence": "Besides , there is also an instance - weighted IoU score for each of the two , which assigns high scores to those pixels of small instances . [ 1 3 ] uses ResNet - 1 0 1 [ 1 2 ] , while others use VGG 1 6 [ 2 8 ] .", "ner": [["ResNet - 1 0 1", "Method"], ["VGG 1 6", "Method"]], "rel": [["ResNet - 1 0 1", "Compare-With", "VGG 1 6"]], "rel_plus": [["ResNet - 1 0 1:Method", "Compare-With", "VGG 1 6:Method"]]}
{"doc_id": "7507210", "sentence": "It is notable that these significant improvements show the strength of our pre - trained features , considering that DeepLab - v 2 [ 3 ] uses ResNet - 1 0 1 , and LRR [ 9 ] uses much more data for training .", "ner": [["DeepLab - v 2", "Method"], ["ResNet - 1 0 1", "Method"], ["LRR", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "7507210", "sentence": "Using this new architecture we designed a group of correspondingly shallow networks , and showed that they outperform the previous very deep residual networks not only on the ImageNet classification dataset , but also when applied to semantic image segmentation .", "ner": [["ImageNet classification", "Dataset"], ["semantic image segmentation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "7507210", "sentence": "According to the results in Fig. 4 , ResNet - 1 1 0 trained on CIFAR - 1 0 , as well as ResNet - 1 0 1 and ResNet - 1 5 2 trained on ILSVRC 2 0 1 2 , generate much smaller gradients from their long paths than from their short paths .", "ner": [["ResNet - 1 1 0", "Method"], ["CIFAR - 1 0", "Dataset"], ["ResNet - 1 0 1", "Method"], ["ResNet - 1 5 2", "Method"], ["ILSVRC 2 0 1 2", "Dataset"]], "rel": [["ResNet - 1 1 0", "Trained-With", "CIFAR - 1 0"], ["ResNet - 1 5 2", "Trained-With", "ILSVRC 2 0 1 2"], ["ResNet - 1 0 1", "Trained-With", "ILSVRC 2 0 1 2"]], "rel_plus": [["ResNet - 1 1 0:Method", "Trained-With", "CIFAR - 1 0:Dataset"], ["ResNet - 1 5 2:Method", "Trained-With", "ILSVRC 2 0 1 2:Dataset"], ["ResNet - 1 0 1:Method", "Trained-With", "ILSVRC 2 0 1 2:Dataset"]]}
{"doc_id": "202676714", "sentence": "We propose Absum , which is a regularization method for improving adversarial robustness of convolutional neural networks ( CNNs ) .", "ner": [["Absum", "Method"], ["regularization method", "Method"], ["adversarial robustness", "Task"], ["convolutional neural networks", "Method"], ["CNNs", "Method"]], "rel": [["Absum", "SubClass-Of", "regularization method"], ["convolutional neural networks", "Used-For", "adversarial robustness"], ["Absum", "Used-For", "adversarial robustness"], ["CNNs", "Synonym-Of", "convolutional neural networks"], ["Absum", "Part-Of", "convolutional neural networks"]], "rel_plus": [["Absum:Method", "SubClass-Of", "regularization method:Method"], ["convolutional neural networks:Method", "Used-For", "adversarial robustness:Task"], ["Absum:Method", "Used-For", "adversarial robustness:Task"], ["CNNs:Method", "Synonym-Of", "convolutional neural networks:Method"], ["Absum:Method", "Part-Of", "convolutional neural networks:Method"]]}
{"doc_id": "202676714", "sentence": "Although CNNs can accurately recognize images , recent studies have shown that the convolution operations in CNNs commonly have structural sensitivity to specific noise composed of Fourier basis functions .", "ner": [["CNNs", "Method"], ["CNNs", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202676714", "sentence": "Absum can improve robustness against single Fourier attack while being as simple and efficient as standard regularization methods ( e.g. , weight decay and L 1 regularization ) .", "ner": [["Absum", "Method"], ["robustness against single Fourier attack", "Task"], ["standard regularization methods", "Method"], ["weight decay", "Method"], ["L 1 regularization", "Method"]], "rel": [["Absum", "Used-For", "robustness against single Fourier attack"], ["weight decay", "SubClass-Of", "standard regularization methods"], ["L 1 regularization", "SubClass-Of", "standard regularization methods"]], "rel_plus": [["Absum:Method", "Used-For", "robustness against single Fourier attack:Task"], ["weight decay:Method", "SubClass-Of", "standard regularization methods:Method"], ["L 1 regularization:Method", "SubClass-Of", "standard regularization methods:Method"]]}
{"doc_id": "202676714", "sentence": "Our experiments demonstrate that Absum improves robustness against single Fourier attack more than standard regularization methods .", "ner": [["Absum", "Method"], ["robustness against single Fourier attack", "Task"], ["standard regularization methods", "Method"]], "rel": [["Absum", "Used-For", "robustness against single Fourier attack"], ["Absum", "Compare-With", "standard regularization methods"]], "rel_plus": [["Absum:Method", "Used-For", "robustness against single Fourier attack:Task"], ["Absum:Method", "Compare-With", "standard regularization methods:Method"]]}
{"doc_id": "202676714", "sentence": "Furthermore , we reveal that robust CNNs with Absum are more robust against transferred attacks due to decreasing the common sensitivity and against high - frequency noise than standard regularization methods .", "ner": [["CNNs", "Method"], ["Absum", "Method"]], "rel": [["Absum", "Part-Of", "CNNs"]], "rel_plus": [["Absum:Method", "Part-Of", "CNNs:Method"]]}
{"doc_id": "202676714", "sentence": "We also reveal that Absum can improve robustness against gradient - based attacks ( projected gradient descent ) when used with adversarial training .", "ner": [["Absum", "Method"], ["robustness against gradient - based attacks", "Task"], ["projected gradient descent", "Method"], ["adversarial training", "Method"]], "rel": [["Absum", "Used-For", "robustness against gradient - based attacks"], ["projected gradient descent", "Used-For", "robustness against gradient - based attacks"], ["Absum", "Part-Of", "adversarial training"]], "rel_plus": [["Absum:Method", "Used-For", "robustness against gradient - based attacks:Task"], ["projected gradient descent:Method", "Used-For", "robustness against gradient - based attacks:Task"], ["Absum:Method", "Part-Of", "adversarial training:Method"]]}
{"doc_id": "202676714", "sentence": "Deep neural networks have achieved great success in many applications , e.g. , image recognition ( He et al. 2 0 1 6 ) and machine translation ( Vaswani et al. 2 0 1 7 ) .", "ner": [["Deep neural networks", "Method"], ["image recognition", "Task"], ["machine translation", "Task"]], "rel": [["Deep neural networks", "Used-For", "image recognition"], ["Deep neural networks", "Used-For", "machine translation"]], "rel_plus": [["Deep neural networks:Method", "Used-For", "image recognition:Task"], ["Deep neural networks:Method", "Used-For", "machine translation:Task"]]}
{"doc_id": "202676714", "sentence": "Specifically , CNNs and rectified linear units ( ReLUs ) have resulted in breakthroughs in image recognition ( LeCun et al. 1 9 8 9 ; Nair and Hinton 2 0 1 0 ) and are de facto standards for image recognition and other applications ( He et al. 2 0 1 6 ; Radford , Metz , and Chintala 2 0 1 6 ) .", "ner": [["CNNs", "Method"], ["rectified linear units", "Method"], ["ReLUs", "Method"], ["image recognition", "Task"], ["image recognition", "Task"]], "rel": [["ReLUs", "Synonym-Of", "rectified linear units"], ["CNNs", "Used-For", "image recognition"]], "rel_plus": [["ReLUs:Method", "Synonym-Of", "rectified linear units:Method"], ["CNNs:Method", "Used-For", "image recognition:Task"]]}
{"doc_id": "202676714", "sentence": "In fact , Tsuzuku and Sato ( 2 0 1 9 ) have recently shown that CNNs have the structural sensitivity from the perspective that convolution can be regarded as the product of the circulant matrix and proposed single Fourier attack ( SFA ) . 1 Fourier basis functions create singular vectors of circulant matrices , and SFA uses these singular vectors since the dominant singular vector can be the worst noise for a matrix - vector product .", "ner": [["CNNs", "Method"], ["single Fourier attack", "Method"], ["SFA", "Method"], ["SFA", "Method"]], "rel": [["SFA", "Synonym-Of", "single Fourier attack"]], "rel_plus": [["SFA:Method", "Synonym-Of", "single Fourier attack:Method"]]}
{"doc_id": "202676714", "sentence": "Although SFA is a very simple attack composed of a single - frequency component , it is universal adversarial perturbations for CNNs , i.e. , it can decrease the classification accuracy of various CNNbased models without using the information about the model parameters and without depending on input images .", "ner": [["SFA", "Task"], ["CNNs", "Method"], ["classification", "Task"], ["CNNbased models", "Method"]], "rel": [["CNNbased models", "Used-For", "classification"]], "rel_plus": [["CNNbased models:Method", "Used-For", "classification:Task"]]}
{"doc_id": "202676714", "sentence": "To defend CNNs against SFA , we first reveal that the spectral norm constraint ( Sedghi , Gupta , and Long 2 0 1 9 ) ( hereinafter , we call it SNC ) can reduce the structural sensitivity .", "ner": [["CNNs", "Method"], ["against SFA", "Task"], ["spectral norm constraint", "Method"], ["SNC", "Method"]], "rel": [["CNNs", "Used-For", "against SFA"], ["SNC", "Synonym-Of", "spectral norm constraint"]], "rel_plus": [["CNNs:Method", "Used-For", "against SFA:Task"], ["SNC:Method", "Synonym-Of", "spectral norm constraint:Method"]]}
{"doc_id": "202676714", "sentence": "However , SNC is not so practical since it requires high computational cost to compute the spectral norm ( the largest singular value ) .", "ner": [["SNC", "Method"], ["spectral norm", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202676714", "sentence": "We then develop Absum ; an efficient regularization method for reducing the structural sensitivity of CNNs .", "ner": [["Absum", "Method"], ["regularization method", "Method"], ["CNNs", "Method"]], "rel": [["Absum", "SubClass-Of", "regularization method"], ["Absum", "Part-Of", "CNNs"]], "rel_plus": [["Absum:Method", "SubClass-Of", "regularization method:Method"], ["Absum:Method", "Part-Of", "CNNs:Method"]]}
{"doc_id": "202676714", "sentence": "Instead of the spectral norm , we use the induced \u221e - norm ( L \u221e operator norm ) since it is the upper bound of the spectral norm for convolution .", "ner": [["spectral norm", "Method"], ["\u221e - norm", "Method"], ["L \u221e operator norm", "Method"], ["spectral norm", "Method"], ["convolution", "Method"]], "rel": [["\u221e - norm", "Synonym-Of", "L \u221e operator norm"]], "rel_plus": [["\u221e - norm:Method", "Synonym-Of", "L \u221e operator norm:Method"]]}
{"doc_id": "202676714", "sentence": "Absum is as simple as standard regularization methods such as weight decay , but it can reduce sensitivity to SFA .", "ner": [["Absum", "Method"], ["standard regularization methods", "Method"], ["weight decay", "Method"], ["SFA", "Method"]], "rel": [["Absum", "SubClass-Of", "standard regularization methods"], ["weight decay", "SubClass-Of", "standard regularization methods"], ["Absum", "Used-For", "SFA"]], "rel_plus": [["Absum:Method", "SubClass-Of", "standard regularization methods:Method"], ["weight decay:Method", "SubClass-Of", "standard regularization methods:Method"], ["Absum:Method", "Used-For", "SFA:Method"]]}
{"doc_id": "202676714", "sentence": "Image recognition experiments on MNIST , Fashion - MNIST ( FMNIST ) , CIFAR 1 0 , CIFAR 1 0 0 , and SVHN demonstrate that Absum and SNC outperform L 1 and L 2 regularization methods in terms of improving robustness against SFA , and the computation time of Absum is about one - tenth that of SNC .", "ner": [["Image recognition", "Task"], ["MNIST", "Dataset"], ["Fashion - MNIST", "Dataset"], ["FMNIST", "Dataset"], ["CIFAR 1 0", "Dataset"], ["CIFAR 1 0 0", "Dataset"], ["SVHN", "Dataset"], ["Absum", "Method"], ["SNC", "Method"], ["L 1", "Method"], ["L 2 regularization", "Method"], ["robustness against SFA", "Task"], ["Absum", "Method"], ["SNC", "Method"]], "rel": [["MNIST", "Benchmark-For", "Image recognition"], ["Fashion - MNIST", "Benchmark-For", "Image recognition"], ["CIFAR 1 0", "Benchmark-For", "Image recognition"], ["CIFAR 1 0 0", "Benchmark-For", "Image recognition"], ["SVHN", "Benchmark-For", "Image recognition"], ["FMNIST", "Synonym-Of", "Fashion - MNIST"], ["Absum", "Compare-With", "L 1"], ["SNC", "Compare-With", "L 1"], ["Absum", "Compare-With", "L 2 regularization"], ["SNC", "Compare-With", "L 2 regularization"], ["SNC", "Used-For", "robustness against SFA"], ["Absum", "Used-For", "robustness against SFA"], ["L 1", "Used-For", "robustness against SFA"], ["L 2 regularization", "Used-For", "robustness against SFA"], ["Absum", "Compare-With", "SNC"]], "rel_plus": [["MNIST:Dataset", "Benchmark-For", "Image recognition:Task"], ["Fashion - MNIST:Dataset", "Benchmark-For", "Image recognition:Task"], ["CIFAR 1 0:Dataset", "Benchmark-For", "Image recognition:Task"], ["CIFAR 1 0 0:Dataset", "Benchmark-For", "Image recognition:Task"], ["SVHN:Dataset", "Benchmark-For", "Image recognition:Task"], ["FMNIST:Dataset", "Synonym-Of", "Fashion - MNIST:Dataset"], ["Absum:Method", "Compare-With", "L 1:Method"], ["SNC:Method", "Compare-With", "L 1:Method"], ["Absum:Method", "Compare-With", "L 2 regularization:Method"], ["SNC:Method", "Compare-With", "L 2 regularization:Method"], ["SNC:Method", "Used-For", "robustness against SFA:Task"], ["Absum:Method", "Used-For", "robustness against SFA:Task"], ["L 1:Method", "Used-For", "robustness against SFA:Task"], ["L 2 regularization:Method", "Used-For", "robustness against SFA:Task"], ["Absum:Method", "Compare-With", "SNC:Method"]]}
{"doc_id": "202676714", "sentence": "In the additional empirical evaluation , we reveal that robust CNNs against SFA can be robust against transferred attacks by using white - box attacks ( projected gradient descent : PGD ( Kurakin , Goodfellow , and Bengio 2 0 1 6 ; Madry et al. 2 0 1 8) ) .", "ner": [["CNNs", "Method"], ["against SFA", "Task"], ["white - box attacks", "Method"], ["projected gradient descent", "Method"], ["PGD", "Method"]], "rel": [["CNNs", "Used-For", "against SFA"], ["projected gradient descent", "SubClass-Of", "white - box attacks"], ["PGD", "Synonym-Of", "projected gradient descent"]], "rel_plus": [["CNNs:Method", "Used-For", "against SFA:Task"], ["projected gradient descent:Method", "SubClass-Of", "white - box attacks:Method"], ["PGD:Method", "Synonym-Of", "projected gradient descent:Method"]]}
{"doc_id": "202676714", "sentence": "As a further investigation of Absum and SNC , we reveal that adversarial perturbations for CNNs trained with Absum and SNC have little high - frequency components , i.e. , these CNNs are robust against high - frequency noise .", "ner": [["Absum", "Method"], ["SNC", "Method"], ["CNNs", "Method"], ["Absum", "Method"], ["SNC", "Method"], ["CNNs", "Method"]], "rel": [["Absum", "Part-Of", "CNNs"], ["SNC", "Part-Of", "CNNs"]], "rel_plus": [["Absum:Method", "Part-Of", "CNNs:Method"], ["SNC:Method", "Part-Of", "CNNs:Method"]]}
{"doc_id": "202676714", "sentence": "Furthermore , our experiments show that Absum is effective against PGD when using adversarial training .", "ner": [["Absum", "Method"], ["against PGD", "Task"], ["adversarial training", "Method"]], "rel": [["Absum", "Used-For", "against PGD"], ["Absum", "Part-Of", "adversarial training"]], "rel_plus": [["Absum:Method", "Used-For", "against PGD:Task"], ["Absum:Method", "Part-Of", "adversarial training:Method"]]}
{"doc_id": "202676714", "sentence": "The following are main contributions of this paper : \u2022 We show that SNC improves robustness against SFA .", "ner": [["SNC", "Method"], ["robustness against SFA", "Task"]], "rel": [["SNC", "Used-For", "robustness against SFA"]], "rel_plus": [["SNC:Method", "Used-For", "robustness against SFA:Task"]]}
{"doc_id": "202676714", "sentence": "SNC was proposed to improve generalization performance , but effectiveness in robustness against SFA had not been evaluated . \u2022 We propose Absum and its proximal operator .", "ner": [["SNC", "Method"], ["robustness against SFA", "Task"], ["Absum", "Method"], ["proximal operator", "Method"]], "rel": [["SNC", "Used-For", "robustness against SFA"], ["Absum", "SubClass-Of", "proximal operator"]], "rel_plus": [["SNC:Method", "Used-For", "robustness against SFA:Task"], ["Absum:Method", "SubClass-Of", "proximal operator:Method"]]}
{"doc_id": "202676714", "sentence": "Absum improves robustness against SFA as well as SNC while its computational cost is lower than that of SNC . \u2022 In the futher empirical evaluation , Absum and SNC can also improve robustness against other black - box attacks ( transferred attacks and High - Frequency attacks ) .", "ner": [["Absum", "Method"], ["robustness against SFA", "Task"], ["SNC", "Method"], ["SNC", "Method"], ["Absum", "Method"], ["SNC", "Method"], ["black - box attacks", "Method"], ["transferred attacks", "Method"], ["High - Frequency attacks", "Method"]], "rel": [["Absum", "Used-For", "robustness against SFA"], ["SNC", "Used-For", "robustness against SFA"], ["Absum", "Compare-With", "SNC"], ["High - Frequency attacks", "SubClass-Of", "black - box attacks"], ["transferred attacks", "SubClass-Of", "black - box attacks"]], "rel_plus": [["Absum:Method", "Used-For", "robustness against SFA:Task"], ["SNC:Method", "Used-For", "robustness against SFA:Task"], ["Absum:Method", "Compare-With", "SNC:Method"], ["High - Frequency attacks:Method", "SubClass-Of", "black - box attacks:Method"], ["transferred attacks:Method", "SubClass-Of", "black - box attacks:Method"]]}
{"doc_id": "202676714", "sentence": "In addition , Absum can improve robustness against PGD when used with adversarial training .", "ner": [["Absum", "Method"], ["robustness against PGD", "Task"], ["adversarial training", "Method"]], "rel": [["Absum", "Used-For", "robustness against PGD"], ["Absum", "Part-Of", "adversarial training"]], "rel_plus": [["Absum:Method", "Used-For", "robustness against PGD:Task"], ["Absum:Method", "Part-Of", "adversarial training:Method"]]}
{"doc_id": "202676714", "sentence": "In this section , we outline CNNs , ReLUs , and a circulant matrix for convolution operation .", "ner": [["CNNs", "Method"], ["ReLUs", "Method"], ["circulant matrix", "Method"], ["convolution operation", "Method"]], "rel": [["circulant matrix", "Part-Of", "convolution operation"]], "rel_plus": [["circulant matrix:Method", "Part-Of", "convolution operation:Method"]]}
{"doc_id": "202676714", "sentence": "After the convolution , we usually use ReLU activations as the following function : Typical model architectures use a combination of convolution and ReLU .", "ner": [["convolution", "Method"], ["ReLU", "Method"], ["convolution", "Method"], ["ReLU", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202676714", "sentence": "For example , a standard block of ResNet ( He et al. 2 0 1 6 ) is composed as where BN is batch normalization ( Ioffe and Szegedy 2 0 1 5 ) .", "ner": [["ResNet", "Method"], ["BN", "Method"], ["batch normalization", "Method"]], "rel": [["BN", "Part-Of", "ResNet"], ["BN", "Synonym-Of", "batch normalization"]], "rel_plus": [["BN:Method", "Part-Of", "ResNet:Method"], ["BN:Method", "Synonym-Of", "batch normalization:Method"]]}
{"doc_id": "202676714", "sentence": "Since SFA and Absum are based on a circulant matrix for convolution operation , we show that the convolution can be expressed as a product of a vector and doubly block circulant matrix .", "ner": [["SFA", "Method"], ["Absum", "Method"], ["circulant matrix", "Method"], ["convolution operation", "Method"], ["convolution", "Method"], ["block circulant matrix", "Method"]], "rel": [["circulant matrix", "Part-Of", "SFA"], ["circulant matrix", "Part-Of", "Absum"], ["circulant matrix", "Part-Of", "convolution operation"], ["block circulant matrix", "Part-Of", "convolution"]], "rel_plus": [["circulant matrix:Method", "Part-Of", "SFA:Method"], ["circulant matrix:Method", "Part-Of", "Absum:Method"], ["circulant matrix:Method", "Part-Of", "convolution operation:Method"], ["block circulant matrix:Method", "Part-Of", "convolution:Method"]]}
{"doc_id": "202676714", "sentence": "SFA is composed of ( F ) l \u2297 ( F ) m and its complex conjugate ( F ) n\u2212l \u2297 ( F ) n\u2212m to create a perturbation that has real values since inputs of CNNs are assumed to be real values .", "ner": [["SFA", "Method"], ["CNNs", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202676714", "sentence": "Figure 1 shows examples of CIFAR 1 0 perturbed by SFA .", "ner": [["CIFAR 1 0", "Dataset"], ["SFA", "Method"]], "rel": [["SFA", "Used-For", "CIFAR 1 0"]], "rel_plus": [["SFA:Method", "Used-For", "CIFAR 1 0:Dataset"]]}
{"doc_id": "202676714", "sentence": "To understand the vulnerability of CNNs , several studies focused on sensitivity of CNNs in the frequency domain ( Yin et al. 2 0 1 9 ; Das et al. 2 0 1 8 ; Liu et al. 2 0 1 9 ) .", "ner": [["CNNs", "Method"], ["CNNs", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202676714", "sentence": "Jo and Bengio ( 2 0 1 7 ) and show that CNNs misclassify images processed by low - pass filters and call this a High - Frequency attack , which is a simple black - box adversarial attack .", "ner": [["CNNs", "Method"], ["High - Frequency attack", "Method"], ["black - box adversarial attack", "Method"]], "rel": [["High - Frequency attack", "SubClass-Of", "black - box adversarial attack"]], "rel_plus": [["High - Frequency attack:Method", "SubClass-Of", "black - box adversarial attack:Method"]]}
{"doc_id": "202676714", "sentence": "Note that Absum can be used with adversarial training .", "ner": [["Absum", "Method"], ["adversarial training", "Method"]], "rel": [["Absum", "Part-Of", "adversarial training"]], "rel_plus": [["Absum:Method", "Part-Of", "adversarial training:Method"]]}
{"doc_id": "202676714", "sentence": "On the other hand , SFA only uses the information that the target model is composed of CNNs and is more practical .", "ner": [["SFA", "Method"], ["CNNs", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202676714", "sentence": "As standard regularization methods , L 2 regularization ( weight decay ) is commonly used for improving generalization performance due to its simplicity .", "ner": [["standard regularization methods", "Method"], ["L 2 regularization", "Method"], ["weight decay", "Method"]], "rel": [["L 2 regularization", "SubClass-Of", "standard regularization methods"], ["weight decay", "SubClass-Of", "standard regularization methods"]], "rel_plus": [["L 2 regularization:Method", "SubClass-Of", "standard regularization methods:Method"], ["weight decay:Method", "SubClass-Of", "standard regularization methods:Method"]]}
{"doc_id": "202676714", "sentence": "In this section , we first show that SNC can improve robustness against SFA .", "ner": [["SNC", "Method"], ["robustness against SFA", "Task"]], "rel": [["SNC", "Used-For", "robustness against SFA"]], "rel_plus": [["SNC:Method", "Used-For", "robustness against SFA:Task"]]}
{"doc_id": "202676714", "sentence": "Finally , we discuss Absum and its proximal operator , which is an efficient defense method against SFA .", "ner": [["Absum", "Method"], ["proximal operator", "Method"], ["against SFA", "Task"]], "rel": [["proximal operator", "Part-Of", "Absum"], ["Absum", "Used-For", "against SFA"]], "rel_plus": [["proximal operator:Method", "Part-Of", "Absum:Method"], ["Absum:Method", "Used-For", "against SFA:Task"]]}
{"doc_id": "202676714", "sentence": "SFA is based on the following properties of linear transform : where \u03c3 is the largest singular value ( spectral norm or induced 2 - norm ) , and v is the right singular vector corresponding to \u03c3 .", "ner": [["SFA", "Method"], ["spectral norm", "Method"], ["induced 2 - norm", "Method"]], "rel": [["spectral norm", "Part-Of", "SFA"], ["induced 2 - norm", "Part-Of", "SFA"]], "rel_plus": [["spectral norm:Method", "Part-Of", "SFA:Method"], ["induced 2 - norm:Method", "Part-Of", "SFA:Method"]]}
{"doc_id": "202676714", "sentence": "Since the spectral norm determines the impact of SFA , we can reduce sensitivity to SFA by constraining the spectral norm .", "ner": [["spectral norm", "Method"], ["SFA", "Method"], ["SFA", "Method"], ["spectral norm", "Method"]], "rel": [["spectral norm", "Part-Of", "SFA"]], "rel_plus": [["spectral norm:Method", "Part-Of", "SFA:Method"]]}
{"doc_id": "202676714", "sentence": "The constraint of the spectral norm for CNNs ( i.e. , SNC ) ( Sedghi , Gupta , and Long 2 0 1 9 ; Gouk et al. 2 0 1 8 ) was proposed in the context of improving generalization performance .", "ner": [["spectral norm", "Method"], ["CNNs", "Method"], ["SNC", "Method"]], "rel": [["spectral norm", "Part-Of", "CNNs"]], "rel_plus": [["spectral norm:Method", "Part-Of", "CNNs:Method"]]}
{"doc_id": "202676714", "sentence": "However , the constraints of the exact spectral norm 2 of A incurs large computation cost ; the O(n 2 c 2 ( c + log(n ) ) ) time for each convolution when input size is n \u00d7 n , and the numbers of input and output channels are c even if we use the efficient spectral norm constraints ( Sedghi , Gupta , and Long 2 0 1 9 ) .", "ner": [["spectral norm", "Method"], ["spectral norm", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202676714", "sentence": "Figure 2 shows the test accuracy of models , which is trained with L 1 regularization , on data perturbed by SFA against the regularization weight \u03bb .", "ner": [["L 1 regularization", "Method"], ["SFA", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202676714", "sentence": "Note that weight decay can also penalize the spectral norm ( in the appendix ) and imposes tight regularization , as discussed in the experiments section .", "ner": [["weight decay", "Method"], ["spectral norm", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202676714", "sentence": "As described in eq. ( 3 ) , ReLUs are used before convolution as K * ReLU ( \u00b7 ) .", "ner": [["ReLUs", "Method"], ["convolution", "Method"], ["ReLU", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202676714", "sentence": "The K ( i ) is the filter matrix of the i - th convolution , and L is the number of convolution filters . 3 Figure 3 shows search spaces of Absum ( blue ) and L 1 regularization ( red ) when we have two parameters .", "ner": [["convolution", "Method"], ["convolution", "Method"], ["Absum", "Method"], ["L 1 regularization", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202676714", "sentence": "The constraint of Absum is looser than L 1 regularization because a large element k Note that the search space of weight decay is also the point K = O when ||K|| F = 0 .", "ner": [["Absum", "Method"], ["L 1 regularization", "Method"], ["weight decay", "Method"]], "rel": [["Absum", "Compare-With", "L 1 regularization"]], "rel_plus": [["Absum:Method", "Compare-With", "L 1 regularization:Method"]]}
{"doc_id": "202676714", "sentence": "Therefore , the loss function with Absum can be lower than that with L 1 regularization and weight decay if we use a large \u03bb .", "ner": [["Absum", "Method"], ["L 1 regularization", "Method"], ["weight decay", "Method"]], "rel": [["Absum", "Compare-With", "L 1 regularization"], ["Absum", "Compare-With", "weight decay"]], "rel_plus": [["Absum:Method", "Compare-With", "L 1 regularization:Method"], ["Absum:Method", "Compare-With", "weight decay:Method"]]}
{"doc_id": "202676714", "sentence": "By using the proximal operator after stochastic gradient descent ( SGD ) , we update the i - th convolution filter : where \u03b7 is a learning rate , and B is a minibatch size .", "ner": [["stochastic gradient descent", "Method"], ["SGD", "Method"], ["convolution", "Method"]], "rel": [["SGD", "Synonym-Of", "stochastic gradient descent"]], "rel_plus": [["SGD:Method", "Synonym-Of", "stochastic gradient descent:Method"]]}
{"doc_id": "202676714", "sentence": "We can also compute weight decay and L 1 regularization in O(h 2 ) since the number of parameters in each convolution is h 2 .", "ner": [["weight decay", "Method"], ["L 1 regularization", "Method"], ["convolution", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202676714", "sentence": "Therefore , the order of computational complexity of Absum is the same as those of weight decay and L 1 regularization .", "ner": [["Absum", "Method"], ["weight decay", "Method"], ["L 1 regularization", "Method"]], "rel": [["Absum", "Compare-With", "weight decay"], ["Absum", "Compare-With", "L 1 regularization"]], "rel_plus": [["Absum:Method", "Compare-With", "weight decay:Method"], ["Absum:Method", "Compare-With", "L 1 regularization:Method"]]}
{"doc_id": "202676714", "sentence": "When we have c input channels and c output channels , the computational costs of Absum , weight decay , and L 1 regularization are O(c 2 h 2 ) and less than that of SNC O(c 2 n 2 ( c+log(n ) ) ) where n \u2265 h. Note that the loss function f for training deep neural networks is usually non - convex while g(K ) is convex .", "ner": [["Absum", "Method"], ["weight decay", "Method"], ["L 1 regularization", "Method"], ["SNC", "Method"], ["deep neural networks", "Method"]], "rel": [["L 1 regularization", "Compare-With", "SNC"], ["weight decay", "Compare-With", "SNC"], ["Absum", "Compare-With", "SNC"]], "rel_plus": [["L 1 regularization:Method", "Compare-With", "SNC:Method"], ["weight decay:Method", "Compare-With", "SNC:Method"], ["Absum:Method", "Compare-With", "SNC:Method"]]}
{"doc_id": "202676714", "sentence": "We discuss the evaluation of the effectiveness of SNC and Absum in improving robustness against SFA .", "ner": [["SNC", "Method"], ["Absum", "Method"], ["robustness against SFA", "Task"]], "rel": [["SNC", "Used-For", "robustness against SFA"], ["Absum", "Used-For", "robustness against SFA"]], "rel_plus": [["SNC:Method", "Used-For", "robustness against SFA:Task"], ["Absum:Method", "Used-For", "robustness against SFA:Task"]]}
{"doc_id": "202676714", "sentence": "Next , we show that Absum is more efficient than SNC especially when the size of input images and models are large .", "ner": [["Absum", "Method"], ["SNC", "Method"]], "rel": [["Absum", "Compare-With", "SNC"]], "rel_plus": [["Absum:Method", "Compare-With", "SNC:Method"]]}
{"doc_id": "202676714", "sentence": "Finally , as the further investigation , we discuss the evaluation of the performance of Absum and SNC in terms of robustness against transferred attacks , vulnerability in frequency domain , and robustness against PGD when used with adversarial training .", "ner": [["Absum", "Method"], ["SNC", "Method"], ["robustness against transferred attacks", "Task"], ["vulnerability in frequency domain", "Task"], ["robustness against PGD", "Task"], ["adversarial training", "Method"]], "rel": [["Absum", "Used-For", "robustness against transferred attacks"], ["SNC", "Used-For", "robustness against transferred attacks"], ["SNC", "Used-For", "vulnerability in frequency domain"], ["Absum", "Used-For", "vulnerability in frequency domain"], ["Absum", "Used-For", "robustness against PGD"], ["SNC", "Used-For", "robustness against PGD"], ["Absum", "Part-Of", "adversarial training"], ["SNC", "Part-Of", "adversarial training"]], "rel_plus": [["Absum:Method", "Used-For", "robustness against transferred attacks:Task"], ["SNC:Method", "Used-For", "robustness against transferred attacks:Task"], ["SNC:Method", "Used-For", "vulnerability in frequency domain:Task"], ["Absum:Method", "Used-For", "vulnerability in frequency domain:Task"], ["Absum:Method", "Used-For", "robustness against PGD:Task"], ["SNC:Method", "Used-For", "robustness against PGD:Task"], ["Absum:Method", "Part-Of", "adversarial training:Method"], ["SNC:Method", "Part-Of", "adversarial training:Method"]]}
{"doc_id": "202676714", "sentence": "To evaluate effectiveness , we conducted experiments of image recognition on MNIST ( LeCun et al. 1 9 9 8   We provide details of the experimental conditions in the appendix .", "ner": [["image recognition", "Task"], ["MNIST", "Dataset"]], "rel": [["MNIST", "Benchmark-For", "image recognition"]], "rel_plus": [["MNIST:Dataset", "Benchmark-For", "image recognition:Task"]]}
{"doc_id": "202676714", "sentence": "In all experiments , we selected the best regularization weight from among [ 1 0 1 , 1 0 0 , . . . , 1 0 \u2212 7 ] for Absum and standard regularization methods , and the best spectral norm \u03c3 from among [ 0.0 1 , 0. 1 , 0. 5 , 1. 0 , 1 0 ] for SNC .", "ner": [["Absum", "Method"], ["standard regularization methods", "Method"], ["spectral norm", "Method"], ["SNC", "Method"]], "rel": [["spectral norm", "Part-Of", "SNC"]], "rel_plus": [["spectral norm:Method", "Part-Of", "SNC:Method"]]}
{"doc_id": "202676714", "sentence": "For MNIST and FMNIST , we stacked two convolutional layers and two fully connected layers and used ReLUs as activation functions .", "ner": [["MNIST", "Dataset"], ["FMNIST", "Dataset"], ["convolutional layers", "Method"], ["fully connected layers", "Method"], ["ReLUs", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202676714", "sentence": "For CIFAR 1 0 , CI - FAR 1 0 0 , and SVHN , the model architecture was ResNet - 1 8 ( He et al. 2 0 1 6 ) .", "ner": [["CIFAR 1 0", "Dataset"], ["CI - FAR 1 0 0", "Dataset"], ["SVHN", "Dataset"], ["ResNet - 1 8", "Method"]], "rel": [["ResNet - 1 8", "Evaluated-With", "CIFAR 1 0"], ["ResNet - 1 8", "Evaluated-With", "CI - FAR 1 0 0"], ["ResNet - 1 8", "Evaluated-With", "SVHN"]], "rel_plus": [["ResNet - 1 8:Method", "Evaluated-With", "CIFAR 1 0:Dataset"], ["ResNet - 1 8:Method", "Evaluated-With", "CI - FAR 1 0 0:Dataset"], ["ResNet - 1 8:Method", "Evaluated-With", "SVHN:Dataset"]]}
{"doc_id": "202676714", "sentence": "We used SFA with l , m \u2208 { 0 , 1 , . . . , 2 7 } and \u03b5 = 8 0 / 2 5 5 on MNIST and FMNIST , and l , m \u2208 { 0 , 1 , . . . , 3 1 } and \u03b5 = 1 0 / 2 5 5 on CIFAR 1 0 , CIFAR 1 0 0 , and SVHN .", "ner": [["SFA", "Method"], ["MNIST", "Dataset"], ["FMNIST", "Dataset"], ["CIFAR 1 0", "Dataset"], ["CIFAR 1 0 0", "Dataset"], ["SVHN", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "202676714", "sentence": "In addition , we used PGD to evaluate robustness against transferred attacks and white - box attacks since PGD is a sophisticated white - box attack .", "ner": [["PGD", "Method"], ["robustness against transferred attacks", "Task"], ["white - box attacks", "Task"], ["PGD", "Method"], ["white - box attack", "Task"]], "rel": [["PGD", "Used-For", "robustness against transferred attacks"], ["PGD", "Used-For", "white - box attacks"]], "rel_plus": [["PGD:Method", "Used-For", "robustness against transferred attacks:Task"], ["PGD:Method", "Used-For", "white - box attacks:Task"]]}
{"doc_id": "202676714", "sentence": "In addition to naive training , we evaluated robustness against PGD when we used adversarial training ( Kurakin , Goodfellow , and Bengio 2 0 1 6 ; Madry et al. 2 0 1 8 ) with each method because Absum can be used with it due to its simplicity .", "ner": [["robustness against PGD", "Task"], ["adversarial training", "Method"], ["Absum", "Method"]], "rel": [["adversarial training", "Used-For", "robustness against PGD"], ["Absum", "Part-Of", "adversarial training"]], "rel_plus": [["adversarial training:Method", "Used-For", "robustness against PGD:Task"], ["Absum:Method", "Part-Of", "adversarial training:Method"]]}
{"doc_id": "202676714", "sentence": "The L \u221e norm of the perturbation \u03b5 was set to \u03b5 = 0. 3 for MNIST and FMNIST and \u03b5 = 8/ 2 5 5 for CIFAR 1 0 , CI - FAR 1 0 0 , and SVHN at training time .", "ner": [["MNIST", "Dataset"], ["FMNIST", "Dataset"], ["CIFAR 1 0", "Dataset"], ["CI - FAR 1 0 0", "Dataset"], ["SVHN", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "202676714", "sentence": "For PGD , we updated the perturbation for 4 0 iterations with a step size of 0.0 1 on MNIST and FMNIST at training and evaluation times , and on CIFAR 1 0 , CIFAR 1 0 0 , and SVHN , for 7 iterations with a step size of 2/ 2 5 5 at training time and 1 0 0 iterations with the same step size at evaluation time .", "ner": [["PGD", "Method"], ["MNIST", "Dataset"], ["FMNIST", "Dataset"], ["CIFAR 1 0", "Dataset"], ["CIFAR 1 0 0", "Dataset"], ["SVHN", "Dataset"]], "rel": [["PGD", "Evaluated-With", "MNIST"], ["PGD", "Evaluated-With", "FMNIST"], ["PGD", "Evaluated-With", "CIFAR 1 0"], ["PGD", "Evaluated-With", "CIFAR 1 0 0"], ["PGD", "Evaluated-With", "SVHN"]], "rel_plus": [["PGD:Method", "Evaluated-With", "MNIST:Dataset"], ["PGD:Method", "Evaluated-With", "FMNIST:Dataset"], ["PGD:Method", "Evaluated-With", "CIFAR 1 0:Dataset"], ["PGD:Method", "Evaluated-With", "CIFAR 1 0 0:Dataset"], ["PGD:Method", "Evaluated-With", "SVHN:Dataset"]]}
{"doc_id": "202676714", "sentence": "Robustness against SFA Table 1 lists the accuracies of each method on test data perturbed by SFA and selected \u03bb and \u03c3 .", "ner": [["Robustness against SFA", "Task"], ["SFA", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202676714", "sentence": "In this table , Avg . denote robust accuracies against SFA averaged over ( l , m ) , and Min . denotes minimum accuracies among hyperparameters ( l , m ) , i.e. , robust accuracies against optimized SFA .", "ner": [["against SFA", "Task"], ["SFA", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202676714", "sentence": "In Tab . 1 , Absum and SNC are more robust against SFA compared with WD and L 1 .", "ner": [["Absum", "Method"], ["SNC", "Method"], ["robust against SFA", "Task"], ["WD", "Method"], ["L 1", "Method"]], "rel": [["SNC", "Used-For", "robust against SFA"], ["Absum", "Used-For", "robust against SFA"], ["WD", "Used-For", "robust against SFA"], ["L 1", "Used-For", "robust against SFA"], ["Absum", "Compare-With", "WD"], ["SNC", "Compare-With", "WD"], ["Absum", "Compare-With", "L 1"], ["SNC", "Compare-With", "L 1"]], "rel_plus": [["SNC:Method", "Used-For", "robust against SFA:Task"], ["Absum:Method", "Used-For", "robust against SFA:Task"], ["WD:Method", "Used-For", "robust against SFA:Task"], ["L 1:Method", "Used-For", "robust against SFA:Task"], ["Absum:Method", "Compare-With", "WD:Method"], ["SNC:Method", "Compare-With", "WD:Method"], ["Absum:Method", "Compare-With", "L 1:Method"], ["SNC:Method", "Compare-With", "L 1:Method"]]}
{"doc_id": "202676714", "sentence": "Although SNC is more robust than Absum on CIFAR 1 0 and CIFAR 1 0 0 , clean accuracies of SNC are less than those of Absum and the computation time of SNC is larger than that of Absum as discussed below .", "ner": [["SNC", "Method"], ["Absum", "Method"], ["CIFAR 1 0", "Dataset"], ["CIFAR 1 0 0", "Dataset"], ["SNC", "Method"], ["Absum", "Method"], ["SNC", "Method"], ["Absum", "Method"]], "rel": [["SNC", "Compare-With", "Absum"], ["SNC", "Evaluated-With", "CIFAR 1 0"], ["Absum", "Evaluated-With", "CIFAR 1 0"], ["SNC", "Evaluated-With", "CIFAR 1 0 0"], ["Absum", "Evaluated-With", "CIFAR 1 0 0"], ["SNC", "Compare-With", "Absum"], ["SNC", "Compare-With", "Absum"]], "rel_plus": [["SNC:Method", "Compare-With", "Absum:Method"], ["SNC:Method", "Evaluated-With", "CIFAR 1 0:Dataset"], ["Absum:Method", "Evaluated-With", "CIFAR 1 0:Dataset"], ["SNC:Method", "Evaluated-With", "CIFAR 1 0 0:Dataset"], ["Absum:Method", "Evaluated-With", "CIFAR 1 0 0:Dataset"], ["SNC:Method", "Compare-With", "Absum:Method"], ["SNC:Method", "Compare-With", "Absum:Method"]]}
{"doc_id": "202676714", "sentence": "Figure 4 shows the test accuracies of the methods on MNIST and CIFAR 1 0 perturbed by SFA against regularization weights .", "ner": [["MNIST", "Dataset"], ["CIFAR 1 0", "Dataset"], ["SFA", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202676714", "sentence": "However , L 1 and WD significantly decrease in accuracy when the regularization weight is higher than 1 0 \u2212 1 .", "ner": [["L 1", "Method"], ["WD", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202676714", "sentence": "WD and L 1 with a large \u03bb prevent minimization of the training loss .", "ner": [["WD", "Method"], ["L 1", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202676714", "sentence": "On the other hand , Absum with a large \u03bb can decrease the training loss because the search space of In conclusion , standard regularization methods might not be effective in improving robustness against SFA because the high regularization weight imposes too tight of constraints to minimize the loss function .", "ner": [["Absum", "Method"], ["standard regularization methods", "Method"], ["robustness against SFA", "Task"]], "rel": [["Absum", "Used-For", "robustness against SFA"], ["standard regularization methods", "Used-For", "robustness against SFA"]], "rel_plus": [["Absum:Method", "Used-For", "robustness against SFA:Task"], ["standard regularization methods:Method", "Used-For", "robustness against SFA:Task"]]}
{"doc_id": "202676714", "sentence": "On the other hand , Absum imposes looser constraints ; thus , we can improve robustness while maintaining classification performance .", "ner": [["Absum", "Method"], ["classification", "Task"]], "rel": [["Absum", "Used-For", "classification"]], "rel_plus": [["Absum:Method", "Used-For", "classification:Task"]]}
{"doc_id": "202676714", "sentence": "As shown in Fig. 6 ( a ) , Absum is about ten times faster than SNC on 3 2 \u00d7 3 2 image datasets with ResNet 1 8 .", "ner": [["Absum", "Method"], ["SNC", "Method"], ["ResNet 1 8", "Method"]], "rel": [["Absum", "Compare-With", "SNC"], ["Absum", "Used-For", "ResNet 1 8"], ["SNC", "Used-For", "ResNet 1 8"]], "rel_plus": [["Absum:Method", "Compare-With", "SNC:Method"], ["Absum:Method", "Used-For", "ResNet 1 8:Method"], ["SNC:Method", "Used-For", "ResNet 1 8:Method"]]}
{"doc_id": "202676714", "sentence": "The runtime of SNC is comparable to those of other methods on MNIST and FMNIST because we use only two convolution layers , and image sizes of these datasets are smaller than other datasets .", "ner": [["SNC", "Method"], ["MNIST", "Dataset"], ["FMNIST", "Dataset"], ["convolution layers", "Method"]], "rel": [["SNC", "Evaluated-With", "MNIST"], ["SNC", "Evaluated-With", "FMNIST"]], "rel_plus": [["SNC:Method", "Evaluated-With", "MNIST:Dataset"], ["SNC:Method", "Evaluated-With", "FMNIST:Dataset"]]}
{"doc_id": "202676714", "sentence": "In Fig. 6 ( b ) , the runtime of Absum does not increase significantly compared with SNC and the increase in the runtime of Absum is similar to those of standard regularization methods .", "ner": [["Absum", "Method"], ["SNC", "Method"], ["Absum", "Method"], ["standard regularization methods", "Method"]], "rel": [["Absum", "Compare-With", "SNC"], ["Absum", "Compare-With", "standard regularization methods"]], "rel_plus": [["Absum:Method", "Compare-With", "SNC:Method"], ["Absum:Method", "Compare-With", "standard regularization methods:Method"]]}
{"doc_id": "202676714", "sentence": "Since SNC incurs large computational cost and depends on the in -   Robustness against Transferred Attacks Sensitivity to SFA is caused by convolution operation and is universal for CNNs .", "ner": [["SNC", "Method"], ["SFA", "Method"], ["convolution operation", "Method"], ["CNNs", "Method"]], "rel": [["convolution operation", "Part-Of", "CNNs"]], "rel_plus": [["convolution operation:Method", "Part-Of", "CNNs:Method"]]}
{"doc_id": "202676714", "sentence": "This sensitivity might be a cause of transferability of adversarial attacks , and robust CNNs against SFA can be robust against transferred attacks .", "ner": [["CNNs", "Method"], ["against SFA", "Task"]], "rel": [["CNNs", "Used-For", "against SFA"]], "rel_plus": [["CNNs:Method", "Used-For", "against SFA:Task"]]}
{"doc_id": "202676714", "sentence": "Absum and SNC improve robustness compared to WD and L 1 .", "ner": [["Absum", "Method"], ["SNC", "Method"], ["WD", "Method"], ["L 1", "Method"]], "rel": [["SNC", "Compare-With", "WD"], ["Absum", "Compare-With", "WD"], ["SNC", "Compare-With", "L 1"], ["Absum", "Compare-With", "L 1"]], "rel_plus": [["SNC:Method", "Compare-With", "WD:Method"], ["Absum:Method", "Compare-With", "WD:Method"], ["SNC:Method", "Compare-With", "L 1:Method"], ["Absum:Method", "Compare-With", "L 1:Method"]]}
{"doc_id": "202676714", "sentence": "Sensitivity in Frequency Domain Several studies show that CNNs are sensitive to high - frequency noise unlike human visual systems since CNNs are biased towards highfrequency information Yin et al. 2 0 1 9 ) .", "ner": [["CNNs", "Method"], ["CNNs", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202676714", "sentence": "From the robustness against SFA , which is regarded as single - frequency noise , Absum and SNC can be expected not to bias CNNs towards high - frequency information .", "ner": [["robustness against SFA", "Task"], ["Absum", "Method"], ["SNC", "Method"], ["CNNs", "Method"]], "rel": [["SNC", "Part-Of", "CNNs"], ["Absum", "Part-Of", "CNNs"]], "rel_plus": [["SNC:Method", "Part-Of", "CNNs:Method"], ["Absum:Method", "Part-Of", "CNNs:Method"]]}
{"doc_id": "202676714", "sentence": "Figure 7 shows the power spectra of PGD perturbations on CIFAR 1 0 and Tab . 3 lists the accuracies on the test data processed by High - Frequency attacks .", "ner": [["PGD", "Method"], ["CIFAR 1 0", "Dataset"], ["Tab", "Dataset"], ["High - Frequency attacks", "Task"]], "rel": [["PGD", "Evaluated-With", "CIFAR 1 0"], ["PGD", "Evaluated-With", "Tab"], ["PGD", "Used-For", "High - Frequency attacks"]], "rel_plus": [["PGD:Method", "Evaluated-With", "CIFAR 1 0:Dataset"], ["PGD:Method", "Evaluated-With", "Tab:Dataset"], ["PGD:Method", "Used-For", "High - Frequency attacks:Task"]]}
{"doc_id": "202676714", "sentence": "This figure shows that vulnerabilities of WD and L 1 are biased in the high - frequency domain , while vulnerability of SNC is highly biased in the low - frequency domain .", "ner": [["WD", "Method"], ["L 1", "Method"], ["SNC", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202676714", "sentence": "Due to these characteristics , SNC and Absum are more robust against High - Frequency attacks than WD and L 1 ( Tab . 3 ) .", "ner": [["SNC", "Method"], ["Absum", "Method"], ["robust against High - Frequency attacks", "Task"], ["WD", "Method"], ["L 1", "Method"]], "rel": [["SNC", "Used-For", "robust against High - Frequency attacks"], ["Absum", "Used-For", "robust against High - Frequency attacks"], ["WD", "Used-For", "robust against High - Frequency attacks"], ["L 1", "Used-For", "robust against High - Frequency attacks"], ["SNC", "Compare-With", "WD"], ["Absum", "Compare-With", "WD"], ["SNC", "Compare-With", "L 1"], ["Absum", "Compare-With", "L 1"]], "rel_plus": [["SNC:Method", "Used-For", "robust against High - Frequency attacks:Task"], ["Absum:Method", "Used-For", "robust against High - Frequency attacks:Task"], ["WD:Method", "Used-For", "robust against High - Frequency attacks:Task"], ["L 1:Method", "Used-For", "robust against High - Frequency attacks:Task"], ["SNC:Method", "Compare-With", "WD:Method"], ["Absum:Method", "Compare-With", "WD:Method"], ["SNC:Method", "Compare-With", "L 1:Method"], ["Absum:Method", "Compare-With", "L 1:Method"]]}
{"doc_id": "202676714", "sentence": "Since human visual systems can perceive low - frequency noise better than high - frequency noise , attacks for Absum and SNC might be more perceptible than attacks for WD and L 1 .", "ner": [["Absum", "Method"], ["SNC", "Method"], ["WD", "Method"], ["L 1", "Method"]], "rel": [["Absum", "Compare-With", "WD"], ["SNC", "Compare-With", "WD"], ["Absum", "Compare-With", "L 1"], ["SNC", "Compare-With", "L 1"]], "rel_plus": [["Absum:Method", "Compare-With", "WD:Method"], ["SNC:Method", "Compare-With", "WD:Method"], ["Absum:Method", "Compare-With", "L 1:Method"], ["SNC:Method", "Compare-With", "L 1:Method"]]}
{"doc_id": "202676714", "sentence": "Note that Absum is more robust against high - pass filtering than SNC , which is presented in the appendix .", "ner": [["Absum", "Method"], ["SNC", "Method"]], "rel": [["Absum", "Compare-With", "SNC"]], "rel_plus": [["Absum:Method", "Compare-With", "SNC:Method"]]}
{"doc_id": "202676714", "sentence": "This result supports that Absum does not bias CNNs towards a specific frequency domain while SNC biases CNNs towards the low - frequency domain .", "ner": [["Absum", "Method"], ["CNNs", "Method"], ["SNC", "Method"], ["CNNs", "Method"]], "rel": [["Absum", "Compare-With", "SNC"]], "rel_plus": [["Absum:Method", "Compare-With", "SNC:Method"]]}
{"doc_id": "202676714", "sentence": "Robustness against PGD with Adversarial Training Table 4 lists the accuracies of models trained by adversarial training on data perturbed by PGD .", "ner": [["Robustness against PGD", "Task"], ["Adversarial Training", "Method"], ["adversarial training", "Method"], ["PGD", "Method"]], "rel": [["Adversarial Training", "Used-For", "Robustness against PGD"], ["PGD", "Used-For", "adversarial training"]], "rel_plus": [["Adversarial Training:Method", "Used-For", "Robustness against PGD:Task"], ["PGD:Method", "Used-For", "adversarial training:Method"]]}
{"doc_id": "202676714", "sentence": "When using adversarial training , Absum improves robustness against PGD , the highest among regularization methods , on almost all datasets .", "ner": [["adversarial training", "Method"], ["Absum", "Method"], ["robustness against PGD", "Task"]], "rel": [["Absum", "Used-For", "robustness against PGD"]], "rel_plus": [["Absum:Method", "Used-For", "robustness against PGD:Task"]]}
{"doc_id": "202676714", "sentence": "This implies that sensitivity to SFA is one of the causes of vulnerabilities of CNNs .", "ner": [["SFA", "Method"], ["CNNs", "Method"]], "rel": [["SFA", "Part-Of", "CNNs"]], "rel_plus": [["SFA:Method", "Part-Of", "CNNs:Method"]]}
{"doc_id": "202676714", "sentence": "The \u03bb of Absum tends to be higher than the \u03bb of WD and L 1 ; thus , Absum can also improve robustness against PGD without deteriorating classification performance due to its looseness .", "ner": [["Absum", "Method"], ["WD", "Method"], ["L 1", "Method"], ["Absum", "Method"], ["robustness against PGD", "Task"], ["classification", "Task"]], "rel": [["Absum", "Compare-With", "WD"], ["Absum", "Compare-With", "L 1"], ["Absum", "Used-For", "robustness against PGD"]], "rel_plus": [["Absum:Method", "Compare-With", "WD:Method"], ["Absum:Method", "Compare-With", "L 1:Method"], ["Absum:Method", "Used-For", "robustness against PGD:Task"]]}
{"doc_id": "202676714", "sentence": "Note that Absum does not improve robustness against PGD whithout adversarial training since the structural sensitivity of CNNs does not necessarily cause all vulnerabilities of CNN - based models ( we discussed this in the appendix ) .", "ner": [["Absum", "Method"], ["robustness against PGD", "Task"], ["adversarial training", "Method"], ["CNNs", "Method"], ["CNN", "Method"]], "rel": [["Absum", "Used-For", "robustness against PGD"]], "rel_plus": [["Absum:Method", "Used-For", "robustness against PGD:Task"]]}
{"doc_id": "202676714", "sentence": "Even so , Absum is more effective than other standard regularizations since it can efficiently improve robustness against black - box attacks ( SFA , transferred attacks , and High - Frequency attacks ) and enhance adversarial training , as mentioned above .", "ner": [["Absum", "Method"], ["standard regularizations", "Method"], ["robustness against black - box attacks", "Task"], ["SFA", "Task"], ["transferred attacks", "Task"], ["High - Frequency attacks", "Task"], ["adversarial training", "Method"]], "rel": [["Absum", "Compare-With", "standard regularizations"], ["Absum", "Used-For", "robustness against black - box attacks"], ["SFA", "SubTask-Of", "robustness against black - box attacks"], ["transferred attacks", "SubTask-Of", "robustness against black - box attacks"], ["High - Frequency attacks", "SubTask-Of", "robustness against black - box attacks"]], "rel_plus": [["Absum:Method", "Compare-With", "standard regularizations:Method"], ["Absum:Method", "Used-For", "robustness against black - box attacks:Task"], ["SFA:Task", "SubTask-Of", "robustness against black - box attacks:Task"], ["transferred attacks:Task", "SubTask-Of", "robustness against black - box attacks:Task"], ["High - Frequency attacks:Task", "SubTask-Of", "robustness against black - box attacks:Task"]]}
{"doc_id": "202676714", "sentence": "We proposed Absum ; an efficient defense method against SFA that can reduce the structural sensitivity of CNNs with ReLUs while its computational cost remains comparable to standard regularizations .", "ner": [["Absum", "Method"], ["against SFA", "Task"], ["CNNs", "Method"], ["ReLUs", "Method"], ["standard regularizations", "Method"]], "rel": [["Absum", "Used-For", "against SFA"], ["ReLUs", "Part-Of", "CNNs"], ["Absum", "Used-For", "CNNs"], ["Absum", "Compare-With", "standard regularizations"]], "rel_plus": [["Absum:Method", "Used-For", "against SFA:Task"], ["ReLUs:Method", "Part-Of", "CNNs:Method"], ["Absum:Method", "Used-For", "CNNs:Method"], ["Absum:Method", "Compare-With", "standard regularizations:Method"]]}
{"doc_id": "202676714", "sentence": "By reducing the structural sensitivity , Absum can improve robustness against not only SFA , but also transferred PGD , and High - Frequency attacks .", "ner": [["Absum", "Method"], ["SFA", "Task"], ["transferred PGD", "Task"], ["High - Frequency attacks", "Task"]], "rel": [["Absum", "Used-For", "SFA"], ["Absum", "Used-For", "transferred PGD"], ["Absum", "Used-For", "High - Frequency attacks"]], "rel_plus": [["Absum:Method", "Used-For", "SFA:Task"], ["Absum:Method", "Used-For", "transferred PGD:Task"], ["Absum:Method", "Used-For", "High - Frequency attacks:Task"]]}
{"doc_id": "202676714", "sentence": "Due to its simplicity , Absum can be used with other methods , and Absum can enhance adversarial training of PGD .   In this section , we provide the proofs of the lemmas . is a convex function , we have g(tx + ( 1 \u2212 t)y ) \u2264 tg(x ) + ( 1 \u2212 t)g(y ) , where t \u2208 [ 0 , 1 ] and \u2200x , y \u2208 Rn .", "ner": [["Absum", "Method"], ["Absum", "Method"], ["adversarial training", "Method"], ["PGD", "Method"]], "rel": [["Absum", "Part-Of", "adversarial training"], ["adversarial training", "Part-Of", "PGD"]], "rel_plus": [["Absum:Method", "Part-Of", "adversarial training:Method"], ["adversarial training:Method", "Part-Of", "PGD:Method"]]}
{"doc_id": "202676714", "sentence": "In this section , we explain that L 2 regularization ( weight decay : WD ) can constrain the induced norm of a convolutional layer .", "ner": [["L 2 regularization", "Method"], ["weight decay", "Method"], ["WD", "Method"], ["convolutional layer", "Method"]], "rel": [["WD", "Synonym-Of", "weight decay"], ["L 2 regularization", "Used-For", "convolutional layer"]], "rel_plus": [["WD:Method", "Synonym-Of", "weight decay:Method"], ["L 2 regularization:Method", "Used-For", "convolutional layer:Method"]]}
{"doc_id": "202676714", "sentence": "The L 2 regularization term of the convolution filter On the other hand , the square of the Frobenius norm of A becomes Therefore , if we use the L 2 regularization , we constrain the Frobenius norm of A. In addition , let M be m \u00d7 m matrices , we have the following inequalities : where || \u00b7 || 2 is the induced 2 - norm , which is the largest singular value .", "ner": [["L 2 regularization", "Method"], ["convolution", "Method"], ["L 2 regularization", "Method"]], "rel": [["L 2 regularization", "Part-Of", "convolution"]], "rel_plus": [["L 2 regularization:Method", "Part-Of", "convolution:Method"]]}
{"doc_id": "202676714", "sentence": "The fast gradient sign method ( FGSM ) and PGD are popular as simple and sophisticated white - box attacks , respectively ( Goodfellow , Shlens , and Szegedy 2 0 1 4 ; Kurakin , Goodfellow , and Bengio 2 0 1 6 ; Madry et al. 2 0 1 8) .", "ner": [["fast gradient sign method", "Method"], ["FGSM", "Method"], ["PGD", "Method"], ["white - box attacks", "Method"]], "rel": [["FGSM", "Synonym-Of", "fast gradient sign method"], ["fast gradient sign method", "SubClass-Of", "white - box attacks"], ["PGD", "SubClass-Of", "white - box attacks"]], "rel_plus": [["FGSM:Method", "Synonym-Of", "fast gradient sign method:Method"], ["fast gradient sign method:Method", "SubClass-Of", "white - box attacks:Method"], ["PGD:Method", "SubClass-Of", "white - box attacks:Method"]]}
{"doc_id": "202676714", "sentence": "Note that Absum can be used with adversarial training and enhances it , as discussed in experiments .", "ner": [["Absum", "Method"], ["adversarial training", "Method"]], "rel": [["Absum", "Part-Of", "adversarial training"]], "rel_plus": [["Absum:Method", "Part-Of", "adversarial training:Method"]]}
{"doc_id": "202676714", "sentence": "On the other hand , SFA only uses the information that the target model is composed of CNNs and is more practical .", "ner": [["SFA", "Method"], ["CNNs", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202676714", "sentence": "Parseval networks are more robust against FGSM than naive models and can enhance adversarial training .", "ner": [["Parseval networks", "Method"], ["robust against FGSM", "Task"], ["adversarial training", "Method"]], "rel": [["Parseval networks", "Used-For", "robust against FGSM"], ["Parseval networks", "Part-Of", "adversarial training"]], "rel_plus": [["Parseval networks:Method", "Used-For", "robust against FGSM:Task"], ["Parseval networks:Method", "Part-Of", "adversarial training:Method"]]}
{"doc_id": "202676714", "sentence": "In addition , its robustness might be less than that of the spectral norm regularization ( Tsuzuku , Sato , and Sugiyama 2 0 1 8) though Parseval networks penalize the spectral norm like the spectral norm constraint .", "ner": [["spectral norm regularization", "Method"], ["Parseval networks", "Method"], ["spectral norm", "Method"], ["spectral norm constraint", "Method"]], "rel": [["spectral norm", "Part-Of", "spectral norm regularization"], ["Parseval networks", "Part-Of", "spectral norm regularization"]], "rel_plus": [["spectral norm:Method", "Part-Of", "spectral norm regularization:Method"], ["Parseval networks:Method", "Part-Of", "spectral norm regularization:Method"]]}
{"doc_id": "202676714", "sentence": "However , the spectral norm in spectral norm regularization is often quite different from that of A ( Gouk et al. 2 0 1 8 ; Sedghi , Gupta , and Long 2 0 1 9 ) for convolution .", "ner": [["spectral norm", "Method"], ["spectral norm regularization", "Method"]], "rel": [["spectral norm", "Part-Of", "spectral norm regularization"]], "rel_plus": [["spectral norm:Method", "Part-Of", "spectral norm regularization:Method"]]}
{"doc_id": "202676714", "sentence": "Therefore , the maxnorm regularization on convolution is ||A i || 2 = l m k 2 l , m \u2264 c and is similar to L 2 regularization .", "ner": [["maxnorm regularization", "Method"], ["convolution", "Method"], ["L 2 regularization", "Method"]], "rel": [["maxnorm regularization", "Part-Of", "convolution"], ["maxnorm regularization", "Compare-With", "L 2 regularization"]], "rel_plus": [["maxnorm regularization:Method", "Part-Of", "convolution:Method"], ["maxnorm regularization:Method", "Compare-With", "L 2 regularization:Method"]]}
{"doc_id": "202676714", "sentence": "In fact , we observed that the effectiveness of maxnorm regularization is similar to weight decay .", "ner": [["maxnorm regularization", "Method"], ["weight decay", "Method"]], "rel": [["maxnorm regularization", "Compare-With", "weight decay"]], "rel_plus": [["maxnorm regularization:Method", "Compare-With", "weight decay:Method"]]}
{"doc_id": "202676714", "sentence": "In all experiments , we selected the best regularization weight from among [ 1 0 1 , 1 0 0 , . . . , 1 0 \u2212 6 , 1 0 \u2212 7 ] for Absum and standard regularization methods and selected the best spectral norm \u03c3 from among [ 0.0 1 , 0. 1 , 0. 5 , 1. 0 , 1 0 ] for spectral norm constraint ( SNC ) ( Sedghi , Gupta , and Long 2 0 1 9 ) .", "ner": [["Absum", "Method"], ["standard regularization methods", "Method"], ["spectral norm", "Method"], ["spectral norm constraint", "Method"], ["SNC", "Method"]], "rel": [["SNC", "Synonym-Of", "spectral norm constraint"]], "rel_plus": [["SNC:Method", "Synonym-Of", "spectral norm constraint:Method"]]}
{"doc_id": "202676714", "sentence": "In addition , MNIST , CIFAR 1 0 and CIFAR 1 0 0 were standardized as ( mean , standard deviation)=( 0 , 1 ) before the images were applied to the models as preprocessing .", "ner": [["MNIST", "Dataset"], ["CIFAR 1 0", "Dataset"], ["CIFAR 1 0 0", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "202676714", "sentence": "For MNIST and Fashion - MNIST ( FMNIST ) , we stacked two convolutional layers and two fully connected layers , the first convolutional layer had the 1 0 output channels and the second convolutional layer had 2 0 output channels .", "ner": [["MNIST", "Dataset"], ["Fashion - MNIST", "Dataset"], ["FMNIST", "Dataset"], ["convolutional layers", "Method"], ["fully connected layers", "Method"], ["convolutional layer", "Method"], ["convolutional layer", "Method"]], "rel": [["FMNIST", "Synonym-Of", "Fashion - MNIST"]], "rel_plus": [["FMNIST:Dataset", "Synonym-Of", "Fashion - MNIST:Dataset"]]}
{"doc_id": "202676714", "sentence": "After each convolutional layer , we applied max pooling ( the stride was 2 ) and ReLU activation .", "ner": [["convolutional layer", "Method"], ["max pooling", "Method"], ["ReLU activation", "Method"]], "rel": [["ReLU activation", "Part-Of", "convolutional layer"], ["max pooling", "Part-Of", "convolutional layer"]], "rel_plus": [["ReLU activation:Method", "Part-Of", "convolutional layer:Method"], ["max pooling:Method", "Part-Of", "convolutional layer:Method"]]}
{"doc_id": "202676714", "sentence": "The output of the second convolutional layer was applied to the first fully connected layer ( the size was 3 2 0 \u00d7 5 0 ) , and we used the ReLU activation after the first fully connected layer .", "ner": [["convolutional layer", "Method"], ["fully connected layer", "Method"], ["ReLU activation", "Method"], ["fully connected layer", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202676714", "sentence": "The size of the second fully connected layer was 5 0 \u00d7 1 0 , and we used softmax as the output function .", "ner": [["fully connected layer", "Method"], ["softmax", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202676714", "sentence": "After the second convolution layer and before the second fully connected layer , we applied 5 0 % dropout .", "ner": [["convolution layer", "Method"], ["fully connected layer", "Method"], ["dropout", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202676714", "sentence": "We changed ( l , m ) to { ( 0 , 0 ) , ( 0 , 1 ) , . . . , ( 2 7 , 2 6 ) , ( 2 7 , 2 7 ) } in SFA since the size of the images was 2 8 \u00d7 2 8 and evaluated the accuracy of the model on the test data perturbed by SFA .", "ner": [["SFA", "Method"], ["SFA", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202676714", "sentence": "CIFAR 1 0 and CIFAR 1 0 0 contain 5 0 , 0 0 0 training images and 1 0 , 0 0 0 test images ( Krizhevsky and Hinton 2 0 0 9 ) .", "ner": [["CIFAR 1 0", "Dataset"], ["CIFAR 1 0 0", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "202676714", "sentence": "The model architecture was ResNet - 1 8 for CIFAR 1 0 , CIFAR 1 0 0 , and SVHN ( He et al. 2 0 1 6 ) . 4 As the preprocessing for training , given images were randomly cropped as 3 2 \u00d7 3 2 after padding a sequence of four on each border of the images .", "ner": [["ResNet - 1 8", "Method"], ["CIFAR 1 0", "Dataset"], ["CIFAR 1 0 0", "Dataset"], ["SVHN", "Dataset"]], "rel": [["ResNet - 1 8", "Evaluated-With", "CIFAR 1 0"], ["ResNet - 1 8", "Evaluated-With", "CIFAR 1 0 0"], ["ResNet - 1 8", "Evaluated-With", "SVHN"]], "rel_plus": [["ResNet - 1 8:Method", "Evaluated-With", "CIFAR 1 0:Dataset"], ["ResNet - 1 8:Method", "Evaluated-With", "CIFAR 1 0 0:Dataset"], ["ResNet - 1 8:Method", "Evaluated-With", "SVHN:Dataset"]]}
{"doc_id": "202676714", "sentence": "We changed ( l , m ) in SFA to { ( 0 , 0 ) , ( 0 , 1 ) , . . . , ( 3 1 , 3 0 ) , ( 3 1 , 3 1 ) } since the size of the images was 3 2 \u00d7 3 2 and evaluated the accuracy of the model on the test data perturbed by SFA .", "ner": [["SFA", "Method"], ["SFA", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202676714", "sentence": "We also evaluated the effectiveness of Absum against PGD .", "ner": [["Absum", "Method"], ["against PGD", "Task"]], "rel": [["Absum", "Used-For", "against PGD"]], "rel_plus": [["Absum:Method", "Used-For", "against PGD:Task"]]}
{"doc_id": "202676714", "sentence": "We evaluated Absum with adversarial training ( Madry et al. 2 0 1 8 ) in addition to naive training because Absum and other regularization methods can be used with adversarial training .", "ner": [["Absum", "Method"], ["adversarial training", "Method"], ["Absum", "Method"], ["adversarial training", "Method"]], "rel": [["Absum", "Part-Of", "adversarial training"]], "rel_plus": [["Absum:Method", "Part-Of", "adversarial training:Method"]]}
{"doc_id": "202676714", "sentence": "The number of epochs for MNIST and FMNIST was set to 1 0 0 .", "ner": [["MNIST", "Dataset"], ["FMNIST", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "202676714", "sentence": "On the other hand , we observed overfitting in the adversarial training on CI - FAR 1 0 , CIFAR 1 0 0 , and SVHN .", "ner": [["adversarial training", "Method"], ["CI - FAR 1 0", "Dataset"], ["CIFAR 1 0 0", "Dataset"], ["SVHN", "Dataset"]], "rel": [["adversarial training", "Trained-With", "CI - FAR 1 0"], ["adversarial training", "Trained-With", "CIFAR 1 0 0"], ["adversarial training", "Trained-With", "SVHN"]], "rel_plus": [["adversarial training:Method", "Trained-With", "CI - FAR 1 0:Dataset"], ["adversarial training:Method", "Trained-With", "CIFAR 1 0 0:Dataset"], ["adversarial training:Method", "Trained-With", "SVHN:Dataset"]]}
{"doc_id": "202676714", "sentence": "We also applied weight decay of 1 0 \u2212 4 to all parameters on CI - FAR 1 0 and CIFAR 1 0 0 in the adversarial training of PGD since overfitting easily occurred in adversarial training on these datasets .", "ner": [["weight decay", "Method"], ["CI - FAR 1 0", "Dataset"], ["CIFAR 1 0 0", "Dataset"], ["adversarial training", "Method"], ["PGD", "Method"], ["adversarial training", "Method"]], "rel": [["adversarial training", "Trained-With", "CI - FAR 1 0"], ["adversarial training", "Trained-With", "CIFAR 1 0 0"], ["weight decay", "Part-Of", "adversarial training"], ["PGD", "Part-Of", "adversarial training"]], "rel_plus": [["adversarial training:Method", "Trained-With", "CI - FAR 1 0:Dataset"], ["adversarial training:Method", "Trained-With", "CIFAR 1 0 0:Dataset"], ["weight decay:Method", "Part-Of", "adversarial training:Method"], ["PGD:Method", "Part-Of", "adversarial training:Method"]]}
{"doc_id": "202676714", "sentence": "In PGD , the L \u221e norm of the perturbation \u03b5 was set to \u03b5 = [ 0 . 0 5 , 0. 1 , 0. 1 5 , 0. 2 , 0. 2 5 , 0. 4 ] for and \u03b5 = [ 4/ 2 5 5 , 8/ 2 5 5 , 1 2 / 2 5 5 , 1 6 / 2 5 5 , 2 0 / 2 5 5 ] for CIFAR 1 0 at evaluation time .", "ner": [["PGD", "Method"], ["L \u221e norm", "Method"], ["CIFAR 1 0", "Dataset"]], "rel": [["L \u221e norm", "Part-Of", "PGD"], ["PGD", "Evaluated-With", "CIFAR 1 0"]], "rel_plus": [["L \u221e norm:Method", "Part-Of", "PGD:Method"], ["PGD:Method", "Evaluated-With", "CIFAR 1 0:Dataset"]]}
{"doc_id": "202676714", "sentence": "For PGD , we updated the perturbation for 4 0 iterations with a step size of 0.0 1 on MNIST and FMNIST at training and evaluation times .", "ner": [["PGD", "Method"], ["MNIST", "Dataset"], ["FMNIST", "Dataset"]], "rel": [["PGD", "Evaluated-With", "MNIST"], ["PGD", "Evaluated-With", "FMNIST"]], "rel_plus": [["PGD:Method", "Evaluated-With", "MNIST:Dataset"], ["PGD:Method", "Evaluated-With", "FMNIST:Dataset"]]}
{"doc_id": "202676714", "sentence": "On CIFAR 1 0 , CIFAR 1 0 0 , and SVHN , we updated the perturbation for 7 iterations with a step size of 2/ 2 5 5 at training time and 1 0 0 iterations at evaluation time .", "ner": [["CIFAR 1 0", "Dataset"], ["CIFAR 1 0 0", "Dataset"], ["SVHN", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "202676714", "sentence": "For adversarial training , we used training data perturbed by PGD with \u03b5 = 0. 3 on MNIST and \u03b5 = 8/ 2 5 5 on CIFAR 1 0 , CIFAR 1 0 0 , and SVHN .", "ner": [["adversarial training", "Method"], ["PGD", "Method"], ["MNIST", "Dataset"], ["CIFAR 1 0", "Dataset"], ["CIFAR 1 0 0", "Dataset"], ["SVHN", "Dataset"]], "rel": [["PGD", "Part-Of", "adversarial training"], ["adversarial training", "Trained-With", "MNIST"], ["adversarial training", "Trained-With", "CIFAR 1 0"], ["adversarial training", "Trained-With", "CIFAR 1 0 0"], ["adversarial training", "Trained-With", "SVHN"]], "rel_plus": [["PGD:Method", "Part-Of", "adversarial training:Method"], ["adversarial training:Method", "Trained-With", "MNIST:Dataset"], ["adversarial training:Method", "Trained-With", "CIFAR 1 0:Dataset"], ["adversarial training:Method", "Trained-With", "CIFAR 1 0 0:Dataset"], ["adversarial training:Method", "Trained-With", "SVHN:Dataset"]]}
{"doc_id": "202676714", "sentence": "Robustness against SFA Figure 8 shows the accuracies on datasets perturbed by SFA against hyperparameters ( l , m ) of SFA .", "ner": [["Robustness against SFA", "Task"], ["SFA", "Method"], ["SFA", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202676714", "sentence": "As shown in Fig. 8 , the models trained with WD and L 1 regularization are sensitive to certain frequency noise ( e.g. , ( 1 7 , 1 7 ) in Figs. 8 ( j ) and ( k ) ) .", "ner": [["WD", "Method"], ["L 1 regularization", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202676714", "sentence": "In this table , Avg . denotes accuracies on data perturbed by SFA averaged over hyperparameters ( l , m ) , and Min . denotes minimum accuracies on data perturbed by SFA among ( l , m ) .", "ner": [["SFA", "Method"], ["SFA", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202676714", "sentence": "Figure 9 shows the accuracies of the methods on FM - NIST , CIFAR 1 0 0 , and SVHN perturbed by SFA against regularization weights .", "ner": [["FM - NIST", "Dataset"], ["CIFAR 1 0 0", "Dataset"], ["SVHN", "Dataset"], ["SFA", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202676714", "sentence": "These results are almost the same as those of MNIST and CIFAR 1 0 .", "ner": [["MNIST", "Dataset"], ["CIFAR 1 0", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "202676714", "sentence": "Figure 1 0 shows the accuracies of SNC on all datasets perturbed by SFA against the threshold of the spectral norm \u03c3 .", "ner": [["SNC", "Method"], ["SFA", "Method"], ["spectral norm", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202676714", "sentence": "We can see that , on MNIST and FMNIST , accuracies increase along with \u03c3 .", "ner": [["MNIST", "Dataset"], ["FMNIST", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "202676714", "sentence": "On the other hand , on CIFAR 1 0 , CIFAR 1 0 0 , and SVHN , where we used ResNet , minimum accuracy decreases , when the spectral norm becomes larger than a certain point , while max accuracy increases along with the spectral norm .", "ner": [["CIFAR 1 0", "Dataset"], ["CIFAR 1 0 0", "Dataset"], ["SVHN", "Dataset"], ["ResNet", "Method"], ["spectral norm", "Method"], ["spectral norm", "Method"]], "rel": [["ResNet", "Evaluated-With", "CIFAR 1 0"], ["ResNet", "Evaluated-With", "CIFAR 1 0 0"], ["ResNet", "Evaluated-With", "SVHN"]], "rel_plus": [["ResNet:Method", "Evaluated-With", "CIFAR 1 0:Dataset"], ["ResNet:Method", "Evaluated-With", "CIFAR 1 0 0:Dataset"], ["ResNet:Method", "Evaluated-With", "SVHN:Dataset"]]}
{"doc_id": "202676714", "sentence": "Figure 1 1 shows the lowest training loss 1 N f in training with SNC on CIFAR 1 0 against \u03c3 .", "ner": [["SNC", "Method"], ["CIFAR 1 0", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "202676714", "sentence": "We can see that Absum and SNC can improve robustness against transferred PGD better than WD and L 1 .", "ner": [["Absum", "Method"], ["SNC", "Method"], ["robustness against transferred PGD", "Task"], ["WD", "Method"], ["L 1", "Method"]], "rel": [["SNC", "Used-For", "robustness against transferred PGD"], ["Absum", "Used-For", "robustness against transferred PGD"], ["WD", "Used-For", "robustness against transferred PGD"], ["L 1", "Used-For", "robustness against transferred PGD"], ["Absum", "Compare-With", "WD"], ["SNC", "Compare-With", "WD"], ["SNC", "Compare-With", "L 1"], ["Absum", "Compare-With", "L 1"]], "rel_plus": [["SNC:Method", "Used-For", "robustness against transferred PGD:Task"], ["Absum:Method", "Used-For", "robustness against transferred PGD:Task"], ["WD:Method", "Used-For", "robustness against transferred PGD:Task"], ["L 1:Method", "Used-For", "robustness against transferred PGD:Task"], ["Absum:Method", "Compare-With", "WD:Method"], ["SNC:Method", "Compare-With", "WD:Method"], ["SNC:Method", "Compare-With", "L 1:Method"], ["Absum:Method", "Compare-With", "L 1:Method"]]}
{"doc_id": "202676714", "sentence": "On the other hand , the models trained using SNC are not more robust against the high - pass filter than WD and L 1 while they are more robust against High - Frequency attacks .", "ner": [["SNC", "Method"], ["WD", "Method"], ["L 1", "Method"], ["High - Frequency attacks", "Task"]], "rel": [["SNC", "Compare-With", "WD"], ["SNC", "Compare-With", "L 1"], ["SNC", "Used-For", "High - Frequency attacks"], ["L 1", "Used-For", "High - Frequency attacks"], ["WD", "Used-For", "High - Frequency attacks"]], "rel_plus": [["SNC:Method", "Compare-With", "WD:Method"], ["SNC:Method", "Compare-With", "L 1:Method"], ["SNC:Method", "Used-For", "High - Frequency attacks:Task"], ["L 1:Method", "Used-For", "High - Frequency attacks:Task"], ["WD:Method", "Used-For", "High - Frequency attacks:Task"]]}
{"doc_id": "202676714", "sentence": "Therefore , SNC biases CNNs towards low - frequency components of image data .", "ner": [["SNC", "Method"], ["CNNs", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202676714", "sentence": "CLN is accuracy on clean data . \u03bb and \u03c3 are selected so that average accuracies ( Avg ) against SFA would achieve largest values .     We evaluated the computation time for convergence on CI - FAR 1 0 .", "ner": [["SFA", "Method"], ["CI - FAR 1 0", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "202676714", "sentence": "In this figure , Absum converges as fast as L 1 regularization .", "ner": [["Absum", "Method"], ["L 1 regularization", "Method"]], "rel": [["Absum", "Compare-With", "L 1 regularization"]], "rel_plus": [["Absum:Method", "Compare-With", "L 1 regularization:Method"]]}
{"doc_id": "202676714", "sentence": "Table 8 lists the test accuracies of the models trained by naive training and adversarial training on the data perturbed by PGD .", "ner": [["adversarial training", "Method"], ["PGD", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202676714", "sentence": "We can see that when we train the models without using adversarial training , Absum does not improve robustness against PGD .", "ner": [["adversarial training", "Method"], ["Absum", "Method"], ["robustness against PGD", "Task"]], "rel": [["Absum", "Used-For", "robustness against PGD"]], "rel_plus": [["Absum:Method", "Used-For", "robustness against PGD:Task"]]}
{"doc_id": "202676714", "sentence": "This implies that the structural sensitivity of CNNs does not necessarily cause all vulnerabilities of CNN - based models .", "ner": [["CNNs", "Method"], ["CNN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202676714", "sentence": "However , when we use adversarial training , Absum improves robustness against PGD , the highest among regularization methods on almost all datasets .", "ner": [["adversarial training", "Method"], ["Absum", "Method"], ["robustness against PGD", "Task"]], "rel": [["Absum", "Used-For", "robustness against PGD"]], "rel_plus": [["Absum:Method", "Used-For", "robustness against PGD:Task"]]}
{"doc_id": "202676714", "sentence": "We can see that SNC can slightly improve the robustness against PGD in naive training .", "ner": [["SNC", "Method"], ["robustness against PGD", "Task"]], "rel": [["SNC", "Used-For", "robustness against PGD"]], "rel_plus": [["SNC:Method", "Used-For", "robustness against PGD:Task"]]}
{"doc_id": "202676714", "sentence": "However , when using adversarial training , it does not improve robustness more than Absum .", "ner": [["adversarial training", "Method"], ["Absum", "Method"]], "rel": [["adversarial training", "Compare-With", "Absum"]], "rel_plus": [["adversarial training:Method", "Compare-With", "Absum:Method"]]}
{"doc_id": "202676714", "sentence": "The best regularization weights for WD and L 1 regularization in adversarial training tend to be lower , and \u03c3 in adversarial training is higher compared with naive training .", "ner": [["WD", "Method"], ["L 1 regularization", "Method"], ["adversarial training", "Method"], ["adversarial training", "Method"]], "rel": [["WD", "Part-Of", "adversarial training"], ["L 1 regularization", "Part-Of", "adversarial training"]], "rel_plus": [["WD:Method", "Part-Of", "adversarial training:Method"], ["L 1 regularization:Method", "Part-Of", "adversarial training:Method"]]}
{"doc_id": "202676714", "sentence": "On the other hand , the best regularization weights of Absum in adversarial training tend to be higher than those in naive training .", "ner": [["Absum", "Method"], ["adversarial training", "Method"]], "rel": [["Absum", "Part-Of", "adversarial training"]], "rel_plus": [["Absum:Method", "Part-Of", "adversarial training:Method"]]}
{"doc_id": "202676714", "sentence": "Thus , Absum can improve robustness without deteriorating classification performance due to its looseness .", "ner": [["Absum", "Method"], ["classification", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "60440450", "sentence": "Detecting and segmenting individual objects , regardless of their category , is crucial for many applications such as action detection or robotic interaction .", "ner": [["Detecting and segmenting individual objects", "Task"], ["action detection", "Task"], ["robotic interaction", "Task"]], "rel": [["Detecting and segmenting individual objects", "Used-For", "action detection"], ["Detecting and segmenting individual objects", "Used-For", "robotic interaction"]], "rel_plus": [["Detecting and segmenting individual objects:Task", "Used-For", "action detection:Task"], ["Detecting and segmenting individual objects:Task", "Used-For", "robotic interaction:Task"]]}
{"doc_id": "60440450", "sentence": "Consider , for instance , a sample video clip from the Charades dataset [ 3 6 ] for action recognition in Figure 1 .", "ner": [["Charades dataset", "Dataset"], ["action recognition", "Task"]], "rel": [["Charades dataset", "Benchmark-For", "action recognition"]], "rel_plus": [["Charades dataset:Dataset", "Benchmark-For", "action recognition:Task"]]}
{"doc_id": "60440450", "sentence": "Indeed , state - of - the - art methods for action recognition [ 4 1 , 1 3 , 4 6 ] rely heavily on object detection frameworks , such as Mask R - CNN [ 1 4 ] , to localize these objects .", "ner": [["action recognition", "Task"], ["object detection", "Task"], ["Mask R - CNN", "Method"]], "rel": [["Mask R - CNN", "Used-For", "action recognition"], ["Mask R - CNN", "Used-For", "object detection"]], "rel_plus": [["Mask R - CNN:Method", "Used-For", "action recognition:Task"], ["Mask R - CNN:Method", "Used-For", "object detection:Task"]]}
{"doc_id": "60440450", "sentence": "We validate the performance of the proposed approach using two datasets : the Freiburg Berkeley Motion Segmentation dataset ( FBMS ) [ 2 9 ] for video instance segmentation and DAVIS 2 0 1 6 [ 3 3 ] for binary motion foregroundbackground segmentation .", "ner": [["Freiburg Berkeley Motion Segmentation", "Dataset"], ["FBMS", "Dataset"], ["video instance segmentation", "Task"], ["DAVIS 2 0 1 6", "Dataset"], ["segmentation", "Task"]], "rel": [["FBMS", "Synonym-Of", "Freiburg Berkeley Motion Segmentation"], ["Freiburg Berkeley Motion Segmentation", "Benchmark-For", "video instance segmentation"], ["DAVIS 2 0 1 6", "Benchmark-For", "segmentation"]], "rel_plus": [["FBMS:Dataset", "Synonym-Of", "Freiburg Berkeley Motion Segmentation:Dataset"], ["Freiburg Berkeley Motion Segmentation:Dataset", "Benchmark-For", "video instance segmentation:Task"], ["DAVIS 2 0 1 6:Dataset", "Benchmark-For", "segmentation:Task"]]}
{"doc_id": "60440450", "sentence": "To sum up , our contributions are three - fold : ( 1 ) we propose the first deep learning - based method for instance segmentation of generic moving objects from video ; ( 2 ) we study the standard evaluation approach on the benchmark FBMS dataset and propose a more informative measure ; ( 3 ) we report state - of - the - art result for both video instance and foreground/background segmentation .", "ner": [["deep learning", "Method"], ["instance segmentation", "Task"], ["FBMS", "Dataset"], ["video instance", "Task"], ["foreground/background segmentation", "Task"]], "rel": [["deep learning", "Used-For", "instance segmentation"]], "rel_plus": [["deep learning:Method", "Used-For", "instance segmentation:Task"]]}
{"doc_id": "60440450", "sentence": "Very recently , Bideau et al. [ 6 ] proposed to combine a heuristicbased motion segmentation method [ 2 8 , 5 ] with a CNN trained for semantic segmentation for the task of moving object segmentation .", "ner": [["motion segmentation", "Task"], ["CNN", "Method"], ["semantic segmentation", "Task"], ["object segmentation", "Task"]], "rel": [["CNN", "Used-For", "semantic segmentation"], ["CNN", "Used-For", "object segmentation"]], "rel_plus": [["CNN:Method", "Used-For", "semantic segmentation:Task"], ["CNN:Method", "Used-For", "object segmentation:Task"]]}
{"doc_id": "60440450", "sentence": "Foreground/Background Video Segmentation : Several works have focused on the binary version of the video segmentation task , separating all the moving objects object from the background .", "ner": [["Foreground/Background Video Segmentation", "Task"], ["video segmentation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "60440450", "sentence": "Object Detection : The task of segmenting object instances from still images has seen immense success in recent years , bolstered by large , standard datasets such as COCO [ 2 5 ] .", "ner": [["Object Detection", "Task"], ["COCO", "Dataset"]], "rel": [["COCO", "Benchmark-For", "Object Detection"]], "rel_plus": [["COCO:Dataset", "Benchmark-For", "Object Detection:Task"]]}
{"doc_id": "60440450", "sentence": "These features are combined and passed to the joint object detection and segmentation module .", "ner": [["object detection", "Task"], ["segmentation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "60440450", "sentence": "Here , we train on the MS COCO dataset [ 2 5 ] , which contains approximately 1 2 0 , 0 0 0 training images with instance segmentation masks for each object in 8 0 categories .", "ner": [["MS COCO", "Dataset"], ["instance segmentation", "Task"]], "rel": [["MS COCO", "Benchmark-For", "instance segmentation"]], "rel_plus": [["MS COCO:Dataset", "Benchmark-For", "instance segmentation:Task"]]}
{"doc_id": "60440450", "sentence": "Mask R - CNN contains three stages : ( 1 ) Feature extraction : a \" backbone \" network , such as ResNet [ 1 5 ] , is used to extract features from an image . ( 2 ) Region proposal : A region proposal layer uses these features to selects regions likely to contain an object .", "ner": [["Mask R - CNN", "Method"], ["Feature extraction", "Method"], ["ResNet", "Method"], ["Region proposal", "Method"], ["region proposal layer", "Method"]], "rel": [["Feature extraction", "Part-Of", "Mask R - CNN"], ["ResNet", "SubClass-Of", "Feature extraction"]], "rel_plus": [["Feature extraction:Method", "Part-Of", "Mask R - CNN:Method"], ["ResNet:Method", "SubClass-Of", "Feature extraction:Method"]]}
{"doc_id": "60440450", "sentence": "To build a two - stream instance segmentation model , we extract the backbone from our individual appearance - based and motion - based segmentation models .", "ner": [["instance segmentation", "Task"], ["motion - based segmentation models", "Method"]], "rel": [["motion - based segmentation models", "Used-For", "instance segmentation"]], "rel_plus": [["motion - based segmentation models:Method", "Used-For", "instance segmentation:Task"]]}
{"doc_id": "60440450", "sentence": "Although this may appear similar to prior approaches for building a two - stream detection model , it differs in a key detail : prior approaches obtain region proposals either only from appearance features [ 1 3 , 1 1 , 1 0 ] , or from appearance and motion features individually [ 3 2 ] .", "ner": [["two - stream detection model", "Method"], ["region proposals", "Task"]], "rel": [["two - stream detection model", "Used-For", "region proposals"]], "rel_plus": [["two - stream detection model:Method", "Used-For", "region proposals:Task"]]}
{"doc_id": "60440450", "sentence": "We train our joint model on subsets of the DAVIS and YouTube Video Object Segmentation datasets ( as detailed in Section 5. 1 ) .", "ner": [["DAVIS", "Dataset"], ["YouTube Video Object Segmentation", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "60440450", "sentence": "To evaluate methods for video instance segmentation , we desire a metric that appropriately rewards segmenting and tracking moving objects , but penalizes the detection of static objects or background .", "ner": [["video instance segmentation", "Task"], ["detection", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "60440450", "sentence": "In our experiments , we report results using both the official FBMS measure as well as our proposed measure , and empirically show that reducing the detection of static objects does not improve results on the FBMS measure , but significantly improves accuracy on our proposed measure .", "ner": [["FBMS", "Dataset"], ["FBMS", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "60440450", "sentence": "Next , we compare our approach to prior work on the task of motion foreground/background segmentation and video instance segmentation .", "ner": [["motion foreground/background segmentation", "Task"], ["video instance segmentation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "60440450", "sentence": "Three candidate datasets exist for this task : YouTube Video Object Segmentation ( YTVOS ) [ 4 4 ] , DAVIS 2 0 1 6 [ 3 3 ] , and FBMS [ 2 9 ] .", "ner": [["YouTube Video Object Segmentation", "Dataset"], ["YTVOS", "Dataset"], ["DAVIS 2 0 1 6", "Dataset"], ["FBMS", "Dataset"]], "rel": [["YTVOS", "Synonym-Of", "YouTube Video Object Segmentation"]], "rel_plus": [["YTVOS:Dataset", "Synonym-Of", "YouTube Video Object Segmentation:Dataset"]]}
{"doc_id": "60440450", "sentence": "While YTVOS contains over 3, 0 0 0 short videos with instance segmentation labels , not all objects in these videos are necessarily labeled , and both moving as well as static objects may be labeled .", "ner": [["YTVOS", "Dataset"], ["instance segmentation", "Task"]], "rel": [["YTVOS", "Benchmark-For", "instance segmentation"]], "rel_plus": [["YTVOS:Dataset", "Benchmark-For", "instance segmentation:Task"]]}
{"doc_id": "60440450", "sentence": "The DAVIS 2 0 1 6 dataset contains instance segmentation masks ( provided with DAVIS 2 0 1 7 ) for only the moving objects , but only contains 3 0 training videos .", "ner": [["DAVIS 2 0 1 6", "Dataset"], ["instance segmentation", "Task"], ["DAVIS 2 0 1 7", "Dataset"]], "rel": [["DAVIS 2 0 1 6", "Benchmark-For", "instance segmentation"], ["DAVIS 2 0 1 7", "Benchmark-For", "instance segmentation"]], "rel_plus": [["DAVIS 2 0 1 6:Dataset", "Benchmark-For", "instance segmentation:Task"], ["DAVIS 2 0 1 7:Dataset", "Benchmark-For", "instance segmentation:Task"]]}
{"doc_id": "60440450", "sentence": "Finally , although FBMS contains a total of 5 9 sequences with labeled instance segmentation masks for moving objects , prior work evaluates on the entire dataset , preventing us from training on any sequences in the dataset in order to provide a fair comparison .", "ner": [["FBMS", "Dataset"], ["instance segmentation", "Task"]], "rel": [["FBMS", "Benchmark-For", "instance segmentation"]], "rel_plus": [["FBMS:Dataset", "Benchmark-For", "instance segmentation:Task"]]}
{"doc_id": "60440450", "sentence": "As described earlier , we use COCO [ 2 5 ] , an image - based object segmentation dataset , to train our appearance stream .", "ner": [["COCO", "Dataset"], ["object segmentation", "Task"]], "rel": [["COCO", "Benchmark-For", "object segmentation"]], "rel_plus": [["COCO:Dataset", "Benchmark-For", "object segmentation:Task"]]}
{"doc_id": "60440450", "sentence": "Finally , we create a subset of YTVOS [ 4 4 ] , which we call ' YTVOS - moving ' , of about 5 0 0 videos where every moving object is labeled , and no static object is labeled .", "ner": [["YTVOS", "Dataset"], ["YTVOS - moving", "Dataset"]], "rel": [["YTVOS - moving", "SubClass-Of", "YTVOS"]], "rel_plus": [["YTVOS - moving:Dataset", "SubClass-Of", "YTVOS:Dataset"]]}
{"doc_id": "60440450", "sentence": "The backbone for every model , including the motion stream , is initialized using ImageNet [ 3 4 ] pre - training due to difficulties with training Mask R - CNN from scratch .", "ner": [["ImageNet", "Dataset"], ["Mask R - CNN", "Method"]], "rel": [["Mask R - CNN", "Trained-With", "ImageNet"]], "rel_plus": [["Mask R - CNN:Method", "Trained-With", "ImageNet:Dataset"]]}
{"doc_id": "60440450", "sentence": "We train our motion stream on FlyingThings 3 D using Groundtruth flow as input , and then with FlowNet 2 flow [ 1 8 ] for 1 0 , 0 0 0 iterations each .", "ner": [["FlyingThings 3 D", "Dataset"], ["FlowNet 2", "Method"]], "rel": [["FlowNet 2", "Trained-With", "FlyingThings 3 D"]], "rel_plus": [["FlowNet 2:Method", "Trained-With", "FlyingThings 3 D:Dataset"]]}
{"doc_id": "60440450", "sentence": "We train the joint model on YTVOS and then on DAVIS for 5, 0 0 0 iterations each .", "ner": [["YTVOS", "Dataset"], ["DAVIS", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "60440450", "sentence": "For ablation , we found it helpful to use the standard detection mean average precision ( mAP ) metric [ 2 5 ] in place of video object segmentation metrics , which require tracking and obfuscate analysis of our architecture choices .", "ner": [["detection", "Task"], ["video object segmentation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "60440450", "sentence": "We report both detection and segmentation mAP at an IoU threshold of 0. 5 To begin , we explore training strategies for the motion stream of our model .", "ner": [["detection", "Task"], ["segmentation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "60440450", "sentence": "In order to match flow in the real world , we start by estimating flow on FlyingThings 3 D using two optical flow estimation methods : FlowNet 2 and LiteFlowNet .", "ner": [["FlyingThings 3 D", "Dataset"], ["optical flow estimation", "Task"], ["FlowNet 2", "Method"], ["LiteFlowNet", "Method"]], "rel": [["FlowNet 2", "Evaluated-With", "FlyingThings 3 D"], ["LiteFlowNet", "Evaluated-With", "FlyingThings 3 D"], ["FlyingThings 3 D", "Benchmark-For", "optical flow estimation"], ["FlowNet 2", "Used-For", "optical flow estimation"], ["LiteFlowNet", "Used-For", "optical flow estimation"]], "rel_plus": [["FlowNet 2:Method", "Evaluated-With", "FlyingThings 3 D:Dataset"], ["LiteFlowNet:Method", "Evaluated-With", "FlyingThings 3 D:Dataset"], ["FlyingThings 3 D:Dataset", "Benchmark-For", "optical flow estimation:Task"], ["FlowNet 2:Method", "Used-For", "optical flow estimation:Task"], ["LiteFlowNet:Method", "Used-For", "optical flow estimation:Task"]]}
{"doc_id": "60440450", "sentence": "We start by training using only FlowNet 2 flow as input ( \" FlowNet 2 \" ) .", "ner": [["FlowNet 2", "Method"], ["FlowNet 2", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "60440450", "sentence": "To overcome this , we train a variant starting with groundtruth flow , and fine - tune on FlowNet 2 flow ( \" FlowNet 2 \u2190 Groundtruth \" row ) .", "ner": [["FlowNet 2", "Method"], ["FlowNet 2", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "60440450", "sentence": "We swap out FlowNet 2 with LiteFlowNet [ 1 7 ] ( \" LiteFlowNet \u2190 Groundtruth \" row ) .", "ner": [["FlowNet 2", "Method"], ["LiteFlowNet", "Method"], ["LiteFlowNet", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "60440450", "sentence": "Surprisingly , we find that FlowNet 2 provides significant improvements for segmentation , despite performing worse on standard flow estimation benchmarks .", "ner": [["FlowNet 2", "Method"], ["segmentation", "Task"]], "rel": [["FlowNet 2", "Used-For", "segmentation"]], "rel_plus": [["FlowNet 2:Method", "Used-For", "segmentation:Task"]]}
{"doc_id": "60440450", "sentence": "Qualitatively , we found that FlowNet 2 provides sharper results along boundaries than LiteFlowNet , which may make it difficult to localize objects .", "ner": [["FlowNet 2", "Method"], ["LiteFlowNet", "Method"]], "rel": [["FlowNet 2", "Compare-With", "LiteFlowNet"]], "rel_plus": [["FlowNet 2:Method", "Compare-With", "LiteFlowNet:Method"]]}
{"doc_id": "60440450", "sentence": "First , we train a standard , \" class - specific \" Mask R - CNN , that outputs a set of boxes and masks for each of the 8 0 categories in the COCO Dataset .", "ner": [["Mask R - CNN", "Method"], ["COCO", "Dataset"]], "rel": [["Mask R - CNN", "Evaluated-With", "COCO"]], "rel_plus": [["Mask R - CNN:Method", "Evaluated-With", "COCO:Dataset"]]}
{"doc_id": "60440450", "sentence": "Second , we train an \" objectness \" Mask R - CNN , by collapsing all the categories in COCO to a single category before training .", "ner": [["Mask R - CNN", "Method"], ["COCO", "Dataset"]], "rel": [["Mask R - CNN", "Evaluated-With", "COCO"]], "rel_plus": [["Mask R - CNN:Method", "Evaluated-With", "COCO:Dataset"]]}
{"doc_id": "60440450", "sentence": "Generally , we use the objectness model trained in Section 5. 3 . 2 to initialize the appearance stream , the bounding box and mask prediction heads , and the region proposal network ( RPN ) .", "ner": [["mask prediction", "Task"], ["region proposal network", "Method"], ["RPN", "Method"]], "rel": [["region proposal network", "Used-For", "mask prediction"], ["RPN", "Synonym-Of", "region proposal network"]], "rel_plus": [["region proposal network:Method", "Used-For", "mask prediction:Task"], ["RPN:Method", "Synonym-Of", "region proposal network:Method"]]}
{"doc_id": "60440450", "sentence": "However , finetuning this model on the DAVIS 1 6 training set leads to our best model , as seen in Table 4 .   Although our approach provides instance segmentation masks for each moving object in a video , we verify the ability of our method to detect moving objects by comparing to prior work on the motion foreground - background segmentation task on DAVIS 2 0 1 6 [ 3 3 ] .", "ner": [["DAVIS 1 6", "Dataset"], ["instance segmentation", "Task"], ["foreground - background segmentation", "Task"], ["DAVIS 2 0 1 6", "Dataset"]], "rel": [["DAVIS 2 0 1 6", "Benchmark-For", "foreground - background segmentation"]], "rel_plus": [["DAVIS 2 0 1 6:Dataset", "Benchmark-For", "foreground - background segmentation:Task"]]}
{"doc_id": "60440450", "sentence": "In order to evaluate this subtask , [ 6 ] uses an alternative labeling for FBMS introduced in [ 4 ] , and supplements the official FBMS measure with a \u2206 Obj metric , which indicates the average absolute difference between the number of predicted objects and groundtruth objects in each sequence .", "ner": [["FBMS", "Dataset"], ["FBMS", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "60440450", "sentence": "Finally , we present video instance segmentation evaluations on the Freiburg Berkeley Motion Segmentation ( FBMS ) dataset [ 2 9 ] on the official metric as well as our proposed metric ( Section 4 ) , followed by qualitative results .", "ner": [["video instance segmentation", "Task"], ["Freiburg Berkeley Motion Segmentation", "Dataset"], ["FBMS", "Dataset"]], "rel": [["Freiburg Berkeley Motion Segmentation", "Benchmark-For", "video instance segmentation"], ["FBMS", "Synonym-Of", "Freiburg Berkeley Motion Segmentation"]], "rel_plus": [["Freiburg Berkeley Motion Segmentation:Dataset", "Benchmark-For", "video instance segmentation:Task"], ["FBMS:Dataset", "Synonym-Of", "Freiburg Berkeley Motion Segmentation:Dataset"]]}
{"doc_id": "60440450", "sentence": "In the final row , both [ 2 1 ] and [ 6 ] exhibit segmentation and tracking errors ; the region corresponding to the man 's foot ( colored yellow for Keuper et al. and red for Bideau et al. ) are mistakenly tracked into a background region thus segmenting part of the background as a moving object .", "ner": [["segmentation", "Task"], ["tracking", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "60440450", "sentence": "We evaluate our proposed approach on the FBMS and DAVIS benchmark datasets , achieving state - of - the - art results .", "ner": [["FBMS", "Dataset"], ["DAVIS", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "44148233", "sentence": "The past few years have seen a surge of research in this area due to the unprecedented success of deep learning in computer vision and natural language processing .", "ner": [["deep learning", "Method"], ["computer vision", "Task"], ["natural language processing", "Task"]], "rel": [["deep learning", "Used-For", "computer vision"], ["deep learning", "Used-For", "natural language processing"]], "rel_plus": [["deep learning:Method", "Used-For", "computer vision:Task"], ["deep learning:Method", "Used-For", "natural language processing:Task"]]}
{"doc_id": "44148233", "sentence": "Automatic video description involves understanding of many entities and the detection of their occurrences in a video employing computer vision techniques .", "ner": [["Automatic video description", "Task"], ["computer vision techniques", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "44148233", "sentence": "All this information must then be articulated using a comprehensible and grammatically correct text employing Natural Language Processing ( NLP ) techniques .", "ner": [["Natural Language Processing", "Task"], ["NLP", "Task"]], "rel": [["NLP", "Synonym-Of", "Natural Language Processing"]], "rel_plus": [["NLP:Task", "Synonym-Of", "Natural Language Processing:Task"]]}
{"doc_id": "44148233", "sentence": "Over the past few years , these two traditionally independent fields , Computer Vision ( CV ) and Natural Language Processing ( NLP ) have joined forces to address the upsurge of research interests in understanding and describing images and videos .", "ner": [["Computer Vision", "Task"], ["CV", "Task"], ["Natural Language Processing", "Task"], ["NLP", "Task"]], "rel": [["CV", "Synonym-Of", "Computer Vision"], ["NLP", "Synonym-Of", "Natural Language Processing"]], "rel_plus": [["CV:Task", "Synonym-Of", "Computer Vision:Task"], ["NLP:Task", "Synonym-Of", "Natural Language Processing:Task"]]}
{"doc_id": "44148233", "sentence": "Special issues of journals are published focusing on language in vision [ 9 ] and workshops uniting the two areas have also been held regularly at both NLP and CV conferences [ 1 5 ] , [ 1 6 ] , [ 1 7 ] , [ 1 0 5 ] .", "ner": [["NLP", "Task"], ["CV", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "44148233", "sentence": "Automatic video description has many applications in human - robot interaction , automatic video subtitling and video surveillance .", "ner": [["Automatic video description", "Task"], ["human - robot interaction", "Task"], ["automatic video subtitling", "Task"], ["video surveillance", "Task"]], "rel": [["Automatic video description", "Used-For", "human - robot interaction"], ["Automatic video description", "Used-For", "automatic video subtitling"], ["Automatic video description", "Used-For", "video surveillance"]], "rel_plus": [["Automatic video description:Task", "Used-For", "human - robot interaction:Task"], ["Automatic video description:Task", "Used-For", "automatic video subtitling:Task"], ["Automatic video description:Task", "Used-For", "video surveillance:Task"]]}
{"doc_id": "44148233", "sentence": "Leveraging the recent developments in deep neural net - works for NLP and CV , and the increased availability of large multi - modal datasets , automatically generating stories from pixels is no longer a science fiction .", "ner": [["NLP", "Task"], ["CV", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "44148233", "sentence": "This growing body of work has mainly originated from the robotics community and can be labeled broadly as language grounded meaning from vision to robotic perception [ 1 3 7 ] .", "ner": [["robotic perception", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "44148233", "sentence": "Related research areas include , connecting words to pictures [ 2 5 ] , [ 2 6 ] , [ 4 3 ] , narrating images in natural language sentences [ 5 0 ] , [ 9 0 ] , [ 9 6 ] and understanding natural language instructions for robotic applications [ 6 5 ] , [ 1 0 6 ] , [ 1 5 2 ] .", "ner": [["connecting words to pictures", "Task"], ["narrating images in natural language sentences", "Task"], ["understanding natural language instructions for robotic applications", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "44148233", "sentence": "Thanks to the release of benchmark datasets MS COCO [ 9 9 ] and Flicker 3 0 k [ 1 8 0 ] , research in image captioning and retrieval [ 4 5 ] , [ 4 9 ] , [ 8 2 ] , [ 1 0 4 ] , and image question answering [ 1 8 ] , [ 1 0 3 ] , [ 1 2 7 ] , [ 1 8 4 ] has also become very active .", "ner": [["MS COCO", "Dataset"], ["Flicker 3 0 k", "Dataset"], ["image captioning", "Task"], ["retrieval", "Task"], ["image question answering", "Task"]], "rel": [["MS COCO", "Benchmark-For", "image captioning"], ["Flicker 3 0 k", "Benchmark-For", "image captioning"], ["MS COCO", "Benchmark-For", "retrieval"], ["Flicker 3 0 k", "Benchmark-For", "retrieval"], ["MS COCO", "Benchmark-For", "image question answering"], ["Flicker 3 0 k", "Benchmark-For", "image question answering"]], "rel_plus": [["MS COCO:Dataset", "Benchmark-For", "image captioning:Task"], ["Flicker 3 0 k:Dataset", "Benchmark-For", "image captioning:Task"], ["MS COCO:Dataset", "Benchmark-For", "retrieval:Task"], ["Flicker 3 0 k:Dataset", "Benchmark-For", "retrieval:Task"], ["MS COCO:Dataset", "Benchmark-For", "image question answering:Task"], ["Flicker 3 0 k:Dataset", "Benchmark-For", "image question answering:Task"]]}
{"doc_id": "44148233", "sentence": "The task of video description is relatively more challenging , compared to image captioning , because not all objects in the video are relevant to the description such as the detected objects that do not play any role in the observed activity [ 2 4 ] .", "ner": [["video description", "Task"], ["image captioning", "Task"]], "rel": [["video description", "Compare-With", "image captioning"]], "rel_plus": [["video description:Task", "Compare-With", "image captioning:Task"]]}
{"doc_id": "44148233", "sentence": "The example illustrates differences between three related areas of research , namely , image captioning , video captioning and dense video captioning .", "ner": [["image captioning", "Task"], ["video captioning", "Task"], ["dense video captioning", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "44148233", "sentence": "Below we define some terminologies used in this paper . \u2022 Visual Description : The unifying concept encompassing ( see Fig. 3 ) the automatic generation of single or multiple natural language sentences that convey the information in still images or video clips . \u2022 Video Captioning : Conveying the information of a video clip as a whole through a single automatically generated natural language sentence based on the premise that short video clips usually contain one main event [ 2 1 ] , [ 4 5 ] , [ 5 5 ] , [ 1 1 7 ] , [ 1 6 0 ] , [ 1 7 8 ] . \u2022 Video Description : Automatically generating multiple natural language sentences that provide a narrative of a relatively longer video clip .", "ner": [["Visual Description", "Task"], ["Video Captioning", "Task"], ["Video Description", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "44148233", "sentence": "Video description is sometimes also referred to as story telling or paragraph generation [ 1 3 0 ] , [ 1 8 3 ] . \u2022 Dense Video Captioning : Detection and conveying information of all , possibly overlapping , events of different lengths in a video using a natural language sentence per event .", "ner": [["Video description", "Task"], ["story telling", "Task"], ["paragraph generation", "Task"], ["Dense Video Captioning", "Task"]], "rel": [["story telling", "Synonym-Of", "Video description"], ["paragraph generation", "Synonym-Of", "Video description"]], "rel_plus": [["story telling:Task", "Synonym-Of", "Video description:Task"], ["paragraph generation:Task", "Synonym-Of", "Video description:Task"]]}
{"doc_id": "44148233", "sentence": "However , the advent of deep learning and the tremendous advancements in CV and NLP have equally affected the area of video captioning .", "ner": [["deep learning", "Method"], ["CV", "Task"], ["NLP", "Task"], ["video captioning", "Task"]], "rel": [["deep learning", "Used-For", "CV"], ["deep learning", "Used-For", "NLP"], ["deep learning", "Used-For", "video captioning"]], "rel_plus": [["deep learning:Method", "Used-For", "CV:Task"], ["deep learning:Method", "Used-For", "NLP:Task"], ["deep learning:Method", "Used-For", "video captioning:Task"]]}
{"doc_id": "44148233", "sentence": "Hence , latest approaches follow deep learning based architectures [ 1 3 3 ] , [ 1 6 0 ] that encode the visual features with 2D/ 3 D - CNN and use LSTM/GRU to learn the sequence .", "ner": [["deep learning based architectures", "Method"], ["2D/ 3 D - CNN", "Method"], ["LSTM/GRU", "Method"]], "rel": [["2D/ 3 D - CNN", "Part-Of", "deep learning based architectures"], ["LSTM/GRU", "Part-Of", "deep learning based architectures"]], "rel_plus": [["2D/ 3 D - CNN:Method", "Part-Of", "deep learning based architectures:Method"], ["LSTM/GRU:Method", "Part-Of", "deep learning based architectures:Method"]]}
{"doc_id": "44148233", "sentence": "The output of both approaches is either a single sentence [ 1 1 6 ] , [ 1 7 6 ] , or multiple sentences [ 2 4 ] , [ 4 1 ] , [ 7 8 ] , [ 1 3 0 ] , [ 1 4 5 ] , [ 1 8 3 ] per video Fig. 2 : Illustration of differences between image captioning , video captioning and dense video captioning .", "ner": [["image captioning", "Task"], ["video captioning", "Task"], ["dense video captioning", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "44148233", "sentence": "Methods that follow CNN - LSTM/GRU framework mainly differ from each other in the different types of CNNs and language models ( vanilla RNN , LSTM , and GRUs ) they employ and as well as how they pass the extracted visual features to the language model ( at the first time step only or all time steps ) .", "ner": [["CNN - LSTM/GRU", "Method"], ["CNNs", "Method"], ["language models", "Method"], ["RNN", "Method"], ["LSTM", "Method"], ["GRUs", "Method"]], "rel": [["CNNs", "Part-Of", "CNN - LSTM/GRU"], ["language models", "Part-Of", "CNN - LSTM/GRU"], ["RNN", "SubClass-Of", "language models"], ["LSTM", "SubClass-Of", "language models"], ["GRUs", "SubClass-Of", "language models"]], "rel_plus": [["CNNs:Method", "Part-Of", "CNN - LSTM/GRU:Method"], ["language models:Method", "Part-Of", "CNN - LSTM/GRU:Method"], ["RNN:Method", "SubClass-Of", "language models:Method"], ["LSTM:Method", "SubClass-Of", "language models:Method"], ["GRUs:Method", "SubClass-Of", "language models:Method"]]}
{"doc_id": "44148233", "sentence": "More recently , video based visual description problem has evolved towards dense video captioning and video story telling .", "ner": [["video based visual description problem", "Task"], ["dense video captioning", "Task"], ["video story telling", "Task"]], "rel": [["dense video captioning", "SubTask-Of", "video based visual description problem"], ["video story telling", "SubTask-Of", "video based visual description problem"]], "rel_plus": [["dense video captioning:Task", "SubTask-Of", "video based visual description problem:Task"], ["video story telling:Task", "SubTask-Of", "video based visual description problem:Task"]]}
{"doc_id": "44148233", "sentence": "Currently , automatic evaluations are typically performed using machine translation and image captioning metrics , including Bilingual Evaluation Understudy ( BLEU ) [ 1 1 8 ] , Recall Oriented Understudy for Gisting Evaluation ( ROUGE ) [ 9 8 ] , Metric for Evaluation of Translation with Explicit Ordering ( METEOR ) [ 2 2 ] , Consensus based Image Description Evaluation ( CIDEr ) [ 1 5 8 ] , and the recently proposed Semantic Propositional Image Captioning Evaluation ( SPICE ) [ 1 4 ] and Word Mover 's Distance ( WMD ) [ 9 2 ] metrics .", "ner": [["machine translation and image captioning metrics", "Method"], ["Bilingual Evaluation Understudy", "Method"], ["BLEU", "Method"], ["Recall Oriented Understudy for Gisting Evaluation", "Method"], ["ROUGE", "Method"], ["Metric for Evaluation of Translation with Explicit Ordering", "Method"], ["METEOR", "Method"], ["Consensus based Image Description Evaluation", "Method"], ["CIDEr", "Method"], ["Semantic Propositional Image Captioning Evaluation", "Method"], ["SPICE", "Method"], ["Word Mover 's Distance", "Method"], ["WMD", "Method"]], "rel": [["Bilingual Evaluation Understudy", "SubClass-Of", "machine translation and image captioning metrics"], ["Recall Oriented Understudy for Gisting Evaluation", "SubClass-Of", "machine translation and image captioning metrics"], ["Metric for Evaluation of Translation with Explicit Ordering", "SubClass-Of", "machine translation and image captioning metrics"], ["Consensus based Image Description Evaluation", "SubClass-Of", "machine translation and image captioning metrics"], ["Semantic Propositional Image Captioning Evaluation", "SubClass-Of", "machine translation and image captioning metrics"], ["Word Mover 's Distance", "SubClass-Of", "machine translation and image captioning metrics"], ["BLEU", "Synonym-Of", "Bilingual Evaluation Understudy"], ["ROUGE", "Synonym-Of", "Recall Oriented Understudy for Gisting Evaluation"], ["METEOR", "Synonym-Of", "Metric for Evaluation of Translation with Explicit Ordering"], ["CIDEr", "Synonym-Of", "Consensus based Image Description Evaluation"], ["SPICE", "Synonym-Of", "Semantic Propositional Image Captioning Evaluation"], ["WMD", "Synonym-Of", "Word Mover 's Distance"]], "rel_plus": [["Bilingual Evaluation Understudy:Method", "SubClass-Of", "machine translation and image captioning metrics:Method"], ["Recall Oriented Understudy for Gisting Evaluation:Method", "SubClass-Of", "machine translation and image captioning metrics:Method"], ["Metric for Evaluation of Translation with Explicit Ordering:Method", "SubClass-Of", "machine translation and image captioning metrics:Method"], ["Consensus based Image Description Evaluation:Method", "SubClass-Of", "machine translation and image captioning metrics:Method"], ["Semantic Propositional Image Captioning Evaluation:Method", "SubClass-Of", "machine translation and image captioning metrics:Method"], ["Word Mover 's Distance:Method", "SubClass-Of", "machine translation and image captioning metrics:Method"], ["BLEU:Method", "Synonym-Of", "Bilingual Evaluation Understudy:Method"], ["ROUGE:Method", "Synonym-Of", "Recall Oriented Understudy for Gisting Evaluation:Method"], ["METEOR:Method", "Synonym-Of", "Metric for Evaluation of Translation with Explicit Ordering:Method"], ["CIDEr:Method", "Synonym-Of", "Consensus based Image Description Evaluation:Method"], ["SPICE:Method", "Synonym-Of", "Semantic Propositional Image Captioning Evaluation:Method"], ["WMD:Method", "Synonym-Of", "Word Mover 's Distance:Method"]]}
{"doc_id": "44148233", "sentence": "CIDEr is a human - consensus - based evaluation metric , which was developed specifically for evaluating image captioning methods but has also been used in video description tasks .", "ner": [["CIDEr", "Method"], ["human - consensus - based evaluation metric", "Method"], ["image captioning", "Task"], ["video description", "Task"]], "rel": [["CIDEr", "SubClass-Of", "human - consensus - based evaluation metric"], ["CIDEr", "Used-For", "image captioning"], ["CIDEr", "Used-For", "video description"]], "rel_plus": [["CIDEr:Method", "SubClass-Of", "human - consensus - based evaluation metric:Method"], ["CIDEr:Method", "Used-For", "image captioning:Task"], ["CIDEr:Method", "Used-For", "video description:Task"]]}
{"doc_id": "44148233", "sentence": "We first highlight the important applications and major trends of video description in Section 1 and then classify automatic video description methods into three groups , giving an overview of the models from each group in Section 2 .", "ner": [["video description", "Task"], ["automatic video description", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "44148233", "sentence": "The classical methods phase , where pioneering visual description research employed classical CV and NLP methods to first detect entities ( objects , actions , scenes ) in videos and then fit them to standard sentence templates .", "ner": [["CV", "Task"], ["NLP", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "44148233", "sentence": "The first stage known as content identification focuses on visual recognition and classification of the main objects in the video clip .", "ner": [["visual recognition", "Task"], ["classification", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "44148233", "sentence": "Below we summarize the recognition techniques used in the Stage I of the SVO tuples based approaches . \u2022 Object Recognition : Object recognition in SVO approaches was performed typically using conventional methods , including model - based shape matching through edge detection or color matching [ 8 4 ] , HAAR features matching [ 1 6 4 ] , context - based object recognition [ 1 5 6 ] , Scale Invariant Feature Transform ( SIFT ) [ 1 0 1 ] , discriminatively trained partbased models [ 5 4 ] and Deformable Parts Model ( DPM ) [ 5 2 ] , [ 5 3 ] . \u2022 Human and Activity Detection : Human detection methods employed features such as Histograms of Oriented Gradient ( HOG ) [ 3 9 ] followed by SVM .", "ner": [["SVO", "Method"], ["Object Recognition", "Task"], ["Object recognition", "Task"], ["SVO", "Method"], ["model - based shape matching", "Method"], ["edge detection", "Method"], ["color matching", "Method"], ["HAAR features matching", "Method"], ["context - based object recognition", "Method"], ["Scale Invariant Feature Transform", "Method"], ["SIFT", "Method"], ["discriminatively trained partbased models", "Method"], ["Deformable Parts Model", "Method"], ["DPM", "Method"], ["Human and Activity Detection", "Task"], ["Human detection", "Task"], ["Histograms of Oriented Gradient", "Method"], ["HOG", "Method"], ["SVM", "Method"]], "rel": [["model - based shape matching", "Used-For", "Object recognition"], ["HAAR features matching", "Used-For", "Object recognition"], ["context - based object recognition", "Used-For", "Object recognition"], ["Scale Invariant Feature Transform", "Used-For", "Object recognition"], ["discriminatively trained partbased models", "Used-For", "Object recognition"], ["Deformable Parts Model", "Used-For", "Object recognition"], ["SVO", "Used-For", "Object recognition"], ["color matching", "Part-Of", "model - based shape matching"], ["edge detection", "Part-Of", "model - based shape matching"], ["SIFT", "Synonym-Of", "Scale Invariant Feature Transform"], ["DPM", "Synonym-Of", "Deformable Parts Model"], ["Histograms of Oriented Gradient", "Used-For", "Human detection"], ["SVM", "Used-For", "Human detection"], ["HOG", "Synonym-Of", "Histograms of Oriented Gradient"]], "rel_plus": [["model - based shape matching:Method", "Used-For", "Object recognition:Task"], ["HAAR features matching:Method", "Used-For", "Object recognition:Task"], ["context - based object recognition:Method", "Used-For", "Object recognition:Task"], ["Scale Invariant Feature Transform:Method", "Used-For", "Object recognition:Task"], ["discriminatively trained partbased models:Method", "Used-For", "Object recognition:Task"], ["Deformable Parts Model:Method", "Used-For", "Object recognition:Task"], ["SVO:Method", "Used-For", "Object recognition:Task"], ["color matching:Method", "Part-Of", "model - based shape matching:Method"], ["edge detection:Method", "Part-Of", "model - based shape matching:Method"], ["SIFT:Method", "Synonym-Of", "Scale Invariant Feature Transform:Method"], ["DPM:Method", "Synonym-Of", "Deformable Parts Model:Method"], ["Histograms of Oriented Gradient:Method", "Used-For", "Human detection:Task"], ["SVM:Method", "Used-For", "Human detection:Task"], ["HOG:Method", "Synonym-Of", "Histograms of Oriented Gradient:Method"]]}
{"doc_id": "44148233", "sentence": "For activity detection , features like Spatiotemporal Interest Points such as Histogram of Oriented Optical Flow ( HOOF ) [ 3 2 ] , Bayesian Networks ( BN ) [ 7 2 ] , Dynamic Bayesian Networks ( DBNs ) [ 5 9 ] , Hidden Markov Models ( HMM ) [ 2 7 ] , state machines [ 8 5 ] , and PNF Networks [ 1 2 1 ] have been used by SVO approaches . \u2022 Integrated Approaches : Instead of detecting the description - relevant entities separately , Stochastic Attribute Image Grammar ( SAIG ) [ 1 9 2 ] and Stochastic Context Free Grammars ( SCFG ) [ 1 1 0 ] , allow for compositional representation of visual entities present in a video , an image or a scene based on their spatial and functional relations .", "ner": [["activity detection", "Task"], ["Spatiotemporal Interest Points", "Method"], ["Histogram of Oriented Optical Flow", "Method"], ["HOOF", "Method"], ["Bayesian Networks", "Method"], ["BN", "Method"], ["Dynamic Bayesian Networks", "Method"], ["DBNs", "Method"], ["Hidden Markov Models", "Method"], ["HMM", "Method"], ["state machines", "Method"], ["PNF Networks", "Method"], ["SVO", "Method"], ["Stochastic Attribute Image Grammar", "Method"], ["SAIG", "Method"], ["Stochastic Context Free Grammars", "Method"], ["SCFG", "Method"]], "rel": [["Histogram of Oriented Optical Flow", "SubClass-Of", "Spatiotemporal Interest Points"], ["Bayesian Networks", "SubClass-Of", "Spatiotemporal Interest Points"], ["Dynamic Bayesian Networks", "SubClass-Of", "Spatiotemporal Interest Points"], ["Hidden Markov Models", "SubClass-Of", "Spatiotemporal Interest Points"], ["state machines", "SubClass-Of", "Spatiotemporal Interest Points"], ["PNF Networks", "SubClass-Of", "Spatiotemporal Interest Points"], ["HOOF", "Synonym-Of", "Histogram of Oriented Optical Flow"], ["BN", "Synonym-Of", "Bayesian Networks"], ["DBNs", "Synonym-Of", "Dynamic Bayesian Networks"], ["HMM", "Synonym-Of", "Hidden Markov Models"], ["PNF Networks", "Part-Of", "SVO"], ["state machines", "Part-Of", "SVO"], ["Hidden Markov Models", "Part-Of", "SVO"], ["Dynamic Bayesian Networks", "Part-Of", "SVO"], ["Bayesian Networks", "Part-Of", "SVO"], ["Histogram of Oriented Optical Flow", "Part-Of", "SVO"], ["SAIG", "Synonym-Of", "Stochastic Attribute Image Grammar"], ["SCFG", "Synonym-Of", "Stochastic Context Free Grammars"]], "rel_plus": [["Histogram of Oriented Optical Flow:Method", "SubClass-Of", "Spatiotemporal Interest Points:Method"], ["Bayesian Networks:Method", "SubClass-Of", "Spatiotemporal Interest Points:Method"], ["Dynamic Bayesian Networks:Method", "SubClass-Of", "Spatiotemporal Interest Points:Method"], ["Hidden Markov Models:Method", "SubClass-Of", "Spatiotemporal Interest Points:Method"], ["state machines:Method", "SubClass-Of", "Spatiotemporal Interest Points:Method"], ["PNF Networks:Method", "SubClass-Of", "Spatiotemporal Interest Points:Method"], ["HOOF:Method", "Synonym-Of", "Histogram of Oriented Optical Flow:Method"], ["BN:Method", "Synonym-Of", "Bayesian Networks:Method"], ["DBNs:Method", "Synonym-Of", "Dynamic Bayesian Networks:Method"], ["HMM:Method", "Synonym-Of", "Hidden Markov Models:Method"], ["PNF Networks:Method", "Part-Of", "SVO:Method"], ["state machines:Method", "Part-Of", "SVO:Method"], ["Hidden Markov Models:Method", "Part-Of", "SVO:Method"], ["Dynamic Bayesian Networks:Method", "Part-Of", "SVO:Method"], ["Bayesian Networks:Method", "Part-Of", "SVO:Method"], ["Histogram of Oriented Optical Flow:Method", "Part-Of", "SVO:Method"], ["SAIG:Method", "Synonym-Of", "Stochastic Attribute Image Grammar:Method"], ["SCFG:Method", "Synonym-Of", "Stochastic Context Free Grammars:Method"]]}
{"doc_id": "44148233", "sentence": "For Stage II , sentence generation , a variety of methods have been proposed including HALogen representation [ 9 3 ] , Head - driven Phrase Structure Grammar ( HPSG ) [ 1 2 2 ] , planner and surface realizer [ 1 2 6 ] .", "ner": [["HALogen representation", "Method"], ["Head - driven Phrase Structure Grammar", "Method"], ["HPSG", "Method"], ["planner and surface realizer", "Method"]], "rel": [["HPSG", "Synonym-Of", "Head - driven Phrase Structure Grammar"]], "rel_plus": [["HPSG:Method", "Synonym-Of", "Head - driven Phrase Structure Grammar:Method"]]}
{"doc_id": "44148233", "sentence": "Verb is obtained from action/activity detection methods using spatio - temporal features whereas subject and object are obtained from object detection methods using spatial features . the selection of appropriate lexicons for sentence generation .", "ner": [["action/activity detection", "Task"], ["object detection", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "44148233", "sentence": "Hakeem et . al. [ 6 6 ] addressed the shortcomings of Kojima et . al 's [ 8 4 ] work and proposed an extended case framework ( CASE E ) using hierarchical CASE representations .", "ner": [["CASE E", "Method"], ["hierarchical CASE representations", "Method"]], "rel": [["hierarchical CASE representations", "Part-Of", "CASE E"]], "rel_plus": [["hierarchical CASE representations:Method", "Part-Of", "CASE E:Method"]]}
{"doc_id": "44148233", "sentence": "Moreover , they incorporated temporal information into CASE using temporal logic to encode the relationship between sub - events .", "ner": [["CASE", "Method"], ["temporal logic", "Method"]], "rel": [["temporal logic", "Part-Of", "CASE"]], "rel_plus": [["temporal logic:Method", "Part-Of", "CASE:Method"]]}
{"doc_id": "44148233", "sentence": "They implemented a suite of conventional image processing techniques , including face detection [ 8 9 ] , emotion detection [ 1 0 2 ] , action detection [ 2 7 ] , non - human object detection [ 1 6 4 ] and scene classification [ 8 1 ] , to extract the high level entities of interest from video frames .", "ner": [["image processing", "Task"], ["face detection", "Task"], ["emotion detection", "Task"], ["action detection", "Task"], ["non - human object detection", "Task"], ["scene classification", "Task"]], "rel": [["scene classification", "SubTask-Of", "image processing"], ["face detection", "SubTask-Of", "image processing"], ["emotion detection", "SubTask-Of", "image processing"], ["action detection", "SubTask-Of", "image processing"], ["non - human object detection", "SubTask-Of", "image processing"]], "rel_plus": [["scene classification:Task", "SubTask-Of", "image processing:Task"], ["face detection:Task", "SubTask-Of", "image processing:Task"], ["emotion detection:Task", "SubTask-Of", "image processing:Task"], ["action detection:Task", "SubTask-Of", "image processing:Task"], ["non - human object detection:Task", "SubTask-Of", "image processing:Task"]]}
{"doc_id": "44148233", "sentence": "An \" image parsing engine \" using stochastic attribute image grammar ( SAIG ) [ 1 9 2 ] is employed to produce a visual vocabulary i.e. a list of visual entities present in the frame along with their relationships .", "ner": [["stochastic attribute image grammar", "Method"], ["SAIG", "Method"]], "rel": [["SAIG", "Synonym-Of", "stochastic attribute image grammar"]], "rel_plus": [["SAIG:Method", "Synonym-Of", "stochastic attribute image grammar:Method"]]}
{"doc_id": "44148233", "sentence": "Video Event Markup Language ( VEML ) [ 1 1 1 ] is used to represent semantic information .", "ner": [["Video Event Markup Language", "Method"], ["VEML", "Method"]], "rel": [["VEML", "Synonym-Of", "Video Event Markup Language"]], "rel_plus": [["VEML:Method", "Synonym-Of", "Video Event Markup Language:Method"]]}
{"doc_id": "44148233", "sentence": "In the final stage , head - driven phrase structure grammar ( HPSG ) [ 1 2 2 ] is used to generate text description from the semantic representation .", "ner": [["head - driven phrase structure grammar", "Method"], ["HPSG", "Method"]], "rel": [["HPSG", "Synonym-Of", "head - driven phrase structure grammar"]], "rel_plus": [["HPSG:Method", "Synonym-Of", "head - driven phrase structure grammar:Method"]]}
{"doc_id": "44148233", "sentence": "Verb labels corresponding to actions in the videos are then produced using Hidden Markov Models ( HMMs ) .", "ner": [["Hidden Markov Models", "Method"], ["HMMs", "Method"]], "rel": [["HMMs", "Synonym-Of", "Hidden Markov Models"]], "rel_plus": [["HMMs:Method", "Synonym-Of", "Hidden Markov Models:Method"]]}
{"doc_id": "44148233", "sentence": "Then by using grammar rules and parts of speech ( POS ) tagging , most probable subjects , objects and verbs are selected .", "ner": [["parts of speech", "Method"], ["POS", "Method"]], "rel": [["POS", "Synonym-Of", "parts of speech"]], "rel_plus": [["POS:Method", "Synonym-Of", "parts of speech:Method"]]}
{"doc_id": "44148233", "sentence": "Compared to Krishnamoorthy et al. [ 8 7 ] , instead of using only 2 0 objects in the PASCAL dataset [ 4 8 ] , all videos of the YouTube corpora are used for the detection of 2 4 1 objects , 4 5 subjects , and 2 1 8 verbs .", "ner": [["PASCAL", "Dataset"], ["YouTube corpora", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "44148233", "sentence": "To further improve the accuracy of assigning visually detected entities to the right category , probabilities using language statistics obtained from four \" out of domain \" English text corpora : English Gigaword , British National Corpus ( BNC ) , ukWac and WaCkypedia EN are used to enhance the confidence of word - category alignment for sentence generation .", "ner": [["English Gigaword", "Dataset"], ["British National Corpus", "Dataset"], ["BNC", "Dataset"], ["ukWac", "Dataset"], ["WaCkypedia EN", "Dataset"]], "rel": [["BNC", "Synonym-Of", "British National Corpus"]], "rel_plus": [["BNC:Dataset", "Synonym-Of", "British National Corpus:Dataset"]]}
{"doc_id": "44148233", "sentence": "Na\u00efve SVO tuple rule - based engineering approaches are indeed inadequate to describe open domain videos and large datasets , such as YouTubeClips [ 3 4 ] , TACoS - MultiLevel [ 1 3 0 ] , MPII - MD [ 1 3 2 ] , and M - VAD [ 1 5 5 ] .", "ner": [["Na\u00efve SVO tuple rule - based engineering approaches", "Method"], ["YouTubeClips", "Dataset"], ["TACoS - MultiLevel", "Dataset"], ["MPII - MD", "Dataset"], ["M - VAD", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "44148233", "sentence": "First , it learns to represent the video as intermediate semantic labels using maximum posterior estimate ( MAP ) .", "ner": [["maximum posterior estimate", "Method"], ["MAP", "Method"]], "rel": [["MAP", "Synonym-Of", "maximum posterior estimate"]], "rel_plus": [["MAP:Method", "Synonym-Of", "maximum posterior estimate:Method"]]}
{"doc_id": "44148233", "sentence": "Then , it translates the semantic labels into natural language sentences by using techniques borrowed from Statistical Machine Translation ( SMT ) [ 8 3 ] .", "ner": [["Statistical Machine Translation", "Method"], ["SMT", "Method"]], "rel": [["SMT", "Synonym-Of", "Statistical Machine Translation"]], "rel_plus": [["SMT:Method", "Synonym-Of", "Statistical Machine Translation:Method"]]}
{"doc_id": "44148233", "sentence": "For the object and activity recognition stages , the research moved from earlier threshold - based detection [ 8 4 ] to manual feature engineering and traditional classifiers [ 4 1 ] , [ 6 4 ] , [ 8 7 ] , [ 1 5 3 ] .", "ner": [["object and activity recognition", "Task"], ["threshold - based detection", "Task"], ["manual feature engineering", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "44148233", "sentence": "The whirlwind success of deep learning in almost all subfields of computer vision , has also revolutionized video description approaches .", "ner": [["deep learning", "Method"], ["computer vision", "Task"], ["video description", "Task"]], "rel": [["video description", "SubTask-Of", "computer vision"], ["deep learning", "Used-For", "computer vision"], ["deep learning", "Used-For", "video description"]], "rel_plus": [["video description:Task", "SubTask-Of", "computer vision:Task"], ["deep learning:Method", "Used-For", "computer vision:Task"], ["deep learning:Method", "Used-For", "video description:Task"]]}
{"doc_id": "44148233", "sentence": "In particular , Convolutional Neural Networks ( CNNs ) [ 8 8 ] are the state of the art for modeling visual data and excel at tasks such as object recognition [ 8 8 ] , [ 1 4 7 ] , [ 1 5 1 ] .", "ner": [["Convolutional Neural Networks", "Method"], ["CNNs", "Method"], ["object recognition", "Task"]], "rel": [["CNNs", "Synonym-Of", "Convolutional Neural Networks"], ["Convolutional Neural Networks", "Used-For", "object recognition"]], "rel_plus": [["CNNs:Method", "Synonym-Of", "Convolutional Neural Networks:Method"], ["Convolutional Neural Networks:Method", "Used-For", "object recognition:Task"]]}
{"doc_id": "44148233", "sentence": "Long Short - Term Memory ( LSTMs ) [ 7 1 ] and the more general deep Recurrent Neural Networks ( RNNs ) , on the other hand , are now dominating the area of sequence modeling , setting new benchmarks in machine translation [ 3 7 ] , [ 1 5 0 ] , speech recognition [ 6 2 ] and the closely related task of image captioning [ 4 5 ] , [ 1 6 3 ] .", "ner": [["Long Short - Term Memory", "Method"], ["LSTMs", "Method"], ["Recurrent Neural Networks", "Method"], ["RNNs", "Method"], ["sequence modeling", "Task"], ["machine translation", "Task"], ["speech recognition", "Task"], ["image captioning", "Task"]], "rel": [["LSTMs", "Synonym-Of", "Long Short - Term Memory"], ["RNNs", "Synonym-Of", "Recurrent Neural Networks"], ["Recurrent Neural Networks", "Used-For", "sequence modeling"], ["Long Short - Term Memory", "Used-For", "sequence modeling"], ["Long Short - Term Memory", "Used-For", "machine translation"], ["Recurrent Neural Networks", "Used-For", "machine translation"], ["Long Short - Term Memory", "Used-For", "speech recognition"], ["Recurrent Neural Networks", "Used-For", "speech recognition"], ["Long Short - Term Memory", "Used-For", "image captioning"], ["Recurrent Neural Networks", "Used-For", "image captioning"]], "rel_plus": [["LSTMs:Method", "Synonym-Of", "Long Short - Term Memory:Method"], ["RNNs:Method", "Synonym-Of", "Recurrent Neural Networks:Method"], ["Recurrent Neural Networks:Method", "Used-For", "sequence modeling:Task"], ["Long Short - Term Memory:Method", "Used-For", "sequence modeling:Task"], ["Long Short - Term Memory:Method", "Used-For", "machine translation:Task"], ["Recurrent Neural Networks:Method", "Used-For", "machine translation:Task"], ["Long Short - Term Memory:Method", "Used-For", "speech recognition:Task"], ["Recurrent Neural Networks:Method", "Used-For", "speech recognition:Task"], ["Long Short - Term Memory:Method", "Used-For", "image captioning:Task"], ["Recurrent Neural Networks:Method", "Used-For", "image captioning:Task"]]}
{"doc_id": "44148233", "sentence": "As shown in Figure 7 , the deep learning approaches to video description can also be divided into two sequential stages , namely , visual content extraction and text generation .", "ner": [["deep learning", "Method"], ["video description", "Task"], ["visual content extraction", "Task"], ["text generation", "Task"]], "rel": [["deep learning", "Used-For", "video description"], ["text generation", "SubTask-Of", "video description"], ["visual content extraction", "SubTask-Of", "video description"], ["deep learning", "Used-For", "visual content extraction"], ["deep learning", "Used-For", "text generation"]], "rel_plus": [["deep learning:Method", "Used-For", "video description:Task"], ["text generation:Task", "SubTask-Of", "video description:Task"], ["visual content extraction:Task", "SubTask-Of", "video description:Task"], ["deep learning:Method", "Used-For", "visual content extraction:Task"], ["deep learning:Method", "Used-For", "text generation:Task"]]}
{"doc_id": "44148233", "sentence": "CNN , RNN or Long Short - Term Memory ( LSTM ) are used in this encoding stage to learn these visual features , that are then used in the second stage for text generation , also known as the decoding stage .", "ner": [["CNN", "Method"], ["RNN", "Method"], ["Long Short - Term Memory", "Method"], ["LSTM", "Method"], ["text generation", "Task"]], "rel": [["LSTM", "Synonym-Of", "Long Short - Term Memory"], ["Long Short - Term Memory", "Used-For", "text generation"], ["RNN", "Used-For", "text generation"], ["CNN", "Used-For", "text generation"]], "rel_plus": [["LSTM:Method", "Synonym-Of", "Long Short - Term Memory:Method"], ["Long Short - Term Memory:Method", "Used-For", "text generation:Task"], ["RNN:Method", "Used-For", "text generation:Task"], ["CNN:Method", "Used-For", "text generation:Task"]]}
{"doc_id": "44148233", "sentence": "For decoding , different flavours of RNNs are used , such as deep RNN , Bi - directional RNN , LSTM or Gated Recurrent Units ( GRU ) .", "ner": [["RNNs", "Method"], ["RNN", "Method"], ["Bi - directional RNN", "Method"], ["LSTM", "Method"], ["Gated Recurrent Units", "Method"], ["GRU", "Method"]], "rel": [["RNN", "SubClass-Of", "RNNs"], ["Bi - directional RNN", "SubClass-Of", "RNNs"], ["Gated Recurrent Units", "SubClass-Of", "RNNs"], ["LSTM", "SubClass-Of", "RNNs"], ["GRU", "Synonym-Of", "Gated Recurrent Units"]], "rel_plus": [["RNN:Method", "SubClass-Of", "RNNs:Method"], ["Bi - directional RNN:Method", "SubClass-Of", "RNNs:Method"], ["Gated Recurrent Units:Method", "SubClass-Of", "RNNs:Method"], ["LSTM:Method", "SubClass-Of", "RNNs:Method"], ["GRU:Method", "Synonym-Of", "Gated Recurrent Units:Method"]]}
{"doc_id": "44148233", "sentence": "Figure 8 illustrates a typical end - to - end video description system with encoder - decoder stages .", "ner": [["video description", "Task"], ["encoder - decoder", "Method"]], "rel": [["encoder - decoder", "Used-For", "video description"]], "rel_plus": [["encoder - decoder:Method", "Used-For", "video description:Task"]]}
{"doc_id": "44148233", "sentence": "We group the literature based on the different combinations of deep learning architectures for encoding and decoding stages , namely : \u2022 CNN -RNN Video Description , where convolution architectures are used for visual encoding and recurrent structures are used for decoding .", "ner": [["deep learning", "Method"], ["CNN -RNN Video Description", "Method"], ["convolution", "Method"]], "rel": [["convolution", "Part-Of", "CNN -RNN Video Description"]], "rel_plus": [["convolution:Method", "Part-Of", "CNN -RNN Video Description:Method"]]}
{"doc_id": "44148233", "sentence": "This is the most common architecture employed in deep learning based video description methods ; \u2022 RNN -RNN Video Description , where recurrent networks are used for both stages ; and Deep reinforcement networks , the relatively new research area for video description .", "ner": [["deep learning", "Method"], ["video description", "Task"], ["RNN -RNN Video Description", "Method"], ["recurrent networks", "Method"], ["Deep reinforcement networks", "Method"], ["video description", "Task"]], "rel": [["deep learning", "Used-For", "video description"], ["recurrent networks", "Part-Of", "RNN -RNN Video Description"], ["Deep reinforcement networks", "Used-For", "video description"]], "rel_plus": [["deep learning:Method", "Used-For", "video description:Task"], ["recurrent networks:Method", "Part-Of", "RNN -RNN Video Description:Method"], ["Deep reinforcement networks:Method", "Used-For", "video description:Task"]]}
{"doc_id": "44148233", "sentence": "Given its success in computer vision and simplicity , CNN is still by far the most popular network structure used Donahue et al. [ 4 5 ] were the first to use a deep neural networks to solve the video captioning problem .", "ner": [["computer vision", "Task"], ["CNN", "Method"], ["video captioning", "Task"]], "rel": [["CNN", "Used-For", "computer vision"], ["CNN", "Used-For", "video captioning"]], "rel_plus": [["CNN:Method", "Used-For", "computer vision:Task"], ["CNN:Method", "Used-For", "video captioning:Task"]]}
{"doc_id": "44148233", "sentence": "The first architecture , LSTM encoderdecoder with CRF max , is motivated by the statistical machine translation ( SMT ) based video description approach by Rohrbach et al. [ 1 3 5 ] mentioned earlier in Section 2. 2 .", "ner": [["LSTM encoderdecoder", "Method"], ["CRF max", "Method"], ["statistical machine translation", "Method"], ["SMT", "Method"], ["video description", "Task"]], "rel": [["CRF max", "Part-Of", "LSTM encoderdecoder"], ["SMT", "Synonym-Of", "statistical machine translation"], ["statistical machine translation", "Used-For", "video description"]], "rel_plus": [["CRF max:Method", "Part-Of", "LSTM encoderdecoder:Method"], ["SMT:Method", "Synonym-Of", "statistical machine translation:Method"], ["statistical machine translation:Method", "Used-For", "video description:Task"]]}
{"doc_id": "44148233", "sentence": "Recognizing the state of the art machine translation performance of LSTMs , the SMT module in [ 1 3 5 ] is replaced with a stacked LSTM comprising two layers for encoding and decoding .", "ner": [["machine translation", "Task"], ["LSTMs", "Method"], ["SMT", "Method"], ["LSTM", "Method"]], "rel": [["LSTMs", "Used-For", "machine translation"], ["SMT", "Used-For", "machine translation"]], "rel_plus": [["LSTMs:Method", "Used-For", "machine translation:Task"], ["SMT:Method", "Used-For", "machine translation:Task"]]}
{"doc_id": "44148233", "sentence": "Another variant of the architecture , LSTM decoder with CRF max , incorporates max predictions .", "ner": [["LSTM decoder", "Method"], ["CRF max", "Method"]], "rel": [["CRF max", "Part-Of", "LSTM decoder"]], "rel_plus": [["CRF max:Method", "Part-Of", "LSTM decoder:Method"]]}
{"doc_id": "44148233", "sentence": "This virtue of LSTM is exploited in the third variant of the architecture , LSTM decoder with CRF probabilities .", "ner": [["LSTM", "Method"], ["LSTM decoder", "Method"], ["CRF", "Method"]], "rel": [["CRF", "Part-Of", "LSTM decoder"]], "rel_plus": [["CRF:Method", "Part-Of", "LSTM decoder:Method"]]}
{"doc_id": "44148233", "sentence": "Instead of using max predication like in second variant ( LSTM decoder with CRF max ) , this architecture incorporates probability distributions .", "ner": [["LSTM decoder", "Method"], ["CRF max", "Method"]], "rel": [["CRF max", "Part-Of", "LSTM decoder"]], "rel_plus": [["CRF max:Method", "Part-Of", "LSTM decoder:Method"]]}
{"doc_id": "44148233", "sentence": "Although the LSTM outperformed the SMT based approach of [ 1 3 5 ] , it was still not trainable in an end - to - end fashion .", "ner": [["LSTM", "Method"], ["SMT", "Method"]], "rel": [["LSTM", "Compare-With", "SMT"]], "rel_plus": [["LSTM:Method", "Compare-With", "SMT:Method"]]}
{"doc_id": "44148233", "sentence": "To avoid supervised intermediate representations , they connected an LSTM directly to the output of the CNN .", "ner": [["LSTM", "Method"], ["CNN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "44148233", "sentence": "The CNN extracts visual features whereas the LSTM models the sequence dynamics .", "ner": [["CNN", "Method"], ["LSTM", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "44148233", "sentence": "They transformed a short video into a fixed length visual input using a CNN model [ 7 4 ] that is slightly different from AlexNet [ 8 8 ] .", "ner": [["CNN", "Method"], ["AlexNet", "Method"]], "rel": [["AlexNet", "SubClass-Of", "CNN"]], "rel_plus": [["AlexNet:Method", "SubClass-Of", "CNN:Method"]]}
{"doc_id": "44148233", "sentence": "The CNN model [ 7 4 ] was learned using the ILSVRC - 2 0 1 2 object classification dataset ( comprising 1. 2 M images ) , which is a subset of ImageNet [ 1 4 0 ] .", "ner": [["CNN", "Method"], ["ILSVRC - 2 0 1 2 object classification", "Dataset"], ["ImageNet", "Dataset"]], "rel": [["CNN", "Trained-With", "ILSVRC - 2 0 1 2 object classification"], ["ILSVRC - 2 0 1 2 object classification", "SubClass-Of", "ImageNet"]], "rel_plus": [["CNN:Method", "Trained-With", "ILSVRC - 2 0 1 2 object classification:Dataset"], ["ILSVRC - 2 0 1 2 object classification:Dataset", "SubClass-Of", "ImageNet:Dataset"]]}
{"doc_id": "44148233", "sentence": "It provides a robust and efficient way without manual feature selection for initialization object recognition in the videos .", "ner": [["feature selection", "Task"], ["object recognition", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "44148233", "sentence": "The feature vectors from CNN form the input to the first layer of the LSTM .", "ner": [["CNN", "Method"], ["LSTM", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "44148233", "sentence": "A second LSTM layer is stacked on top of first LSTM layer , where the hidden state of the first LSTM layer becomes the input to the second LSTM unit for caption generation .", "ner": [["LSTM", "Method"], ["LSTM", "Method"], ["LSTM", "Method"], ["LSTM", "Method"], ["caption generation", "Task"]], "rel": [["LSTM", "Used-For", "caption generation"]], "rel_plus": [["LSTM:Method", "Used-For", "caption generation:Task"]]}
{"doc_id": "44148233", "sentence": "In essence , the transforming of multiple frame - based feature vectors into a single aggregated video - based vector , reduces the video description problem into an image captioning one .", "ner": [["video description", "Task"], ["image captioning", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "44148233", "sentence": "With the success of C 3 D [ 1 5 7 ] in capturing spatio - temporal action dynamics in videos , Li et al. [ 1 7 8 ] proposed a novel 3D - CNN to model the spatiotemporal information in videos .", "ner": [["C 3 D", "Method"], ["3D - CNN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "44148233", "sentence": "Their 3D - CNN is based on GoogLeNet [ 1 5 1 ] and pre - trained on an activity recognition dataset .", "ner": [["3D - CNN", "Method"], ["GoogLeNet", "Method"], ["activity recognition dataset", "Dataset"]], "rel": [["GoogLeNet", "Part-Of", "3D - CNN"], ["3D - CNN", "Trained-With", "activity recognition dataset"]], "rel_plus": [["GoogLeNet:Method", "Part-Of", "3D - CNN:Method"], ["3D - CNN:Method", "Trained-With", "activity recognition dataset:Dataset"]]}
{"doc_id": "44148233", "sentence": "Using 3D CNN and attention mechanisms in RNN , they were able to improve results .", "ner": [["3D CNN", "Method"], ["attention mechanisms", "Method"], ["RNN", "Method"]], "rel": [["3D CNN", "Part-Of", "RNN"], ["attention mechanisms", "Part-Of", "RNN"]], "rel_plus": [["3D CNN:Method", "Part-Of", "RNN:Method"], ["attention mechanisms:Method", "Part-Of", "RNN:Method"]]}
{"doc_id": "44148233", "sentence": "Recently , GRU - EVE [ 1 2 ] was proposed as an effective and computationally efficient technique for video captioning .", "ner": [["GRU - EVE", "Method"], ["video captioning", "Task"]], "rel": [["GRU - EVE", "Used-For", "video captioning"]], "rel_plus": [["GRU - EVE:Method", "Used-For", "video captioning:Task"]]}
{"doc_id": "44148233", "sentence": "GRU - EVE uses a standard GRU for language modeling but with Enriched Visual Encoding as follows .", "ner": [["GRU - EVE", "Method"], ["GRU", "Method"]], "rel": [["GRU", "Part-Of", "GRU - EVE"]], "rel_plus": [["GRU:Method", "Part-Of", "GRU - EVE:Method"]]}
{"doc_id": "44148233", "sentence": "It applies the Short Fourier Transform on 2D/ 3 D - CNN features in a hierarchical manner to encapsulate the spatiotemporal video dynamics .", "ner": [["Short Fourier Transform", "Method"], ["2D/ 3 D - CNN", "Method"]], "rel": [["Short Fourier Transform", "Part-Of", "2D/ 3 D - CNN"]], "rel_plus": [["Short Fourier Transform:Method", "Part-Of", "2D/ 3 D - CNN:Method"]]}
{"doc_id": "44148233", "sentence": "Interestingly , the enriched features obtained by applying Short Fourier Transform on 2D - CNN features alone [ 1 2 ] , outperform C 3 D [ 1 5 7 ] features .", "ner": [["Short Fourier Transform", "Method"], ["2D - CNN", "Method"], ["C 3 D", "Method"]], "rel": [["Short Fourier Transform", "Used-For", "2D - CNN"], ["2D - CNN", "Compare-With", "C 3 D"]], "rel_plus": [["Short Fourier Transform:Method", "Used-For", "2D - CNN:Method"], ["2D - CNN:Method", "Compare-With", "C 3 D:Method"]]}
{"doc_id": "44148233", "sentence": "For that purpose they used a two - layered LSTM framework , where the sequence of video frames is input to the first layer of the LSTM .", "ner": [["LSTM", "Method"], ["LSTM", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "44148233", "sentence": "The hidden state of the first LSTM layer forms the input to the second layer of the LSTM .", "ner": [["LSTM", "Method"], ["LSTM", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "44148233", "sentence": "Although sequenceto - sequence learning had previously been used in machine translation [ 1 5 0 ] , this is the first method [ 1 6 0 ] to use a sequence - to - sequence approach in video captioning .", "ner": [["sequenceto - sequence", "Method"], ["machine translation", "Task"], ["sequence - to - sequence", "Method"], ["video captioning", "Task"]], "rel": [["sequenceto - sequence", "Used-For", "machine translation"], ["sequence - to - sequence", "Used-For", "video captioning"]], "rel_plus": [["sequenceto - sequence:Method", "Used-For", "machine translation:Task"], ["sequence - to - sequence:Method", "Used-For", "video captioning:Task"]]}
{"doc_id": "44148233", "sentence": "Yu et al. [ 1 8 3 ] proposed a hierarchical recurrent neural network ( h - RNN ) that applies the attention mechanisms on both the temporal and spatial aspects .", "ner": [["hierarchical recurrent neural network", "Method"], ["h - RNN", "Method"], ["attention mechanisms", "Method"]], "rel": [["h - RNN", "Synonym-Of", "hierarchical recurrent neural network"], ["attention mechanisms", "Part-Of", "hierarchical recurrent neural network"]], "rel_plus": [["h - RNN:Method", "Synonym-Of", "hierarchical recurrent neural network:Method"], ["attention mechanisms:Method", "Part-Of", "hierarchical recurrent neural network:Method"]]}
{"doc_id": "44148233", "sentence": "First , a Gated Recurrent Unit ( GRU ) layer takes video features as input and generates a single short sentence .", "ner": [["Gated Recurrent Unit", "Method"], ["GRU", "Method"]], "rel": [["GRU", "Synonym-Of", "Gated Recurrent Unit"]], "rel_plus": [["GRU:Method", "Synonym-Of", "Gated Recurrent Unit:Method"]]}
{"doc_id": "44148233", "sentence": "Recently , Krishna et al. [ 8 6 ] introduced the concept of dense - captioning of events in a video and employed action detection techniques to predict the temporal intervals .", "ner": [["action detection", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "44148233", "sentence": "Although not as popular as the CNN - RNN framework , another approach is to also encode the visual information using RNNs .", "ner": [["CNN - RNN", "Method"], ["RNNs", "Method"]], "rel": [["RNNs", "Part-Of", "CNN - RNN"]], "rel_plus": [["RNNs:Method", "Part-Of", "CNN - RNN:Method"]]}
{"doc_id": "44148233", "sentence": "Srivastava et al. [ 1 4 8 ] use one LSTM to extract features from video frames ( i.e. encoding ) and then pass the feature vector through another LSTM for decoding .", "ner": [["LSTM", "Method"], ["LSTM", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "44148233", "sentence": "The authors adopted a machine translation model [ 1 5 0 ] for visual recognition but could not achieve significant improvement in classification accuracy .", "ner": [["machine translation", "Task"], ["visual recognition", "Task"], ["classification", "Task"]], "rel": [["machine translation", "Used-For", "visual recognition"]], "rel_plus": [["machine translation:Task", "Used-For", "visual recognition:Task"]]}
{"doc_id": "44148233", "sentence": "Yu et al. [ 1 8 3 ] proposed a similar approach and used two RNN structures for the video description task .", "ner": [["RNN", "Method"], ["video description", "Task"]], "rel": [["RNN", "Used-For", "video description"]], "rel_plus": [["RNN:Method", "Used-For", "video description:Task"]]}
{"doc_id": "44148233", "sentence": "Their configuration is a hierarchical decoder with multiple Gated Recurrent Units ( GRU ) for sentence generation .", "ner": [["Gated Recurrent Units", "Method"], ["GRU", "Method"]], "rel": [["GRU", "Synonym-Of", "Gated Recurrent Units"]], "rel_plus": [["GRU:Method", "Synonym-Of", "Gated Recurrent Units:Method"]]}
{"doc_id": "44148233", "sentence": "For example , variants of Hierarchical Reinforcement Learning ( HRL ) framework have been applied to Atari games [ 9 1 ] , [ 1 6 2 ] .", "ner": [["Hierarchical Reinforcement Learning", "Method"], ["HRL", "Method"], ["Atari games", "Task"]], "rel": [["HRL", "Synonym-Of", "Hierarchical Reinforcement Learning"], ["Hierarchical Reinforcement Learning", "Used-For", "Atari games"]], "rel_plus": [["HRL:Method", "Synonym-Of", "Hierarchical Reinforcement Learning:Method"], ["Hierarchical Reinforcement Learning:Method", "Used-For", "Atari games:Task"]]}
{"doc_id": "44148233", "sentence": "Similarly , different variants of DRL have been used to meet the challenging requirements of image captioning [ 1 2 8 ] as well as video description [ 3 6 ] , [ 9 5 ] , [ 1 1 9 ] , [ 1 2 0 ] , [ 1 7 1 ] .", "ner": [["DRL", "Method"], ["image captioning", "Task"], ["video description", "Task"]], "rel": [["DRL", "Used-For", "image captioning"], ["DRL", "Used-For", "video description"]], "rel_plus": [["DRL:Method", "Used-For", "image captioning:Task"], ["DRL:Method", "Used-For", "video description:Task"]]}
{"doc_id": "44148233", "sentence": "Xwang et al. [ 1 7 1 ] proposed a fully - differentiable neural network architecture using reinforcement learning for video description .", "ner": [["fully - differentiable neural network", "Method"], ["reinforcement learning", "Method"], ["video description", "Task"]], "rel": [["reinforcement learning", "Used-For", "fully - differentiable neural network"], ["fully - differentiable neural network", "Used-For", "video description"]], "rel_plus": [["reinforcement learning:Method", "Used-For", "fully - differentiable neural network:Method"], ["fully - differentiable neural network:Method", "Used-For", "video description:Task"]]}
{"doc_id": "44148233", "sentence": "The frame level features are processed through two stage encoder i.e. low level LSTM [ 1 4 1 ] followed by a high level LSTM [ 7 1 ] .", "ner": [["LSTM", "Method"], ["LSTM", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "44148233", "sentence": "Textually Annotated Cooking Scenes ( TACoS ) is a subset of MP - II Composites [ 1 3 6 ] .", "ner": [["Textually Annotated Cooking Scenes", "Dataset"], ["TACoS", "Dataset"], ["MP - II Composites", "Dataset"]], "rel": [["TACoS", "Synonym-Of", "Textually Annotated Cooking Scenes"], ["Textually Annotated Cooking Scenes", "SubClass-Of", "MP - II Composites"]], "rel_plus": [["TACoS:Dataset", "Synonym-Of", "Textually Annotated Cooking Scenes:Dataset"], ["Textually Annotated Cooking Scenes:Dataset", "SubClass-Of", "MP - II Composites:Dataset"]]}
{"doc_id": "44148233", "sentence": "Note that MP - II Composites contain more videos but less activities than the MP - II Cooking [ 1 3 4 ] .", "ner": [["MP - II Composites", "Dataset"], ["MP - II Cooking", "Dataset"]], "rel": [["MP - II Composites", "Compare-With", "MP - II Cooking"]], "rel_plus": [["MP - II Composites:Dataset", "Compare-With", "MP - II Cooking:Dataset"]]}
{"doc_id": "44148233", "sentence": "AMT workers were employed to align the sentences and associated videos for example : \" preparing carrots \" , \" cutting a cucumber \" or \" separating eggs \" etc .   TACoS Multilevel [ 1 3 0 ] corpus annotations were also collected via AMT workers on the TACoS corpus [ 1 2 5 ] .", "ner": [["TACoS Multilevel", "Dataset"], ["TACoS corpus", "Dataset"]], "rel": [["TACoS Multilevel", "SubClass-Of", "TACoS corpus"]], "rel_plus": [["TACoS Multilevel:Dataset", "SubClass-Of", "TACoS corpus:Dataset"]]}
{"doc_id": "44148233", "sentence": "Example clips and descriptions are shown in Figure 9 ( f ) . [ 1 5 5 ] is based on the Descriptive Video Service ( DVS ) and contains 4 8 , 9 8 6 video clips from 9 2 different movies .", "ner": [["Descriptive Video Service", "Method"], ["DVS", "Method"]], "rel": [["DVS", "Synonym-Of", "Descriptive Video Service"]], "rel_plus": [["DVS:Method", "Synonym-Of", "Descriptive Video Service:Method"]]}
{"doc_id": "44148233", "sentence": "ActivityNet Entities dataset ( or ANet - Entities ) [ 1 8 9 ] is the first video dataset with entities grounding and annotations .", "ner": [["ActivityNet Entities", "Dataset"], ["ANet - Entities", "Dataset"]], "rel": [["ANet - Entities", "Synonym-Of", "ActivityNet Entities"]], "rel_plus": [["ANet - Entities:Dataset", "Synonym-Of", "ActivityNet Entities:Dataset"]]}
{"doc_id": "44148233", "sentence": "However , validation set of ActivityNet Captions is randomly and evenly split into ANet - Entities validation ( 2. 5 k ) and testing ( 2. 5 k ) sets .   Microsoft Video Description ( MSVD ) dataset [ 3 3 ] comprises of 1, 9 7 0 YouTube clips with human annotated sentences .", "ner": [["ActivityNet Captions", "Dataset"], ["ANet - Entities", "Dataset"], ["Microsoft Video Description", "Dataset"], ["MSVD", "Dataset"]], "rel": [["ANet - Entities", "SubClass-Of", "ActivityNet Captions"], ["MSVD", "Synonym-Of", "Microsoft Video Description"]], "rel_plus": [["ANet - Entities:Dataset", "SubClass-Of", "ActivityNet Captions:Dataset"], ["MSVD:Dataset", "Synonym-Of", "Microsoft Video Description:Dataset"]]}
{"doc_id": "44148233", "sentence": "Video Titles in the Wild ( VTW ) [ 1 8 7 ] contains 1 8 1 0 0 video clips with an average of 1. 5 minutes duration per clip .", "ner": [["Video Titles in the Wild", "Dataset"], ["VTW", "Dataset"]], "rel": [["VTW", "Synonym-Of", "Video Titles in the Wild"]], "rel_plus": [["VTW:Dataset", "Synonym-Of", "Video Titles in the Wild:Dataset"]]}
{"doc_id": "44148233", "sentence": "ActivityNet Captions dataset [ 8 6 ] contains 1 0 0 k dense natural language descriptions of about 2 0 k videos from ActivityNet [ 1 9 2 ] that correspond to approximately 8 4 9 hours .", "ner": [["ActivityNet Captions", "Dataset"], ["ActivityNet", "Dataset"]], "rel": [["ActivityNet Captions", "SubClass-Of", "ActivityNet"]], "rel_plus": [["ActivityNet Captions:Dataset", "SubClass-Of", "ActivityNet:Dataset"]]}
{"doc_id": "44148233", "sentence": "Some of the major competitions are listed below .   The Large Scale Movie Description Challenge ( LSMDC ) [ 3 ] started in 2 0 1 5 in conjunction with ICCV 2 0 1 5 , and as an ECCV workshop in 2 0 1 6 .", "ner": [["Large Scale Movie Description Challenge", "Dataset"], ["LSMDC", "Dataset"]], "rel": [["LSMDC", "Synonym-Of", "Large Scale Movie Description Challenge"]], "rel_plus": [["LSMDC:Dataset", "Synonym-Of", "Large Scale Movie Description Challenge:Dataset"]]}
{"doc_id": "44148233", "sentence": "Since 2 0 1 7 , the MovieQA challenge has also been included in LSMDC in addition to the previous three tasks .", "ner": [["MovieQA", "Dataset"], ["LSMDC", "Dataset"]], "rel": [["MovieQA", "SubClass-Of", "LSMDC"]], "rel_plus": [["MovieQA:Dataset", "SubClass-Of", "LSMDC:Dataset"]]}
{"doc_id": "44148233", "sentence": "The LSMDC dataset basically combines two benchmark datasets , M - VAD [ 1 5 5 ] and MPII - MD [ 1 3 2 ] which were initially collected independently ( see Section 3. 2 ) .", "ner": [["LSMDC", "Dataset"], ["M - VAD", "Dataset"], ["MPII - MD", "Dataset"]], "rel": [["M - VAD", "SubClass-Of", "LSMDC"], ["MPII - MD", "SubClass-Of", "LSMDC"]], "rel_plus": [["M - VAD:Dataset", "SubClass-Of", "LSMDC:Dataset"], ["MPII - MD:Dataset", "SubClass-Of", "LSMDC:Dataset"]]}
{"doc_id": "44148233", "sentence": "A survey of benchmark results on video description ( Section - 6 ) shows that LSMDC has emerged as the most challenging dataset , evident by the poor performances of several models .", "ner": [["video description", "Task"], ["LSMDC", "Dataset"]], "rel": [["LSMDC", "Benchmark-For", "video description"]], "rel_plus": [["LSMDC:Dataset", "Benchmark-For", "video description:Task"]]}
{"doc_id": "44148233", "sentence": "Submission protocol of the challenge is similar to the MSCOCO Image Captioning Challenge [ 3 5 ] , and uses the same protocol for automatic evaluation .", "ner": [["MSCOCO", "Dataset"], ["Image Captioning", "Task"]], "rel": [["MSCOCO", "Benchmark-For", "Image Captioning"]], "rel_plus": [["MSCOCO:Dataset", "Benchmark-For", "Image Captioning:Task"]]}
{"doc_id": "44148233", "sentence": "In 2 0 1 6 , to further motivate and challenge the academic and the tech industry research community , Microsoft started the Microsoft Research -Video to Text ( MSR - VTT ) [ 4 ] competition aiming at bringing together computer vision and language researchers .", "ner": [["Microsoft Research -Video to Text", "Dataset"], ["MSR - VTT", "Dataset"], ["computer vision", "Task"]], "rel": [["MSR - VTT", "Synonym-Of", "Microsoft Research -Video to Text"], ["Microsoft Research -Video to Text", "Benchmark-For", "computer vision"]], "rel_plus": [["MSR - VTT:Dataset", "Synonym-Of", "Microsoft Research -Video to Text:Dataset"], ["Microsoft Research -Video to Text:Dataset", "Benchmark-For", "computer vision:Task"]]}
{"doc_id": "44148233", "sentence": "Unlike LSMDC , MSR - VTT challenge focuses only on the video to text task .", "ner": [["LSMDC", "Dataset"], ["MSR - VTT", "Dataset"]], "rel": [["MSR - VTT", "Compare-With", "LSMDC"]], "rel_plus": [["MSR - VTT:Dataset", "Compare-With", "LSMDC:Dataset"]]}
{"doc_id": "44148233", "sentence": "Text Retrieval Conference ( TREC ) is a series of workshops emphasizing various subareas of Information Retrieval ( IR ) research .", "ner": [["Text Retrieval Conference", "Dataset"], ["TREC", "Dataset"], ["Information Retrieval", "Task"], ["IR", "Task"]], "rel": [["TREC", "Synonym-Of", "Text Retrieval Conference"], ["Text Retrieval Conference", "Benchmark-For", "Information Retrieval"]], "rel_plus": [["TREC:Dataset", "Synonym-Of", "Text Retrieval Conference:Dataset"], ["Text Retrieval Conference:Dataset", "Benchmark-For", "Information Retrieval:Task"]]}
{"doc_id": "44148233", "sentence": "In particular , the TREC Video Retrieval Evaluation ( TRECVID ) [ 1 ] workshops , started in 2 0 0 1 , are dedicated to research efforts on content - based exploitation of digital videos .", "ner": [["TREC Video Retrieval Evaluation", "Dataset"], ["TRECVID", "Dataset"]], "rel": [["TRECVID", "Synonym-Of", "TREC Video Retrieval Evaluation"]], "rel_plus": [["TRECVID:Dataset", "Synonym-Of", "TREC Video Retrieval Evaluation:Dataset"]]}
{"doc_id": "44148233", "sentence": "The primary areas of interests include \" semantic indexing , video summarization , video copy detection , multimedia event detection and ad - hoc video search \" [ 1 ] .", "ner": [["semantic indexing", "Task"], ["video summarization", "Task"], ["video copy detection", "Task"], ["multimedia event detection", "Task"], ["ad - hoc video search", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "44148233", "sentence": "Since TREC - 2 0 1 6 , Video to Text Description ( VTT ) [ 1 9 ] using natural language has also been included in the challenge tasks .", "ner": [["Video to Text Description", "Dataset"], ["VTT", "Dataset"]], "rel": [["VTT", "Synonym-Of", "Video to Text Description"]], "rel_plus": [["VTT:Dataset", "Synonym-Of", "Video to Text Description:Dataset"]]}
{"doc_id": "44148233", "sentence": "Human evaluations are also employed to gauge the quality of the automatically generated descriptions following the Direct Assessment ( DA ) [ 6 1 ] method .", "ner": [["Direct Assessment", "Method"], ["DA", "Method"]], "rel": [["DA", "Synonym-Of", "Direct Assessment"]], "rel_plus": [["DA:Method", "Synonym-Of", "Direct Assessment:Method"]]}
{"doc_id": "44148233", "sentence": "Due to its high reliability , DA is now employed as the official ranking method for machine translation benchmark evaluations [ 2 8 ] .", "ner": [["DA", "Method"], ["machine translation", "Task"]], "rel": [["DA", "Used-For", "machine translation"]], "rel_plus": [["DA:Method", "Used-For", "machine translation:Task"]]}
{"doc_id": "44148233", "sentence": "As per DA based video description evaluation , human assessors are shown video - sentence pairs to rate how well the sentence describes the events in the video on a scale of 0 \u2212 1 0 0 [ 6 0 ] .", "ner": [["DA", "Method"], ["video description", "Task"]], "rel": [["DA", "Used-For", "video description"]], "rel_plus": [["DA:Method", "Used-For", "video description:Task"]]}
{"doc_id": "44148233", "sentence": "ActivityNet Dense - Captioning Events in Videos [ 7 ] was first introduced in 2 0 1 7 as a task of the ActivityNet Large Scale Activity Recognition Challenge [ 8 ] , [ 5 7 ] , running as a CVPR Workshop since 2 0 1 6 .", "ner": [["ActivityNet Dense - Captioning Events in Videos", "Dataset"], ["ActivityNet Large Scale Activity Recognition Challenge", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "44148233", "sentence": "Automatic evaluations are performed using six different metrics which were originally designed for machine translation and image captioning .", "ner": [["machine translation", "Task"], ["image captioning", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "44148233", "sentence": "For automatic evaluation , when comparing the generated sentences with ground truth descriptions , three evaluation metrics are borrowed from machine translation , namely , Bilingual Evaluation Understudy ( BLEU ) [ 1 1 8 ] , Recall Oriented Understudy of Gisting Evaluation ( ROUGE ) [ 9 8 ] and Metric for Evaluation of Translation with Explicit Ordering ( METEOR ) [ 2 2 ] .", "ner": [["machine translation", "Task"], ["Bilingual Evaluation Understudy", "Method"], ["BLEU", "Method"], ["Recall Oriented Understudy of Gisting Evaluation", "Method"], ["ROUGE", "Method"], ["Metric for Evaluation of Translation with Explicit Ordering", "Method"], ["METEOR", "Method"]], "rel": [["Bilingual Evaluation Understudy", "Used-For", "machine translation"], ["Recall Oriented Understudy of Gisting Evaluation", "Used-For", "machine translation"], ["Metric for Evaluation of Translation with Explicit Ordering", "Used-For", "machine translation"], ["BLEU", "Synonym-Of", "Bilingual Evaluation Understudy"], ["ROUGE", "Synonym-Of", "Recall Oriented Understudy of Gisting Evaluation"], ["METEOR", "Synonym-Of", "Metric for Evaluation of Translation with Explicit Ordering"]], "rel_plus": [["Bilingual Evaluation Understudy:Method", "Used-For", "machine translation:Task"], ["Recall Oriented Understudy of Gisting Evaluation:Method", "Used-For", "machine translation:Task"], ["Metric for Evaluation of Translation with Explicit Ordering:Method", "Used-For", "machine translation:Task"], ["BLEU:Method", "Synonym-Of", "Bilingual Evaluation Understudy:Method"], ["ROUGE:Method", "Synonym-Of", "Recall Oriented Understudy of Gisting Evaluation:Method"], ["METEOR:Method", "Synonym-Of", "Metric for Evaluation of Translation with Explicit Ordering:Method"]]}
{"doc_id": "44148233", "sentence": "Consensus based Image Description Evaluation ( CIDEr ) [ 1 5 8 ] and Semantic Propositional Image Captioning Evaluation ( SPICE ) [ 1 4 ] are two other recently introduced metrics specifically designed for image captioning tasks , that are also being used for automatic evaluation of video description .", "ner": [["Consensus based Image Description Evaluation", "Method"], ["CIDEr", "Method"], ["Semantic Propositional Image Captioning Evaluation", "Method"], ["SPICE", "Method"], ["image captioning", "Task"], ["video description", "Task"]], "rel": [["CIDEr", "Synonym-Of", "Consensus based Image Description Evaluation"], ["SPICE", "Synonym-Of", "Semantic Propositional Image Captioning Evaluation"], ["Semantic Propositional Image Captioning Evaluation", "Used-For", "image captioning"], ["Consensus based Image Description Evaluation", "Used-For", "image captioning"], ["Consensus based Image Description Evaluation", "Used-For", "video description"], ["Semantic Propositional Image Captioning Evaluation", "Used-For", "video description"]], "rel_plus": [["CIDEr:Method", "Synonym-Of", "Consensus based Image Description Evaluation:Method"], ["SPICE:Method", "Synonym-Of", "Semantic Propositional Image Captioning Evaluation:Method"], ["Semantic Propositional Image Captioning Evaluation:Method", "Used-For", "image captioning:Task"], ["Consensus based Image Description Evaluation:Method", "Used-For", "image captioning:Task"], ["Consensus based Image Description Evaluation:Method", "Used-For", "video description:Task"], ["Semantic Propositional Image Captioning Evaluation:Method", "Used-For", "video description:Task"]]}
{"doc_id": "44148233", "sentence": "The chunk includes unigrams that are adjacent in candidate [ 1 1 8 ] Machine translation n - gram precision ROUGE [ 9 8 ] Document summarization n - gram recall METEOR [ 2 2 ] Machine translation n - gram with synonym matching CIDEr [ 1 5 8 ] Image captioning tf - idf weighted n - gram similarity SPICE [ 1 4 ] Image captioning Scene - graph synonym matching WMD [ 9 2 ] Document similarity Earth mover distance on word 2 vec as well as reference sentences .", "ner": [["Machine translation", "Task"], ["ROUGE", "Method"], ["Document summarization", "Task"], ["METEOR", "Method"], ["Machine translation", "Task"], ["CIDEr", "Method"], ["Image captioning", "Task"], ["Image captioning", "Task"], ["WMD", "Method"], ["Document similarity", "Task"], ["word 2 vec", "Method"]], "rel": [["ROUGE", "Used-For", "Machine translation"], ["METEOR", "Used-For", "Document summarization"], ["CIDEr", "Used-For", "Machine translation"], ["WMD", "Used-For", "Image captioning"]], "rel_plus": [["ROUGE:Method", "Used-For", "Machine translation:Task"], ["METEOR:Method", "Used-For", "Document summarization:Task"], ["CIDEr:Method", "Used-For", "Machine translation:Task"], ["WMD:Method", "Used-For", "Image captioning:Task"]]}
{"doc_id": "44148233", "sentence": "There is no metric specifically designed for evaluating video description , instead machine translation and image captioning metrics have been extended for this task .", "ner": [["video description", "Task"], ["machine translation", "Task"], ["image captioning", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "44148233", "sentence": "GRU - EVE [ 1 2 ] achieves the best performance on METEOR and ROUGE L metrics and the second best on CIDEr metric whereas LSTM - TSA [ 1 1 7 ] and M 3 -IC [ 1 6 9 ] report the best BLEU scores .", "ner": [["GRU - EVE", "Method"], ["METEOR", "Method"], ["ROUGE L", "Method"], ["CIDEr", "Method"], ["LSTM - TSA", "Method"], ["M 3 -IC", "Method"], ["BLEU", "Method"]], "rel": [["GRU - EVE", "Compare-With", "LSTM - TSA"], ["GRU - EVE", "Compare-With", "M 3 -IC"]], "rel_plus": [["GRU - EVE:Method", "Compare-With", "LSTM - TSA:Method"], ["GRU - EVE:Method", "Compare-With", "M 3 -IC:Method"]]}
{"doc_id": "44148233", "sentence": "As shown in Table 6 , on TACoS Multilevel dataset , h - RNN [ 1 8 3 ] has the best results on all reported metrics i.e. BLEU , METEOR and CIDEr .", "ner": [["TACoS Multilevel", "Dataset"], ["h - RNN", "Method"], ["BLEU", "Method"], ["METEOR", "Method"], ["CIDEr", "Method"]], "rel": [["h - RNN", "Evaluated-With", "TACoS Multilevel"]], "rel_plus": [["h - RNN:Method", "Evaluated-With", "TACoS Multilevel:Dataset"]]}
{"doc_id": "44148233", "sentence": "On the more challenging M - VAD dataset , overall the reported results ( Table 7 ) are very poor , however , within the presented results we see that so far only Temporal - Attention [ 1 7 8 ] , and HRNE [ 1 1 5 ] reported results using the BLEU metric with a BLEU score of 0. 7 each .", "ner": [["M - VAD", "Dataset"], ["Temporal - Attention", "Method"], ["HRNE", "Method"], ["BLEU", "Method"], ["BLEU", "Method"]], "rel": [["Temporal - Attention", "Evaluated-With", "M - VAD"], ["HRNE", "Evaluated-With", "M - VAD"]], "rel_plus": [["Temporal - Attention:Method", "Evaluated-With", "M - VAD:Dataset"], ["HRNE:Method", "Evaluated-With", "M - VAD:Dataset"]]}
{"doc_id": "44148233", "sentence": "All the papers using this dataset report METEOR results and so far BAE [ 2 3 ] has produced the best METEOR score followed by LSTM - TSA [ 1 1 7 ] .", "ner": [["METEOR", "Method"], ["BAE", "Method"], ["METEOR", "Method"], ["LSTM - TSA", "Method"]], "rel": [["BAE", "Compare-With", "LSTM - TSA"]], "rel_plus": [["BAE:Method", "Compare-With", "LSTM - TSA:Method"]]}
{"doc_id": "44148233", "sentence": "HRNE [ 1 1 5 ] and Glove+Deep Fusion Ensemble [ 1 5 9 ] share the third place for METEOR score .", "ner": [["HRNE", "Method"], ["Glove+Deep Fusion Ensemble", "Method"], ["METEOR", "Method"]], "rel": [["HRNE", "Compare-With", "Glove+Deep Fusion Ensemble"]], "rel_plus": [["HRNE:Method", "Compare-With", "Glove+Deep Fusion Ensemble:Method"]]}
{"doc_id": "44148233", "sentence": "MPII - MD is another very challenging dataset and still has very low benchmark results , as shown in Table 8 , similar to the M - VAD dataset .", "ner": [["MPII - MD", "Dataset"], ["M - VAD", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "44148233", "sentence": "LSTM - TSA [ 1 1 7 ] has achieved the best METEOR score followed by LSTM - E [ 1 1 6 ] and S 2 VT [ 1 6 0 ] at second and third place respectively .", "ner": [["LSTM - TSA", "Method"], ["METEOR", "Method"], ["LSTM - E", "Method"], ["S 2 VT", "Method"]], "rel": [["LSTM - TSA", "Compare-With", "LSTM - E"], ["LSTM - TSA", "Compare-With", "S 2 VT"]], "rel_plus": [["LSTM - TSA:Method", "Compare-With", "LSTM - E:Method"], ["LSTM - TSA:Method", "Compare-With", "S 2 VT:Method"]]}
{"doc_id": "44148233", "sentence": "Results on another popular dataset , MSR - VTT , are overall better than the M - VAD and MPII - II datasets .", "ner": [["MSR - VTT", "Dataset"], ["M - VAD", "Dataset"], ["MPII - II", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "44148233", "sentence": "DenseVidCap [ 1 4 2 ] and HRL [ 1 7 1 ] respectively report the second and third best scores on BLEU metric .", "ner": [["DenseVidCap", "Method"], ["HRL", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "44148233", "sentence": "Highest scores in CIDEr and ROUGE metrics are achieved by methods DVC [ 9 7 ] and JEDDi - Net [ 1 7 4 ] respectively .", "ner": [["CIDEr", "Method"], ["ROUGE", "Method"], ["DVC", "Method"], ["JEDDi - Net", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "44148233", "sentence": "Finally , in Table 1 1 , we report two results for LSMDC and Charades each and only one result for YouCook - II datasets .", "ner": [["LSMDC", "Dataset"], ["Charades", "Dataset"], ["YouCook - II", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "44148233", "sentence": "Visual Reasoning : Although video VQA is still in nascent stage , beyond VQA is the visual reasoning problem .", "ner": [["Visual Reasoning", "Task"], ["video VQA", "Task"], ["VQA", "Task"], ["visual reasoning", "Task"]], "rel": [["VQA", "SubTask-Of", "visual reasoning"]], "rel_plus": [["VQA:Task", "SubTask-Of", "visual reasoning:Task"]]}
{"doc_id": "44148233", "sentence": "An example of visual reasoning models is the MAC Network [ 7 3 ] which is able to think and reason giving promising results on CLEVR [ 7 6 ] , a visual reasoning dataset .", "ner": [["visual reasoning", "Task"], ["MAC Network", "Method"], ["CLEVR", "Dataset"], ["visual reasoning", "Task"]], "rel": [["MAC Network", "Used-For", "visual reasoning"], ["CLEVR", "Benchmark-For", "visual reasoning"]], "rel_plus": [["MAC Network:Method", "Used-For", "visual reasoning:Task"], ["CLEVR:Dataset", "Benchmark-For", "visual reasoning:Task"]]}
{"doc_id": "44148233", "sentence": "Visual Dialogue : Similar to audio dialogue ( e.g. Siri , Hello Google , Alexa and ECHO ) , visual dialogue [ 4 2 ] is another promising and flourishing field , especially in an era where we look forward to interact with robots .", "ner": [["Visual Dialogue", "Task"], ["audio dialogue", "Task"], ["visual dialogue", "Task"]], "rel": [["audio dialogue", "Compare-With", "visual dialogue"]], "rel_plus": [["audio dialogue:Task", "Compare-With", "visual dialogue:Task"]]}
{"doc_id": "44148233", "sentence": "While the majority of computer vision research has focused on video description , without the help of audio , audio is naturally present in most of videos .", "ner": [["computer vision", "Task"], ["video description", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "44148233", "sentence": "This approach has shown significantly better performance in visual question answering methods and is likely to improve video description accuracy .", "ner": [["visual question answering", "Task"], ["video description", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "44148233", "sentence": "In conjunction with machine translation , video captioning may be used for automatic video subtitling .", "ner": [["machine translation", "Task"], ["video captioning", "Task"], ["automatic video subtitling", "Task"]], "rel": [["video captioning", "Used-For", "automatic video subtitling"]], "rel_plus": [["video captioning:Task", "Used-For", "automatic video subtitling:Task"]]}
{"doc_id": "44148233", "sentence": "So far video description has relied on automatic metrics designed for machine translation and image captioning tasks .", "ner": [["video description", "Task"], ["machine translation", "Task"], ["image captioning", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "44148233", "sentence": "To date there is no automatic video description ( or even captioning ) evaluation metric that is purpose designed .", "ner": [["automatic video description", "Task"], ["captioning", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "44148233", "sentence": "This problem is going to exacerbate in the future with dense video captioning and story telling tasks .", "ner": [["dense video captioning", "Task"], ["story telling", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "44148233", "sentence": "From an algorithm design perspective , although LSTMs have shown competitive caption generation performance , the interpretablity and intelligibility of the underlying models are low .", "ner": [["LSTMs", "Method"], ["caption generation", "Task"]], "rel": [["LSTMs", "Used-For", "caption generation"]], "rel_plus": [["LSTMs:Method", "Used-For", "caption generation:Task"]]}
{"doc_id": "44148233", "sentence": "Current metrics have been adopted either from machine translation or image captioning and fall short in measuring the quality of machine generated video captions and their agreement with human judgments .", "ner": [["machine translation", "Task"], ["image captioning", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "209532167", "sentence": "A significant advancement in natural - language understanding has come with the development of pre - trained contextual embeddings , such as BERT , which can be fine - tuned for downstream tasks with less labeled data and training budget , while achieving better accuracies .", "ner": [["natural - language understanding", "Task"], ["BERT", "Method"]], "rel": [["BERT", "Used-For", "natural - language understanding"]], "rel_plus": [["BERT:Method", "Used-For", "natural - language understanding:Task"]]}
{"doc_id": "209532167", "sentence": "Specifically , first , we curate a massive , deduplicated corpus of 7. 4 M Python files from GitHub , which we use to pre - train CuBERT , an open - sourced code - understanding BERT model ; and , second , we create an open - sourced benchmark that comprises five classification tasks and one program - repair task , akin to code - understanding tasks proposed in the literature before .", "ner": [["CuBERT", "Method"], ["BERT", "Method"], ["classification", "Task"], ["program - repair", "Task"], ["code - understanding", "Task"]], "rel": [["CuBERT", "SubClass-Of", "BERT"]], "rel_plus": [["CuBERT:Method", "SubClass-Of", "BERT:Method"]]}
{"doc_id": "209532167", "sentence": "We fine - tune CuBERT on our benchmark tasks , and compare the resulting models to different variants of Word 2 Vec token embeddings , BiLSTM and Transformer models , as well as published state - of - the - art models , showing that CuBERT outperforms them all , even with shorter training , and with fewer labeled examples .", "ner": [["CuBERT", "Method"], ["Word 2 Vec", "Method"], ["BiLSTM", "Method"], ["Transformer models", "Method"], ["CuBERT", "Method"]], "rel": [["CuBERT", "Compare-With", "Word 2 Vec"], ["CuBERT", "Compare-With", "BiLSTM"], ["CuBERT", "Compare-With", "Transformer models"]], "rel_plus": [["CuBERT:Method", "Compare-With", "Word 2 Vec:Method"], ["CuBERT:Method", "Compare-With", "BiLSTM:Method"], ["CuBERT:Method", "Compare-With", "Transformer models:Method"]]}
{"doc_id": "209532167", "sentence": "Sequence modeling on source code has been shown to be successful in a variety of software - engineering tasks , such as code completion ( Hindle et al. , 2 0 1 2 ; Raychev et al. , 2 0 1 4 ) , source code to pseudocode mapping ( Oda et al. , 2 0 1 5 ) , API - sequence prediction ( Gu et al. , 2 0 1 6 ) , program repair ( Pu et al. , 2 0 1 6 ; Gupta et al. , 2 0 1 7 ) , and natural language to code mapping ( Iyer et al. , 2 0 1 8) , among others .", "ner": [["Sequence modeling", "Method"], ["code completion", "Task"], ["source code to pseudocode mapping", "Task"], ["API - sequence prediction", "Task"], ["program repair", "Task"], ["natural language to code mapping", "Task"]], "rel": [["Sequence modeling", "Used-For", "code completion"], ["Sequence modeling", "Used-For", "source code to pseudocode mapping"], ["Sequence modeling", "Used-For", "API - sequence prediction"], ["Sequence modeling", "Used-For", "program repair"], ["Sequence modeling", "Used-For", "natural language to code mapping"]], "rel_plus": [["Sequence modeling:Method", "Used-For", "code completion:Task"], ["Sequence modeling:Method", "Used-For", "source code to pseudocode mapping:Task"], ["Sequence modeling:Method", "Used-For", "API - sequence prediction:Task"], ["Sequence modeling:Method", "Used-For", "program repair:Task"], ["Sequence modeling:Method", "Used-For", "natural language to code mapping:Task"]]}
{"doc_id": "209532167", "sentence": "In particular , BERT ( Devlin et al. , 2 0 1 9 ) produces a bidirectional Transformer encoder ( Vaswani et al. , 2 0 1 7 ) by training it to predict values of masked tokens and whether two sentences follow each other in a natural discourse .", "ner": [["BERT", "Method"], ["bidirectional Transformer encoder", "Method"]], "rel": [["BERT", "SubClass-Of", "bidirectional Transformer encoder"]], "rel_plus": [["BERT:Method", "SubClass-Of", "bidirectional Transformer encoder:Method"]]}
{"doc_id": "209532167", "sentence": "We call our model CuBERT , short for Code Understanding BERT .", "ner": [["CuBERT", "Method"], ["Code Understanding BERT", "Method"]], "rel": [["CuBERT", "Synonym-Of", "Code Understanding BERT"]], "rel_plus": [["CuBERT:Method", "Synonym-Of", "Code Understanding BERT:Method"]]}
{"doc_id": "209532167", "sentence": "We also train Word 2 Vec embeddings ( Mikolov et al. , 2 0 1 3 a , b ) , namely , continuous bag - of - words ( CBOW ) and Skipgram embeddings , on the same corpus .", "ner": [["Word 2 Vec", "Method"], ["continuous bag - of - words", "Method"], ["CBOW", "Method"], ["Skipgram", "Method"]], "rel": [["continuous bag - of - words", "SubClass-Of", "Word 2 Vec"], ["Skipgram", "SubClass-Of", "Word 2 Vec"], ["CBOW", "Synonym-Of", "continuous bag - of - words"]], "rel_plus": [["continuous bag - of - words:Method", "SubClass-Of", "Word 2 Vec:Method"], ["Skipgram:Method", "SubClass-Of", "Word 2 Vec:Method"], ["CBOW:Method", "Synonym-Of", "continuous bag - of - words:Method"]]}
{"doc_id": "209532167", "sentence": "For evaluating CuBERT , we create a benchmark of five classification tasks , ranging from classification of source code according to presence or absense of certain classes of bugs , to mismatch between a function 's natural language description and its body , to predicting the right kind of exception to catch for a given code fragment .", "ner": [["CuBERT", "Method"], ["classification", "Task"], ["classification", "Task"]], "rel": [["CuBERT", "Used-For", "classification"]], "rel_plus": [["CuBERT:Method", "Used-For", "classification:Task"]]}
{"doc_id": "209532167", "sentence": "To evaluate Cu - BERT 's effectiveness on a more complex task , we create a task for joint classification , localization and repair of variable misuse bugs ( Vasic et al. , 2 0 1 9 ) , which involves predicting two pointers .", "ner": [["Cu - BERT", "Method"], ["joint classification", "Task"]], "rel": [["Cu - BERT", "Used-For", "joint classification"]], "rel_plus": [["Cu - BERT:Method", "Used-For", "joint classification:Task"]]}
{"doc_id": "209532167", "sentence": "We fine - tune CuBERT on each of the classification tasks and compare the results with multilayered bidirectional LSTM ( Hochreiter & Schmidhuber , 1 9 9 7 ) models .", "ner": [["CuBERT", "Method"], ["classification", "Task"], ["multilayered bidirectional LSTM", "Method"]], "rel": [["CuBERT", "Used-For", "classification"], ["multilayered bidirectional LSTM", "Used-For", "classification"], ["CuBERT", "Compare-With", "multilayered bidirectional LSTM"]], "rel_plus": [["CuBERT:Method", "Used-For", "classification:Task"], ["multilayered bidirectional LSTM:Method", "Used-For", "classification:Task"], ["CuBERT:Method", "Compare-With", "multilayered bidirectional LSTM:Method"]]}
{"doc_id": "209532167", "sentence": "We train the LSTM models from scratch and also using pre - trainined Word 2 Vec embeddings .", "ner": [["LSTM", "Method"], ["Word 2 Vec", "Method"]], "rel": [["Word 2 Vec", "Used-For", "LSTM"]], "rel_plus": [["Word 2 Vec:Method", "Used-For", "LSTM:Method"]]}
{"doc_id": "209532167", "sentence": "We perform a number of additional studies by varying the sampling strategies used for training Word 2 Vec models , by varying program lengths , and by comparing against Transformer models trained from scratch .", "ner": [["Word 2 Vec", "Method"], ["Transformer", "Method"]], "rel": [["Word 2 Vec", "Compare-With", "Transformer"]], "rel_plus": [["Word 2 Vec:Method", "Compare-With", "Transformer:Method"]]}
{"doc_id": "209532167", "sentence": "CuBERT when fine - tuned on the variable misuse localization and repair task , produces high classification , localization and localization+repair accuracies .", "ner": [["CuBERT", "Method"], ["classification", "Task"], ["localization", "Task"], ["localization+repair", "Task"]], "rel": [["CuBERT", "Used-For", "classification"], ["CuBERT", "Used-For", "localization"], ["CuBERT", "Used-For", "localization+repair"]], "rel_plus": [["CuBERT:Method", "Used-For", "classification:Task"], ["CuBERT:Method", "Used-For", "localization:Task"], ["CuBERT:Method", "Used-For", "localization+repair:Task"]]}
{"doc_id": "209532167", "sentence": "The contributions of this paper are as follows : \u2022 We present the first attempt at pre - training a BERT contextual embedding of source code . \u2022 We show the efficacy of the pre - trained contextual embedding on five classification tasks .", "ner": [["BERT", "Method"], ["classification", "Task"]], "rel": [["BERT", "Used-For", "classification"]], "rel_plus": [["BERT:Method", "Used-For", "classification:Task"]]}
{"doc_id": "209532167", "sentence": "Our results show that the fine - tuned models outperform the baseline LSTM models supported by Word 2 Vec embeddings , and Transformers trained from scratch .", "ner": [["LSTM", "Method"], ["Word 2 Vec", "Method"], ["Transformers", "Method"]], "rel": [["Word 2 Vec", "Used-For", "LSTM"]], "rel_plus": [["Word 2 Vec:Method", "Used-For", "LSTM:Method"]]}
{"doc_id": "209532167", "sentence": "Word 2 Vec ( Mikolov et al. , 2 0 1 3 a , b ) computed word embeddings based on word co - occurrence and proximity , but the same embedding is used regardless of the context .", "ner": [["Word 2 Vec", "Method"], ["word embeddings", "Method"]], "rel": [["Word 2 Vec", "Used-For", "word embeddings"]], "rel_plus": [["Word 2 Vec:Method", "Used-For", "word embeddings:Method"]]}
{"doc_id": "209532167", "sentence": "BERT ( Devlin et al. , 2 0 1 9 ) improved natural - language pre - training by using a denoising autoencoder .", "ner": [["BERT", "Method"], ["denoising autoencoder", "Method"]], "rel": [["denoising autoencoder", "Part-Of", "BERT"]], "rel_plus": [["denoising autoencoder:Method", "Part-Of", "BERT:Method"]]}
{"doc_id": "209532167", "sentence": "These pre - training objectives , along with the use of a Transformer - based architecture , gave BERT an accuracy boost in a number of NLP tasks over the state - of - the - art .", "ner": [["Transformer - based architecture", "Method"], ["BERT", "Method"], ["NLP", "Task"]], "rel": [["Transformer - based architecture", "Used-For", "BERT"], ["BERT", "Used-For", "NLP"]], "rel_plus": [["Transformer - based architecture:Method", "Used-For", "BERT:Method"], ["BERT:Method", "Used-For", "NLP:Task"]]}
{"doc_id": "209532167", "sentence": "Both the BERT model and the Word 2 Vec embeddings are built on the subword vocabulary .", "ner": [["BERT", "Method"], ["Word 2 Vec", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "209532167", "sentence": "To evaluate CuBERT , we design five classification tasks and a multi - headed pointer task .", "ner": [["CuBERT", "Method"], ["classification", "Task"], ["multi - headed pointer", "Task"]], "rel": [["CuBERT", "Used-For", "classification"], ["CuBERT", "Used-For", "multi - headed pointer"]], "rel_plus": [["CuBERT:Method", "Used-For", "classification:Task"], ["CuBERT:Method", "Used-For", "multi - headed pointer:Task"]]}
{"doc_id": "209532167", "sentence": "We take the classification version restated by Vasic et al. ( 2 0 1 9 ) , wherein , given a function , the task is to predict whether there is a variable misuse at some location in the function , without specifying a particular location to consider .", "ner": [["variable misuse", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "209532167", "sentence": "The negative examples are created by randomly replacing some binary operator with another type - compatible operator .   Swapped Operand Pradel & Sen ( 2 0 1 8) propose the wrong binary operand task where a variable or constant is used incorrectly in an expression , but that task is quite similar to the variable misuse task we already use .", "ner": [["binary operator", "Method"], ["type - compatible operator", "Method"], ["variable misuse", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "209532167", "sentence": "This provides parallel corpora between code and natural language sentences that have been used for machine translation between the two ( Barone & Sennrich , 2 0 1 7 ) , detecting uninformative docstrings ( Louis et al. , 2 0 1 8) and to evaluate their utility to provide supervision in neural code search ( Cambronero et al. , 2 0 1 9 ) .", "ner": [["machine translation", "Task"], ["detecting uninformative docstrings", "Task"], ["neural code search", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "209532167", "sentence": "The task is the multi - class classification problem of predicting the original exception type .", "ner": [["multi - class classification", "Task"], ["exception type", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "209532167", "sentence": "As an instance of a non - classification task , we consider the joint classification , localization and repair version of the variable misuse task from Vasic et al. ( 2 0 1 9 ) .", "ner": [["non - classification task", "Task"], ["joint classification", "Task"], ["localization and repair version of the variable misuse", "Task"]], "rel": [["joint classification", "SubTask-Of", "non - classification task"], ["localization and repair version of the variable misuse", "SubTask-Of", "non - classification task"]], "rel_plus": [["joint classification:Task", "SubTask-Of", "non - classification task:Task"], ["localization and repair version of the variable misuse:Task", "SubTask-Of", "non - classification task:Task"]]}
{"doc_id": "209532167", "sentence": "Thus , each example consists of one ( for MLM ) or two ( for NSP ) sentences , where a sentence is the concatenation of contiguous lines from the source corpus , sized to fit the target example length .", "ner": [["MLM", "Task"], ["NSP", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "209532167", "sentence": "To ensure that every sentence is treated in multiple instances of both MLM and NSP , BERT by default duplicates the corpus 1 0 times , and generates independently derived examples from each duplicate .", "ner": [["MLM", "Task"], ["NSP", "Task"], ["BERT", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "209532167", "sentence": "CuBERT is similarly formulated , but a CuBERT sentence is a logical code line , as defined by the Python standard .", "ner": [["CuBERT", "Method"], ["CuBERT", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "209532167", "sentence": "For the pointer prediction task , the pointer is computed over the sequence of outputs generated by the last layer of the BERT model .   We train Word 2 Vec models using the same pre - training corpus as the BERT model .", "ner": [["pointer prediction", "Task"], ["BERT", "Method"], ["Word 2 Vec", "Method"], ["BERT", "Method"]], "rel": [["BERT", "Used-For", "pointer prediction"]], "rel_plus": [["BERT:Method", "Used-For", "pointer prediction:Task"]]}
{"doc_id": "209532167", "sentence": "To maintain parity , we generate the dataset for Word 2 Vec using the same pipeline as BERT but by disabling masking and generation of negative examples for NSP .", "ner": [["Word 2 Vec", "Method"], ["BERT", "Method"], ["NSP", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "209532167", "sentence": "We train both CBOW and Skipgram models using GenSim ( \u0158eh\u016f\u0159ek & Sojka , 2 0 1 0 ) .", "ner": [["CBOW", "Method"], ["Skipgram", "Method"], ["GenSim", "Dataset"]], "rel": [["Skipgram", "Trained-With", "GenSim"], ["CBOW", "Trained-With", "GenSim"]], "rel_plus": [["Skipgram:Method", "Trained-With", "GenSim:Dataset"], ["CBOW:Method", "Trained-With", "GenSim:Dataset"]]}
{"doc_id": "209532167", "sentence": "In order to obtain context - sensitive encodings of input sequences for the fine - tuning tasks , we use multi - layered bidirectional LSTMs ( Hochreiter & Schmidhuber , 1 9 9 7 ) ( BiLSTMs ) .", "ner": [["multi - layered bidirectional LSTMs", "Method"], ["BiLSTMs", "Method"]], "rel": [["BiLSTMs", "Synonym-Of", "multi - layered bidirectional LSTMs"]], "rel_plus": [["BiLSTMs:Method", "Synonym-Of", "multi - layered bidirectional LSTMs:Method"]]}
{"doc_id": "209532167", "sentence": "Additionally , to further evaluate whether LSTMs alone are sufficient without pre - training , we try initializing the BiLSTM with an embedding matrix that is trained from scratch .", "ner": [["LSTMs", "Method"], ["BiLSTM", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "209532167", "sentence": "We used BERT 's own Transformer implementation , to ensure comparability of results .", "ner": [["BERT", "Method"], ["Transformer", "Method"]], "rel": [["Transformer", "Part-Of", "BERT"]], "rel_plus": [["Transformer:Method", "Part-Of", "BERT:Method"]]}
{"doc_id": "209532167", "sentence": "For comparison with prior work , we use the unidirectional LSTM and pointer model from ( Vasic et al. , 2 0 1 9 ) for the Variable Misuse Localization and Repair task .   As stated above , CuBERT 's dataset generation duplicates the corpus 1 0 times , whereas Word 2 Vec is trained without duplication .", "ner": [["unidirectional LSTM", "Method"], ["Variable Misuse Localization and Repair", "Task"], ["CuBERT", "Method"], ["Word 2 Vec", "Method"]], "rel": [["unidirectional LSTM", "Used-For", "Variable Misuse Localization and Repair"], ["CuBERT", "Compare-With", "Word 2 Vec"]], "rel_plus": [["unidirectional LSTM:Method", "Used-For", "Variable Misuse Localization and Repair:Task"], ["CuBERT:Method", "Compare-With", "Word 2 Vec:Method"]]}
{"doc_id": "209532167", "sentence": "To compensate for this difference , we trained Word 2 Vec for 1 0 epochs and CuBERT for 1 epoch .", "ner": [["Word 2 Vec", "Method"], ["CuBERT", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "209532167", "sentence": "We pre - train CuBERT with the default configuration of the BERT Large model .", "ner": [["CuBERT", "Method"], ["BERT", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "209532167", "sentence": "For the baseline BiLSTM models , we did extensive experimentation on the Variable Misuse task by varying the number of layers ( 1 - 3 ) and the number of hidden units ( 1 2 8 , 2 5 6 , 5 1 2 ) .", "ner": [["BiLSTM", "Method"], ["Variable Misuse", "Task"]], "rel": [["BiLSTM", "Used-For", "Variable Misuse"]], "rel_plus": [["BiLSTM:Method", "Used-For", "Variable Misuse:Task"]]}
{"doc_id": "209532167", "sentence": "We also tried LSTM output dropout probability ( 0. 1 , 0. 5 ) , optimizers ( Adam ( Kingma & Ba , 2 0 1 4 ) and AdaGrad ( Duchi et al. , 2 0 1 1 ) ) , and learning rates ( 1e - 3 , 1e - 4 , 1e - 5 ) .", "ner": [["LSTM", "Method"], ["dropout", "Method"], ["Adam", "Method"], ["AdaGrad", "Method"]], "rel": [["dropout", "Part-Of", "LSTM"], ["Adam", "Part-Of", "LSTM"], ["AdaGrad", "Part-Of", "LSTM"]], "rel_plus": [["dropout:Method", "Part-Of", "LSTM:Method"], ["Adam:Method", "Part-Of", "LSTM:Method"], ["AdaGrad:Method", "Part-Of", "LSTM:Method"]]}
{"doc_id": "209532167", "sentence": "The most promising combination was a 3 - layered BiLSTM with 5 1 2 hidden units per layer , LSTM output dropout probability of 0. 1 and Adam optimizer with learning rate of 1e - 3 .", "ner": [["BiLSTM", "Method"], ["LSTM", "Method"], ["dropout", "Method"], ["Adam optimizer", "Method"]], "rel": [["Adam optimizer", "Part-Of", "LSTM"], ["dropout", "Part-Of", "LSTM"]], "rel_plus": [["Adam optimizer:Method", "Part-Of", "LSTM:Method"], ["dropout:Method", "Part-Of", "LSTM:Method"]]}
{"doc_id": "209532167", "sentence": "For the baseline Transformer models , we originally attempted to train a Transformer model of the same configuration as CuBERT .", "ner": [["Transformer", "Method"], ["Transformer", "Method"], ["CuBERT", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "209532167", "sentence": "Instead , we performed a hyperparameter search over transformer layers ( 1 - 6 ) , hidden units ( 1 2 8 , 2 5 6 , 5 1 2 ) , learning rates ( 5e - 5 , 1e - 4 , 5e - 4 , 1e - 3 ) and batch sizes ( 6 4 , 2 5 6 , 1 0 2 4 , 2 0 4 8 , 4 0 9 6 , 8 1 9 2 ) on the Variable Misuse task .", "ner": [["transformer", "Method"], ["Variable Misuse", "Task"]], "rel": [["transformer", "Used-For", "Variable Misuse"]], "rel_plus": [["transformer:Method", "Used-For", "Variable Misuse:Task"]]}
{"doc_id": "209532167", "sentence": "Finally , for our baseline pointer model ( referred to as LSTM+pointer below ) we searched over the following hyperparameter choices : hidden sizes of 5 1 2 and 1 0 2 4 , token embedding sizes of 5 1 2 and 1 0 2 4 , learning rates of 0. 1 , 0.0 1 , and 0.0 0 1 , and the AdaGrad and Gradient Descent optimizers .", "ner": [["LSTM+pointer", "Method"], ["token embedding", "Method"], ["AdaGrad", "Method"], ["Gradient Descent optimizers", "Method"]], "rel": [["token embedding", "Part-Of", "LSTM+pointer"], ["AdaGrad", "Part-Of", "LSTM+pointer"], ["Gradient Descent optimizers", "Part-Of", "LSTM+pointer"]], "rel_plus": [["token embedding:Method", "Part-Of", "LSTM+pointer:Method"], ["AdaGrad:Method", "Part-Of", "LSTM+pointer:Method"], ["Gradient Descent optimizers:Method", "Part-Of", "LSTM+pointer:Method"]]}
{"doc_id": "209532167", "sentence": "We compare CuBERT to BiLSTM models with and without pre - trained Word 2 Vec embeddings on the classification tasks ( Section 4. 3 ) . 2 .", "ner": [["CuBERT", "Method"], ["BiLSTM", "Method"], ["Word 2 Vec", "Method"], ["classification", "Task"]], "rel": [["CuBERT", "Compare-With", "BiLSTM"], ["Word 2 Vec", "Part-Of", "BiLSTM"], ["CuBERT", "Used-For", "classification"], ["BiLSTM", "Used-For", "classification"]], "rel_plus": [["CuBERT:Method", "Compare-With", "BiLSTM:Method"], ["Word 2 Vec:Method", "Part-Of", "BiLSTM:Method"], ["CuBERT:Method", "Used-For", "classification:Task"], ["BiLSTM:Method", "Used-For", "classification:Task"]]}
{"doc_id": "209532167", "sentence": "Does fine - tuning actually help , or is the Transformer model behind CuBERT the main power behind the approach ?", "ner": [["Transformer", "Method"], ["CuBERT", "Method"]], "rel": [["Transformer", "Part-Of", "CuBERT"]], "rel_plus": [["Transformer:Method", "Part-Of", "CuBERT:Method"]]}
{"doc_id": "209532167", "sentence": "We compare fine - tuned CuBERT models to Transformer - based models trained from scratch on the classification tasks ( Section 4. 4 ) .", "ner": [["CuBERT", "Method"], ["Transformer - based models", "Method"], ["classification", "Task"]], "rel": [["CuBERT", "Compare-With", "Transformer - based models"], ["CuBERT", "Used-For", "classification"], ["Transformer - based models", "Used-For", "classification"]], "rel_plus": [["CuBERT:Method", "Compare-With", "Transformer - based models:Method"], ["CuBERT:Method", "Used-For", "classification:Task"], ["Transformer - based models:Method", "Used-For", "classification:Task"]]}
{"doc_id": "209532167", "sentence": "We implemented and fine - tuned a model for a multi - headed pointer prediction task , namely , the Variable - Misuse Localization and Repair task ( Section 4. 7 ) .", "ner": [["multi - headed pointer prediction", "Task"], ["Variable - Misuse Localization and Repair task", "Task"]], "rel": [["Variable - Misuse Localization and Repair task", "SubTask-Of", "multi - headed pointer prediction"]], "rel_plus": [["Variable - Misuse Localization and Repair task:Task", "SubTask-Of", "multi - headed pointer prediction:Task"]]}
{"doc_id": "209532167", "sentence": "For each classification task , we trained BiLSTM models starting with each of our baseline Word 2 Vec embeddings , namely , continuous bag of words ( CBOW ) and Skipgram trained with negative sampling or hierarchical softmax .", "ner": [["classification", "Task"], ["BiLSTM", "Method"], ["Word 2 Vec", "Method"], ["continuous bag of words", "Method"], ["CBOW", "Method"], ["Skipgram", "Method"], ["negative sampling", "Method"], ["hierarchical softmax", "Method"]], "rel": [["BiLSTM", "Used-For", "classification"], ["Word 2 Vec", "Part-Of", "BiLSTM"], ["continuous bag of words", "Part-Of", "BiLSTM"], ["Skipgram", "Part-Of", "BiLSTM"], ["continuous bag of words", "SubClass-Of", "Word 2 Vec"], ["Skipgram", "SubClass-Of", "Word 2 Vec"], ["CBOW", "Synonym-Of", "continuous bag of words"], ["negative sampling", "Part-Of", "Skipgram"], ["hierarchical softmax", "Part-Of", "Skipgram"]], "rel_plus": [["BiLSTM:Method", "Used-For", "classification:Task"], ["Word 2 Vec:Method", "Part-Of", "BiLSTM:Method"], ["continuous bag of words:Method", "Part-Of", "BiLSTM:Method"], ["Skipgram:Method", "Part-Of", "BiLSTM:Method"], ["continuous bag of words:Method", "SubClass-Of", "Word 2 Vec:Method"], ["Skipgram:Method", "SubClass-Of", "Word 2 Vec:Method"], ["CBOW:Method", "Synonym-Of", "continuous bag of words:Method"], ["negative sampling:Method", "Part-Of", "Skipgram:Method"], ["hierarchical softmax:Method", "Part-Of", "Skipgram:Method"]]}
{"doc_id": "209532167", "sentence": "On the CuBERT side , we fine - tuned the pre - trained model for 2 0 epochs , with similar model selection .", "ner": [["CuBERT", "Method"], ["model selection", "Method"]], "rel": [["model selection", "Used-For", "CuBERT"]], "rel_plus": [["model selection:Method", "Used-For", "CuBERT:Method"]]}
{"doc_id": "209532167", "sentence": "CuBERT consistently outperforms BiLSTM ( with the best task - wise Word 2 Vec configuration ) on all tasks , by a margin of 2. 9 - 2 2 % .", "ner": [["CuBERT", "Method"], ["BiLSTM", "Method"], ["Word 2 Vec", "Method"]], "rel": [["CuBERT", "Compare-With", "BiLSTM"], ["Word 2 Vec", "Part-Of", "BiLSTM"]], "rel_plus": [["CuBERT:Method", "Compare-With", "BiLSTM:Method"], ["Word 2 Vec:Method", "Part-Of", "BiLSTM:Method"]]}
{"doc_id": "209532167", "sentence": "Thus , the pre - trained contextual embedding provides superior results even with a smaller budget of Table 2 : Test accuracies of fine - tuned CuBERT against BiLSTM ( with and without Word 2 Vec embeddings ) and Transformer trained from scratch on the classification tasks . \" ns \" and \" hs \" respectively refer to negative sampling and hierarchical softmax settings used for training CBOW and Skipgram models . \" From scratch \" refers to training with freshly initialized token embeddings , that is , without pre - trained Word 2 Vec embeddings . 2 0 epochs , compared to the 1 0 0 epochs used for BiLSTMs .", "ner": [["CuBERT", "Method"], ["BiLSTM", "Method"], ["Word 2 Vec", "Method"], ["Transformer", "Method"], ["classification", "Task"], ["ns", "Method"], ["hs", "Method"], ["negative sampling", "Method"], ["hierarchical softmax", "Method"], ["CBOW", "Method"], ["Skipgram", "Method"], ["Word 2 Vec", "Method"], ["BiLSTMs", "Method"]], "rel": [["Word 2 Vec", "Part-Of", "BiLSTM"], ["CuBERT", "Compare-With", "BiLSTM"], ["CuBERT", "Compare-With", "Transformer"], ["CuBERT", "Used-For", "classification"], ["BiLSTM", "Used-For", "classification"], ["Transformer", "Used-For", "classification"], ["ns", "Synonym-Of", "negative sampling"], ["hs", "Synonym-Of", "hierarchical softmax"], ["negative sampling", "Part-Of", "CBOW"], ["hierarchical softmax", "Part-Of", "Skipgram"], ["hierarchical softmax", "Part-Of", "Skipgram"], ["negative sampling", "Part-Of", "Skipgram"]], "rel_plus": [["Word 2 Vec:Method", "Part-Of", "BiLSTM:Method"], ["CuBERT:Method", "Compare-With", "BiLSTM:Method"], ["CuBERT:Method", "Compare-With", "Transformer:Method"], ["CuBERT:Method", "Used-For", "classification:Task"], ["BiLSTM:Method", "Used-For", "classification:Task"], ["Transformer:Method", "Used-For", "classification:Task"], ["ns:Method", "Synonym-Of", "negative sampling:Method"], ["hs:Method", "Synonym-Of", "hierarchical softmax:Method"], ["negative sampling:Method", "Part-Of", "CBOW:Method"], ["hierarchical softmax:Method", "Part-Of", "Skipgram:Method"], ["hierarchical softmax:Method", "Part-Of", "Skipgram:Method"], ["negative sampling:Method", "Part-Of", "Skipgram:Method"]]}
{"doc_id": "209532167", "sentence": "The difference between the performance of BiLSTM and CuBERT is the highest for this task .", "ner": [["BiLSTM", "Method"], ["CuBERT", "Method"]], "rel": [["BiLSTM", "Compare-With", "CuBERT"]], "rel_plus": [["BiLSTM:Method", "Compare-With", "CuBERT:Method"]]}
{"doc_id": "209532167", "sentence": "Except for the Operand task , CuBERT outperforms BiLSTM within 2 finetuning epochs .", "ner": [["CuBERT", "Method"], ["BiLSTM", "Method"]], "rel": [["CuBERT", "Compare-With", "BiLSTM"]], "rel_plus": [["CuBERT:Method", "Compare-With", "BiLSTM:Method"]]}
{"doc_id": "209532167", "sentence": "On the Operand task , the performance difference between CuBERT with 2 or 1 0 fine - tuning epochs and BiLSTM is about 1% .", "ner": [["CuBERT", "Method"], ["BiLSTM", "Method"]], "rel": [["CuBERT", "Compare-With", "BiLSTM"]], "rel_plus": [["CuBERT:Method", "Compare-With", "BiLSTM:Method"]]}
{"doc_id": "209532167", "sentence": "For the rest of the tasks , CuBERT with only 2 finetuning epochs outperforms BiLSTM ( with the best task - wise Word 2 Vec configuration ) by a margin of 0. 7 - 1 2 % .", "ner": [["CuBERT", "Method"], ["BiLSTM", "Method"], ["Word 2 Vec", "Method"]], "rel": [["CuBERT", "Compare-With", "BiLSTM"], ["Word 2 Vec", "Part-Of", "BiLSTM"]], "rel_plus": [["CuBERT:Method", "Compare-With", "BiLSTM:Method"], ["Word 2 Vec:Method", "Part-Of", "BiLSTM:Method"]]}
{"doc_id": "209532167", "sentence": "This shows that CuBERT can reach accuracies that are comparable to or better than those of BiLSTMs trained with Word 2 Vec embeddings within only a few epochs .", "ner": [["CuBERT", "Method"], ["BiLSTMs", "Method"], ["Word 2 Vec", "Method"]], "rel": [["Word 2 Vec", "Part-Of", "BiLSTMs"], ["CuBERT", "Compare-With", "BiLSTMs"]], "rel_plus": [["Word 2 Vec:Method", "Part-Of", "BiLSTMs:Method"], ["CuBERT:Method", "Compare-With", "BiLSTMs:Method"]]}
{"doc_id": "209532167", "sentence": "We also trained the BiLSTM models from scratch , that is , without using the Word 2 Vec embeddings .", "ner": [["BiLSTM", "Method"], ["Word 2 Vec", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "209532167", "sentence": "Though no single Word 2 Vec configuration is the best , CBOW trained with negative sampling gives the most consistent results overall .", "ner": [["Word 2 Vec", "Method"], ["CBOW", "Method"], ["negative sampling", "Method"]], "rel": [["negative sampling", "Part-Of", "CBOW"]], "rel_plus": [["negative sampling:Method", "Part-Of", "CBOW:Method"]]}
{"doc_id": "209532167", "sentence": "One may wonder if CuBERT 's promising results derive more from using a Transformer - based model for its classification tasks , and less from the actual , unsupervised pre - training .", "ner": [["CuBERT", "Method"], ["Transformer", "Method"], ["classification", "Task"], ["unsupervised pre - training", "Task"]], "rel": [["CuBERT", "Compare-With", "Transformer"], ["CuBERT", "Used-For", "classification"], ["Transformer", "Used-For", "classification"]], "rel_plus": [["CuBERT:Method", "Compare-With", "Transformer:Method"], ["CuBERT:Method", "Used-For", "classification:Task"], ["Transformer:Method", "Used-For", "classification:Task"]]}
{"doc_id": "209532167", "sentence": "Here we compare our results on the classification tasks to a Transformer - based model trained from scratch , i.e. , without the benefit of a pre - trained embedding .", "ner": [["classification", "Task"], ["Transformer", "Method"]], "rel": [["Transformer", "Used-For", "classification"]], "rel_plus": [["Transformer:Method", "Used-For", "classification:Task"]]}
{"doc_id": "209532167", "sentence": "As seen from the last row of Table 2 , the performance of CuBERT is substantially higher than the Transformer models trained from scratch .", "ner": [["CuBERT", "Method"], ["Transformer", "Method"]], "rel": [["CuBERT", "Compare-With", "Transformer"]], "rel_plus": [["CuBERT:Method", "Compare-With", "Transformer:Method"]]}
{"doc_id": "209532167", "sentence": "The Function Docstring task seems robust to the reduction of the training dataset , both early and late in the fine - tuning process ( that is , within 2 vs. 2 0 epochs ) , whereas the Exception Classification task is heavily impacted by the dataset reduction , given that it has relatively few training examples to begin with .", "ner": [["Function Docstring", "Task"], ["Exception Classification", "Task"]], "rel": [["Function Docstring", "Compare-With", "Exception Classification"]], "rel_plus": [["Function Docstring:Task", "Compare-With", "Exception Classification:Task"]]}
{"doc_id": "209532167", "sentence": "For example , for both Variable Misuse and Function Docstring , CuBERT at 2 epochs and 1/ 3 rd training data outperforms the BiLSTM with Word 2 Vec and the Transformer baselines .", "ner": [["Variable Misuse", "Task"], ["Function Docstring", "Task"], ["CuBERT", "Method"], ["BiLSTM", "Method"], ["Word 2 Vec", "Method"], ["Transformer", "Method"]], "rel": [["CuBERT", "Used-For", "Variable Misuse"], ["CuBERT", "Used-For", "Function Docstring"], ["CuBERT", "Compare-With", "BiLSTM"], ["CuBERT", "Compare-With", "Word 2 Vec"], ["CuBERT", "Compare-With", "Transformer"]], "rel_plus": [["CuBERT:Method", "Used-For", "Variable Misuse:Task"], ["CuBERT:Method", "Used-For", "Function Docstring:Task"], ["CuBERT:Method", "Compare-With", "BiLSTM:Method"], ["CuBERT:Method", "Compare-With", "Word 2 Vec:Method"], ["CuBERT:Method", "Compare-With", "Transformer:Method"]]}
{"doc_id": "209532167", "sentence": "For comparison , we also evaluated the BiLSTM model on sequences of length 1 2 8 and 2 5 6 for the Variable Misuse task .", "ner": [["BiLSTM", "Method"], ["Variable Misuse", "Task"]], "rel": [["BiLSTM", "Used-For", "Variable Misuse"]], "rel_plus": [["BiLSTM:Method", "Used-For", "Variable Misuse:Task"]]}
{"doc_id": "209532167", "sentence": "We obtained accuracies of 7 1 . 3 4 % and 7 3 . 6 3 % respectively , which are lower than the best BiLSTM accuracy on sequence length 5 1 2 and also lower than the accuracies of CuBERT for the corresponding lengths ( see Table 4 ) .", "ner": [["BiLSTM", "Method"], ["CuBERT", "Method"]], "rel": [["BiLSTM", "Compare-With", "CuBERT"]], "rel_plus": [["BiLSTM:Method", "Compare-With", "CuBERT:Method"]]}
{"doc_id": "209532167", "sentence": "We now discuss the results of fine - tuning CuBERT to predict the localization and repair pointers for the variable misuse task .", "ner": [["CuBERT", "Method"], ["variable misuse", "Task"]], "rel": [["CuBERT", "Used-For", "variable misuse"]], "rel_plus": [["CuBERT:Method", "Used-For", "variable misuse:Task"]]}
{"doc_id": "209532167", "sentence": "We refer to these respectively as CuBERT+pointer and LSTM+pointer models , respectively .", "ner": [["CuBERT+pointer", "Method"], ["LSTM+pointer", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "209532167", "sentence": "As seen from Table 5 , the CuBERT+pointer model outperforms the LSTM+pointer model consistently across all the metrics , and even within 2 and 1 0 epochs .", "ner": [["CuBERT+pointer", "Method"], ["LSTM+pointer", "Method"]], "rel": [["CuBERT+pointer", "Compare-With", "LSTM+pointer"]], "rel_plus": [["CuBERT+pointer:Method", "Compare-With", "LSTM+pointer:Method"]]}
{"doc_id": "209532167", "sentence": "We present the first attempt at pre - trained contextual embedding of source code by training a BERT model , called CuBERT , which we fine - tuned on five classification tasks and compared against BiL - STM with Word 2 Vec embeddings and Transformer models .", "ner": [["BERT", "Method"], ["CuBERT", "Method"], ["classification", "Task"], ["BiL - STM", "Method"], ["Word 2 Vec", "Method"], ["Transformer", "Method"]], "rel": [["CuBERT", "SubClass-Of", "BERT"], ["CuBERT", "Used-For", "classification"], ["CuBERT", "Compare-With", "BiL - STM"], ["CuBERT", "Compare-With", "Word 2 Vec"], ["CuBERT", "Compare-With", "Transformer"]], "rel_plus": [["CuBERT:Method", "SubClass-Of", "BERT:Method"], ["CuBERT:Method", "Used-For", "classification:Task"], ["CuBERT:Method", "Compare-With", "BiL - STM:Method"], ["CuBERT:Method", "Compare-With", "Word 2 Vec:Method"], ["CuBERT:Method", "Compare-With", "Transformer:Method"]]}
{"doc_id": "209532167", "sentence": "As a more challenging task , we also evaluated CuBERT on a multi - headed pointer prediction task .", "ner": [["CuBERT", "Method"], ["multi - headed pointer prediction", "Task"]], "rel": [["CuBERT", "Used-For", "multi - headed pointer prediction"]], "rel_plus": [["CuBERT:Method", "Used-For", "multi - headed pointer prediction:Task"]]}
{"doc_id": "209532167", "sentence": "Once a location is chosen , we replace it with a special HOLE token , and create a classification example that pairs the function ( with the masked exception location ) with the true label ( the removed exception type ) .", "ner": [["classification", "Task"], ["exception type", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "209532167", "sentence": "The CuBERT variable - misuse model correctly predicts that the code has an error .", "ner": [["CuBERT variable - misuse", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "208513596", "sentence": "Based on this intuition , we propose Cross - Modal Deep Clustering ( XDC ) , a novel self - supervised method that leverages unsupervised clustering in one modality ( e.g. , audio ) as a supervisory signal for the other modality ( e.g. , video ) .", "ner": [["Cross - Modal Deep Clustering", "Method"], ["XDC", "Method"]], "rel": [["XDC", "Synonym-Of", "Cross - Modal Deep Clustering"]], "rel_plus": [["XDC:Method", "Synonym-Of", "Cross - Modal Deep Clustering:Method"]]}
{"doc_id": "208513596", "sentence": "Most importantly , our video model pretrained on large - scale unlabeled data significantly outperforms the same model pretrained with full - supervision on ImageNet and Kinetics for action recognition on HMDB 5 1 and UCF 1 0 1 .", "ner": [["ImageNet", "Dataset"], ["Kinetics", "Dataset"], ["action recognition", "Task"], ["HMDB 5 1", "Dataset"], ["UCF 1 0 1", "Dataset"]], "rel": [["HMDB 5 1", "Benchmark-For", "action recognition"], ["UCF 1 0 1", "Benchmark-For", "action recognition"], ["Kinetics", "Used-For", "action recognition"], ["ImageNet", "Used-For", "action recognition"]], "rel_plus": [["HMDB 5 1:Dataset", "Benchmark-For", "action recognition:Task"], ["UCF 1 0 1:Dataset", "Benchmark-For", "action recognition:Task"], ["Kinetics:Dataset", "Used-For", "action recognition:Task"], ["ImageNet:Dataset", "Used-For", "action recognition:Task"]]}
{"doc_id": "208513596", "sentence": "To the best of our knowledge , XDC is the first self - supervised learning method that outperforms large - scale fully - supervised pretraining for action recognition on the same architecture .", "ner": [["XDC", "Method"], ["action recognition", "Task"]], "rel": [["XDC", "Used-For", "action recognition"]], "rel_plus": [["XDC:Method", "Used-For", "action recognition:Task"]]}
{"doc_id": "208513596", "sentence": "The recent creation of large - scale datasets for action recognition [ 4 , 2 6 , 2 7 ] has undoubtedly enabled a major leap forward in the accuracy of models for video understanding .", "ner": [["action recognition", "Task"], ["video understanding", "Task"]], "rel": [["action recognition", "Used-For", "video understanding"]], "rel_plus": [["action recognition:Task", "Used-For", "video understanding:Task"]]}
{"doc_id": "208513596", "sentence": "This suggests that the definition of the \" right \" label space for action recognition , and more generally for video understanding , is still very much up for debate .", "ner": [["action recognition", "Task"], ["video understanding", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "208513596", "sentence": "In two of our approachesMulti - Head Deep Clustering ( MDC ) and Concatenation Deep Clustering (CDC) - the pseudo - labels from the second modality are supplementary , i.e. , they complement the pseudo - labels generated on the first modality .", "ner": [["approachesMulti - Head Deep Clustering", "Method"], ["MDC", "Method"], ["Concatenation Deep Clustering", "Method"], ["(CDC)", "Method"]], "rel": [["MDC", "Synonym-Of", "approachesMulti - Head Deep Clustering"], ["(CDC)", "Synonym-Of", "Concatenation Deep Clustering"]], "rel_plus": [["MDC:Method", "Synonym-Of", "approachesMulti - Head Deep Clustering:Method"], ["(CDC):Method", "Synonym-Of", "Concatenation Deep Clustering:Method"]]}
{"doc_id": "208513596", "sentence": "The third approach - Cross - Modal Deep Clustering (XDC) - instead uses the pseudo - labels from the other modality as exclusive supervisory signal .", "ner": [["Cross - Modal Deep Clustering", "Method"], ["(XDC)", "Method"]], "rel": [["(XDC)", "Synonym-Of", "Cross - Modal Deep Clustering"]], "rel_plus": [["(XDC):Method", "Synonym-Of", "Cross - Modal Deep Clustering:Method"]]}
{"doc_id": "208513596", "sentence": "Our experiments provide support for several interesting conclusions : \u2022 All three of our cross - modal deep clustering methods yield audio and video representations that generalize better to the downstream tasks of action recognition and audio classification , compared to their withinmodality counterparts . \u2022 XDC ( i.e. , the cross - modal deep clustering relying on the other modality as an exclusive supervisory signal ) outperforms all the other approaches .", "ner": [["cross - modal deep clustering", "Method"], ["action recognition", "Task"], ["audio classification", "Task"], ["XDC", "Method"], ["cross - modal deep clustering", "Method"]], "rel": [["cross - modal deep clustering", "Used-For", "action recognition"], ["cross - modal deep clustering", "Used-For", "audio classification"], ["XDC", "Synonym-Of", "cross - modal deep clustering"]], "rel_plus": [["cross - modal deep clustering:Method", "Used-For", "action recognition:Task"], ["cross - modal deep clustering:Method", "Used-For", "audio classification:Task"], ["XDC:Method", "Synonym-Of", "cross - modal deep clustering:Method"]]}
{"doc_id": "208513596", "sentence": "This underscores the complementarity of audio and video and the benefits of learning label - spaces across modalities . \u2022 We demonstrate that self - supervised cross - modal learning with XDC on a large - scale video dataset yields an action recognition model that achieves higher accuracy when finetuned on HMDB 5 1 or UCF 1 0 1 , compared to that produced by fully - supervised pretraining on Kinetics .", "ner": [["XDC", "Method"], ["action recognition", "Task"], ["HMDB 5 1", "Dataset"], ["UCF 1 0 1", "Dataset"], ["Kinetics", "Dataset"]], "rel": [["XDC", "Used-For", "action recognition"], ["HMDB 5 1", "Benchmark-For", "action recognition"], ["UCF 1 0 1", "Benchmark-For", "action recognition"]], "rel_plus": [["XDC:Method", "Used-For", "action recognition:Task"], ["HMDB 5 1:Dataset", "Benchmark-For", "action recognition:Task"], ["UCF 1 0 1:Dataset", "Benchmark-For", "action recognition:Task"]]}
{"doc_id": "208513596", "sentence": "Pioneering work in unsupervised learning of video models includes deep belief networks [ 2 1 ] , autoencoders [ 2 2 , 6 3 ] , shiftinvariant decoders [ 5 2 ] , sparse coding algorithms [ 3 3 ] , and stacked ISAs [ 3 2 ] .", "ner": [["deep belief networks", "Method"], ["autoencoders", "Method"], ["shiftinvariant decoders", "Method"], ["sparse coding algorithms", "Method"], ["stacked ISAs", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "208513596", "sentence": "Other selfsupervised tasks include colorization [ 7 4 ] , inpainting [ 4 7 ] , motion segmentation [ 4 6 ] , and instance counting [ 4 2 ] .", "ner": [["colorization", "Task"], ["inpainting", "Task"], ["motion segmentation", "Task"], ["instance counting", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "208513596", "sentence": "Other pretext tasks for video modeling include temporal frame ordering [ 9 , 3 4 , 3 8 , 7 2 ] , establishing region or object correspondences across frames [ 2 3 , 2 4 , 6 9 , 7 0 ] , predicting flow [ 3 1 ] or colors [ 6 6 ] , as well as tracking [ 7 1 ] .", "ner": [["temporal frame ordering", "Task"], ["region or object correspondences across frames", "Task"], ["predicting flow", "Task"], ["tracking", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "208513596", "sentence": "Other approaches learn from unlabeled multi - modal data for a specific target task , e.g. sound source localization [ 7 5 ] and audio - visual co - segmentation [ 5 4 ] .", "ner": [["sound source localization", "Task"], ["audio - visual co - segmentation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "208513596", "sentence": "Previous cross - modal self - supervised representation learning methods most relevant to our work include audio - visual correspondence [ 1 ] , audio - visual temporal synchronization [ 2 9 , 4 4 ] , and learning image representations using ambient sound [ 4 5 ] .", "ner": [["audio - visual correspondence", "Method"], ["audio - visual temporal synchronization", "Method"], ["learning image representations using ambient sound", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "208513596", "sentence": "DeepCluster uses a 2D CNN ( e.g. ResNet - 5 0 ) as its image encoder E and clusters the features after each epoch using k - means .", "ner": [["DeepCluster", "Method"], ["2D CNN", "Method"], ["ResNet - 5 0", "Method"], ["k - means", "Method"]], "rel": [["k - means", "Part-Of", "DeepCluster"], ["2D CNN", "Part-Of", "DeepCluster"], ["ResNet - 5 0", "Synonym-Of", "2D CNN"]], "rel_plus": [["k - means:Method", "Part-Of", "DeepCluster:Method"], ["2D CNN:Method", "Part-Of", "DeepCluster:Method"], ["ResNet - 5 0:Method", "Synonym-Of", "2D CNN:Method"]]}
{"doc_id": "208513596", "sentence": "We refer to the DeepCluster baseline in our paper as SingleModality Deep Clustering ( SDC ) .", "ner": [["DeepCluster", "Method"], ["SingleModality Deep Clustering", "Method"], ["SDC", "Method"]], "rel": [["SingleModality Deep Clustering", "SubClass-Of", "DeepCluster"], ["SDC", "Synonym-Of", "SingleModality Deep Clustering"]], "rel_plus": [["SingleModality Deep Clustering:Method", "SubClass-Of", "DeepCluster:Method"], ["SDC:Method", "Synonym-Of", "SingleModality Deep Clustering:Method"]]}
{"doc_id": "208513596", "sentence": "We describe three approaches ( MDC , CDC , and XDC ) by detailing the steps taken at each deep clustering iteration .", "ner": [["MDC", "Method"], ["CDC", "Method"], ["XDC", "Method"], ["deep clustering", "Task"]], "rel": [["XDC", "Used-For", "deep clustering"], ["CDC", "Used-For", "deep clustering"], ["MDC", "Used-For", "deep clustering"]], "rel_plus": [["XDC:Method", "Used-For", "deep clustering:Task"], ["CDC:Method", "Used-For", "deep clustering:Task"], ["MDC:Method", "Used-For", "deep clustering:Task"]]}
{"doc_id": "208513596", "sentence": "Multi - Head Deep Clustering ( MDC ) .", "ner": [["Multi - Head Deep Clustering", "Method"], ["MDC", "Method"]], "rel": [["MDC", "Synonym-Of", "Multi - Head Deep Clustering"]], "rel_plus": [["MDC:Method", "Synonym-Of", "Multi - Head Deep Clustering:Method"]]}
{"doc_id": "208513596", "sentence": "At each deep clustering iteration , MDC uses the cluster assignments of F v as pseudo - labels for one head and that of F a as pseudo - labels for the other head .", "ner": [["deep clustering", "Method"], ["MDC", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "208513596", "sentence": "Concatenation Deep Clustering ( CDC ) .", "ner": [["Concatenation Deep Clustering", "Method"], ["CDC", "Method"]], "rel": [["CDC", "Synonym-Of", "Concatenation Deep Clustering"]], "rel_plus": [["CDC:Method", "Synonym-Of", "Concatenation Deep Clustering:Method"]]}
{"doc_id": "208513596", "sentence": "Specifically , at each deep clustering iteration , CDC clusters vectors obtained by concatenating the visual and audio feature vectors , separately l 2 -normalized .", "ner": [["deep clustering", "Task"], ["CDC", "Method"], ["l 2 -normalized", "Method"]], "rel": [["CDC", "Used-For", "deep clustering"]], "rel_plus": [["CDC:Method", "Used-For", "deep clustering:Task"]]}
{"doc_id": "208513596", "sentence": "Cross - Modal Deep Clustering ( XDC ) .", "ner": [["Cross - Modal Deep Clustering", "Method"], ["XDC", "Method"]], "rel": [["XDC", "Synonym-Of", "Cross - Modal Deep Clustering"]], "rel_plus": [["XDC:Method", "Synonym-Of", "Cross - Modal Deep Clustering:Method"]]}
{"doc_id": "208513596", "sentence": "At each deep clustering iteration , XDC clusters the audio deep features , F a , and uses their cluster assignments as pseudo - labels to train the visual encoder , E v .", "ner": [["deep clustering", "Task"], ["XDC", "Method"]], "rel": [["XDC", "Used-For", "deep clustering"]], "rel_plus": [["XDC:Method", "Used-For", "deep clustering:Task"]]}
{"doc_id": "208513596", "sentence": "We use three large video datasets for our self - supervised pretraining : Kinetics [ 2 7 ] , AudioSet [ 1 0 ] , and IG 6 5 M [ 1 2 ] .", "ner": [["self - supervised pretraining", "Method"], ["Kinetics", "Dataset"], ["AudioSet", "Dataset"], ["IG 6 5 M", "Dataset"]], "rel": [["Kinetics", "Evaluated-With", "self - supervised pretraining"], ["AudioSet", "Evaluated-With", "self - supervised pretraining"], ["IG 6 5 M", "Evaluated-With", "self - supervised pretraining"]], "rel_plus": [["Kinetics:Dataset", "Evaluated-With", "self - supervised pretraining:Method"], ["AudioSet:Dataset", "Evaluated-With", "self - supervised pretraining:Method"], ["IG 6 5 M:Dataset", "Evaluated-With", "self - supervised pretraining:Method"]]}
{"doc_id": "208513596", "sentence": "While Kinetics and AudioSet are supervised benchmarks for action recognition and audio classification , IG 6 5 M is a large - scale weaklysupervised dataset collected from a social media website .", "ner": [["Kinetics", "Dataset"], ["AudioSet", "Dataset"], ["action recognition", "Task"], ["audio classification", "Task"], ["IG 6 5 M", "Dataset"]], "rel": [["AudioSet", "Benchmark-For", "action recognition"], ["Kinetics", "Benchmark-For", "action recognition"], ["AudioSet", "Benchmark-For", "audio classification"], ["Kinetics", "Benchmark-For", "audio classification"]], "rel_plus": [["AudioSet:Dataset", "Benchmark-For", "action recognition:Task"], ["Kinetics:Dataset", "Benchmark-For", "action recognition:Task"], ["AudioSet:Dataset", "Benchmark-For", "audio classification:Task"], ["Kinetics:Dataset", "Benchmark-For", "audio classification:Task"]]}
{"doc_id": "208513596", "sentence": "The videos in Kinetics and AudioSet are about 1 0 - second long , while the ones from IG 6 5 M last from 1 to 6 0 seconds .", "ner": [["Kinetics", "Dataset"], ["AudioSet", "Dataset"], ["IG 6 5 M", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "208513596", "sentence": "These training sets include 2 4 0 K , 2M , and 6 5 M videos for Kinetics , AudioSet , and IG 6 5 M , respectively .", "ner": [["Kinetics", "Dataset"], ["AudioSet", "Dataset"], ["IG 6 5 M", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "208513596", "sentence": "Furthermore , we randomly sample 2 4 0 K videos from AudioSet and denote this subset as AudioSet - 2 4 0 K .", "ner": [["AudioSet", "Dataset"], ["AudioSet - 2 4 0 K", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "208513596", "sentence": "We generate this subset to have an AudioSet data of the same size as Kinetics , in order to study the effects of pretraining with the same data size but on a different data distribution and domain .", "ner": [["AudioSet", "Dataset"], ["Kinetics", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "208513596", "sentence": "We evaluate our pretraining performance on three downstream benchmarks , UCF 1 0 1 [ 5 8 ] , HMBD 5 1 [ 3 0 ] , and ESC 5 0 [ 4 9 ] .", "ner": [["UCF 1 0 1", "Dataset"], ["HMBD 5 1", "Dataset"], ["ESC 5 0", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "208513596", "sentence": "UCF 1 0 1 and HMDB 5 1 are video benchmarks for human action recognition , while ESC 5 0 is an environmental sound classification dataset .", "ner": [["UCF 1 0 1", "Dataset"], ["HMDB 5 1", "Dataset"], ["human action recognition", "Task"], ["ESC 5 0", "Dataset"], ["environmental sound classification", "Task"]], "rel": [["HMDB 5 1", "Benchmark-For", "human action recognition"], ["UCF 1 0 1", "Benchmark-For", "human action recognition"], ["ESC 5 0", "Benchmark-For", "environmental sound classification"]], "rel_plus": [["HMDB 5 1:Dataset", "Benchmark-For", "human action recognition:Task"], ["UCF 1 0 1:Dataset", "Benchmark-For", "human action recognition:Task"], ["ESC 5 0:Dataset", "Benchmark-For", "environmental sound classification:Task"]]}
{"doc_id": "208513596", "sentence": "UCF 1 0 1 contains about 1 3 K videos from 1 0 1 human action classes , and HMDB 5 1 consists of 7K clips spanning 5 1 different human activities .", "ner": [["UCF 1 0 1", "Dataset"], ["HMDB 5 1", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "208513596", "sentence": "UCF 1 0 1 and HMDB 5 1 have 3 official train/test splits , while ESC 5 0 has 5 splits .", "ner": [["UCF 1 0 1", "Dataset"], ["HMDB 5 1", "Dataset"], ["ESC 5 0", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "208513596", "sentence": "We employ R( 2 + 1 )D [ 6 2 ] and ResNet [ 1 9 ]   the RGB channels , L is the number of frames , and H and W are the frame height and width .", "ner": [["R( 2 + 1 )D", "Method"], ["ResNet", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "208513596", "sentence": "We choose the 1 8 - layer variants of R( 2 + 1 )D and ResNet encoders .", "ner": [["R( 2 + 1 )D", "Method"], ["ResNet", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "208513596", "sentence": "The first study compares between the single - modality deep clustering baseline and the three multi - modal deep clustering models proposed in Section 3 .", "ner": [["single - modality deep clustering", "Task"], ["multi - modal deep clustering", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "208513596", "sentence": "We pretrain SDC , MDC , CDC , and XDC on Kinetics and show their performance on the downstream tasks in Table 1 .", "ner": [["SDC", "Method"], ["MDC", "Method"], ["CDC", "Method"], ["XDC", "Method"], ["Kinetics", "Dataset"]], "rel": [["XDC", "Trained-With", "Kinetics"], ["CDC", "Trained-With", "Kinetics"], ["MDC", "Trained-With", "Kinetics"], ["SDC", "Trained-With", "Kinetics"]], "rel_plus": [["XDC:Method", "Trained-With", "Kinetics:Dataset"], ["CDC:Method", "Trained-With", "Kinetics:Dataset"], ["MDC:Method", "Trained-With", "Kinetics:Dataset"], ["SDC:Method", "Trained-With", "Kinetics:Dataset"]]}
{"doc_id": "208513596", "sentence": "This shows that our self - supervised pretraining is effective and generalizes well to multiple tasks . ( II ) All multimodal models ( MDC , CDC , and XDC ) significantly outperform SDC by up to 1 2 . 4 % , 7. 6 % , and 1 1 . 5 % on UCF 1 0 1 , HMDB 5 1 , and ESC 5 0 , respectively .", "ner": [["self - supervised pretraining", "Method"], ["MDC", "Method"], ["CDC", "Method"], ["XDC", "Method"], ["SDC", "Method"], ["UCF 1 0 1", "Dataset"], ["HMDB 5 1", "Dataset"], ["ESC 5 0", "Dataset"]], "rel": [["XDC", "Compare-With", "SDC"], ["CDC", "Compare-With", "SDC"], ["MDC", "Compare-With", "SDC"], ["MDC", "Evaluated-With", "UCF 1 0 1"], ["CDC", "Evaluated-With", "UCF 1 0 1"], ["XDC", "Evaluated-With", "UCF 1 0 1"], ["SDC", "Evaluated-With", "UCF 1 0 1"], ["MDC", "Evaluated-With", "HMDB 5 1"], ["CDC", "Evaluated-With", "HMDB 5 1"], ["XDC", "Evaluated-With", "HMDB 5 1"], ["SDC", "Evaluated-With", "HMDB 5 1"], ["SDC", "Evaluated-With", "ESC 5 0"], ["XDC", "Evaluated-With", "ESC 5 0"], ["CDC", "Evaluated-With", "ESC 5 0"], ["CDC", "Evaluated-With", "ESC 5 0"], ["MDC", "Evaluated-With", "ESC 5 0"]], "rel_plus": [["XDC:Method", "Compare-With", "SDC:Method"], ["CDC:Method", "Compare-With", "SDC:Method"], ["MDC:Method", "Compare-With", "SDC:Method"], ["MDC:Method", "Evaluated-With", "UCF 1 0 1:Dataset"], ["CDC:Method", "Evaluated-With", "UCF 1 0 1:Dataset"], ["XDC:Method", "Evaluated-With", "UCF 1 0 1:Dataset"], ["SDC:Method", "Evaluated-With", "UCF 1 0 1:Dataset"], ["MDC:Method", "Evaluated-With", "HMDB 5 1:Dataset"], ["CDC:Method", "Evaluated-With", "HMDB 5 1:Dataset"], ["XDC:Method", "Evaluated-With", "HMDB 5 1:Dataset"], ["SDC:Method", "Evaluated-With", "HMDB 5 1:Dataset"], ["SDC:Method", "Evaluated-With", "ESC 5 0:Dataset"], ["XDC:Method", "Evaluated-With", "ESC 5 0:Dataset"], ["CDC:Method", "Evaluated-With", "ESC 5 0:Dataset"], ["CDC:Method", "Evaluated-With", "ESC 5 0:Dataset"], ["MDC:Method", "Evaluated-With", "ESC 5 0:Dataset"]]}
{"doc_id": "208513596", "sentence": "What distinguishes XDC from the other three models is that each modality encoder in XDC is self - supervised purely by the signal from the other modality .", "ner": [["XDC", "Method"], ["XDC", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "208513596", "sentence": "The encoders in CDC , MDC , and SDC all employ a self - supervision sig - Table 3 : Pretraining data type and size .", "ner": [["CDC", "Method"], ["MDC", "Method"], ["SDC", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "208513596", "sentence": "XDC significantly outperforms fully - supervised pretraining on HMDB 5 1 and UCF 1 0 1 . nal coming from the same modality .", "ner": [["XDC", "Method"], ["HMDB 5 1", "Dataset"], ["UCF 1 0 1", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "208513596", "sentence": "Overall , we find from this study that XDC performs the best among the three multi - modal approaches , so we opt to use XDC in the rest of the experiments .", "ner": [["XDC", "Method"], ["XDC", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "208513596", "sentence": "We pretrain XDC on three datasets , Kinetics , AudioSet - 2 4 0 K , and AudioSet , using k = 6 4 , 1 2 8 , 2 5 6 , 5 1 2 , and 1 0 2 4 clusters .", "ner": [["XDC", "Method"], ["Kinetics", "Dataset"], ["AudioSet - 2 4 0 K", "Dataset"], ["AudioSet", "Dataset"]], "rel": [["XDC", "Trained-With", "Kinetics"], ["XDC", "Trained-With", "AudioSet - 2 4 0 K"], ["XDC", "Trained-With", "AudioSet"]], "rel_plus": [["XDC:Method", "Trained-With", "Kinetics:Dataset"], ["XDC:Method", "Trained-With", "AudioSet - 2 4 0 K:Dataset"], ["XDC:Method", "Trained-With", "AudioSet:Dataset"]]}
{"doc_id": "208513596", "sentence": "Observations : ( I ) the best value for k is not sensitive to the number of semantic labels in the downstream datasets , For example , although HMDB 5 1 and ESC 5 0 have about the same number of labels , they do not share the same best k value . ( II ) Similarly , the best value for k seems uncorrelated with the number of original semantic labels of the pretraining dataset , e.g. 4 0 0 in Kinetics .", "ner": [["HMDB 5 1", "Dataset"], ["ESC 5 0", "Dataset"], ["Kinetics", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "208513596", "sentence": "For example , the best k for HMDB 5 1 shifts from 1 2 8 to 2 5 6 when moving from pretraining on AudioSet - 2 4 0 K to the full AudioSet .", "ner": [["HMDB 5 1", "Dataset"], ["AudioSet - 2 4 0 K", "Dataset"], ["AudioSet", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "208513596", "sentence": "XDC fixed features outperform several fully - finetuned supervised models . ( 2 4 0 K examples ) , AudioSet ( 2M examples ) , and IG 6 5 M ( 6 5 M examples ) .", "ner": [["XDC", "Method"], ["AudioSet", "Dataset"], ["IG 6 5 M", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "208513596", "sentence": "Kinetics and IG 6 5 M videos are collected originally for activity recognition , while AudioSet contains videos aimed for audio event classification .", "ner": [["Kinetics", "Dataset"], ["IG 6 5 M", "Dataset"], ["activity recognition", "Task"], ["AudioSet", "Dataset"], ["audio event classification", "Task"]], "rel": [["IG 6 5 M", "Benchmark-For", "activity recognition"], ["Kinetics", "Benchmark-For", "activity recognition"], ["AudioSet", "Benchmark-For", "audio event classification"]], "rel_plus": [["IG 6 5 M:Dataset", "Benchmark-For", "activity recognition:Task"], ["Kinetics:Dataset", "Benchmark-For", "activity recognition:Task"], ["AudioSet:Dataset", "Benchmark-For", "audio event classification:Task"]]}
{"doc_id": "208513596", "sentence": "Table 3 presents the results of XDC self - supervised pretraining with different data types and sizes , and compares it to fully - supervised pretraining on ImageNet , Kinetics , and AudioSet .", "ner": [["XDC", "Method"], ["ImageNet", "Dataset"], ["Kinetics", "Dataset"], ["AudioSet", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "208513596", "sentence": "For example , XDC performance on HMDB 5 1 improves by 9. 8 % and 2 4 . 1 % when pretrained on AudioSet and IG 6 5 M , respectively , compared to the results when pretrained on Kinetics . ( II ) XDC outperforms fullysupervised pretraining by 5. 1 % on HMDB 5 1 and by 0. 6 % on UCF 1 0 1 .", "ner": [["XDC", "Method"], ["HMDB 5 1", "Dataset"], ["AudioSet", "Dataset"], ["IG 6 5 M", "Dataset"], ["Kinetics", "Dataset"], ["XDC", "Method"], ["HMDB 5 1", "Dataset"], ["UCF 1 0 1", "Dataset"]], "rel": [["XDC", "Evaluated-With", "HMDB 5 1"], ["XDC", "Trained-With", "AudioSet"], ["XDC", "Trained-With", "IG 6 5 M"], ["XDC", "Trained-With", "Kinetics"]], "rel_plus": [["XDC:Method", "Evaluated-With", "HMDB 5 1:Dataset"], ["XDC:Method", "Trained-With", "AudioSet:Dataset"], ["XDC:Method", "Trained-With", "IG 6 5 M:Dataset"], ["XDC:Method", "Trained-With", "Kinetics:Dataset"]]}
{"doc_id": "208513596", "sentence": "To the best of our knowledge , XDC is the first method to demonstrate that self - supervision can outperform large - scale full - supervision in representation learning for action recognition . ( III ) The performance of the fullysupervised pretrained model is influenced by the taxonomy of the pretraining data more than the size .", "ner": [["XDC", "Method"], ["action recognition", "Task"]], "rel": [["XDC", "Used-For", "action recognition"]], "rel_plus": [["XDC:Method", "Used-For", "action recognition:Task"]]}
{"doc_id": "208513596", "sentence": "For example , supervised - pretraining on Kinetics gives better performance on both UCF 1 0 1 and HMDB 5 1 compared to supervisedpretraining on AudioSet ( which is more than 8 times larger than Kinetics ) and ImageNet .", "ner": [["Kinetics", "Dataset"], ["UCF 1 0 1", "Dataset"], ["HMDB 5 1", "Dataset"], ["AudioSet", "Dataset"], ["Kinetics", "Dataset"], ["ImageNet", "Dataset"]], "rel": [["AudioSet", "Compare-With", "Kinetics"]], "rel_plus": [["AudioSet:Dataset", "Compare-With", "Kinetics:Dataset"]]}
{"doc_id": "208513596", "sentence": "This suggests that XDC pretraining is useful both as a fixed feature extractor and as a pretraining initialization . ( II ) The performance of XDC as a fixed feature extractor exceeds the results of many fully - finetuned supervised models .", "ner": [["XDC", "Method"], ["XDC", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "208513596", "sentence": "For example , fc - only XDC outperforms , by significant margins , the fully - finetuned supervised AudioSetand ImageNet - pretrained models on both UCF 1 0 1 and HMDB 5 1 . ( III ) We observe that fully - supervised pretraining , followed by fc - only finetuning performs well when the pretraining latent space ( taxonomy ) is well aligned with the downstream task .", "ner": [["XDC", "Method"], ["AudioSetand", "Dataset"], ["ImageNet", "Dataset"], ["UCF 1 0 1", "Dataset"], ["HMDB 5 1", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "208513596", "sentence": "For example , pretraining on Kinetics with learning fc - only on HMDB 5 1 and UCF 1 0 1 gives the best performance .", "ner": [["Kinetics", "Dataset"], ["HMDB 5 1", "Dataset"], ["UCF 1 0 1", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "208513596", "sentence": "This is expected as the label space of HMBD 5 1 and UCF 1 0 1 overlap largely with that of Kinetics .", "ner": [["HMBD 5 1", "Dataset"], ["UCF 1 0 1", "Dataset"], ["Kinetics", "Dataset"]], "rel": [["HMBD 5 1", "Compare-With", "Kinetics"], ["UCF 1 0 1", "Compare-With", "Kinetics"]], "rel_plus": [["HMBD 5 1:Dataset", "Compare-With", "Kinetics:Dataset"], ["UCF 1 0 1:Dataset", "Compare-With", "Kinetics:Dataset"]]}
{"doc_id": "208513596", "sentence": "This observation suggests that fully - supervised pretraining is more taxonomy/downstream - task dependent , while our self - supervised XDC is taxonomy - independent . \" What does XDC actually learn ?", "ner": [["XDC", "Method"], ["XDC", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "208513596", "sentence": "What semantic signals does the algorithm use to train its encoders ? \" Here , we try to answer these questions by inspecting the k - means clustering results produced by the last iteration of XDC .", "ner": [["k - means clustering", "Method"], ["XDC", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "208513596", "sentence": "Table 5 and Table 6 list the most common Kinetics concepts of some audio and video clusters , respectively , learned by XDC when trained on Kinetics .", "ner": [["Kinetics", "Dataset"], ["XDC", "Method"], ["Kinetics", "Dataset"]], "rel": [["XDC", "Trained-With", "Kinetics"]], "rel_plus": [["XDC:Method", "Trained-With", "Kinetics:Dataset"]]}
{"doc_id": "208513596", "sentence": "Kinetics concepts is low , we still find some coherence , mostly at the scene level : a farm setting in audio # 1 2 7 ( \" grooming horse \" , \" milking cow \" ) , or gym activities in video # 6 3 ( \" pull ups \" , \" gymnastics tumbling \" , \" punch - Table 5 : XDC audio clusters .", "ner": [["Kinetics", "Dataset"], ["XDC", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "208513596", "sentence": "Figure 2 visualizes and compares conv 1 spatial and temporal filters of R( 2 + 1 )D learned by selfsupervised XDC pretraining on IG 6 5 M versus fullysupervised pretraining on Kinetics .", "ner": [["R( 2 + 1 )D", "Method"], ["XDC", "Method"], ["IG 6 5 M", "Dataset"], ["Kinetics", "Dataset"]], "rel": [["XDC", "Trained-With", "IG 6 5 M"], ["XDC", "Trained-With", "Kinetics"]], "rel_plus": [["XDC:Method", "Trained-With", "IG 6 5 M:Dataset"], ["XDC:Method", "Trained-With", "Kinetics:Dataset"]]}
{"doc_id": "208513596", "sentence": "Here , we compare XDC with state - of - the - art selfsupervised methods on action recognition in UCF 1 0 1 [ 5 8 ] and HMDB 5 1 [ 3 0 ] , and on audio event classification in ESC 5 0 [ 4 9 ] and DCASE [ 6 0 ] .", "ner": [["XDC", "Method"], ["action recognition", "Task"], ["UCF 1 0 1", "Dataset"], ["HMDB 5 1", "Dataset"], ["audio event classification", "Task"], ["ESC 5 0", "Dataset"], ["DCASE", "Dataset"]], "rel": [["UCF 1 0 1", "Benchmark-For", "action recognition"], ["HMDB 5 1", "Benchmark-For", "action recognition"], ["XDC", "Used-For", "action recognition"], ["XDC", "Evaluated-With", "UCF 1 0 1"], ["XDC", "Evaluated-With", "HMDB 5 1"], ["XDC", "Used-For", "audio event classification"], ["XDC", "Evaluated-With", "ESC 5 0"], ["XDC", "Evaluated-With", "DCASE"]], "rel_plus": [["UCF 1 0 1:Dataset", "Benchmark-For", "action recognition:Task"], ["HMDB 5 1:Dataset", "Benchmark-For", "action recognition:Task"], ["XDC:Method", "Used-For", "action recognition:Task"], ["XDC:Method", "Evaluated-With", "UCF 1 0 1:Dataset"], ["XDC:Method", "Evaluated-With", "HMDB 5 1:Dataset"], ["XDC:Method", "Used-For", "audio event classification:Task"], ["XDC:Method", "Evaluated-With", "ESC 5 0:Dataset"], ["XDC:Method", "Evaluated-With", "DCASE:Dataset"]]}
{"doc_id": "208513596", "sentence": "We use our XDC models pretrained on Kinetics , AudioSet , and IG 5 6 M .", "ner": [["XDC", "Method"], ["Kinetics", "Dataset"], ["AudioSet", "Dataset"], ["IG 5 6 M", "Dataset"]], "rel": [["XDC", "Trained-With", "Kinetics"], ["XDC", "Trained-With", "AudioSet"], ["XDC", "Trained-With", "IG 5 6 M"]], "rel_plus": [["XDC:Method", "Trained-With", "Kinetics:Dataset"], ["XDC:Method", "Trained-With", "AudioSet:Dataset"], ["XDC:Method", "Trained-With", "IG 5 6 M:Dataset"]]}
{"doc_id": "208513596", "sentence": "Similarly , we finetune on the action recognition downstream tasks using 3 2 - frame clips for both XDC and the fully - supervised pretraining baselines .", "ner": [["action recognition", "Task"], ["XDC", "Method"]], "rel": [["XDC", "Used-For", "action recognition"]], "rel_plus": [["XDC:Method", "Used-For", "action recognition:Task"]]}
{"doc_id": "208513596", "sentence": "For DCASE , following [ 2 9 ] we extract conv 5 features for 6 0 uniformly - sampled clips from each audio sample and use a linear SVM on these features .", "ner": [["DCASE", "Dataset"], ["SVM", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "208513596", "sentence": "Table 7 compares XDC pretrained on three large - scale datasets against state - of - the - art self - supervised methods , after finetuning on the UCF 1 0 1 and HMDB 5 1 benchmarks .", "ner": [["XDC", "Method"], ["UCF 1 0 1", "Dataset"], ["HMDB 5 1", "Dataset"]], "rel": [["XDC", "Trained-With", "UCF 1 0 1"], ["XDC", "Trained-With", "HMDB 5 1"]], "rel_plus": [["XDC:Method", "Trained-With", "UCF 1 0 1:Dataset"], ["XDC:Method", "Trained-With", "HMDB 5 1:Dataset"]]}
{"doc_id": "208513596", "sentence": "We also compare against two fully - supervised methods pretrained on ImageNet and Kinetics and then finetuned on UCF 1 0 1 and HMDB 5 1 .", "ner": [["ImageNet", "Dataset"], ["Kinetics", "Dataset"], ["UCF 1 0 1", "Dataset"], ["HMDB 5 1", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "208513596", "sentence": "Results : ( I ) XDC pretrained on IG 6 5 M significantly outperforms fully - supervised pretraining on Kinetics : by 3. 8 % on HMDB 5 1 and by 1. 1 % on UCF 1 0 1 .", "ner": [["XDC", "Method"], ["IG 6 5 M", "Dataset"], ["Kinetics", "Dataset"], ["HMDB 5 1", "Dataset"], ["UCF 1 0 1", "Dataset"]], "rel": [["XDC", "Trained-With", "IG 6 5 M"], ["XDC", "Trained-With", "Kinetics"], ["XDC", "Trained-With", "HMDB 5 1"], ["XDC", "Trained-With", "UCF 1 0 1"]], "rel_plus": [["XDC:Method", "Trained-With", "IG 6 5 M:Dataset"], ["XDC:Method", "Trained-With", "Kinetics:Dataset"], ["XDC:Method", "Trained-With", "HMDB 5 1:Dataset"], ["XDC:Method", "Trained-With", "UCF 1 0 1:Dataset"]]}
{"doc_id": "208513596", "sentence": "To the best of our knowledge , XDC is the first method to demonstrate that self - supervision can outperform large - scale full - supervision in representation learning for action recognition . ( II ) XDC pretrained on IG 6 5 M sets new state - of - the - art performance for self - supervised methods on both datasets , as it outperforms the current state - of - the - art self - supervised method AVTS [ 2 9 ] by 5. 8 % on HMDB 5 1 and 5. 2 % on UCF 1 0 1 . ( III ) When constrained to the same pretraining dataset ( AudioSet ) , XDC outperforms AVTS by 2. 2 % on UCF 1 0 1 and is only slightly worse than AVTS on HMDB 5 1 ( by 0. 6 % ) . [ 2 9 ] 9 4 XDC ( AudioSet ) 9 3 ( b ) DCASE Table 8 : State - of - the - art on audio event classification .", "ner": [["XDC", "Method"], ["action recognition", "Task"], ["XDC", "Method"], ["IG 6 5 M", "Dataset"], ["AVTS", "Method"], ["HMDB 5 1", "Dataset"], ["UCF 1 0 1", "Dataset"], ["AudioSet", "Dataset"], ["XDC", "Method"], ["AVTS", "Method"], ["UCF 1 0 1", "Dataset"], ["AVTS", "Method"], ["HMDB 5 1", "Dataset"], ["XDC", "Method"], ["AudioSet", "Dataset"], ["DCASE", "Dataset"], ["audio event classification", "Task"]], "rel": [["XDC", "Used-For", "action recognition"], ["XDC", "Trained-With", "IG 6 5 M"], ["XDC", "Compare-With", "AVTS"], ["XDC", "Evaluated-With", "HMDB 5 1"], ["AVTS", "Evaluated-With", "HMDB 5 1"], ["XDC", "Evaluated-With", "UCF 1 0 1"], ["AVTS", "Evaluated-With", "UCF 1 0 1"], ["XDC", "Compare-With", "AVTS"], ["AVTS", "Evaluated-With", "UCF 1 0 1"], ["XDC", "Evaluated-With", "UCF 1 0 1"], ["XDC", "Compare-With", "AVTS"], ["XDC", "Evaluated-With", "HMDB 5 1"], ["AVTS", "Evaluated-With", "HMDB 5 1"]], "rel_plus": [["XDC:Method", "Used-For", "action recognition:Task"], ["XDC:Method", "Trained-With", "IG 6 5 M:Dataset"], ["XDC:Method", "Compare-With", "AVTS:Method"], ["XDC:Method", "Evaluated-With", "HMDB 5 1:Dataset"], ["AVTS:Method", "Evaluated-With", "HMDB 5 1:Dataset"], ["XDC:Method", "Evaluated-With", "UCF 1 0 1:Dataset"], ["AVTS:Method", "Evaluated-With", "UCF 1 0 1:Dataset"], ["XDC:Method", "Compare-With", "AVTS:Method"], ["AVTS:Method", "Evaluated-With", "UCF 1 0 1:Dataset"], ["XDC:Method", "Evaluated-With", "UCF 1 0 1:Dataset"], ["XDC:Method", "Compare-With", "AVTS:Method"], ["XDC:Method", "Evaluated-With", "HMDB 5 1:Dataset"], ["AVTS:Method", "Evaluated-With", "HMDB 5 1:Dataset"]]}
{"doc_id": "208513596", "sentence": "We compare XDC with self - supervised methods on ESC 5 0 and DCASE benchmarks .", "ner": [["XDC", "Method"], ["ESC 5 0", "Dataset"], ["DCASE", "Dataset"]], "rel": [["XDC", "Evaluated-With", "ESC 5 0"], ["XDC", "Evaluated-With", "DCASE"]], "rel_plus": [["XDC:Method", "Evaluated-With", "ESC 5 0:Dataset"], ["XDC:Method", "Evaluated-With", "DCASE:Dataset"]]}
{"doc_id": "208513596", "sentence": "XDC shows competitive performance . pretrained on AudioSet with the state - of - the - art in selfsupervised methods for audio .", "ner": [["XDC", "Method"], ["AudioSet", "Dataset"]], "rel": [["XDC", "Trained-With", "AudioSet"]], "rel_plus": [["XDC:Method", "Trained-With", "AudioSet:Dataset"]]}
{"doc_id": "208513596", "sentence": "XDC achieves competitive results with only a 1. 7 % gap separating it from the the stateof - the - art [ 5 5 ] on ESC 5 0 and a 1% gap with the result of [ 2 9 ] on DCASE .", "ner": [["XDC", "Method"], ["ESC 5 0", "Dataset"], ["DCASE", "Dataset"]], "rel": [["XDC", "Evaluated-With", "ESC 5 0"], ["XDC", "Evaluated-With", "DCASE"]], "rel_plus": [["XDC:Method", "Evaluated-With", "ESC 5 0:Dataset"], ["XDC:Method", "Evaluated-With", "DCASE:Dataset"]]}
{"doc_id": "208513596", "sentence": "We have presented Cross - Modal Deep Clustering ( XDC ) , a novel self - supervised learning method for video and audio .", "ner": [["Cross - Modal Deep Clustering", "Method"], ["XDC", "Method"]], "rel": [["XDC", "Synonym-Of", "Cross - Modal Deep Clustering"]], "rel_plus": [["XDC:Method", "Synonym-Of", "Cross - Modal Deep Clustering:Method"]]}
{"doc_id": "208513596", "sentence": "Our experiments showed that XDC outperforms not only existing self - supervised representation learning methods but also fully - supervised ImageNet - and Kineticspretrained models in action recognition .", "ner": [["XDC", "Method"], ["ImageNet", "Dataset"], ["Kineticspretrained", "Dataset"], ["action recognition", "Task"]], "rel": [["ImageNet", "Used-For", "action recognition"], ["Kineticspretrained", "Used-For", "action recognition"], ["XDC", "Used-For", "action recognition"]], "rel_plus": [["ImageNet:Dataset", "Used-For", "action recognition:Task"], ["Kineticspretrained:Dataset", "Used-For", "action recognition:Task"], ["XDC:Method", "Used-For", "action recognition:Task"]]}
{"doc_id": "208513596", "sentence": "To the best of our knowledge , XDC is the first method to demonstrate self - supervision outperforming large - scale full - supervision in representation learning for action recognition .", "ner": [["XDC", "Method"], ["action recognition", "Task"]], "rel": [["XDC", "Used-For", "action recognition"]], "rel_plus": [["XDC:Method", "Used-For", "action recognition:Task"]]}
{"doc_id": "208513596", "sentence": "Tables 1 3 and 1 4 present the top and bottom 1 0 audio and video clusters learned with XDC on Kinetics , ranked by their purity with respect to Kinetics labels .", "ner": [["XDC", "Method"], ["Kinetics", "Dataset"], ["Kinetics", "Dataset"]], "rel": [["XDC", "Trained-With", "Kinetics"]], "rel_plus": [["XDC:Method", "Trained-With", "Kinetics:Dataset"]]}
{"doc_id": "208513596", "sentence": "Early stopping is used for pretraining on small datasets such as Kinetics [ 2 7 ] and AudioSet [ 1 0 ] to stop before the model starts overfitting on the pretext task .", "ner": [["Early stopping", "Method"], ["Kinetics", "Dataset"], ["AudioSet", "Dataset"]], "rel": [["Early stopping", "Used-For", "Kinetics"], ["Early stopping", "Used-For", "AudioSet"]], "rel_plus": [["Early stopping:Method", "Used-For", "Kinetics:Dataset"], ["Early stopping:Method", "Used-For", "AudioSet:Dataset"]]}
{"doc_id": "208513596", "sentence": "We pretrain XDC on IG 6 5 M longer in the last deep clustering iteration ( denoted as IG 6 5 M * in Table 1 0 ) .", "ner": [["XDC", "Method"], ["IG 6 5 M", "Dataset"], ["deep clustering", "Task"], ["IG 6 5 M", "Dataset"]], "rel": [["XDC", "Trained-With", "IG 6 5 M"], ["XDC", "Used-For", "deep clustering"]], "rel_plus": [["XDC:Method", "Trained-With", "IG 6 5 M:Dataset"], ["XDC:Method", "Used-For", "deep clustering:Task"]]}
{"doc_id": "202565512", "sentence": "In this work , we study commonsense question answering , a challenging task which requires machines to collect Figure 1 : An example from the CommonsenseQA dataset which requires multiple external knowledge to make the correct prediction .", "ner": [["commonsense question answering", "Task"], ["CommonsenseQA", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "202565512", "sentence": "To better use the relational structure of the evidence , we construct graphs for both sources , including extracted graph paths from ConceptNet and triples derived from Wikipedia sentences by Semantic Role Labeling ( SRL ) .", "ner": [["ConceptNet", "Dataset"], ["Role Labeling", "Method"], ["SRL", "Method"]], "rel": [["SRL", "Synonym-Of", "Role Labeling"]], "rel_plus": [["SRL:Method", "Synonym-Of", "Role Labeling:Method"]]}
{"doc_id": "202565512", "sentence": "We contribute by developing two graph - based modules , including ( 1 ) a graph - based contextual word representation learning module , which utilizes graph structural information to re - define the distance between words for learning better contextual word representations , and ( 2 ) a graphbased inference module , which first adopts Graph Convolutional Network ( Kipf and Welling 2 0 1 6 ) to encode neighbor information into the representations of nodes , followed by a graph attention mechanism for evidence aggregation .", "ner": [["graph - based modules", "Method"], ["graph - based contextual word representation learning module", "Method"], ["graphbased inference module", "Method"], ["Graph Convolutional Network", "Method"], ["graph attention mechanism", "Method"]], "rel": [["graph - based contextual word representation learning module", "Part-Of", "graph - based modules"], ["graphbased inference module", "Part-Of", "graph - based modules"], ["Graph Convolutional Network", "Part-Of", "graphbased inference module"], ["graph attention mechanism", "Part-Of", "graphbased inference module"]], "rel_plus": [["graph - based contextual word representation learning module:Method", "Part-Of", "graph - based modules:Method"], ["graphbased inference module:Method", "Part-Of", "graph - based modules:Method"], ["Graph Convolutional Network:Method", "Part-Of", "graphbased inference module:Method"], ["graph attention mechanism:Method", "Part-Of", "graphbased inference module:Method"]]}
{"doc_id": "202565512", "sentence": "Our contributions of this paper can be summarized as follows : \u2022 We introduce a graph - based approach to leverage evidence from heterogeneous knowledge sources for commonsense question answering . \u2022 We propose a graph - based contextual representation learning module and a graph - based inference module to make better use of the graph information for commonsense question answering . \u2022 Results show that our model achieves a new state - of - theart performance on the CommonsenseQA leaderboard .", "ner": [["commonsense question answering", "Task"], ["graph - based contextual representation learning module", "Method"], ["graph - based inference module", "Method"], ["commonsense question answering", "Task"], ["CommonsenseQA", "Dataset"]], "rel": [["graph - based contextual representation learning module", "Used-For", "commonsense question answering"], ["graph - based inference module", "Used-For", "commonsense question answering"]], "rel_plus": [["graph - based contextual representation learning module:Method", "Used-For", "commonsense question answering:Task"], ["graph - based inference module:Method", "Used-For", "commonsense question answering:Task"]]}
{"doc_id": "202565512", "sentence": "This paper utilizes CommonsenseQA ( Talmor et al. 2 0 1 9 ) , an influential dataset for commonsense question answering task for experiments .", "ner": [["CommonsenseQA", "Dataset"], ["commonsense question answering", "Task"]], "rel": [["CommonsenseQA", "Benchmark-For", "commonsense question answering"]], "rel_plus": [["CommonsenseQA:Dataset", "Benchmark-For", "commonsense question answering:Task"]]}
{"doc_id": "202565512", "sentence": "The second module adopts Graph Convolutional Network ( Kipf and Welling 2 0 1 6 ) to get node representations by using neighbor information and utilizes graph attention to aggregate graph representations to make final predictions .", "ner": [["Graph Convolutional Network", "Method"], ["graph attention", "Method"]], "rel": [["graph attention", "Part-Of", "Graph Convolutional Network"]], "rel_plus": [["graph attention:Method", "Part-Of", "Graph Convolutional Network:Method"]]}
{"doc_id": "202565512", "sentence": "In order to obtain contextual word representations for ConceptNet nodes , we transfer the triple into a natural language sequence according to the relation template in ConceptNet .", "ner": [["ConceptNet", "Dataset"], ["ConceptNet", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "202565512", "sentence": "We utilize Semantic Role Labeling ( SRL ) to extract arguments ( subjective , objective ) for each predicate in one sentence .", "ner": [["Semantic Role Labeling", "Method"], ["SRL", "Method"]], "rel": [["SRL", "Synonym-Of", "Semantic Role Labeling"]], "rel_plus": [["SRL:Method", "Synonym-Of", "Semantic Role Labeling:Method"]]}
{"doc_id": "202565512", "sentence": "Formally , the input of XLNet is the concatenation of sorted ConceptNet evidence sentences S T , sorted Wikipedia evidence sentences S , question q , and choice c. The output of XLNet is contextual word piece representations .", "ner": [["XLNet", "Method"], ["ConceptNet", "Dataset"], ["XLNet", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202565512", "sentence": "Specifically , we regard the two evidence graphs ConceptGraph and Wiki - Graph as one graph and adopt Graph Convolutional Networks ( GCNs ) ( Kipf and Welling 2 0 1 6 ) to obtain node representations by encoding graph - structural information .", "ner": [["ConceptGraph", "Method"], ["Wiki - Graph", "Method"], ["Graph Convolutional Networks", "Method"], ["GCNs", "Method"]], "rel": [["GCNs", "Synonym-Of", "Graph Convolutional Networks"]], "rel_plus": [["GCNs:Method", "Synonym-Of", "Graph Convolutional Networks:Method"]]}
{"doc_id": "202565512", "sentence": "Because relational GCNs usually over - parameterize the model ( Marcheggiani and Titov 2 0 1 7 ; Zhang , Qi , and Manning 2 0 1 8) , we apply GCNs on the undirected graph .", "ner": [["GCNs", "Method"], ["GCNs", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202565512", "sentence": "The i - th node representation h 0 i is obtained by averaging hidden states of the corresponding evidence in the output of XLNet and reducing dimension via a non - linear transformation . h where s i = { w 0 , \u00b7 \u00b7 \u00b7 , w t } is the corresponding evidence to the i - th node , h wj is the contextual token representation of XLNet for the token w j , W \u2208 R d \u00d7 k is to reduce high dimension d into low dimension k , and \u03c3 is an activation function .", "ner": [["XLNet", "Method"], ["XLNet", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202565512", "sentence": "The graph representation is computed the same as the multiplicative attention ( Luong , Pham , and Manning 2 0 1 5 ) , where h L i is the i - th node representation at the last layer , h c is the representation of the last token in XLNet and can be regarded as the input representation , \u03b1 i is the importance of the i - th node , and h g is the graph representation .", "ner": [["multiplicative attention", "Method"], ["XLNet", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202565512", "sentence": "These models include SGN - lite , BECON ( single ) , BECON ( ensemble ) , CSR - KG and CSR - KG ( AI 2 IR ) . \u2022 Group 2 : models without extracted knowledge , including BERT - large ( Devlin et al. 2 0 1 9 ) , XLNet - large ( Yang et al. 2 0 1 9 ) and RoBERTa ( Liu et al. 2 0 1 9 ) .", "ner": [["SGN - lite", "Method"], ["BECON ( single )", "Method"], ["BECON ( ensemble )", "Method"], ["CSR - KG", "Method"], ["CSR - KG ( AI 2 IR )", "Method"], ["BERT - large", "Method"], ["XLNet - large", "Method"], ["RoBERTa", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202565512", "sentence": "These models adopt pre - trained language models to finetune on the training data and make predictions directly on the test dataset without extracted knowledge . \u2022 Group 3 : models with extracted structured knowledge , including BERT + AMS ( Ye et al. 2 0 1 9 ) and BERT + CSPT .", "ner": [["BERT + AMS", "Method"], ["BERT + CSPT", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202565512", "sentence": "BERT + AMS ( Ye et al. 2 0 1 9 ) constructs a commonsenserelated multi - choice question answering dataset according to ConcepNet and pre - train on the generated dataset .", "ner": [["BERT + AMS", "Method"], ["ConcepNet", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "202565512", "sentence": "BERT + CSPT first trains a generation model to generate synthetic data from ConceptNet , then finetunes RoBERTa on the synthetic data and Open Mind Common Sense ( OMCS ) corpus . \u2022 HyKAS and BERT + OMCS models pre - train BERT whole word masking model on the OMCS corpus .", "ner": [["BERT + CSPT", "Method"], ["ConceptNet", "Dataset"], ["RoBERTa", "Method"], ["Open Mind Common Sense", "Dataset"], ["OMCS", "Dataset"], ["HyKAS", "Method"], ["BERT + OMCS", "Method"], ["BERT", "Method"], ["OMCS", "Dataset"]], "rel": [["BERT + CSPT", "Trained-With", "ConceptNet"], ["OMCS", "Synonym-Of", "Open Mind Common Sense"], ["RoBERTa", "Trained-With", "Open Mind Common Sense"], ["HyKAS", "Trained-With", "OMCS"], ["BERT + OMCS", "Trained-With", "OMCS"]], "rel_plus": [["BERT + CSPT:Method", "Trained-With", "ConceptNet:Dataset"], ["OMCS:Dataset", "Synonym-Of", "Open Mind Common Sense:Dataset"], ["RoBERTa:Method", "Trained-With", "Open Mind Common Sense:Dataset"], ["HyKAS:Method", "Trained-With", "OMCS:Dataset"], ["BERT + OMCS:Method", "Trained-With", "OMCS:Dataset"]]}
{"doc_id": "202565512", "sentence": "AristoBERTv 7 utilizes the information from machine reading comprehension data RACE ( Lai et al. 2 0 1 7 ) and extracts evidence from text sources such as Wikipedia , SimpleWikipedia , etc .", "ner": [["AristoBERTv 7", "Method"], ["machine reading comprehension", "Task"], ["RACE", "Dataset"], ["Wikipedia", "Dataset"], ["SimpleWikipedia", "Dataset"]], "rel": [["RACE", "Used-For", "AristoBERTv 7"], ["Wikipedia", "Used-For", "AristoBERTv 7"], ["SimpleWikipedia", "Used-For", "AristoBERTv 7"], ["RACE", "Benchmark-For", "machine reading comprehension"], ["AristoBERTv 7", "Used-For", "machine reading comprehension"]], "rel_plus": [["RACE:Dataset", "Used-For", "AristoBERTv 7:Method"], ["Wikipedia:Dataset", "Used-For", "AristoBERTv 7:Method"], ["SimpleWikipedia:Dataset", "Used-For", "AristoBERTv 7:Method"], ["RACE:Dataset", "Benchmark-For", "machine reading comprehension:Task"], ["AristoBERTv 7:Method", "Used-For", "machine reading comprehension:Task"]]}
{"doc_id": "202565512", "sentence": "DREAM adopts XLNet - large as the baseline and extracts evidence from Wikipedia .", "ner": [["DREAM", "Method"], ["XLNet - large", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202565512", "sentence": "RoBERT + KE , RoBERTa + IR and RoBERTa + CSPT adopt RoBERTa as the baseline and utilize the evidence from Wikipedia , search engine and OMCS , respectively .", "ner": [["RoBERT + KE", "Method"], ["RoBERTa + IR", "Method"], ["RoBERTa + CSPT", "Method"], ["RoBERTa", "Method"], ["Wikipedia", "Dataset"], ["OMCS", "Dataset"]], "rel": [["Wikipedia", "Used-For", "RoBERT + KE"], ["OMCS", "Used-For", "RoBERT + KE"], ["OMCS", "Used-For", "RoBERTa + IR"], ["Wikipedia", "Used-For", "RoBERTa + CSPT"], ["OMCS", "Used-For", "RoBERTa + CSPT"], ["Wikipedia", "Used-For", "RoBERTa"], ["OMCS", "Used-For", "RoBERTa"]], "rel_plus": [["Wikipedia:Dataset", "Used-For", "RoBERT + KE:Method"], ["OMCS:Dataset", "Used-For", "RoBERT + KE:Method"], ["OMCS:Dataset", "Used-For", "RoBERTa + IR:Method"], ["Wikipedia:Dataset", "Used-For", "RoBERTa + CSPT:Method"], ["OMCS:Dataset", "Used-For", "RoBERTa + CSPT:Method"], ["Wikipedia:Dataset", "Used-For", "RoBERTa:Method"], ["OMCS:Dataset", "Used-For", "RoBERTa:Method"]]}
{"doc_id": "202565512", "sentence": "RoBERT + CSPT adopts knowledge from ConceptNet and OMCS , but the model pre - trains on the sources without explicit reasoning over the evidence , which is different from our approach .", "ner": [["RoBERT + CSPT", "Method"], ["ConceptNet", "Dataset"], ["OMCS", "Dataset"]], "rel": [["ConceptNet", "Used-For", "RoBERT + CSPT"], ["OMCS", "Used-For", "RoBERT + CSPT"]], "rel_plus": [["ConceptNet:Dataset", "Used-For", "RoBERT + CSPT:Method"], ["OMCS:Dataset", "Used-For", "RoBERT + CSPT:Method"]]}
{"doc_id": "202565512", "sentence": "Compared to models with extracted structured knowledge in group 3 , our model extracts graph paths from ConceptNet for graph - based reasoning rather than for pre - training , and we also extract evidence from Wikipedia plain texts , which brings 1 3 . 1 % and 5. 7 % gains over BERT + AMS and ROBERTa + CSPT respectively .", "ner": [["ConceptNet", "Dataset"], ["Wikipedia", "Dataset"], ["BERT + AMS", "Method"], ["ROBERTa + CSPT", "Method"]], "rel": [["ConceptNet", "Used-For", "BERT + AMS"], ["Wikipedia", "Used-For", "BERT + AMS"], ["Wikipedia", "Used-For", "ROBERTa + CSPT"], ["ConceptNet", "Used-For", "ROBERTa + CSPT"]], "rel_plus": [["ConceptNet:Dataset", "Used-For", "BERT + AMS:Method"], ["Wikipedia:Dataset", "Used-For", "BERT + AMS:Method"], ["Wikipedia:Dataset", "Used-For", "ROBERTa + CSPT:Method"], ["ConceptNet:Dataset", "Used-For", "ROBERTa + CSPT:Method"]]}
{"doc_id": "202565512", "sentence": "In the graph - based reasoning part , we dive into the effect of topology sort algorithm for learning contextual word representations and graph inferences with GCN and graph attention .", "ner": [["GCN", "Method"], ["graph attention", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202565512", "sentence": "The graph inference module brings 1. 4 % benefit , showing that GCN can obtain proper node representations and graph attention can aggregate both word and node representations to infer answers .", "ner": [["GCN", "Method"], ["graph attention", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202565512", "sentence": "SWAG and HellaSWAG are two similar datasets trying to predict the next event given an initial event .", "ner": [["SWAG", "Dataset"], ["HellaSWAG", "Dataset"]], "rel": [["SWAG", "Compare-With", "HellaSWAG"]], "rel_plus": [["SWAG:Dataset", "Compare-With", "HellaSWAG:Dataset"]]}
{"doc_id": "202565512", "sentence": "The SWAG dataset has been well solved by pre - trained language models like BERT ( Devlin et al. 2 0 1 9 ) .", "ner": [["SWAG", "Dataset"], ["BERT", "Method"]], "rel": [["SWAG", "Used-For", "BERT"]], "rel_plus": [["SWAG:Dataset", "Used-For", "BERT:Method"]]}
{"doc_id": "202565512", "sentence": "Recently proposed CommonsenseQA ( Talmor et al. 2 0 1 9 ) dataset derived from ConceptNet ( Speer , Chin , and Havasi 2 0 1 7 ) and the choices have the same relation with the concept in the question .", "ner": [["CommonsenseQA", "Dataset"], ["ConceptNet", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "202565512", "sentence": "Knowledge Transfer in NLP Transfer learning has palyed a vital role in the NLP community .", "ner": [["NLP Transfer learning", "Task"], ["NLP", "Task"]], "rel": [["NLP Transfer learning", "SubTask-Of", "NLP"]], "rel_plus": [["NLP Transfer learning:Task", "SubTask-Of", "NLP:Task"]]}
{"doc_id": "202565512", "sentence": "Pre - trained language models from large - scale unstructured data like ELMo ( Peters et al. 2 0 1 8) , GPT ( Radford et al. 2 0 1 8) , BERT ( Devlin et al. 2 0 1 9 ) , XLNet ( Yang et al. 2 0 1 9 ) , RoBERTa ( Liu et al. 2 0 1 9 ) have achieved significant improvements on many tasks .", "ner": [["ELMo", "Method"], ["GPT", "Method"], ["BERT", "Method"], ["XLNet", "Method"], ["RoBERTa", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202565512", "sentence": "This paper utilizes XLNet ( Yang et al. 2 0 1 9 ) as the backend and propose our approach to study the commonsense question answering problem .", "ner": [["XLNet", "Method"], ["commonsense question answering", "Task"]], "rel": [["XLNet", "Used-For", "commonsense question answering"]], "rel_plus": [["XLNet:Method", "Used-For", "commonsense question answering:Task"]]}
{"doc_id": "202565512", "sentence": "Graph Neural Networks for NLP Recently , Graph Neural Networks ( GNN ) has been utilized widely in NLP .", "ner": [["Graph Neural Networks", "Method"], ["NLP", "Task"], ["Graph Neural Networks", "Method"], ["GNN", "Method"], ["NLP", "Task"]], "rel": [["Graph Neural Networks", "Used-For", "NLP"], ["GNN", "Synonym-Of", "Graph Neural Networks"], ["Graph Neural Networks", "Used-For", "NLP"]], "rel_plus": [["Graph Neural Networks:Method", "Used-For", "NLP:Task"], ["GNN:Method", "Synonym-Of", "Graph Neural Networks:Method"], ["Graph Neural Networks:Method", "Used-For", "NLP:Task"]]}
{"doc_id": "202565512", "sentence": "For example , Sun et al. ( 2 0 1 9 ) utilizes Graph Convolutional Networks ( GCN ) to jointly extract entity and relation .", "ner": [["Graph Convolutional Networks", "Method"], ["GCN", "Method"]], "rel": [["GCN", "Synonym-Of", "Graph Convolutional Networks"]], "rel_plus": [["GCN:Method", "Synonym-Of", "Graph Convolutional Networks:Method"]]}
{"doc_id": "202565512", "sentence": "Zhang , Qi , and Manning ( 2 0 1 8) applies GNN to relation extraction over pruned dependency trees and achieves remarkable improvements .", "ner": [["GNN", "Method"], ["relation extraction", "Task"]], "rel": [["GNN", "Used-For", "relation extraction"]], "rel_plus": [["GNN:Method", "Used-For", "relation extraction:Task"]]}
{"doc_id": "202565512", "sentence": "GNN has also been applied into muli - hop reading comprehension tasks ( Tu et al. 2 0 1 9 ; Kundu et al. 2 0 1 9 ; ) .", "ner": [["GNN", "Method"], ["muli - hop reading comprehension", "Task"]], "rel": [["GNN", "Used-For", "muli - hop reading comprehension"]], "rel_plus": [["GNN:Method", "Used-For", "muli - hop reading comprehension:Task"]]}
{"doc_id": "202565512", "sentence": "This paper utilizes GCN to represent graph nodes by utilizing the graph structure information , followed by graph attention which aggregates the graph representations to make the prediction .", "ner": [["GCN", "Method"], ["graph attention", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202565512", "sentence": "In this work , we focus on commonsense question answering and select CommonsenseQA ( Talmor et al. 2 0 1 9 ) dataset as the testbed .", "ner": [["commonsense question answering", "Task"], ["CommonsenseQA", "Dataset"]], "rel": [["CommonsenseQA", "Benchmark-For", "commonsense question answering"]], "rel_plus": [["CommonsenseQA:Dataset", "Benchmark-For", "commonsense question answering:Task"]]}
{"doc_id": "202565512", "sentence": "The second module adopts Graph Convolutional Net - work to encode neighbor information into the representations of nodes , followed by a graph attention mechanism for evidence aggregation to infer fina lanswers .", "ner": [["Graph Convolutional Net - work", "Method"], ["graph attention", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "21683040", "sentence": "SSD ( Single Shot Multibox Detector ) is one of the best object detection algorithms with both high accuracy and fast speed .", "ner": [["SSD", "Method"], ["Single Shot Multibox Detector", "Method"], ["object detection", "Task"]], "rel": [["SSD", "Synonym-Of", "Single Shot Multibox Detector"], ["SSD", "Used-For", "object detection"]], "rel_plus": [["SSD:Method", "Synonym-Of", "Single Shot Multibox Detector:Method"], ["SSD:Method", "Used-For", "object detection:Task"]]}
{"doc_id": "21683040", "sentence": "However , SSD 's feature pyramid detection method makes it hard to fuse the features from different scales .", "ner": [["SSD", "Method"], ["feature pyramid detection", "Method"]], "rel": [["feature pyramid detection", "Part-Of", "SSD"]], "rel_plus": [["feature pyramid detection:Method", "Part-Of", "SSD:Method"]]}
{"doc_id": "21683040", "sentence": "In this paper , we proposed FSSD ( Feature Fusion Single Shot Multibox Detector ) , an enhanced SSD with a novel and lightweight feature fusion module which can improve the performance significantly over SSD with just a little speed drop .", "ner": [["FSSD", "Method"], ["Feature Fusion Single Shot Multibox Detector", "Method"], ["SSD", "Method"], ["feature fusion module", "Method"], ["SSD", "Method"]], "rel": [["feature fusion module", "Part-Of", "FSSD"], ["FSSD", "Synonym-Of", "Feature Fusion Single Shot Multibox Detector"], ["FSSD", "SubClass-Of", "SSD"], ["FSSD", "Compare-With", "SSD"]], "rel_plus": [["feature fusion module:Method", "Part-Of", "FSSD:Method"], ["FSSD:Method", "Synonym-Of", "Feature Fusion Single Shot Multibox Detector:Method"], ["FSSD:Method", "SubClass-Of", "SSD:Method"], ["FSSD:Method", "Compare-With", "SSD:Method"]]}
{"doc_id": "21683040", "sentence": "In addition , our result on COCO is also better than the conventional SSD with a large margin .", "ner": [["COCO", "Dataset"], ["SSD", "Method"]], "rel": [["SSD", "Trained-With", "COCO"]], "rel_plus": [["SSD:Method", "Trained-With", "COCO:Dataset"]]}
{"doc_id": "21683040", "sentence": "Our FSSD outperforms a lot of state - of - the - art object detection algorithms in both aspects of accuracy and speed .", "ner": [["FSSD", "Method"], ["object detection", "Task"]], "rel": [["FSSD", "Used-For", "object detection"]], "rel_plus": [["FSSD:Method", "Used-For", "object detection:Task"]]}
{"doc_id": "21683040", "sentence": "Object detection is one of the core tasks in computer vision .", "ner": [["Object detection", "Task"], ["computer vision", "Task"]], "rel": [["Object detection", "SubTask-Of", "computer vision"]], "rel_plus": [["Object detection:Task", "SubTask-Of", "computer vision:Task"]]}
{"doc_id": "21683040", "sentence": "In recent years , a lot of detectors based on ConvNets have been proposed to improve the accuracy and speed in object detection task [ 8 , 2 7 , 3 , 2 0 , 2 6 ] .", "ner": [["ConvNets", "Method"], ["object detection", "Task"]], "rel": [["ConvNets", "Used-For", "object detection"]], "rel_plus": [["ConvNets:Method", "Used-For", "object detection:Task"]]}
{"doc_id": "21683040", "sentence": "This method is adopted by Faster RCNN [ 2 7 ] , RFCN [ 3 ] and so on .", "ner": [["Faster RCNN", "Method"], ["RFCN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "21683040", "sentence": "Top - Down structure like ( c ) in Fig. 1 is popular recently and has been proved working well in FPN [ 1 8 ] , DSSD [ 7 ] and SharpMask [ 2 4 ] .", "ner": [["FPN", "Method"], ["DSSD", "Method"], ["SharpMask", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "21683040", "sentence": "The main trade - off of object detectors which are based on ConvNets is that the contradiction between object recognition and location .", "ner": [["ConvNets", "Method"], ["object recognition", "Task"]], "rel": [["ConvNets", "Used-For", "object recognition"]], "rel_plus": [["ConvNets:Method", "Used-For", "object recognition:Task"]]}
{"doc_id": "21683040", "sentence": "With deeper ConvNet , the feature maps can represent more semantic information with translation invariance , which is beneficial to object recognition but harmful to object location .", "ner": [["ConvNet", "Method"], ["object recognition", "Task"]], "rel": [["ConvNet", "Used-For", "object recognition"]], "rel_plus": [["ConvNet:Method", "Used-For", "object recognition:Task"]]}
{"doc_id": "21683040", "sentence": "To solve this problem , SSD adopts feature pyramid to detect objects with different scales .", "ner": [["SSD", "Method"], ["feature pyramid", "Method"], ["detect objects", "Task"]], "rel": [["feature pyramid", "Part-Of", "SSD"], ["SSD", "Used-For", "detect objects"]], "rel_plus": [["feature pyramid:Method", "Part-Of", "SSD:Method"], ["SSD:Method", "Used-For", "detect objects:Task"]]}
{"doc_id": "21683040", "sentence": "In this paper , to tackle these problems mentioned above , we propose Feature Fusion SSD(FSSD ) by adding a lightweight and efficient feature fusion module to the conventional SSD .", "ner": [["Feature Fusion SSD(FSSD )", "Method"], ["feature fusion module", "Method"], ["SSD", "Method"]], "rel": [["SSD", "Part-Of", "Feature Fusion SSD(FSSD )"], ["feature fusion module", "Part-Of", "Feature Fusion SSD(FSSD )"]], "rel_plus": [["SSD:Method", "Part-Of", "Feature Fusion SSD(FSSD ):Method"], ["feature fusion module:Method", "Part-Of", "Feature Fusion SSD(FSSD ):Method"]]}
{"doc_id": "21683040", "sentence": "Then we append some downsampling blocks to generate new feature pyramid , which are fed to multibox detectors to produce the final detection re - features are used to detect objects , which is used in some two stage detectors such as Faster R - CNN [ 2 7 ] and R - FCN [ 3 ] . ( c ) Feature fusion method adopted by [ 1 8 , 2 4 ]   Using the proposed architecture , our FSSD improves a lot in performance at a slight expense of speed compared with conventional SSD .", "ner": [["downsampling blocks", "Method"], ["generate new feature pyramid", "Task"], ["two stage detectors", "Method"], ["Faster R - CNN", "Method"], ["R - FCN", "Method"], ["FSSD", "Method"], ["SSD", "Method"]], "rel": [["downsampling blocks", "Used-For", "generate new feature pyramid"], ["Faster R - CNN", "SubClass-Of", "two stage detectors"], ["R - FCN", "SubClass-Of", "two stage detectors"], ["downsampling blocks", "Part-Of", "two stage detectors"], ["downsampling blocks", "Part-Of", "Faster R - CNN"], ["downsampling blocks", "Part-Of", "R - FCN"], ["FSSD", "Compare-With", "SSD"]], "rel_plus": [["downsampling blocks:Method", "Used-For", "generate new feature pyramid:Task"], ["Faster R - CNN:Method", "SubClass-Of", "two stage detectors:Method"], ["R - FCN:Method", "SubClass-Of", "two stage detectors:Method"], ["downsampling blocks:Method", "Part-Of", "two stage detectors:Method"], ["downsampling blocks:Method", "Part-Of", "Faster R - CNN:Method"], ["downsampling blocks:Method", "Part-Of", "R - FCN:Method"], ["FSSD:Method", "Compare-With", "SSD:Method"]]}
{"doc_id": "21683040", "sentence": "We evaluate the FSSD in VOC PASCAL [ 6 ] dataset and MSCOCO [ 1 9 ] dataset .", "ner": [["FSSD", "Method"], ["VOC PASCAL", "Dataset"], ["MSCOCO", "Dataset"]], "rel": [["FSSD", "Evaluated-With", "VOC PASCAL"], ["FSSD", "Evaluated-With", "MSCOCO"]], "rel_plus": [["FSSD:Method", "Evaluated-With", "VOC PASCAL:Dataset"], ["FSSD:Method", "Evaluated-With", "MSCOCO:Dataset"]]}
{"doc_id": "21683040", "sentence": "The results indicate that our FSSD can improve the conventional SSD with a large margin especially for small objects without any bells and whistles .", "ner": [["FSSD", "Method"], ["SSD", "Method"]], "rel": [["FSSD", "Compare-With", "SSD"]], "rel_plus": [["FSSD:Method", "Compare-With", "SSD:Method"]]}
{"doc_id": "21683040", "sentence": "Besides , our FSSD also outperforms a lot of state - of - the - art object detectors based on VG - GNet including ION [ 1 ] and Faster RCNN [ 2 7 ] .", "ner": [["FSSD", "Method"], ["object detectors", "Method"], ["VG - GNet", "Method"], ["ION", "Method"], ["Faster RCNN", "Method"]], "rel": [["Faster RCNN", "SubClass-Of", "object detectors"], ["VG - GNet", "SubClass-Of", "object detectors"], ["FSSD", "Compare-With", "object detectors"], ["ION", "Part-Of", "VG - GNet"], ["FSSD", "Compare-With", "VG - GNet"], ["FSSD", "Compare-With", "Faster RCNN"]], "rel_plus": [["Faster RCNN:Method", "SubClass-Of", "object detectors:Method"], ["VG - GNet:Method", "SubClass-Of", "object detectors:Method"], ["FSSD:Method", "Compare-With", "object detectors:Method"], ["ION:Method", "Part-Of", "VG - GNet:Method"], ["FSSD:Method", "Compare-With", "VG - GNet:Method"], ["FSSD:Method", "Compare-With", "Faster RCNN:Method"]]}
{"doc_id": "21683040", "sentence": "Our feature fusion module can also work better than FPN [ 1 8 ] in the object detection task .", "ner": [["feature fusion module", "Method"], ["FPN", "Method"], ["object detection", "Task"]], "rel": [["feature fusion module", "Compare-With", "FPN"], ["feature fusion module", "Used-For", "object detection"], ["FPN", "Used-For", "object detection"]], "rel_plus": [["feature fusion module:Method", "Compare-With", "FPN:Method"], ["feature fusion module:Method", "Used-For", "object detection:Task"], ["FPN:Method", "Used-For", "object detection:Task"]]}
{"doc_id": "21683040", "sentence": "Our main contributions are summarized as follows : ( 1 ) We define the feature fusion framework and investigate the factors to confirm the structure of the feature fusion module . ( 2 ) We introduce a novel and lightweight way of combining feature maps from different levels and generating feature pyramid to fully utilize the features . ( 3 ) With quantitative and qualitative experiments , we prove that our FSSD has a significant improvement over the conventional SSD with slight speed drop .", "ner": [["feature fusion framework", "Method"], ["feature fusion module", "Method"], ["feature pyramid", "Method"], ["FSSD", "Method"], ["SSD", "Method"]], "rel": [["FSSD", "Compare-With", "SSD"]], "rel_plus": [["FSSD:Method", "Compare-With", "SSD:Method"]]}
{"doc_id": "21683040", "sentence": "FSSD can achieve state - of - the - art performance on both PASCAL VOC dataset and MS COCO dataset .", "ner": [["FSSD", "Method"], ["PASCAL VOC", "Dataset"], ["MS COCO", "Dataset"]], "rel": [["FSSD", "Evaluated-With", "PASCAL VOC"], ["FSSD", "Evaluated-With", "MS COCO"]], "rel_plus": [["FSSD:Method", "Evaluated-With", "PASCAL VOC:Dataset"], ["FSSD:Method", "Evaluated-With", "MS COCO:Dataset"]]}
{"doc_id": "21683040", "sentence": "Object detector with deep ConvNet Benefited from the power of Deep ConvNes , object detector such as Over - Feat [ 2 9 ] and R - CNN [ 9 ] have began to show the dramatic improvements in accuracy .", "ner": [["Object detector", "Method"], ["ConvNet", "Method"], ["Deep ConvNes", "Method"], ["object detector", "Method"], ["Over - Feat", "Method"], ["R - CNN", "Method"]], "rel": [["ConvNet", "Part-Of", "Object detector"], ["Over - Feat", "SubClass-Of", "object detector"], ["R - CNN", "SubClass-Of", "object detector"], ["Deep ConvNes", "Part-Of", "object detector"], ["Deep ConvNes", "Part-Of", "Over - Feat"], ["Deep ConvNes", "Part-Of", "R - CNN"]], "rel_plus": [["ConvNet:Method", "Part-Of", "Object detector:Method"], ["Over - Feat:Method", "SubClass-Of", "object detector:Method"], ["R - CNN:Method", "SubClass-Of", "object detector:Method"], ["Deep ConvNes:Method", "Part-Of", "object detector:Method"], ["Deep ConvNes:Method", "Part-Of", "Over - Feat:Method"], ["Deep ConvNes:Method", "Part-Of", "R - CNN:Method"]]}
{"doc_id": "21683040", "sentence": "OverFeat applies a ConvNet as a feature extractor in the sliding window on an image pyramid .", "ner": [["OverFeat", "Method"], ["ConvNet", "Method"], ["feature extractor", "Method"]], "rel": [["ConvNet", "Part-Of", "OverFeat"], ["ConvNet", "Used-For", "feature extractor"]], "rel_plus": [["ConvNet:Method", "Part-Of", "OverFeat:Method"], ["ConvNet:Method", "Used-For", "feature extractor:Method"]]}
{"doc_id": "21683040", "sentence": "R - CNN [ 9 ] uses the region proposals generated from selective search [ 3 2 ] or Edge boxes [ 3 3 ] to generate the region - based feature from a pre - trained ConvNet and SVMs are adopted to do classification .", "ner": [["R - CNN", "Method"], ["region proposals", "Method"], ["selective search", "Method"], ["Edge boxes", "Method"], ["generate the region - based feature", "Task"], ["ConvNet", "Method"], ["SVMs", "Method"], ["classification", "Task"]], "rel": [["region proposals", "Part-Of", "R - CNN"], ["selective search", "Part-Of", "R - CNN"], ["Edge boxes", "Part-Of", "R - CNN"], ["selective search", "Used-For", "region proposals"], ["Edge boxes", "Used-For", "region proposals"], ["ConvNet", "Used-For", "generate the region - based feature"], ["SVMs", "Used-For", "classification"]], "rel_plus": [["region proposals:Method", "Part-Of", "R - CNN:Method"], ["selective search:Method", "Part-Of", "R - CNN:Method"], ["Edge boxes:Method", "Part-Of", "R - CNN:Method"], ["selective search:Method", "Used-For", "region proposals:Method"], ["Edge boxes:Method", "Used-For", "region proposals:Method"], ["ConvNet:Method", "Used-For", "generate the region - based feature:Task"], ["SVMs:Method", "Used-For", "classification:Task"]]}
{"doc_id": "21683040", "sentence": "SPPNet [ 1 1 ] adopts a spatial pyramid pooling layer which allows the classification module to reuse the ConvNet feature regardless of the input image resolutions .", "ner": [["SPPNet", "Method"], ["spatial pyramid pooling", "Method"], ["classification module", "Method"], ["ConvNet", "Method"]], "rel": [["spatial pyramid pooling", "Part-Of", "SPPNet"], ["classification module", "Part-Of", "SPPNet"], ["ConvNet", "Part-Of", "SPPNet"]], "rel_plus": [["spatial pyramid pooling:Method", "Part-Of", "SPPNet:Method"], ["classification module:Method", "Part-Of", "SPPNet:Method"], ["ConvNet:Method", "Part-Of", "SPPNet:Method"]]}
{"doc_id": "21683040", "sentence": "Fast R - CNN [ 8 ] introduces to train the ConvNet with both the classification and location regression loss end to end .", "ner": [["Fast R - CNN", "Method"], ["ConvNet", "Method"], ["location regression loss", "Method"]], "rel": [["ConvNet", "Part-Of", "Fast R - CNN"], ["location regression loss", "Part-Of", "ConvNet"]], "rel_plus": [["ConvNet:Method", "Part-Of", "Fast R - CNN:Method"], ["location regression loss:Method", "Part-Of", "ConvNet:Method"]]}
{"doc_id": "21683040", "sentence": "Faster R - CNN [ 2 7 ] suggests to replace selective search with a region proposal network ( RPN ) .", "ner": [["Faster R - CNN", "Method"], ["selective search", "Method"], ["region proposal network", "Method"], ["RPN", "Method"]], "rel": [["region proposal network", "Part-Of", "Faster R - CNN"], ["RPN", "Synonym-Of", "region proposal network"]], "rel_plus": [["region proposal network:Method", "Part-Of", "Faster R - CNN:Method"], ["RPN:Method", "Synonym-Of", "region proposal network:Method"]]}
{"doc_id": "21683040", "sentence": "R - FCN [ 3 ] replaces ROI pooling in the Faster RCNN with position sensitive ROI pooling ( PSROI ) to improve the detector 's quality with both aspects of accuracy and speed .", "ner": [["R - FCN", "Method"], ["ROI pooling", "Method"], ["Faster RCNN", "Method"], ["position sensitive ROI pooling", "Method"], ["PSROI", "Method"]], "rel": [["position sensitive ROI pooling", "Part-Of", "R - FCN"], ["ROI pooling", "Part-Of", "Faster RCNN"], ["PSROI", "Synonym-Of", "position sensitive ROI pooling"]], "rel_plus": [["position sensitive ROI pooling:Method", "Part-Of", "R - FCN:Method"], ["ROI pooling:Method", "Part-Of", "Faster RCNN:Method"], ["PSROI:Method", "Synonym-Of", "position sensitive ROI pooling:Method"]]}
{"doc_id": "21683040", "sentence": "Recently , Deformable Convolutional Network [ 4 ] proposes deformable convolution and deformable PSROI to enhance the RFCN further with better accuracy .", "ner": [["Deformable Convolutional Network", "Method"], ["deformable convolution", "Method"], ["PSROI", "Method"], ["RFCN", "Method"]], "rel": [["deformable convolution", "Part-Of", "Deformable Convolutional Network"], ["PSROI", "Part-Of", "Deformable Convolutional Network"], ["RFCN", "Part-Of", "Deformable Convolutional Network"]], "rel_plus": [["deformable convolution:Method", "Part-Of", "Deformable Convolutional Network:Method"], ["PSROI:Method", "Part-Of", "Deformable Convolutional Network:Method"], ["RFCN:Method", "Part-Of", "Deformable Convolutional Network:Method"]]}
{"doc_id": "21683040", "sentence": "YOLO ( you only look once ) [ 2 5 ] divides the input image into several grids and performs localization and classification on each part of image .", "ner": [["YOLO", "Method"], ["you only look once", "Method"], ["localization", "Task"], ["classification", "Task"]], "rel": [["YOLO", "Synonym-Of", "you only look once"], ["YOLO", "Used-For", "localization"], ["YOLO", "Used-For", "classification"]], "rel_plus": [["YOLO:Method", "Synonym-Of", "you only look once:Method"], ["YOLO:Method", "Used-For", "localization:Task"], ["YOLO:Method", "Used-For", "classification:Task"]]}
{"doc_id": "21683040", "sentence": "Benefited from this method , YOLO can run object detection at a very high speed but the accuracy is not satisfactory enough .", "ner": [["YOLO", "Method"], ["object detection", "Task"]], "rel": [["YOLO", "Used-For", "object detection"]], "rel_plus": [["YOLO:Method", "Used-For", "object detection:Task"]]}
{"doc_id": "21683040", "sentence": "YOLOv 2 [ 2 6 ] is an enhanced version of YOLO and it improves the YOLO by removing the fully connected layers and adopts anchor boxes like the RPN .", "ner": [["YOLOv 2", "Method"], ["YOLO", "Method"], ["YOLO", "Method"], ["fully connected layers", "Method"], ["anchor boxes", "Method"], ["RPN", "Method"]], "rel": [["anchor boxes", "Part-Of", "YOLOv 2"], ["RPN", "Part-Of", "YOLOv 2"], ["YOLOv 2", "SubClass-Of", "YOLO"], ["YOLOv 2", "Compare-With", "YOLO"], ["fully connected layers", "Part-Of", "YOLO"], ["RPN", "SubClass-Of", "anchor boxes"]], "rel_plus": [["anchor boxes:Method", "Part-Of", "YOLOv 2:Method"], ["RPN:Method", "Part-Of", "YOLOv 2:Method"], ["YOLOv 2:Method", "SubClass-Of", "YOLO:Method"], ["YOLOv 2:Method", "Compare-With", "YOLO:Method"], ["fully connected layers:Method", "Part-Of", "YOLO:Method"], ["RPN:Method", "SubClass-Of", "anchor boxes:Method"]]}
{"doc_id": "21683040", "sentence": "Then NMS ( non - maximum suppression ) is used to post - process the final detection results .", "ner": [["NMS", "Method"], ["non - maximum suppression", "Method"], ["detection", "Task"]], "rel": [["non - maximum suppression", "Synonym-Of", "NMS"], ["NMS", "Used-For", "detection"]], "rel_plus": [["non - maximum suppression:Method", "Synonym-Of", "NMS:Method"], ["NMS:Method", "Used-For", "detection:Task"]]}
{"doc_id": "21683040", "sentence": "Because SSD detects objects directly from the plane Con - vNet feature maps , it can achieve real - time object detection and process faster than most of other state - of - the - art object detectors .", "ner": [["SSD", "Method"], ["Con - vNet", "Method"], ["real - time object detection", "Task"], ["object detectors", "Method"]], "rel": [["Con - vNet", "Part-Of", "SSD"], ["SSD", "Used-For", "real - time object detection"]], "rel_plus": [["Con - vNet:Method", "Part-Of", "SSD:Method"], ["SSD:Method", "Used-For", "real - time object detection:Task"]]}
{"doc_id": "21683040", "sentence": "In order to improve the accuracy , DSSD [ 7 ] suggests to augment SSD+ResNet - 1 0 1 with deconvolution layers to introduce additional larges - scale context .", "ner": [["DSSD", "Method"], ["SSD+ResNet - 1 0 1", "Method"], ["deconvolution", "Method"]], "rel": [["deconvolution", "Part-Of", "SSD+ResNet - 1 0 1"]], "rel_plus": [["deconvolution:Method", "Part-Of", "SSD+ResNet - 1 0 1:Method"]]}
{"doc_id": "21683040", "sentence": "RSSD [ 1 5 ] uses rainbow concatenation through both pooling and concate - nation to fully utilize the relationship between the layers in the feature pyramid to enhance the accuracy with a little speed lost .", "ner": [["RSSD", "Method"], ["rainbow concatenation", "Method"], ["pooling", "Method"], ["concate - nation", "Method"], ["feature pyramid", "Method"]], "rel": [["rainbow concatenation", "Part-Of", "RSSD"], ["pooling", "Used-For", "rainbow concatenation"], ["concate - nation", "Used-For", "rainbow concatenation"]], "rel_plus": [["rainbow concatenation:Method", "Part-Of", "RSSD:Method"], ["pooling:Method", "Used-For", "rainbow concatenation:Method"], ["concate - nation:Method", "Used-For", "rainbow concatenation:Method"]]}
{"doc_id": "21683040", "sentence": "DSOD [ 3 0 ] investigates how to train a object detector from scratch and designs a DenseNet architecture to improve the parameter efficiency .", "ner": [["DSOD", "Method"], ["object detector", "Method"], ["DenseNet", "Method"]], "rel": [["DenseNet", "Part-Of", "DSOD"]], "rel_plus": [["DenseNet:Method", "Part-Of", "DSOD:Method"]]}
{"doc_id": "21683040", "sentence": "Algorithms using feature fusion in ConvNet There are a lot of approaches which attempt to use multiple layers ' features to improve the performance in computer vision tasks .", "ner": [["feature fusion", "Method"], ["ConvNet", "Method"], ["computer vision", "Task"]], "rel": [["feature fusion", "Part-Of", "ConvNet"]], "rel_plus": [["feature fusion:Method", "Part-Of", "ConvNet:Method"]]}
{"doc_id": "21683040", "sentence": "HyperNet [ 1 7 ] , Parsenet [ 2 1 ] and ION [ 1 ] concatenate features from multiple layers before predicting the result .", "ner": [["HyperNet", "Method"], ["Parsenet", "Method"], ["ION", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "21683040", "sentence": "FCN [ 2 2 ] , U - Net [ 2 8 ] , Stacked Hourglass networks [ 2 3 ] also use skip connections to associate low - level and high - level feature maps to fully utilize the synthetic information .", "ner": [["FCN", "Method"], ["U - Net", "Method"], ["Stacked Hourglass networks", "Method"], ["skip connections", "Method"]], "rel": [["skip connections", "Part-Of", "FCN"], ["skip connections", "Part-Of", "U - Net"], ["skip connections", "Part-Of", "Stacked Hourglass networks"]], "rel_plus": [["skip connections:Method", "Part-Of", "FCN:Method"], ["skip connections:Method", "Part-Of", "U - Net:Method"], ["skip connections:Method", "Part-Of", "Stacked Hourglass networks:Method"]]}
{"doc_id": "21683040", "sentence": "Conventional SSD regards these features in different levels as the same level and generates the object detection results directly from them .", "ner": [["SSD", "Method"], ["object detection", "Task"]], "rel": [["SSD", "Used-For", "object detection"]], "rel_plus": [["SSD:Method", "Used-For", "object detection:Task"]]}
{"doc_id": "21683040", "sentence": "This type of feature fusion is used in FPN [ 1 8 ] and DSSD [ 7 ] and is verified to improve the conventional detector 's performance a lot .", "ner": [["feature fusion", "Method"], ["FPN", "Method"], ["DSSD", "Method"]], "rel": [["feature fusion", "Part-Of", "FPN"], ["feature fusion", "Part-Of", "DSSD"]], "rel_plus": [["feature fusion:Method", "Part-Of", "FPN:Method"], ["feature fusion:Method", "Part-Of", "DSSD:Method"]]}
{"doc_id": "21683040", "sentence": "In the conventional SSD 3 0 0 based on VGG 1 6 , the author chooses conv 4 3 , fc 7 of the VGG 1 6 and new added layer conv 6 2 , conv 7 2 , conv 8 2 , conv 9 2 to generate features to process object detections .", "ner": [["SSD 3 0 0", "Method"], ["VGG 1 6", "Method"], ["conv 4 3", "Method"], ["VGG 1 6", "Method"], ["conv 6 2", "Method"], ["conv 7", "Method"], ["conv 8 2", "Method"], ["conv 9 2", "Method"]], "rel": [["VGG 1 6", "Part-Of", "SSD 3 0 0"], ["conv 4 3", "Part-Of", "VGG 1 6"], ["conv 6 2", "Part-Of", "VGG 1 6"], ["conv 7", "Part-Of", "VGG 1 6"], ["conv 8 2", "Part-Of", "VGG 1 6"], ["conv 9 2", "Part-Of", "VGG 1 6"]], "rel_plus": [["VGG 1 6:Method", "Part-Of", "SSD 3 0 0:Method"], ["conv 4 3:Method", "Part-Of", "VGG 1 6:Method"], ["conv 6 2:Method", "Part-Of", "VGG 1 6:Method"], ["conv 7:Method", "Part-Of", "VGG 1 6:Method"], ["conv 8 2:Method", "Part-Of", "VGG 1 6:Method"], ["conv 9 2:Method", "Part-Of", "VGG 1 6:Method"]]}
{"doc_id": "21683040", "sentence": "In ( b ) , we only detect objects on the feature maps after the fusion feature map . \u03c6 p : Adopted from the conventional SSD , we also use the pyramid feature map to generate object detection results .", "ner": [["SSD", "Method"], ["object detection", "Task"]], "rel": [["SSD", "Used-For", "object detection"]], "rel_plus": [["SSD:Method", "Used-For", "object detection:Task"]]}
{"doc_id": "21683040", "sentence": "Firstly , because our FSSD is based on the SSD model , we can adopt the well trained SSD model as our pre - trained model .", "ner": [["FSSD", "Method"], ["SSD", "Method"], ["SSD", "Method"]], "rel": [["SSD", "Part-Of", "FSSD"]], "rel_plus": [["SSD:Method", "Part-Of", "FSSD:Method"]]}
{"doc_id": "21683040", "sentence": "SSD means that training the conventional SSD model with the default settings from a pre - trained VGG 1 6 model .", "ner": [["SSD", "Method"], ["SSD", "Method"], ["VGG 1 6", "Method"]], "rel": [["VGG 1 6", "Part-Of", "SSD"]], "rel_plus": [["VGG 1 6:Method", "Part-Of", "SSD:Method"]]}
{"doc_id": "21683040", "sentence": "FSSD means that we train the FSSD model with a pre - trained VGG model .", "ner": [["FSSD", "Method"], ["FSSD", "Method"], ["VGG", "Method"]], "rel": [["VGG", "Part-Of", "FSSD"]], "rel_plus": [["VGG:Method", "Part-Of", "FSSD:Method"]]}
{"doc_id": "21683040", "sentence": "FSSD 's training parameters are the same with SSD .", "ner": [["FSSD", "Method"], ["SSD", "Method"]], "rel": [["FSSD", "Compare-With", "SSD"]], "rel_plus": [["FSSD:Method", "Compare-With", "SSD:Method"]]}
{"doc_id": "21683040", "sentence": "FSSD+ means that we train the FSSD from a pre - trained SSD model .", "ner": [["FSSD+", "Method"], ["FSSD", "Method"], ["SSD", "Method"]], "rel": [["SSD", "Part-Of", "FSSD+"]], "rel_plus": [["SSD:Method", "Part-Of", "FSSD+:Method"]]}
{"doc_id": "21683040", "sentence": "Another way to train the FSSD is the same as the conventional SSD algorithm .", "ner": [["FSSD", "Method"], ["SSD", "Method"]], "rel": [["FSSD", "Compare-With", "SSD"]], "rel_plus": [["FSSD:Method", "Compare-With", "SSD:Method"]]}
{"doc_id": "21683040", "sentence": "According to the experiments in Table 2 ( rows 2 and 5 ) , these two ways have little difference on the final result but training FSSD from VGG 1 6 has a slightly better results than the one which is trained from SSD model .", "ner": [["FSSD", "Method"], ["VGG 1 6", "Method"], ["SSD", "Method"]], "rel": [["VGG 1 6", "Part-Of", "FSSD"], ["SSD", "Part-Of", "FSSD"]], "rel_plus": [["VGG 1 6:Method", "Part-Of", "FSSD:Method"], ["SSD:Method", "Part-Of", "FSSD:Method"]]}
{"doc_id": "21683040", "sentence": "But as illustrated in Fig. 4 , training FSSD from SSD models can converge faster than from the pre - trained VGGNet model .", "ner": [["FSSD", "Method"], ["SSD", "Method"], ["VGGNet", "Method"]], "rel": [["SSD", "Part-Of", "FSSD"], ["VGGNet", "Part-Of", "FSSD"]], "rel_plus": [["SSD:Method", "Part-Of", "FSSD:Method"], ["VGGNet:Method", "Part-Of", "FSSD:Method"]]}
{"doc_id": "21683040", "sentence": "Besides , our FSSD also coverages faster than the conventional SSD while having the same training hyper - parameters .", "ner": [["FSSD", "Method"], ["SSD", "Method"]], "rel": [["FSSD", "Compare-With", "SSD"]], "rel_plus": [["FSSD:Method", "Compare-With", "SSD:Method"]]}
{"doc_id": "21683040", "sentence": "We use the center code type to encode the bounding boxes and have the same matching strategy , hard negative mining strategy and data augmentation with SSD .", "ner": [["data augmentation", "Method"], ["SSD", "Method"]], "rel": [["data augmentation", "Used-For", "SSD"]], "rel_plus": [["data augmentation:Method", "Used-For", "SSD:Method"]]}
{"doc_id": "21683040", "sentence": "In order to compare our FSSD with the conventional SSD fairly , our experiments are all based on VGG 1 6 [ 3 1 ] which is preprocessed like SSD [ 2 0 ] .", "ner": [["FSSD", "Method"], ["SSD", "Method"], ["VGG 1 6", "Method"], ["SSD", "Method"]], "rel": [["VGG 1 6", "Part-Of", "FSSD"], ["FSSD", "Compare-With", "SSD"]], "rel_plus": [["VGG 1 6:Method", "Part-Of", "FSSD:Method"], ["FSSD:Method", "Compare-With", "SSD:Method"]]}
{"doc_id": "21683040", "sentence": "We conduct experiments on PASCAL VOC 2 0 0 7 , 2 0 1 2 [ 6 ] and MS COCO dataset [ 1 9 ] .", "ner": [["PASCAL VOC 2 0 0 7 , 2 0 1 2", "Dataset"], ["MS COCO", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "21683040", "sentence": "In VOC 2 0 0 7 and VOC 2 0 1 2 , a predicted bounding box is correct if its intersection over union ( IOU ) with the ground truth is higher than 0. 5 .", "ner": [["VOC 2 0 0 7", "Dataset"], ["VOC 2 0 1 2", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "21683040", "sentence": "In these experiments , the models are trained with the combined dataset from 2 0 0 7 trainval and 2 0 1 2 trainval ( VOC 0 7 + 1 2 ) and tested on VOC 2 0 0 7 test set .", "ner": [["VOC 0 7 + 1 2", "Dataset"], ["VOC 2 0 0 7", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "21683040", "sentence": "There are two kinds of blocks to generate lower resolution feature maps , simple block ( one Conv 3 \u00d7 3 followed by a ReLU ) and bottleneck block , which uses Conv 1 \u00d7 1 to reduce the feature dimension first and is adopted by the conventional SSD .", "ner": [["Conv 3 \u00d7 3", "Method"], ["ReLU", "Method"], ["Conv 1 \u00d7 1", "Method"], ["SSD", "Method"]], "rel": [["Conv 1 \u00d7 1", "Part-Of", "SSD"]], "rel_plus": [["Conv 1 \u00d7 1:Method", "Part-Of", "SSD:Method"]]}
{"doc_id": "21683040", "sentence": "Experimental setup According to the ablation study in Section 4. 1 , the architecture of our FSSD is defined as follows : For FSSD with 3 0 0 \u00d7 3 0 0 input(FSSD 3 0 0 ) , we adopt VGG 1 6 as the backbone network .", "ner": [["FSSD", "Method"], ["FSSD", "Method"], ["input(FSSD 3 0 0 )", "Method"], ["VGG 1 6", "Method"]], "rel": [["VGG 1 6", "Part-Of", "FSSD"], ["VGG 1 6", "Part-Of", "input(FSSD 3 0 0 )"]], "rel_plus": [["VGG 1 6:Method", "Part-Of", "FSSD:Method"], ["VGG 1 6:Method", "Part-Of", "input(FSSD 3 0 0 ):Method"]]}
{"doc_id": "21683040", "sentence": "Then several down - sampling blocks(including one 3 \u00d7 3 convolutional layer with stride 2 and one ReLU layer ) are appended one by one to generate the pyramid features .", "ner": [["3 \u00d7 3 convolutional layer", "Method"], ["ReLU", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "21683040", "sentence": "We use VOC 2 0 0 7 trainval and VOC 2 0 1 2 trainval to train FSSD following SSD [ 2 0 ] .", "ner": [["VOC 2 0 0 7", "Dataset"], ["VOC 2 0 1 2", "Dataset"], ["FSSD", "Method"], ["SSD", "Method"]], "rel": [["FSSD", "Trained-With", "VOC 2 0 0 7"], ["SSD", "Trained-With", "VOC 2 0 0 7"], ["FSSD", "Trained-With", "VOC 2 0 1 2"], ["SSD", "Trained-With", "VOC 2 0 1 2"]], "rel_plus": [["FSSD:Method", "Trained-With", "VOC 2 0 0 7:Dataset"], ["SSD:Method", "Trained-With", "VOC 2 0 0 7:Dataset"], ["FSSD:Method", "Trained-With", "VOC 2 0 1 2:Dataset"], ["SSD:Method", "Trained-With", "VOC 2 0 1 2:Dataset"]]}
{"doc_id": "21683040", "sentence": "The initial learning rate is set to 0.0 0 1 and then divided by 1 0 at step 8 0 k , 1 0 0 k and 1 2 0 k. Following the training strategy in SSD [ 2 0 ] , the weight decay is set to 0.0 0 0 5 .", "ner": [["SSD", "Method"], ["weight decay", "Method"]], "rel": [["weight decay", "Part-Of", "SSD"]], "rel_plus": [["weight decay:Method", "Part-Of", "SSD:Method"]]}
{"doc_id": "21683040", "sentence": "We adopt a SGD with momentum 0. 9 to optimize the FSSD which is initialized by a well pre - trained VGG 1 6 on ImageNet .", "ner": [["SGD", "Method"], ["momentum", "Method"], ["FSSD", "Method"], ["VGG 1 6", "Method"], ["ImageNet", "Dataset"]], "rel": [["momentum", "Part-Of", "SGD"], ["SGD", "Part-Of", "FSSD"], ["VGG 1 6", "Part-Of", "FSSD"], ["VGG 1 6", "Trained-With", "ImageNet"]], "rel_plus": [["momentum:Method", "Part-Of", "SGD:Method"], ["SGD:Method", "Part-Of", "FSSD:Method"], ["VGG 1 6:Method", "Part-Of", "FSSD:Method"], ["VGG 1 6:Method", "Trained-With", "ImageNet:Dataset"]]}
{"doc_id": "21683040", "sentence": "In order to use COCO models as the pre - trained model , we first train the COCO model with 8 0 classes which will be described in details in Section 4 .", "ner": [["COCO", "Dataset"], ["COCO", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "21683040", "sentence": "SSD 3 0 0 S \u2020 indicates training SSD 3 0 0 from scratch VGGNet , which is tested in DSOD [ 3 0 ] .", "ner": [["SSD 3 0 0 S", "Method"], ["SSD 3 0 0", "Method"], ["VGGNet", "Method"], ["DSOD", "Method"]], "rel": [["VGGNet", "Part-Of", "SSD 3 0 0"]], "rel_plus": [["VGGNet:Method", "Part-Of", "SSD 3 0 0:Method"]]}
{"doc_id": "21683040", "sentence": "FSSD 3 0 0 S \u2020 is also trained from scratch VGGNet .", "ner": [["FSSD 3 0 0 S", "Method"], ["VGGNet", "Method"]], "rel": [["VGGNet", "Part-Of", "FSSD 3 0 0 S"]], "rel_plus": [["VGGNet:Method", "Part-Of", "FSSD 3 0 0 S:Method"]]}
{"doc_id": "21683040", "sentence": "ResNet - 1 0 1 as the backbone network , which has better performance compared with VGG 1 6 .", "ner": [["ResNet - 1 0 1", "Method"], ["VGG 1 6", "Method"]], "rel": [["ResNet - 1 0 1", "Compare-With", "VGG 1 6"]], "rel_plus": [["ResNet - 1 0 1:Method", "Compare-With", "VGG 1 6:Method"]]}
{"doc_id": "21683040", "sentence": "With COCO as the training data , FSSD 3 0 0 's performance can be further increased to 8 2 . 7 % , which exceeds DSOD by 1% and SSD 3 0 0 by 1. 5 % .", "ner": [["COCO", "Dataset"], ["FSSD 3 0 0", "Method"], ["DSOD", "Method"], ["SSD 3 0 0", "Method"]], "rel": [["FSSD 3 0 0", "Trained-With", "COCO"], ["FSSD 3 0 0", "Compare-With", "DSOD"], ["FSSD 3 0 0", "Compare-With", "SSD 3 0 0"]], "rel_plus": [["FSSD 3 0 0:Method", "Trained-With", "COCO:Dataset"], ["FSSD 3 0 0:Method", "Compare-With", "DSOD:Method"], ["FSSD 3 0 0:Method", "Compare-With", "SSD 3 0 0:Method"]]}
{"doc_id": "21683040", "sentence": "Our FSSD 5 1 2 also improves the SSD 5 1 2 from 7 9 . 8 % to 8 0 . 9 % , which is also a little higher than RSSD 5 1 2 .", "ner": [["FSSD 5 1 2", "Method"], ["SSD 5 1 2", "Method"], ["RSSD 5 1 2", "Method"]], "rel": [["FSSD 5 1 2", "Compare-With", "SSD 5 1 2"], ["FSSD 5 1 2", "Compare-With", "RSSD 5 1 2"]], "rel_plus": [["FSSD 5 1 2:Method", "Compare-With", "SSD 5 1 2:Method"], ["FSSD 5 1 2:Method", "Compare-With", "RSSD 5 1 2:Method"]]}
{"doc_id": "21683040", "sentence": "DSSD 5 1 2 is better than our FSSD 5 1 2 with the aspect of accuracy but we think the ResNet - 1 0 1 backbone plays a crit - ical role in this progress .", "ner": [["DSSD 5 1 2", "Method"], ["FSSD 5 1 2", "Method"], ["ResNet - 1 0 1", "Method"]], "rel": [["ResNet - 1 0 1", "Part-Of", "DSSD 5 1 2"], ["DSSD 5 1 2", "Compare-With", "FSSD 5 1 2"]], "rel_plus": [["ResNet - 1 0 1:Method", "Part-Of", "DSSD 5 1 2:Method"], ["DSSD 5 1 2:Method", "Compare-With", "FSSD 5 1 2:Method"]]}
{"doc_id": "21683040", "sentence": "However , our FSSD 5 1 2 is much faster than DSSD 5 1 2 .", "ner": [["FSSD 5 1 2", "Method"], ["DSSD 5 1 2", "Method"]], "rel": [["FSSD 5 1 2", "Compare-With", "DSSD 5 1 2"]], "rel_plus": [["FSSD 5 1 2:Method", "Compare-With", "DSSD 5 1 2:Method"]]}
{"doc_id": "21683040", "sentence": "The conventional SSD with VGG 1 6 can only achieve 6 9 . 6 % mAP without the pre - trained VGGNet model .", "ner": [["SSD", "Method"], ["VGG 1 6", "Method"], ["VGGNet", "Method"]], "rel": [["VGG 1 6", "Part-Of", "SSD"]], "rel_plus": [["VGG 1 6:Method", "Part-Of", "SSD:Method"]]}
{"doc_id": "21683040", "sentence": "We also investigate whether our FSSD can improve the conventional SSD .", "ner": [["FSSD", "Method"], ["SSD", "Method"]], "rel": [["FSSD", "Compare-With", "SSD"]], "rel_plus": [["FSSD:Method", "Compare-With", "SSD:Method"]]}
{"doc_id": "21683040", "sentence": "FSSD 3 0 0 with COCO can achieve 8 2 . 0 % mAP , which is higher than the conventional SSD( 7 9 . 3 % ) by 2. 7 points .", "ner": [["FSSD 3 0 0", "Method"], ["COCO", "Dataset"], ["SSD(", "Method"]], "rel": [["FSSD 3 0 0", "Evaluated-With", "COCO"], ["SSD(", "Evaluated-With", "COCO"], ["FSSD 3 0 0", "Compare-With", "SSD("]], "rel_plus": [["FSSD 3 0 0:Method", "Evaluated-With", "COCO:Dataset"], ["SSD(:Method", "Evaluated-With", "COCO:Dataset"], ["FSSD 3 0 0:Method", "Compare-With", "SSD(:Method"]]}
{"doc_id": "21683040", "sentence": "In addition , our FSSD 5 1 2 can achieve 8 4 . 2 % mAP , which exceeds the conventional SSD( 8 2 . 2 % ) by 2 points .", "ner": [["FSSD 5 1 2", "Method"], ["SSD(", "Method"]], "rel": [["FSSD 5 1 2", "Compare-With", "SSD("]], "rel_plus": [["FSSD 5 1 2:Method", "Compare-With", "SSD(:Method"]]}
{"doc_id": "21683040", "sentence": "The FSSD 5 1 2 takes the first place in VOC 2 0 1 2 leaderboard among all of the one - stage object detectors as of the time of submission .", "ner": [["FSSD 5 1 2", "Method"], ["VOC 2 0 1 2", "Dataset"]], "rel": [["FSSD 5 1 2", "Evaluated-With", "VOC 2 0 1 2"]], "rel_plus": [["FSSD 5 1 2:Method", "Evaluated-With", "VOC 2 0 1 2:Dataset"]]}
{"doc_id": "21683040", "sentence": "For training FSSD 3 0 0 , the learning rate is set to 0.0 0 1 for the first 2 8 0 k iterations , then divided by 1 0 at the step 3 6 0 k and 4 0 0 k. But if we train the FSSD 3 0 0 from a well trained SSD model , it will only need 1 2 0 k totally to make the FSSD 3 0 0 converge well .", "ner": [["FSSD 3 0 0", "Method"], ["FSSD 3 0 0", "Method"], ["SSD", "Method"], ["FSSD 3 0 0", "Method"]], "rel": [["SSD", "Part-Of", "FSSD 3 0 0"]], "rel_plus": [["SSD:Method", "Part-Of", "FSSD 3 0 0:Method"]]}
{"doc_id": "21683040", "sentence": "For training FSSD 5 1 2 , the learning rate is set to 0.0 0 1 for the first 2 8 0 k iterations and then divided by 1 0 at the step 3 2 0 k and 3 6 0 k. The COCO test results are shown in Table 5 .", "ner": [["FSSD 5 1 2", "Method"], ["COCO", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "21683040", "sentence": "FSSD 3 0 0 achieves 2 7 . 1 % on the test - dev set , which is higher than the SSD 3 0 0 *( 2 5 . 1 % ) with a large margin .", "ner": [["FSSD", "Method"], ["SSD 3 0 0", "Method"]], "rel": [["FSSD", "Compare-With", "SSD 3 0 0"]], "rel_plus": [["FSSD:Method", "Compare-With", "SSD 3 0 0:Method"]]}
{"doc_id": "21683040", "sentence": "Even though our FSSD does not perform as well as DSOD and DSSD , it should be noted that our base model is VGG 1 6 and FSSD has the best accuracy compared with other algorithms such Figure 5 .", "ner": [["FSSD", "Method"], ["DSOD", "Method"], ["DSSD", "Method"], ["VGG 1 6", "Method"], ["FSSD", "Method"]], "rel": [["VGG 1 6", "Part-Of", "FSSD"], ["FSSD", "Compare-With", "DSOD"], ["FSSD", "Compare-With", "DSSD"]], "rel_plus": [["VGG 1 6:Method", "Part-Of", "FSSD:Method"], ["FSSD:Method", "Compare-With", "DSOD:Method"], ["FSSD:Method", "Compare-With", "DSSD:Method"]]}
{"doc_id": "21683040", "sentence": "As we do not have a Titan X GPU , the speed of our FSSDs is calculated by comparing with SSD 's speed which we have tested on our own Nvidia 1 0 8 0 Ti . as Faster RCNN and ION in Table 5 ( rows 1 and 2 ) which is also based on VGGNet .", "ner": [["FSSDs", "Method"], ["SSD", "Method"], ["Faster RCNN", "Method"], ["ION", "Method"], ["VGGNet", "Method"]], "rel": [["FSSDs", "Compare-With", "SSD"], ["VGGNet", "Part-Of", "Faster RCNN"], ["VGGNet", "Part-Of", "ION"]], "rel_plus": [["FSSDs:Method", "Compare-With", "SSD:Method"], ["VGGNet:Method", "Part-Of", "Faster RCNN:Method"], ["VGGNet:Method", "Part-Of", "ION:Method"]]}
{"doc_id": "21683040", "sentence": "Besides , FSSD 5 1 2 ( 3 1 . 8 % ) outperforms conventional SSD( 2 8 . 8 % ) by 3 points .", "ner": [["FSSD 5 1 2", "Method"], ["SSD(", "Method"]], "rel": [["FSSD 5 1 2", "Compare-With", "SSD("]], "rel_plus": [["FSSD 5 1 2:Method", "Compare-With", "SSD(:Method"]]}
{"doc_id": "21683040", "sentence": "Even though our FSSD 5 1 2 is slightly lower than DSSD 5 1 3 , it should be noted that FSSD 's mAP on small objects is still higher than DSSD 5 1 3 , which proves that our feature fusion module is more powerful than DSSD 's FPN module on small objects ' detection .", "ner": [["FSSD 5 1 2", "Method"], ["DSSD 5 1 3", "Method"], ["FSSD", "Method"], ["DSSD 5 1 3", "Method"], ["feature fusion module", "Method"], ["DSSD", "Method"], ["FPN", "Method"]], "rel": [["feature fusion module", "Part-Of", "FSSD 5 1 2"], ["FSSD 5 1 2", "Compare-With", "DSSD 5 1 3"], ["FSSD", "Compare-With", "DSSD 5 1 3"], ["FPN", "Part-Of", "DSSD"]], "rel_plus": [["feature fusion module:Method", "Part-Of", "FSSD 5 1 2:Method"], ["FSSD 5 1 2:Method", "Compare-With", "DSSD 5 1 3:Method"], ["FSSD:Method", "Compare-With", "DSSD 5 1 3:Method"], ["FPN:Method", "Part-Of", "DSSD:Method"]]}
{"doc_id": "21683040", "sentence": "Because our FSSD adds additional layers on the SSD model , our FSSD consumes about 2 5 % extra time .", "ner": [["FSSD", "Method"], ["SSD", "Method"], ["FSSD", "Method"]], "rel": [["SSD", "Part-Of", "FSSD"]], "rel_plus": [["SSD:Method", "Part-Of", "FSSD:Method"]]}
{"doc_id": "21683040", "sentence": "But compared with DSSD and DSOD , our methods are still much faster than them while the improvement from the SSD is about the same level .", "ner": [["DSSD", "Method"], ["DSOD", "Method"], ["SSD", "Method"]], "rel": [["DSSD", "Compare-With", "DSOD"]], "rel_plus": [["DSSD:Method", "Compare-With", "DSOD:Method"]]}
{"doc_id": "21683040", "sentence": "In Fig. 5 , it is clear that our FSSD is faster than most of the object detection algorithms while having the competitive accuracy .   Our FSSD performs better than conventional SSD mainly in two aspects .", "ner": [["FSSD", "Method"], ["object detection", "Task"], ["FSSD", "Method"], ["SSD", "Method"]], "rel": [["FSSD", "Used-For", "object detection"], ["FSSD", "Compare-With", "SSD"]], "rel_plus": [["FSSD:Method", "Used-For", "object detection:Task"], ["FSSD:Method", "Compare-With", "SSD:Method"]]}
{"doc_id": "21683040", "sentence": "Because SSD only detects small objects from the shallow layers such as conv 4 3 , whose receptive field is too small to observe the lager context information , which leads to the SSD 's bad performance on small objects .", "ner": [["SSD", "Method"], ["conv 4 3", "Method"], ["SSD", "Method"]], "rel": [["conv 4 3", "Part-Of", "SSD"]], "rel_plus": [["conv 4 3:Method", "Part-Of", "SSD:Method"]]}
{"doc_id": "21683040", "sentence": "FSSD detects more small objects than SSD successfully .", "ner": [["FSSD", "Method"], ["SSD", "Method"]], "rel": [["FSSD", "Compare-With", "SSD"]], "rel_plus": [["FSSD:Method", "Compare-With", "SSD:Method"]]}
{"doc_id": "21683040", "sentence": "In this paper , We proposed FSSD , an enhanced SSD by applying a lightweight and efficient feature fusion module on it .", "ner": [["FSSD", "Method"], ["SSD", "Method"], ["feature fusion module", "Method"]], "rel": [["feature fusion module", "Part-Of", "FSSD"], ["FSSD", "SubClass-Of", "SSD"]], "rel_plus": [["feature fusion module:Method", "Part-Of", "FSSD:Method"], ["FSSD:Method", "SubClass-Of", "SSD:Method"]]}
{"doc_id": "21683040", "sentence": "Experiments on VOC PASCAL and MSCOCO prove that our FSSD improves the conventional SSD a lot and outperforms several other state - of - the - art object detectors both in accuracy and efficiency without any bells and whistles .", "ner": [["VOC PASCAL", "Dataset"], ["MSCOCO", "Dataset"], ["FSSD", "Method"], ["SSD", "Method"], ["object detectors", "Method"]], "rel": [["FSSD", "Evaluated-With", "VOC PASCAL"], ["SSD", "Evaluated-With", "VOC PASCAL"], ["FSSD", "Evaluated-With", "MSCOCO"], ["SSD", "Evaluated-With", "MSCOCO"], ["FSSD", "Compare-With", "SSD"], ["FSSD", "Compare-With", "object detectors"]], "rel_plus": [["FSSD:Method", "Evaluated-With", "VOC PASCAL:Dataset"], ["SSD:Method", "Evaluated-With", "VOC PASCAL:Dataset"], ["FSSD:Method", "Evaluated-With", "MSCOCO:Dataset"], ["SSD:Method", "Evaluated-With", "MSCOCO:Dataset"], ["FSSD:Method", "Compare-With", "SSD:Method"], ["FSSD:Method", "Compare-With", "object detectors:Method"]]}
{"doc_id": "21683040", "sentence": "In the future , it is worth enhancing our FSSD with much stronger backbone networks such as ResNet [ 1 2 ] and DenseNet [ 1 3 ] to get better performance on the MSCOCO dataset and replacing the FPN in Mask RCNN [ 1 0 ] with our feature fusion module is also an interesting research field .", "ner": [["FSSD", "Method"], ["ResNet", "Method"], ["DenseNet", "Method"], ["MSCOCO", "Dataset"], ["FPN", "Method"], ["Mask RCNN", "Method"], ["feature fusion module", "Method"]], "rel": [["ResNet", "Part-Of", "FSSD"], ["DenseNet", "Part-Of", "FSSD"], ["FSSD", "Evaluated-With", "MSCOCO"], ["FPN", "Part-Of", "Mask RCNN"], ["feature fusion module", "Part-Of", "Mask RCNN"]], "rel_plus": [["ResNet:Method", "Part-Of", "FSSD:Method"], ["DenseNet:Method", "Part-Of", "FSSD:Method"], ["FSSD:Method", "Evaluated-With", "MSCOCO:Dataset"], ["FPN:Method", "Part-Of", "Mask RCNN:Method"], ["feature fusion module:Method", "Part-Of", "Mask RCNN:Method"]]}
{"doc_id": "11241677", "sentence": "Although large - scale datasets exist for image understanding , such as ImageNet , there are no comparable size video classification datasets .    In this paper , we introduce YouTube - 8 M , the largest multi - label video classification dataset , composed of ~ 8 million videos ( 5 0 0 K hours of video ) , annotated with a vocabulary of 4 8 0 0 visual entities .", "ner": [["image understanding", "Task"], ["ImageNet", "Dataset"], ["video classification", "Task"], ["YouTube - 8 M", "Dataset"], ["multi - label video classification", "Task"]], "rel": [["ImageNet", "Benchmark-For", "image understanding"], ["YouTube - 8 M", "Benchmark-For", "multi - label video classification"]], "rel_plus": [["ImageNet:Dataset", "Benchmark-For", "image understanding:Task"], ["YouTube - 8 M:Dataset", "Benchmark-For", "multi - label video classification:Task"]]}
{"doc_id": "11241677", "sentence": "Then , we decoded each video at one - frame - per - second , and used a Deep CNN pre - trained on ImageNet to extract the hidden representation immediately prior to the classification layer .", "ner": [["Deep CNN", "Method"], ["ImageNet", "Dataset"]], "rel": [["Deep CNN", "Trained-With", "ImageNet"]], "rel_plus": [["Deep CNN:Method", "Trained-With", "ImageNet:Dataset"]]}
{"doc_id": "11241677", "sentence": "Large - scale datasets such as ImageNet [ 6 ] have been key enablers of recent progress in image understanding [ 2 0 , 1 4 , 1 1 ] .", "ner": [["ImageNet", "Dataset"], ["image understanding", "Task"]], "rel": [["ImageNet", "Benchmark-For", "image understanding"]], "rel_plus": [["ImageNet:Dataset", "Benchmark-For", "image understanding:Task"]]}
{"doc_id": "11241677", "sentence": "Furthermore , intermediate layer activations of such networks have proven to be powerful and interpretable for vari - Figure 1 : YouTube - 8 M is a large - scale benchmark for general multi - label video classification .", "ner": [["YouTube - 8 M", "Dataset"], ["multi - label video classification", "Task"]], "rel": [["YouTube - 8 M", "Benchmark-For", "multi - label video classification"]], "rel_plus": [["YouTube - 8 M:Dataset", "Benchmark-For", "multi - label video classification:Task"]]}
{"doc_id": "11241677", "sentence": "The dataset explorer allows browsing and searching of the full vocabulary of Knowledge Graph entities , grouped in 2 4 top - level verticals , along with corresponding videos . ous tasks beyond classification [ 4 1 , 9 , 3 1 ] .", "ner": [["Knowledge Graph", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "11241677", "sentence": "In a similar vein , the amount and size of video benchmarks is growing with the availability of Sports - 1 M [ 1 9 ] for sports videos and ActivityNet [ 1 2 ] for human activities .", "ner": [["Sports - 1 M", "Dataset"], ["ActivityNet", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "11241677", "sentence": "In this paper , we introduce YouTube - 8 M 1 , a large - scale benchmark dataset for general multi - label video classification .", "ner": [["YouTube - 8 M", "Dataset"], ["multi - label video classification", "Task"]], "rel": [["YouTube - 8 M", "Benchmark-For", "multi - label video classification"]], "rel_plus": [["YouTube - 8 M:Dataset", "Benchmark-For", "multi - label video classification:Task"]]}
{"doc_id": "11241677", "sentence": "Therefore , unlike Sports - 1 M and ActivityNet , YouTube - 8 M is not restricted to action classes alone .", "ner": [["Sports - 1 M", "Dataset"], ["ActivityNet", "Dataset"], ["YouTube - 8 M", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "11241677", "sentence": "We hope that the unprecedented scale and diversity of this dataset will be a useful resource for developing advanced video understanding and representation learning techniques .", "ner": [["video understanding", "Task"], ["representation learning", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "11241677", "sentence": "Towards this end , we provide extensive experiments comparing several state - of - the - art techniques for video representation learning , including Deep Networks [ 2 6 ] , and LSTMs ( Long Short - Term Memory Networks ) [ 1 3 ] on this dataset .", "ner": [["video representation learning", "Task"], ["Deep Networks", "Method"], ["LSTMs", "Method"], ["Long Short - Term Memory Networks", "Method"]], "rel": [["Deep Networks", "Used-For", "video representation learning"], ["LSTMs", "Used-For", "video representation learning"], ["LSTMs", "Synonym-Of", "Long Short - Term Memory Networks"]], "rel_plus": [["Deep Networks:Method", "Used-For", "video representation learning:Task"], ["LSTMs:Method", "Used-For", "video representation learning:Task"], ["LSTMs:Method", "Synonym-Of", "Long Short - Term Memory Networks:Method"]]}
{"doc_id": "11241677", "sentence": "In addition , we show that transfering video feature representations learned on this dataset leads to significant improvements on other benchmarks such as Sports - 1 M and ActivityNet .", "ner": [["Sports - 1 M", "Dataset"], ["ActivityNet", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "11241677", "sentence": "Image benchmarks have played a significant role in advancing computer vision algorithms for image understanding .", "ner": [["computer vision", "Task"], ["image understanding", "Task"]], "rel": [["image understanding", "SubClass-Of", "computer vision"]], "rel_plus": [["image understanding:Task", "SubClass-Of", "computer vision:Task"]]}
{"doc_id": "11241677", "sentence": "Starting from a number of well labeled small - scale datasets such as Caltech 1 0 1 / 2 5 6 [ 8 , 1 0 ] , MSRC [ 3 2 ] , PASCAL [ 7 ] , image understanding research has rapidly advanced to utilizing larger datasets such as ImageNet [ 6 ] and SUN [ 3 8 ] for the next generation of vision algorithms .", "ner": [["Caltech 1 0 1 / 2 5 6", "Dataset"], ["MSRC", "Dataset"], ["PASCAL", "Dataset"], ["image understanding", "Task"], ["ImageNet", "Dataset"], ["SUN", "Dataset"]], "rel": [["ImageNet", "Benchmark-For", "image understanding"], ["SUN", "Benchmark-For", "image understanding"]], "rel_plus": [["ImageNet:Dataset", "Benchmark-For", "image understanding:Task"], ["SUN:Dataset", "Benchmark-For", "image understanding:Task"]]}
{"doc_id": "11241677", "sentence": "ImageNet in particular has enabled the development of deep feature learning techniques with millions of parameters such as the AlexNet [ 2 0 ] and Inception [ 1 4 ] architectures due to the number of classes ( 2 1 8 4 1 ) , the diversity of the classes ( 2 7 top - level categories ) and the millions of labeled images available .", "ner": [["ImageNet", "Dataset"], ["AlexNet", "Method"], ["Inception", "Method"]], "rel": [["ImageNet", "Used-For", "AlexNet"], ["ImageNet", "Used-For", "Inception"]], "rel_plus": [["ImageNet:Dataset", "Used-For", "AlexNet:Method"], ["ImageNet:Dataset", "Used-For", "Inception:Method"]]}
{"doc_id": "11241677", "sentence": "A similar effort is in progress in the video understanding domain where the community has quickly progressed from small , well - labeled datasets such as KTH [ 2 2 ] , Hollywood 2 [ 2 3 ] , Weizmann [ 5 ] , with a few thousand video clips , to medium - scale datasets such as UCF 1 0 1 [ 3 3 ] , Thumos' 1 4 [ 1 6 ] and HMDB 5 1 [ 2 1 ] , with more than 5 0 action categories .", "ner": [["video understanding", "Task"], ["KTH", "Dataset"], ["Hollywood 2", "Dataset"], ["Weizmann", "Dataset"], ["UCF 1 0 1", "Dataset"], ["Thumos' 1 4", "Dataset"], ["HMDB 5 1", "Dataset"]], "rel": [["KTH", "Benchmark-For", "video understanding"], ["Hollywood 2", "Benchmark-For", "video understanding"], ["Weizmann", "Benchmark-For", "video understanding"], ["UCF 1 0 1", "Benchmark-For", "video understanding"], ["Thumos' 1 4", "Benchmark-For", "video understanding"], ["HMDB 5 1", "Benchmark-For", "video understanding"]], "rel_plus": [["KTH:Dataset", "Benchmark-For", "video understanding:Task"], ["Hollywood 2:Dataset", "Benchmark-For", "video understanding:Task"], ["Weizmann:Dataset", "Benchmark-For", "video understanding:Task"], ["UCF 1 0 1:Dataset", "Benchmark-For", "video understanding:Task"], ["Thumos' 1 4:Dataset", "Benchmark-For", "video understanding:Task"], ["HMDB 5 1:Dataset", "Benchmark-For", "video understanding:Task"]]}
{"doc_id": "11241677", "sentence": "Currently , the largest available video benchmarks are the Sports - 1 M [ 1 9 ] , with 4 8 7 sports related activities and 1M videos , the YFCC - 1 0 0 M [ 3 4 ] , with 8 0 0 K videos and raw metadata ( titles , descriptions , tags ) for some of them , the FCVID [ 1 7 ] dataset of 9 1 , 2 2 3 videos manually annotated with 2 3 9 categories , and ActivityNet [ 1 2 ] , with \u223c 2 0 0 human activity classes and a few thousand videos .", "ner": [["Sports - 1 M", "Dataset"], ["YFCC - 1 0 0 M", "Dataset"], ["FCVID", "Dataset"], ["ActivityNet", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "11241677", "sentence": "YouTube - 8 M fills the gap in video benchmarks as follows : \u2022 A large - scale video annotation and representation learning benchmark , reflecting the main themes of a video . \u2022 A significant jump in the number and diversity of annotation classes - 4 8 0 0 Knowledge Graph entities vs. less than 5 0 0 categories for all other datasets . \u2022 A substantial increase in the number of labeled videos - over 8 million videos , more than 5 0 0 , 0 0 0 hours of video . \u2022 Availability of pre - computed state - of - the - art features for 1. 9 billion video frames .", "ner": [["YouTube - 8 M", "Dataset"], ["representation learning", "Method"], ["Knowledge Graph", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "11241677", "sentence": "YouTube - 8 M is a benchmark dataset for video understanding , where the main task is to determine the key topical themes of a video .", "ner": [["YouTube - 8 M", "Dataset"], ["video understanding", "Task"]], "rel": [["YouTube - 8 M", "Benchmark-For", "video understanding"]], "rel_plus": [["YouTube - 8 M:Dataset", "Benchmark-For", "video understanding:Task"]]}
{"doc_id": "11241677", "sentence": "Therefore , we pre - process the videos and extract frame - level features using a state - of - the - art deep model : the publicly available Inception network [ 4 ] trained on ImageNet [ 1 4 ] .", "ner": [["Inception network", "Method"], ["ImageNet", "Dataset"]], "rel": [["Inception network", "Trained-With", "ImageNet"]], "rel_plus": [["Inception network:Method", "Trained-With", "ImageNet:Dataset"]]}
{"doc_id": "11241677", "sentence": "Concretely , we decode each video at 1 frame - per - second up to the first 3 6 0 seconds ( 6 minutes ) , feed the decoded frames into the Inception network , and fetch the ReLu activation of the last hidden layer , before the classification layer ( layer name pool_ 3 /_reshape ) .", "ner": [["Inception network", "Method"], ["ReLu", "Method"], ["classification layer", "Method"], ["pool_ 3 /_reshape", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "11241677", "sentence": "For comparison , the average length of videos in UCF - 1 0 1 is 1 0 \u2212 1 5 seconds , Sports - 1 M is 3 3 6 seconds and in this dataset , it is 2 3 0 seconds .", "ner": [["UCF - 1 0 1", "Dataset"], ["Sports - 1 M", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "11241677", "sentence": "A few hidden layers and a classification layer provide the final video - level predictions . pv(e|x ( 1 ) Inspired by the success of various classic bag of words representations for video classification [ 2 3 , 3 6 ] , we next consider a Deep Bag - of - Frames ( DBoF ) approach .", "ner": [["hidden layers", "Method"], ["classification layer", "Method"], ["video classification", "Task"], ["Deep Bag - of - Frames", "Method"], ["DBoF", "Method"]], "rel": [["DBoF", "Synonym-Of", "Deep Bag - of - Frames"]], "rel_plus": [["DBoF:Method", "Synonym-Of", "Deep Bag - of - Frames:Method"]]}
{"doc_id": "11241677", "sentence": "Figure 7 shows the overall architecture of our DBoF network for video classification .", "ner": [["DBoF", "Method"], ["video classification", "Task"]], "rel": [["DBoF", "Used-For", "video classification"]], "rel_plus": [["DBoF:Method", "Used-For", "video classification:Task"]]}
{"doc_id": "11241677", "sentence": "The Ndimensional input frame level features from k randomly selected frames of a video are first fed into a fully connected layer of M units with RELU activations .", "ner": [["fully connected layer", "Method"], ["RELU", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "11241677", "sentence": "The obtained fixed length descriptor of the video can now be classified into the output classes using a Logistic or Softmax layer with additional fully connected layers in between .", "ner": [["Logistic", "Method"], ["Softmax layer", "Method"], ["fully connected layers", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "11241677", "sentence": "The entire network is trained using Stocastic Gradient Descent ( SGD ) with logistic loss for a logistic layer and cross - entropy loss for a softmax layer .", "ner": [["Stocastic Gradient Descent", "Method"], ["SGD", "Method"], ["logistic loss", "Method"], ["logistic layer", "Method"], ["cross - entropy loss", "Method"], ["softmax", "Method"]], "rel": [["SGD", "Synonym-Of", "Stocastic Gradient Descent"], ["logistic loss", "Part-Of", "logistic layer"], ["cross - entropy loss", "Part-Of", "softmax"]], "rel_plus": [["SGD:Method", "Synonym-Of", "Stocastic Gradient Descent:Method"], ["logistic loss:Method", "Part-Of", "logistic layer:Method"], ["cross - entropy loss:Method", "Part-Of", "softmax:Method"]]}
{"doc_id": "11241677", "sentence": "Note that the up - projection layer into sparse codes is similar to what Fisher Vectors [ 2 7 ] and VLAD [ 1 5 ] approaches do but the projection ( i.e. , clustering ) is done discriminatively here .", "ner": [["Fisher Vectors", "Method"], ["VLAD", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "11241677", "sentence": "We also experimented with Fisher Vectors and VLAD but were not able to obtain competitive results using comparable codebook sizes .", "ner": [["Fisher Vectors", "Method"], ["VLAD", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "11241677", "sentence": "The network was trained using SGD with AdaGrad , a learning rate of 0. 1 , and a weight decay penalty of 0.0 0 0 5 .", "ner": [["SGD", "Method"], ["AdaGrad", "Method"], ["weight decay", "Method"]], "rel": [["AdaGrad", "Part-Of", "SGD"]], "rel_plus": [["AdaGrad:Method", "Part-Of", "SGD:Method"]]}
{"doc_id": "11241677", "sentence": "This means that we can only train the LSTM and Softmax layers .", "ner": [["LSTM", "Method"], ["Softmax layers", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "11241677", "sentence": "In order to transfer the learned model to ActivityNet , we used a fully - connected model which uses as inputs the concatenation of the LSTM layers ' outputs as computed at the last frame of the videos in each of these two benchmarks .", "ner": [["ActivityNet", "Dataset"], ["fully - connected model", "Method"], ["LSTM", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "11241677", "sentence": "Unlike traditional transfer learning methods , we do not fine - tune the LSTM layers .", "ner": [["transfer learning", "Task"], ["LSTM", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "11241677", "sentence": "We did perform full fine - tuning experiments on Sports - 1 M , which is large enough to fine - tune the entire LSTM model after pre - training .", "ner": [["Sports - 1 M", "Dataset"], ["LSTM", "Method"]], "rel": [["LSTM", "Trained-With", "Sports - 1 M"]], "rel_plus": [["LSTM:Method", "Trained-With", "Sports - 1 M:Dataset"]]}
{"doc_id": "11241677", "sentence": "We also experimented with Fisher Vectors ( FV ) [ 2 7 ] and VLAD [ 1 5 ] approaches for task - independent video - level representations but were not able to achieve competitive results for FV or VLAD representations of similar dimensionality .", "ner": [["Fisher Vectors", "Method"], ["FV", "Method"], ["VLAD", "Method"], ["FV", "Method"], ["VLAD", "Method"]], "rel": [["FV", "Synonym-Of", "Fisher Vectors"]], "rel_plus": [["FV:Method", "Synonym-Of", "Fisher Vectors:Method"]]}
{"doc_id": "11241677", "sentence": "We leave it as future work to come up with compact FV or VLAD type representations that outperform the much simpler approach described below .   \u03d5(x v 1:Fv ) = \uf8ee \uf8f0 \u00b5(x v 1:Fv ) \u03c3(x v 1:Fv ) Top K ( x v 1:Fv ) \uf8f9 \uf8fb .( 2 ) Standardization of features has been proven to help with online learning algorithms [ 1 4 , 3 7 ] as it makes the updates using Stochastic Gradient Descent ( SGD ) based algorithms ( like Adagrad ) more robust to learning rates , and speeds up convergence .", "ner": [["FV", "Method"], ["VLAD", "Method"], ["Stochastic Gradient Descent", "Method"], ["SGD", "Method"], ["Adagrad", "Method"]], "rel": [["SGD", "Synonym-Of", "Stochastic Gradient Descent"]], "rel_plus": [["SGD:Method", "Synonym-Of", "Stochastic Gradient Descent:Method"]]}
{"doc_id": "11241677", "sentence": "Since training batch SVMs on such a large dataset is impossible , we use the online SVM approach .", "ner": [["SVMs", "Method"], ["SVM", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "11241677", "sentence": "Given a set of training examples ( xi , gi ) i= 1 ... N for a binary classifier , where xi is the feature vector and gi \u2208 [ 0 , 1 ] is the ground - truth , let L(pi , gi ) be the log - loss between the predicted probability and the ground - truth : We could directly write the derivative of L p y|x , g with respect to the softmax weight w h and the logistic weight u h as We use Adagrad with a learning rate of 1. 0 and batch size of 3 2 to learn the weights .", "ner": [["softmax", "Method"], ["Adagrad", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "11241677", "sentence": "In this section , we first provide benchmark baseline results for the above multi - label classification approaches on the YouTube - 8 M dataset .", "ner": [["multi - label classification", "Task"], ["YouTube - 8 M", "Dataset"]], "rel": [["YouTube - 8 M", "Benchmark-For", "multi - label classification"]], "rel_plus": [["YouTube - 8 M:Dataset", "Benchmark-For", "multi - label classification:Task"]]}
{"doc_id": "11241677", "sentence": "We then evaluate the usefulness of video representations learned on this dataset for other tasks , such as Sports - 1 M sports classification and AcitvityNet activity classification .", "ner": [["Sports - 1 M", "Dataset"], ["sports classification", "Task"], ["AcitvityNet", "Dataset"], ["activity classification", "Task"]], "rel": [["Sports - 1 M", "Benchmark-For", "sports classification"], ["AcitvityNet", "Benchmark-For", "activity classification"]], "rel_plus": [["Sports - 1 M:Dataset", "Benchmark-For", "sports classification:Task"], ["AcitvityNet:Dataset", "Benchmark-For", "activity classification:Task"]]}
{"doc_id": "11241677", "sentence": "Frame - level models ( row 1 ) , trained on the strong Inception features and logistic regression , followed by simple averaging of predictions across all frames , perform poorly on this dataset .", "ner": [["Inception features", "Method"], ["logistic regression", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "11241677", "sentence": "Note that the standard deviation and ordinal statistics are more meaningful in the original RELU activation space so we reconstruct the RELU features from the PCA - ed and quantized features by inverting the quantization and the PCA using the provided PCA matrix , computing the collection statistics over the reconstructed frame - level RELU features , and then re - applying PCA , whitening , and L 2 normalization as described in Section 4. 2 . 2 .", "ner": [["RELU activation space", "Method"], ["RELU features", "Method"], ["PCA - ed and quantized features", "Method"], ["PCA", "Method"], ["PCA matrix", "Method"], ["frame - level RELU features", "Method"], ["PCA", "Method"], ["L 2 normalization", "Method"]], "rel": [["PCA - ed and quantized features", "Part-Of", "RELU features"], ["PCA matrix", "Part-Of", "PCA"]], "rel_plus": [["PCA - ed and quantized features:Method", "Part-Of", "RELU features:Method"], ["PCA matrix:Method", "Part-Of", "PCA:Method"]]}
{"doc_id": "11241677", "sentence": "The DBoF architecture ignores sequence information and treats the input video as a bag of frames whereas LSTMs use state information to preserve the video sequence .", "ner": [["DBoF", "Method"], ["LSTMs", "Method"]], "rel": [["DBoF", "Compare-With", "LSTMs"]], "rel_plus": [["DBoF:Method", "Compare-With", "LSTMs:Method"]]}
{"doc_id": "11241677", "sentence": "The mAP results for DBoF are slightly worse than mean pooling + logistic model , which we attribute to slower training and convergence of DBoF on rare classes ( mAP is strongly affected by results on rare classes and the joint class training of DBoF is a disadvantage for those classes ) .", "ner": [["DBoF", "Method"], ["mean pooling + logistic model", "Method"], ["DBoF", "Method"], ["DBoF", "Method"]], "rel": [["DBoF", "Compare-With", "mean pooling + logistic model"]], "rel_plus": [["DBoF:Method", "Compare-With", "mean pooling + logistic model:Method"]]}
{"doc_id": "11241677", "sentence": "We also considered Fisher vectors and VLAD given their recent success in aggregating CNN features at the video - level in [ 3 9 ] .", "ner": [["Fisher vectors", "Method"], ["VLAD", "Method"], ["CNN features", "Method"]], "rel": [["VLAD", "Used-For", "CNN features"], ["Fisher vectors", "Used-For", "CNN features"]], "rel_plus": [["VLAD:Method", "Used-For", "CNN features:Method"], ["Fisher vectors:Method", "Used-For", "CNN features:Method"]]}
{"doc_id": "11241677", "sentence": "However , for the same dimensionality as the video - level representations of the LSTM , DBoF and mean features , they did not produce competitive results .", "ner": [["LSTM", "Method"], ["DBoF", "Method"], ["mean features", "Method"]], "rel": [["LSTM", "Compare-With", "DBoF"], ["LSTM", "Compare-With", "mean features"]], "rel_plus": [["LSTM:Method", "Compare-With", "DBoF:Method"], ["LSTM:Method", "Compare-With", "mean features:Method"]]}
{"doc_id": "11241677", "sentence": "Next , we investigate generalization of the video - level features learned using the YouTube - 8 M dataset and perform transfer learning experiments on the Sports - 1 M dataset .", "ner": [["YouTube - 8 M", "Dataset"], ["transfer learning", "Task"], ["Sports - 1 M", "Dataset"]], "rel": [["Sports - 1 M", "Benchmark-For", "transfer learning"]], "rel_plus": [["Sports - 1 M:Dataset", "Benchmark-For", "transfer learning:Task"]]}
{"doc_id": "11241677", "sentence": "The Sports - 1 M dataset [ 1 9 ] consists of 4 8 7 sports activities with 1. 2 million YouTube videos and is one of the largest benchmarks available for sports/activity recognition .", "ner": [["Sports - 1 M", "Dataset"], ["sports/activity recognition", "Task"]], "rel": [["Sports - 1 M", "Benchmark-For", "sports/activity recognition"]], "rel_plus": [["Sports - 1 M:Dataset", "Benchmark-For", "sports/activity recognition:Task"]]}
{"doc_id": "11241677", "sentence": "To evaluate transfer learning on this dataset , in one experiment we simply use the aggregated video - level descriptors , based on the PCA matrix learned on the YouTube - 8 M dataset , and train MoE or [ 2 4 ] 5 3 . 8 - - Heilbron et al. [ 1 2 ] 4 3 . 0 - - (b ) ActivityNet : Since the dataset is small , we see a substantial boost in performance by pre - training on YouTube - 8 M or using the transfer learnt PCA versus the one learnt from scratch on ActivityNet . logistic models on top using target domain training data .", "ner": [["transfer learning", "Task"], ["PCA matrix", "Method"], ["YouTube - 8 M", "Dataset"], ["MoE", "Method"], ["ActivityNet", "Dataset"], ["YouTube - 8 M", "Dataset"], ["PCA", "Method"], ["ActivityNet", "Dataset"], ["logistic models", "Method"]], "rel": [["YouTube - 8 M", "Benchmark-For", "transfer learning"], ["PCA matrix", "Trained-With", "YouTube - 8 M"]], "rel_plus": [["YouTube - 8 M:Dataset", "Benchmark-For", "transfer learning:Task"], ["PCA matrix:Method", "Trained-With", "YouTube - 8 M:Dataset"]]}
{"doc_id": "11241677", "sentence": "For the LSTM networks , we have two scenarios : 1 ) we use the PCA transformed features and learn a LSTM model from scratch using these features ; or 2 ) we use the LSTM layers pre - trained on the YouTube - 8 M task , and fine - tune them on the Sports - 1 M dataset ( along with a new softmax classifier ) .", "ner": [["LSTM", "Method"], ["PCA transformed features", "Method"], ["LSTM", "Method"], ["LSTM layers", "Method"], ["YouTube - 8 M", "Dataset"], ["Sports - 1 M", "Dataset"], ["softmax classifier", "Method"]], "rel": [["PCA transformed features", "Used-For", "LSTM"], ["softmax classifier", "Part-Of", "LSTM layers"], ["LSTM layers", "Trained-With", "YouTube - 8 M"], ["LSTM layers", "Trained-With", "Sports - 1 M"]], "rel_plus": [["PCA transformed features:Method", "Used-For", "LSTM:Method"], ["softmax classifier:Method", "Part-Of", "LSTM layers:Method"], ["LSTM layers:Method", "Trained-With", "YouTube - 8 M:Dataset"], ["LSTM layers:Method", "Trained-With", "Sports - 1 M:Dataset"]]}
{"doc_id": "11241677", "sentence": "Our learned features are competitive on this dataset , with the best approach beating all but the approach of [ 2 6 ] , which learned directly from the pixels of the videos in the Sports - 1 M dataset , including optical flow , and made use of data augmentation strategies and multiple inferences over several video segments .", "ner": [["Sports - 1 M", "Dataset"], ["optical flow", "Method"], ["data augmentation", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "11241677", "sentence": "We also show that even on such a large dataset ( 1M videos ) , pre - training on YouTube - 8 M still helps , and improves the LSTM performance by \u223c 1 % on all metrics ( vs. no pre - training ) .", "ner": [["YouTube - 8 M", "Dataset"], ["LSTM", "Method"]], "rel": [["LSTM", "Trained-With", "YouTube - 8 M"]], "rel_plus": [["LSTM:Method", "Trained-With", "YouTube - 8 M:Dataset"]]}
{"doc_id": "11241677", "sentence": "Similar to Sports - 1 M experiments , we compare directly training on the ActivityNet dataset against pre - training on YouTube - 8 M for aggregation based and LSTM approaches .", "ner": [["Sports - 1 M", "Dataset"], ["ActivityNet", "Dataset"], ["YouTube - 8 M", "Dataset"], ["LSTM", "Method"]], "rel": [["LSTM", "Trained-With", "ActivityNet"], ["ActivityNet", "Compare-With", "YouTube - 8 M"], ["LSTM", "Trained-With", "YouTube - 8 M"]], "rel_plus": [["LSTM:Method", "Trained-With", "ActivityNet:Dataset"], ["ActivityNet:Dataset", "Compare-With", "YouTube - 8 M:Dataset"], ["LSTM:Method", "Trained-With", "YouTube - 8 M:Dataset"]]}
{"doc_id": "11241677", "sentence": "In this paper , we introduce YouTube - 8 M , a large - scale video benchmark for video classification and representation learning .", "ner": [["YouTube - 8 M", "Dataset"], ["video classification", "Task"], ["representation learning", "Task"]], "rel": [["YouTube - 8 M", "Benchmark-For", "video classification"], ["YouTube - 8 M", "Benchmark-For", "representation learning"]], "rel_plus": [["YouTube - 8 M:Dataset", "Benchmark-For", "video classification:Task"], ["YouTube - 8 M:Dataset", "Benchmark-For", "representation learning:Task"]]}
{"doc_id": "11241677", "sentence": "With YouTube - 8 M , our goal is to advance the field of video understanding , similarly to what large - scale image datasets have done for image understanding .", "ner": [["YouTube - 8 M", "Dataset"], ["video understanding", "Task"], ["image understanding", "Task"]], "rel": [["YouTube - 8 M", "Used-For", "video understanding"]], "rel_plus": [["YouTube - 8 M:Dataset", "Used-For", "video understanding:Task"]]}
{"doc_id": "11241677", "sentence": "We provide extensive experiments comparing several strong baselines for video representation learning , including Deep Networks and LSTMs , on this dataset .", "ner": [["video representation learning", "Task"], ["Deep Networks", "Method"], ["LSTMs", "Method"]], "rel": [["Deep Networks", "Used-For", "video representation learning"], ["LSTMs", "Used-For", "video representation learning"], ["Deep Networks", "Compare-With", "LSTMs"]], "rel_plus": [["Deep Networks:Method", "Used-For", "video representation learning:Task"], ["LSTMs:Method", "Used-For", "video representation learning:Task"], ["Deep Networks:Method", "Compare-With", "LSTMs:Method"]]}
{"doc_id": "11241677", "sentence": "We demonstrate the efficacy of using a fairly unexplored class of models ( mixture - of - experts ) and show that they can outperform popular classifiers like logistic regression and SVMs .", "ner": [["logistic regression", "Method"], ["SVMs", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "11241677", "sentence": "We explore various video - level representations using simple statistics extracted from the framelevel features and model the probability of an entity given the aggregated vector as an MoE. We show that this yields competitive performance compared to more complex approaches ( that directly use frame - level information ) such as LSTM and DBoF. This also demonstrates that if the underlying frame - level features are strong , the need for more sophisticated video - level modeling techniques is reduced .", "ner": [["MoE.", "Method"], ["LSTM", "Method"], ["DBoF.", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "11241677", "sentence": "Finally , we illustrate the usefulness of the dataset by performing transfer learning experiments on existing video benchmarksSports - 1 M and ActivityNet .", "ner": [["transfer learning", "Task"], ["benchmarksSports - 1 M", "Dataset"], ["ActivityNet", "Dataset"]], "rel": [["benchmarksSports - 1 M", "Benchmark-For", "transfer learning"], ["ActivityNet", "Benchmark-For", "transfer learning"]], "rel_plus": [["benchmarksSports - 1 M:Dataset", "Benchmark-For", "transfer learning:Task"], ["ActivityNet:Dataset", "Benchmark-For", "transfer learning:Task"]]}
{"doc_id": "4246700", "sentence": "Recently , noticeable progress has been made in scene classification and target detection .", "ner": [["scene classification", "Task"], ["target detection", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "4246700", "sentence": "However , the studies of the remote sensing images are still concentrate on scene classification [ 5 ] , [ 1 6 ] , [ 1 7 ] , [ 1 8 ] , [ 1 9 ] , object recognition [ 2 0 ] , [ 2 1 ] and segmentation [ 2 2 ] , [ 2 3 ] , [ 2 4 ] , [ 2 5 ] .", "ner": [["scene classification", "Task"], ["object recognition", "Task"], ["segmentation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "4246700", "sentence": "This concise description of the remote sensing scene plays a vital role in numerous fields , such as image retrieval [ 2 7 ] , scene classification [ 9 ] , and military intelligence generation [ 2 8 ] .", "ner": [["image retrieval", "Task"], ["scene classification", "Task"], ["military intelligence generation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "4246700", "sentence": "As for sentence generation , the studies has developed from traditional retrieved - based method to Recurrent Neural Network ( RNN ) .", "ner": [["Recurrent Neural Network", "Method"], ["RNN", "Method"]], "rel": [["RNN", "Synonym-Of", "Recurrent Neural Network"]], "rel_plus": [["RNN:Method", "Synonym-Of", "Recurrent Neural Network:Method"]]}
{"doc_id": "4246700", "sentence": "To decode the image representations into natural language sentences , several methods have been proposed for generating image descriptions [ 2 9 ] , [ 3 2 ] , [ 3 3 ] , [ 3 4 ] , such as Recurrent Neural Network ( RNN ) , Long - Short Term Memory networks ( LSTM ) , retrieve based method and object detection based method .", "ner": [["Recurrent Neural Network", "Method"], ["RNN", "Method"], ["Long - Short Term Memory networks", "Method"], ["LSTM", "Method"], ["retrieve based method", "Method"], ["object detection based method", "Method"]], "rel": [["RNN", "Synonym-Of", "Recurrent Neural Network"], ["LSTM", "Synonym-Of", "Long - Short Term Memory networks"]], "rel_plus": [["RNN:Method", "Synonym-Of", "Recurrent Neural Network:Method"], ["LSTM:Method", "Synonym-Of", "Long - Short Term Memory networks:Method"]]}
{"doc_id": "4246700", "sentence": "Although the aforementioned methods have achieved success in natural image captioning [ 3 5 ] , [ 3 6 ] , [ 3 7 ] , [ 3 2 ] , they may be inappropriate for the remote sensing image captioning .", "ner": [["natural image captioning", "Task"], ["remote sensing image captioning", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "4246700", "sentence": "Specially , the remote sensing image captioning is more complex than the natural image captioning [ 2 6 ] , [ 2 8 ] , and the semantics in remote sensing image become much ambiguous from the \" view of God \" .", "ner": [["remote sensing image captioning", "Task"], ["natural image captioning", "Task"]], "rel": [["remote sensing image captioning", "Compare-With", "natural image captioning"]], "rel_plus": [["remote sensing image captioning:Task", "Compare-With", "natural image captioning:Task"]]}
{"doc_id": "4246700", "sentence": "Shi et al. [ 2 8 ] proposed a remote sensing image captioning framework by leveraging the techniques of Convolutional Neural Network ( CNN ) .", "ner": [["remote sensing image captioning", "Task"], ["Convolutional Neural Network", "Method"], ["CNN", "Method"]], "rel": [["Convolutional Neural Network", "Used-For", "remote sensing image captioning"], ["CNN", "Synonym-Of", "Convolutional Neural Network"]], "rel_plus": [["Convolutional Neural Network:Method", "Used-For", "remote sensing image captioning:Task"], ["CNN:Method", "Synonym-Of", "Convolutional Neural Network:Method"]]}
{"doc_id": "4246700", "sentence": "Both methods used CNN to represent the image and generated the corresponding sentences from the trained model ( recurrent neural networks in [ 2 6 ] and pre - defined templates in [ 2 8 ] ) .", "ner": [["CNN", "Method"], ["recurrent neural networks", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "4246700", "sentence": "To fully advance the task of remote sensing captioning , the representative encoder - decoder frameworks [ 3 0 ] are evaluated with various experimental protocols on the new dataset .", "ner": [["remote sensing captioning", "Task"], ["encoder - decoder", "Method"]], "rel": [["encoder - decoder", "Used-For", "remote sensing captioning"]], "rel_plus": [["encoder - decoder:Method", "Used-For", "remote sensing captioning:Task"]]}
{"doc_id": "4246700", "sentence": "This section comprehensively reviews the existing image captioning including natural image captioning and remote sensing image captioning .", "ner": [["image captioning", "Task"], ["natural image captioning", "Task"], ["remote sensing image captioning", "Task"]], "rel": [["natural image captioning", "SubTask-Of", "image captioning"], ["remote sensing image captioning", "SubTask-Of", "image captioning"]], "rel_plus": [["natural image captioning:Task", "SubTask-Of", "image captioning:Task"], ["remote sensing image captioning:Task", "SubTask-Of", "image captioning:Task"]]}
{"doc_id": "4246700", "sentence": "Natural image captioning , which generates a sentence to describe a natural image , has been studied in computer vision for several years [ 2 9 ] , [ 3 0 ] , [ 3 1 ] , [ 3 2 ] , [ 3 3 ] , [ 3 4 ] .", "ner": [["Natural image captioning", "Task"], ["computer vision", "Task"]], "rel": [["Natural image captioning", "SubTask-Of", "computer vision"]], "rel_plus": [["Natural image captioning:Task", "SubTask-Of", "computer vision:Task"]]}
{"doc_id": "4246700", "sentence": "The methods of the natural image captioning can be divided to three categories , including retrieved - based method , object detection based method and encoder - decoder method .", "ner": [["natural image captioning", "Task"], ["retrieved - based method", "Method"], ["object detection based method", "Method"], ["encoder - decoder method", "Method"]], "rel": [["encoder - decoder method", "Used-For", "natural image captioning"], ["object detection based method", "Used-For", "natural image captioning"], ["retrieved - based method", "Used-For", "natural image captioning"]], "rel_plus": [["encoder - decoder method:Method", "Used-For", "natural image captioning:Task"], ["object detection based method:Method", "Used-For", "natural image captioning:Task"], ["retrieved - based method:Method", "Used-For", "natural image captioning:Task"]]}
{"doc_id": "4246700", "sentence": "The deep neural models , usually using Convolutional Neural Network ( CNN ) to extract features of images and then using language generating models such as Recurrent Neural Network ( RNN ) and a special RNN called Long - Short Term Memory networks ( LSTM ) , have made a great progress in natural image captioning .", "ner": [["Convolutional Neural Network", "Method"], ["CNN", "Method"], ["Recurrent Neural Network", "Method"], ["RNN", "Method"], ["RNN", "Method"], ["Long - Short Term Memory networks", "Method"], ["LSTM", "Method"], ["natural image captioning", "Task"]], "rel": [["CNN", "Synonym-Of", "Convolutional Neural Network"], ["RNN", "Synonym-Of", "Recurrent Neural Network"], ["Long - Short Term Memory networks", "SubClass-Of", "RNN"], ["LSTM", "Synonym-Of", "Long - Short Term Memory networks"], ["Long - Short Term Memory networks", "Used-For", "natural image captioning"], ["Recurrent Neural Network", "Used-For", "natural image captioning"], ["Convolutional Neural Network", "Used-For", "natural image captioning"]], "rel_plus": [["CNN:Method", "Synonym-Of", "Convolutional Neural Network:Method"], ["RNN:Method", "Synonym-Of", "Recurrent Neural Network:Method"], ["Long - Short Term Memory networks:Method", "SubClass-Of", "RNN:Method"], ["LSTM:Method", "Synonym-Of", "Long - Short Term Memory networks:Method"], ["Long - Short Term Memory networks:Method", "Used-For", "natural image captioning:Task"], ["Recurrent Neural Network:Method", "Used-For", "natural image captioning:Task"], ["Convolutional Neural Network:Method", "Used-For", "natural image captioning:Task"]]}
{"doc_id": "4246700", "sentence": "Inspired by recent advances in computer vision and machine translation [ 3 8 ] , a deep model [ 3 9 ] is proposed to maximize the likelihood of the target description sentence given the training image .", "ner": [["computer vision", "Task"], ["machine translation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "4246700", "sentence": "The Multimodal Recurrent Neural Network architecture is a novel combination of three networks , including convolutional neural networks to extract image feature , bidirectional recurrent neural networks to represent sentences , and a structured objective embedding the image feature and sentence representation .", "ner": [["Multimodal Recurrent Neural Network", "Method"], ["convolutional neural networks", "Method"], ["extract image feature", "Task"], ["bidirectional recurrent neural networks", "Method"], ["represent sentences", "Task"], ["structured objective", "Method"], ["embedding the image feature and sentence representation", "Task"]], "rel": [["convolutional neural networks", "Part-Of", "Multimodal Recurrent Neural Network"], ["bidirectional recurrent neural networks", "Part-Of", "Multimodal Recurrent Neural Network"], ["structured objective", "Part-Of", "Multimodal Recurrent Neural Network"], ["convolutional neural networks", "Used-For", "extract image feature"], ["bidirectional recurrent neural networks", "Used-For", "represent sentences"], ["structured objective", "Used-For", "embedding the image feature and sentence representation"]], "rel_plus": [["convolutional neural networks:Method", "Part-Of", "Multimodal Recurrent Neural Network:Method"], ["bidirectional recurrent neural networks:Method", "Part-Of", "Multimodal Recurrent Neural Network:Method"], ["structured objective:Method", "Part-Of", "Multimodal Recurrent Neural Network:Method"], ["convolutional neural networks:Method", "Used-For", "extract image feature:Task"], ["bidirectional recurrent neural networks:Method", "Used-For", "represent sentences:Task"], ["structured objective:Method", "Used-For", "embedding the image feature and sentence representation:Task"]]}
{"doc_id": "4246700", "sentence": "To further complete the task , Johnson et al. [ 3 1 ] proposed a Fully Convolutional Localization Network ( FCLN ) architecture that can localize and describe regions of an image at the same time .", "ner": [["Fully Convolutional Localization Network", "Method"], ["FCLN", "Method"]], "rel": [["FCLN", "Synonym-Of", "Fully Convolutional Localization Network"]], "rel_plus": [["FCLN:Method", "Synonym-Of", "Fully Convolutional Localization Network:Method"]]}
{"doc_id": "4246700", "sentence": "Although many methods have been proposed for natural image captioning , only few studies on remote sensing image captioning can be focused [ 2 8 ] .", "ner": [["natural image captioning", "Task"], ["remote sensing image captioning", "Task"]], "rel": [["natural image captioning", "Compare-With", "remote sensing image captioning"]], "rel_plus": [["natural image captioning:Task", "Compare-With", "remote sensing image captioning:Task"]]}
{"doc_id": "4246700", "sentence": "This is because there is not an accredited dataset like Common Objects in Context ( COCO ) dataset in natural image datasets .", "ner": [["Common Objects in Context", "Dataset"], ["COCO", "Dataset"]], "rel": [["COCO", "Synonym-Of", "Common Objects in Context"]], "rel_plus": [["COCO:Dataset", "Synonym-Of", "Common Objects in Context:Dataset"]]}
{"doc_id": "4246700", "sentence": "Shi et al. [ 2 8 ] proposed a remote sensing image captioning framework by leveraging the techniques of deep learning and Convolutional Neural Network ( CNN ) .", "ner": [["remote sensing image captioning", "Task"], ["deep learning", "Method"], ["Convolutional Neural Network", "Method"], ["CNN", "Method"]], "rel": [["Convolutional Neural Network", "Used-For", "remote sensing image captioning"], ["deep learning", "Used-For", "remote sensing image captioning"], ["CNN", "Synonym-Of", "Convolutional Neural Network"]], "rel_plus": [["Convolutional Neural Network:Method", "Used-For", "remote sensing image captioning:Task"], ["deep learning:Method", "Used-For", "remote sensing image captioning:Task"], ["CNN:Method", "Synonym-Of", "Convolutional Neural Network:Method"]]}
{"doc_id": "4246700", "sentence": "In this section , we first reviews the existed remote sensing image captioning dataset , and then described the proposed Remote Sensing Image Captioning Dataset ( RSICD ) .", "ner": [["remote sensing image captioning dataset", "Dataset"], ["Remote Sensing Image Captioning Dataset", "Dataset"], ["RSICD", "Dataset"]], "rel": [["RSICD", "Synonym-Of", "Remote Sensing Image Captioning Dataset"]], "rel_plus": [["RSICD:Dataset", "Synonym-Of", "Remote Sensing Image Captioning Dataset:Dataset"]]}
{"doc_id": "4246700", "sentence": "A. Existing datasets for remote sensing image captioning 1 ) UCM - captions dataset : This dataset is proposed in [ 2 6 ] , which is based on the UC Merced Land Use Dataset [ 4 1 ] .", "ner": [["remote sensing image captioning", "Task"], ["UCM - captions", "Dataset"], ["UC Merced Land Use Dataset", "Dataset"]], "rel": [["UCM - captions", "Benchmark-For", "remote sensing image captioning"]], "rel_plus": [["UCM - captions:Dataset", "Benchmark-For", "remote sensing image captioning:Task"]]}
{"doc_id": "4246700", "sentence": "The diversity of five coherent sentences for one image are totally different , but the sentence difference between images of the same class is very small . 2 ) Sydney - captions dataset : This dataset is also provided by [ 2 6 ] , which is based on the Sydney Data Set [ 4 2 ] .", "ner": [["Sydney - captions", "Dataset"], ["Sydney Data Set", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "4246700", "sentence": "Both datasets , including UCM - captions and Sydney - captions , only focus on the latter problem , but the first problem should also be considered . 3 ) A untitled dataset : In [ 2 8 ] , a untitled and undisclosed dataset about remote sensing image captioning is proposed .", "ner": [["UCM - captions", "Dataset"], ["Sydney - captions", "Dataset"], ["remote sensing image captioning", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "4246700", "sentence": "To advance the state - of - the - art performances in remote sensing image captioning , we construct a new remote sensing image captioning dataset , named RSICD , for remote sensing image captioning task .", "ner": [["remote sensing image captioning", "Task"], ["remote sensing image captioning", "Task"], ["RSICD", "Dataset"], ["remote sensing image captioning", "Task"]], "rel": [["RSICD", "Benchmark-For", "remote sensing image captioning"]], "rel_plus": [["RSICD:Dataset", "Benchmark-For", "remote sensing image captioning:Task"]]}
{"doc_id": "4246700", "sentence": "We present the outline of encoder - decoder ( encoding an image to a vector , then decoding the vector to a sentence ) for remote sensing image captioning task in Figure 2 .", "ner": [["encoder - decoder", "Method"], ["remote sensing image captioning", "Task"]], "rel": [["encoder - decoder", "Used-For", "remote sensing image captioning"]], "rel_plus": [["encoder - decoder:Method", "Used-For", "remote sensing image captioning:Task"]]}
{"doc_id": "4246700", "sentence": "The handcrafted feature methods first extract the handcrafted features from each image and then obtain image representation by feature encoding techniques such as Bag Of Words ( BOW ) [ 4 3 ] , Fisher Vector ( FV ) [ 4 4 ] and Vector of Locally Aggregated Descriptors ( VLAD ) [ 4 5 ] .", "ner": [["handcrafted feature methods", "Method"], ["feature encoding techniques", "Method"], ["Bag Of Words", "Method"], ["BOW", "Method"], ["Fisher Vector", "Method"], ["FV", "Method"], ["Vector of Locally Aggregated Descriptors", "Method"], ["VLAD", "Method"]], "rel": [["Bag Of Words", "SubClass-Of", "feature encoding techniques"], ["Fisher Vector", "SubClass-Of", "feature encoding techniques"], ["Vector of Locally Aggregated Descriptors", "SubClass-Of", "feature encoding techniques"], ["BOW", "Synonym-Of", "Bag Of Words"], ["FV", "Synonym-Of", "Fisher Vector"], ["VLAD", "Synonym-Of", "Vector of Locally Aggregated Descriptors"]], "rel_plus": [["Bag Of Words:Method", "SubClass-Of", "feature encoding techniques:Method"], ["Fisher Vector:Method", "SubClass-Of", "feature encoding techniques:Method"], ["Vector of Locally Aggregated Descriptors:Method", "SubClass-Of", "feature encoding techniques:Method"], ["BOW:Method", "Synonym-Of", "Bag Of Words:Method"], ["FV:Method", "Synonym-Of", "Fisher Vector:Method"], ["VLAD:Method", "Synonym-Of", "Vector of Locally Aggregated Descriptors:Method"]]}
{"doc_id": "4246700", "sentence": "Lowe et al. [ 4 6 ] proposed a method to extract Scale - Invariant Feature Transform ( SIFT ) features which are invariant to image scale change and rotation .", "ner": [["Scale - Invariant Feature Transform", "Method"], ["SIFT", "Method"]], "rel": [["SIFT", "Synonym-Of", "Scale - Invariant Feature Transform"]], "rel_plus": [["SIFT:Method", "Synonym-Of", "Scale - Invariant Feature Transform:Method"]]}
{"doc_id": "4246700", "sentence": "BOW [ 4 3 ] represented a given image with the frequencies of local visual words , while FV [ 4 4 ] uses gaussian mixture model to encode the handcrafted local features .", "ner": [["BOW", "Method"], ["FV", "Method"]], "rel": [["BOW", "Compare-With", "FV"]], "rel_plus": [["BOW:Method", "Compare-With", "FV:Method"]]}
{"doc_id": "4246700", "sentence": "We use fully connected layers of several CNNs , including AlexNet [ 4 7 ] , VGGNet [ 5 0 ] , GoogLeNet [ 5 1 ] , pre - trained on ImageNet dataset , to extract features of remote sensing images . where I is a remote sensing image , e 0 is the feature of the remote sensing image whose dimension is notated u , and f F R is the feature representations process which the feature can be handcrafted feature or deep feature . 2 ) Representing sentences : In the first method , every word in a sentence is represented by a one - hot K dimension word vector w i , where K is the size of the vocabulary .", "ner": [["fully connected layers", "Method"], ["CNNs", "Method"], ["AlexNet", "Method"], ["VGGNet", "Method"], ["GoogLeNet", "Method"], ["ImageNet", "Dataset"]], "rel": [["AlexNet", "SubClass-Of", "CNNs"], ["VGGNet", "SubClass-Of", "CNNs"], ["GoogLeNet", "SubClass-Of", "CNNs"], ["fully connected layers", "Part-Of", "CNNs"], ["AlexNet", "Trained-With", "ImageNet"], ["VGGNet", "Trained-With", "ImageNet"], ["GoogLeNet", "Trained-With", "ImageNet"], ["CNNs", "Trained-With", "ImageNet"]], "rel_plus": [["AlexNet:Method", "SubClass-Of", "CNNs:Method"], ["VGGNet:Method", "SubClass-Of", "CNNs:Method"], ["GoogLeNet:Method", "SubClass-Of", "CNNs:Method"], ["fully connected layers:Method", "Part-Of", "CNNs:Method"], ["AlexNet:Method", "Trained-With", "ImageNet:Dataset"], ["VGGNet:Method", "Trained-With", "ImageNet:Dataset"], ["GoogLeNet:Method", "Trained-With", "ImageNet:Dataset"], ["CNNs:Method", "Trained-With", "ImageNet:Dataset"]]}
{"doc_id": "4246700", "sentence": "Sentence y is encoded as a sequence of h dimension projected word vectors [ 5 2 ] : where L is the length of the sentence . 3 ) Sentences Generation : In this subsection , a special Recurrent Neural Network ( RNN ) , called Long Short - Term Memory networks ( LSTM ) , is exploited to generate the sentences .", "ner": [["Recurrent Neural Network", "Method"], ["RNN", "Method"], ["Long Short - Term Memory networks", "Method"], ["LSTM", "Method"]], "rel": [["RNN", "Synonym-Of", "Recurrent Neural Network"], ["Long Short - Term Memory networks", "SubClass-Of", "Recurrent Neural Network"], ["LSTM", "Synonym-Of", "Long Short - Term Memory networks"]], "rel_plus": [["RNN:Method", "Synonym-Of", "Recurrent Neural Network:Method"], ["Long Short - Term Memory networks:Method", "SubClass-Of", "Recurrent Neural Network:Method"], ["LSTM:Method", "Synonym-Of", "Long Short - Term Memory networks:Method"]]}
{"doc_id": "4246700", "sentence": "This is because Long Short - Term Memory networks is more complicated than original recurrent neural networks .", "ner": [["Long Short - Term Memory networks", "Method"], ["recurrent neural networks", "Method"]], "rel": [["Long Short - Term Memory networks", "Compare-With", "recurrent neural networks"]], "rel_plus": [["Long Short - Term Memory networks:Method", "Compare-With", "recurrent neural networks:Method"]]}
{"doc_id": "4246700", "sentence": "The Recurrent Neural Networks ( RNNs ) [ 5 3 ] satisfy this property with loops allowing information to persist .", "ner": [["Recurrent Neural Networks", "Method"], ["RNNs", "Method"]], "rel": [["RNNs", "Synonym-Of", "Recurrent Neural Networks"]], "rel_plus": [["RNNs:Method", "Synonym-Of", "Recurrent Neural Networks:Method"]]}
{"doc_id": "4246700", "sentence": "The previous information contained by the state of RNNs can be passed to the present state is the main superiority of RNNs .", "ner": [["RNNs", "Method"], ["RNNs", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "4246700", "sentence": "To address the aforementioned problem , Long Short - Term Memory networks ( LSTM ) is proposed in [ 5 5 ] to handle this long - term dependencies problem .", "ner": [["Long Short - Term Memory networks", "Method"], ["LSTM", "Method"]], "rel": [["LSTM", "Synonym-Of", "Long Short - Term Memory networks"]], "rel_plus": [["LSTM:Method", "Synonym-Of", "Long Short - Term Memory networks:Method"]]}
{"doc_id": "4246700", "sentence": "Then the image feature and the corresponding sentences are fed into RNN or LSTM to train a model to predict word one by one given the image .", "ner": [["RNN", "Method"], ["LSTM", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "4246700", "sentence": "The feature is imported only when t = 1 and the range of t is from 1 to N . g ( \u00b7 ) represents the process of the RNN or LSTM . h t is the output of state t whose dimension is h. w t means the corresponding words in the sentence y , and w 1 and w N are special token representing the START and END vector respectively .", "ner": [["RNN", "Method"], ["LSTM", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "4246700", "sentence": "In order to look different parts of an image , attention based method uses a different image representing method which is introduced as follows . 1 ) Representing remote sensing images : The features of lower convolution layers of CNNs represent the local feature compared with the fully connected layer [ 5 6 ] .", "ner": [["convolution layers", "Method"], ["CNNs", "Method"], ["fully connected layer", "Method"]], "rel": [["convolution layers", "Part-Of", "CNNs"], ["convolution layers", "Compare-With", "fully connected layer"]], "rel_plus": [["convolution layers:Method", "Part-Of", "CNNs:Method"], ["convolution layers:Method", "Compare-With", "fully connected layer:Method"]]}
{"doc_id": "4246700", "sentence": "In order to generate sentences using LSTM , the inputs of LSTM in the second method are y t\u2212 1 and\u1e91 t unlike in the first method which instead are e 0 and w t .\u1e91 t is called context vector which is computed by different attention manners from the annotation vectors a i .", "ner": [["LSTM", "Method"], ["LSTM", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "4246700", "sentence": "The initial states of LSTM are predicted by an average of annotation vectors imported through two separate Multi - layer Perceptron ( MLPs ): In the attention based method , a deep output layer is used to compute output word probability given the context vector , LSTM state and previous word : where , and an embedding matrix E are parameters initialized randomly .", "ner": [["LSTM", "Method"], ["Multi - layer Perceptron", "Method"], ["MLPs", "Method"], ["LSTM", "Method"]], "rel": [["Multi - layer Perceptron", "Part-Of", "LSTM"], ["MLPs", "Synonym-Of", "Multi - layer Perceptron"]], "rel_plus": [["Multi - layer Perceptron:Method", "Part-Of", "LSTM:Method"], ["MLPs:Method", "Synonym-Of", "Multi - layer Perceptron:Method"]]}
{"doc_id": "4246700", "sentence": "The feature used for multimodal method including deep CNNs representations ( models are pre - trained on ImageNet dataset ) and handcrafted representations such as SIFT , BOW , FV and VLAD .", "ner": [["deep CNNs representations", "Method"], ["ImageNet", "Dataset"], ["handcrafted representations", "Method"], ["SIFT", "Method"], ["BOW", "Method"], ["FV", "Method"], ["VLAD", "Method"]], "rel": [["deep CNNs representations", "Trained-With", "ImageNet"], ["SIFT", "SubClass-Of", "handcrafted representations"], ["BOW", "SubClass-Of", "handcrafted representations"], ["FV", "SubClass-Of", "handcrafted representations"], ["VLAD", "SubClass-Of", "handcrafted representations"]], "rel_plus": [["deep CNNs representations:Method", "Trained-With", "ImageNet:Dataset"], ["SIFT:Method", "SubClass-Of", "handcrafted representations:Method"], ["BOW:Method", "SubClass-Of", "handcrafted representations:Method"], ["FV:Method", "SubClass-Of", "handcrafted representations:Method"], ["VLAD:Method", "SubClass-Of", "handcrafted representations:Method"]]}
{"doc_id": "4246700", "sentence": "The experimental setup in this section : the word embedding dimension and hidden state dimension of RNN are respectively set to 2 5 6 and 2 5 6 for multimodal method , and the learning rate of multimodal method is 0.0 0 0 1 .", "ner": [["word embedding", "Method"], ["RNN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "4246700", "sentence": "The word embedding dimension and hidden state dimension of LSTM are respectively set to 5 1 2 and 5 1 2 for attention based method , and the learning rate of attention based method is 0.0 0 0 1 .", "ner": [["word embedding", "Method"], ["LSTM", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "4246700", "sentence": "The metrics used in this paper including BLEU [ 5 7 ] , ROUGE L [ 5 8 ] , METEOR [ 5 9 ] , CIDEr [ 6 0 ] .", "ner": [["BLEU", "Method"], ["ROUGE", "Method"], ["METEOR", "Method"], ["CIDEr", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "4246700", "sentence": "CIDEr measures the consensus by adding a Term Frequency Inverse Document Frequency ( TF - IDF ) weighting for every n - gram .", "ner": [["CIDEr", "Method"], ["Term Frequency Inverse Document Frequency", "Method"], ["TF - IDF", "Method"]], "rel": [["Term Frequency Inverse Document Frequency", "Part-Of", "CIDEr"], ["TF - IDF", "Synonym-Of", "Term Frequency Inverse Document Frequency"]], "rel_plus": [["Term Frequency Inverse Document Frequency:Method", "Part-Of", "CIDEr:Method"], ["TF - IDF:Method", "Synonym-Of", "Term Frequency Inverse Document Frequency:Method"]]}
{"doc_id": "4246700", "sentence": "The CIDEr metric has more reference value compared with BLEU , ROUGE L , METEOR [ 6 0 ] .", "ner": [["CIDEr", "Method"], ["BLEU", "Method"], ["ROUGE L", "Method"], ["METEOR", "Method"]], "rel": [["CIDEr", "Compare-With", "BLEU"], ["CIDEr", "Compare-With", "ROUGE L"], ["CIDEr", "Compare-With", "METEOR"]], "rel_plus": [["CIDEr:Method", "Compare-With", "BLEU:Method"], ["CIDEr:Method", "Compare-With", "ROUGE L:Method"], ["CIDEr:Method", "Compare-With", "METEOR:Method"]]}
{"doc_id": "4246700", "sentence": "The range of the bleu 1 , bleu 2 , bleu 3 , bleu 4 , METEOR and ROUGE L is between 0 and 1 .", "ner": [["bleu 1", "Method"], ["bleu 2", "Method"], ["bleu 3", "Method"], ["bleu 4", "Method"], ["METEOR", "Method"], ["ROUGE L", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "4246700", "sentence": "In this subsection , we evaluate multimodal method based on different kinds of features with randomly 8 0 % for training , 1 0 % for validation and 1 0 % for test on UCM - captions dataset [ 2 6 ] , Sydney - captions [ 2 6 ] datasets and our dataset RSICD .", "ner": [["UCM - captions", "Dataset"], ["Sydney - captions", "Dataset"], ["RSICD", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "4246700", "sentence": "We firstly test four handcrafted representations for captioning , and then use the different CNNs . 1 ) Results based on handcrafted representations : To evaluate the generated sentences based on handcrafted representations , four handcrafted representations are conducted including SIFT , BOW , FV and VLAD .", "ner": [["handcrafted representations", "Method"], ["captioning", "Task"], ["CNNs", "Method"], ["handcrafted representations", "Method"], ["handcrafted representations", "Method"], ["handcrafted representations", "Method"], ["SIFT", "Method"], ["BOW", "Method"], ["FV", "Method"], ["VLAD", "Method"]], "rel": [["handcrafted representations", "Used-For", "captioning"], ["CNNs", "Used-For", "captioning"], ["SIFT", "SubClass-Of", "handcrafted representations"], ["BOW", "SubClass-Of", "handcrafted representations"], ["FV", "SubClass-Of", "handcrafted representations"], ["VLAD", "SubClass-Of", "handcrafted representations"]], "rel_plus": [["handcrafted representations:Method", "Used-For", "captioning:Task"], ["CNNs:Method", "Used-For", "captioning:Task"], ["SIFT:Method", "SubClass-Of", "handcrafted representations:Method"], ["BOW:Method", "SubClass-Of", "handcrafted representations:Method"], ["FV:Method", "SubClass-Of", "handcrafted representations:Method"], ["VLAD:Method", "SubClass-Of", "handcrafted representations:Method"]]}
{"doc_id": "4246700", "sentence": "For each patch , a SIFT feature is obtained by Principal Component Analysis ( PCA ) of the origin SIFT features .", "ner": [["SIFT", "Method"], ["Principal Component Analysis", "Method"], ["PCA", "Method"], ["SIFT", "Method"]], "rel": [["Principal Component Analysis", "Used-For", "SIFT"], ["PCA", "Synonym-Of", "Principal Component Analysis"]], "rel_plus": [["Principal Component Analysis:Method", "Used-For", "SIFT:Method"], ["PCA:Method", "Synonym-Of", "Principal Component Analysis:Method"]]}
{"doc_id": "4246700", "sentence": "Tables II - IV illustrate that the result of LSTM is better than that of RNN on UCM - captions dataset and RSICD dataset .", "ner": [["LSTM", "Method"], ["RNN", "Method"], ["UCM - captions", "Dataset"], ["RSICD", "Dataset"]], "rel": [["LSTM", "Compare-With", "RNN"], ["RNN", "Evaluated-With", "UCM - captions"], ["LSTM", "Evaluated-With", "RSICD"]], "rel_plus": [["LSTM:Method", "Compare-With", "RNN:Method"], ["RNN:Method", "Evaluated-With", "UCM - captions:Dataset"], ["LSTM:Method", "Evaluated-With", "RSICD:Dataset"]]}
{"doc_id": "4246700", "sentence": "Since the LSTM solves the long - term dependencies problem of RNNs , the sentence generated by LSTM can better depict a remote sensing image than the one generated by RNNs [ 5 5 ] .", "ner": [["LSTM", "Method"], ["RNNs", "Method"], ["LSTM", "Method"], ["RNNs", "Method"]], "rel": [["LSTM", "Compare-With", "RNNs"]], "rel_plus": [["LSTM:Method", "Compare-With", "RNNs:Method"]]}
{"doc_id": "4246700", "sentence": "For all four handcrafted representations , VLAD performs the best on UCM - captions dataset and RSICD dataset .", "ner": [["handcrafted representations", "Method"], ["VLAD", "Method"], ["UCM - captions", "Dataset"], ["RSICD", "Dataset"]], "rel": [["VLAD", "Evaluated-With", "UCM - captions"], ["VLAD", "Evaluated-With", "RSICD"]], "rel_plus": [["VLAD:Method", "Evaluated-With", "UCM - captions:Dataset"], ["VLAD:Method", "Evaluated-With", "RSICD:Dataset"]]}
{"doc_id": "4246700", "sentence": "The result of LSTM on Sydney - captions is not as good as RNN .", "ner": [["LSTM", "Method"], ["Sydney - captions", "Dataset"], ["RNN", "Method"]], "rel": [["LSTM", "Evaluated-With", "Sydney - captions"], ["RNN", "Evaluated-With", "Sydney - captions"], ["LSTM", "Compare-With", "RNN"]], "rel_plus": [["LSTM:Method", "Evaluated-With", "Sydney - captions:Dataset"], ["RNN:Method", "Evaluated-With", "Sydney - captions:Dataset"], ["LSTM:Method", "Compare-With", "RNN:Method"]]}
{"doc_id": "4246700", "sentence": "But according to [ 6 1 ] , LSTM should outperform RNN .", "ner": [["LSTM", "Method"], ["RNN", "Method"]], "rel": [["LSTM", "Compare-With", "RNN"]], "rel_plus": [["LSTM:Method", "Compare-With", "RNN:Method"]]}
{"doc_id": "4246700", "sentence": "This is perhaps caused by the imbalance of Sydney - captions and the further analysis is presented in Section V - D 1 . 2 ) Results based on different CNNs : In order to evaluate the generated sentences based on CNNs features , the experiments based on CNNs features are conducted in this subsection .", "ner": [["Sydney - captions", "Dataset"], ["CNNs", "Method"], ["CNNs", "Method"], ["CNNs", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "4246700", "sentence": "The experiments of different CNNs features on dataset UCMcaptions and Sydney - captions have been done in [ 2 6 ] , and the result on our dataset RSICD is shown in Table VI .", "ner": [["CNNs", "Method"], ["UCMcaptions", "Dataset"], ["Sydney - captions", "Dataset"], ["RSICD", "Dataset"]], "rel": [["CNNs", "Evaluated-With", "UCMcaptions"], ["CNNs", "Evaluated-With", "Sydney - captions"], ["CNNs", "Evaluated-With", "RSICD"]], "rel_plus": [["CNNs:Method", "Evaluated-With", "UCMcaptions:Dataset"], ["CNNs:Method", "Evaluated-With", "Sydney - captions:Dataset"], ["CNNs:Method", "Evaluated-With", "RSICD:Dataset"]]}
{"doc_id": "4246700", "sentence": "In CNNs features , AlexNet gets the best result on ROUGE L and CIDEr and VGG 1 9 gets the best result on other objective metrics with a little superiority than others .", "ner": [["CNNs", "Method"], ["AlexNet", "Method"], ["ROUGE L", "Method"], ["CIDEr", "Method"], ["VGG 1 9", "Method"]], "rel": [["AlexNet", "SubClass-Of", "CNNs"]], "rel_plus": [["AlexNet:Method", "SubClass-Of", "CNNs:Method"]]}
{"doc_id": "4246700", "sentence": "This means that representation of CNNs features are powerful for remote sensing image captioning task . 3 ) Results of different training ratios : In order to study the influence of the ratio of training images to the caption result , We use 1 0 % images of dataset for validation , and change the ratio of training and testing images to observe the result based on VGG 1 6 CNNs features .", "ner": [["CNNs", "Method"], ["remote sensing image captioning", "Task"], ["VGG 1 6 CNNs", "Method"]], "rel": [["CNNs", "Used-For", "remote sensing image captioning"]], "rel_plus": [["CNNs:Method", "Used-For", "remote sensing image captioning:Task"]]}
{"doc_id": "4246700", "sentence": "In order to evaluate the reference sentences of RSICD , the most reliable metric CIDEr is used .", "ner": [["RSICD", "Dataset"], ["CIDEr", "Method"]], "rel": [["CIDEr", "Used-For", "RSICD"]], "rel_plus": [["CIDEr:Method", "Used-For", "RSICD:Dataset"]]}
{"doc_id": "4246700", "sentence": "Since the attention based method is based on the convolutional feature of CNNs , the features used in attention based method are all convolutional features extracted by different CNN models .", "ner": [["attention based method", "Method"], ["convolutional feature", "Method"], ["CNNs", "Method"], ["convolutional features", "Method"], ["CNN", "Method"]], "rel": [["convolutional feature", "Used-For", "attention based method"], ["convolutional feature", "Part-Of", "CNNs"], ["CNN", "Used-For", "convolutional features"]], "rel_plus": [["convolutional feature:Method", "Used-For", "attention based method:Method"], ["convolutional feature:Method", "Part-Of", "CNNs:Method"], ["CNN:Method", "Used-For", "convolutional features:Method"]]}
{"doc_id": "4246700", "sentence": "Specifically , for VGG 1 6 , the feature maps of conv 5 3 sized 1 4 \u00d7 1 4 \u00d7 5 1 2 are used ; for VGG 1 9 , the feature maps of conv 5 4 sized 1 4 \u00d7 1 4 \u00d7 5 1 2 are used ; for AlexNet , the feature maps of conv 5 sized 1 3 \u00d7 1 3 \u00d7 2 5 6 are used ; for GoogLeNet , the feature maps of inception 4c/ 3 \u00d7 3 sized 1 4 \u00d7 1 4 \u00d7 5 1 2 are used .", "ner": [["VGG 1 6", "Method"], ["VGG 1 9", "Method"], ["AlexNet", "Method"], ["GoogLeNet", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "4246700", "sentence": "The \" hard \" attention mechanism based on the convolutional features extracted by GoogLeNet gets the best result on UCM - captions dataset and RSICD dataset .", "ner": [["\" hard \" attention mechanism", "Method"], ["convolutional features", "Method"], ["GoogLeNet", "Method"], ["UCM - captions", "Dataset"], ["RSICD", "Dataset"]], "rel": [["convolutional features", "Used-For", "\" hard \" attention mechanism"], ["GoogLeNet", "Used-For", "convolutional features"], ["\" hard \" attention mechanism", "Evaluated-With", "UCM - captions"], ["\" hard \" attention mechanism", "Evaluated-With", "RSICD"]], "rel_plus": [["convolutional features:Method", "Used-For", "\" hard \" attention mechanism:Method"], ["GoogLeNet:Method", "Used-For", "convolutional features:Method"], ["\" hard \" attention mechanism:Method", "Evaluated-With", "UCM - captions:Dataset"], ["\" hard \" attention mechanism:Method", "Evaluated-With", "RSICD:Dataset"]]}
{"doc_id": "4246700", "sentence": "But for Sydney - captions dataset , the result of \" soft \" attention mechanism based on the convolutional feature extracted by VGG 1 6 gets the best result . 2 ) Results of different training ratios : To evaluate the influence of different training ratios , the training ratio is changed to get different results .", "ner": [["Sydney - captions", "Dataset"], ["\" soft \" attention mechanism", "Method"], ["convolutional feature", "Method"], ["VGG 1 6", "Method"]], "rel": [["\" soft \" attention mechanism", "Evaluated-With", "Sydney - captions"], ["convolutional feature", "Used-For", "\" soft \" attention mechanism"], ["VGG 1 6", "Used-For", "convolutional feature"]], "rel_plus": [["\" soft \" attention mechanism:Method", "Evaluated-With", "Sydney - captions:Dataset"], ["convolutional feature:Method", "Used-For", "\" soft \" attention mechanism:Method"], ["VGG 1 6:Method", "Used-For", "convolutional feature:Method"]]}
{"doc_id": "4246700", "sentence": "Features extracted from convolutional layer of AlexNet can represent exhaustive content of remote sensing images .", "ner": [["convolutional layer", "Method"], ["AlexNet", "Method"]], "rel": [["convolutional layer", "Part-Of", "AlexNet"]], "rel_plus": [["convolutional layer:Method", "Part-Of", "AlexNet:Method"]]}
{"doc_id": "4246700", "sentence": "This is because the representation capability of AlexNet for remote sensing image captioning task is powerful .", "ner": [["AlexNet", "Method"], ["remote sensing image captioning", "Task"]], "rel": [["AlexNet", "Used-For", "remote sensing image captioning"]], "rel_plus": [["AlexNet:Method", "Used-For", "remote sensing image captioning:Task"]]}
{"doc_id": "4246700", "sentence": "Then , human subjective evaluation is performed to compare the generalization capabilities of models trained on different datasets . 1 ) The imbalance of Sydney - captions : To verify the influence of unbalance of different kinds image numbers , we present the result of FV using different numbers of cluster center as the results of the FV is related to the number of cluster centers to construct a Gaussian Mixture Model ( GMM ) .", "ner": [["Sydney - captions", "Dataset"], ["FV", "Method"], ["FV", "Method"], ["Gaussian Mixture Model", "Method"], ["GMM", "Method"]], "rel": [["GMM", "Synonym-Of", "Gaussian Mixture Model"], ["FV", "Part-Of", "Gaussian Mixture Model"]], "rel_plus": [["GMM:Method", "Synonym-Of", "Gaussian Mixture Model:Method"], ["FV:Method", "Part-Of", "Gaussian Mixture Model:Method"]]}
{"doc_id": "4246700", "sentence": "From the result of multimodal method based on FV of Sydney - captions dataset in Figure 1 0 , it can be seen that the change of the metrics scores is not significantly related to cluster center numbers when the number of cluster centers are in the range of 1 to 1 0 .", "ner": [["multimodal method based on FV", "Method"], ["Sydney - captions", "Dataset"]], "rel": [["multimodal method based on FV", "Evaluated-With", "Sydney - captions"]], "rel_plus": [["multimodal method based on FV:Method", "Evaluated-With", "Sydney - captions:Dataset"]]}
{"doc_id": "4246700", "sentence": "For simplicity , the metrics shown in Table X are METEOR and CIDEr .", "ner": [["METEOR", "Method"], ["CIDEr", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "4246700", "sentence": "UCM model represents the model trained on UCM - captions datasets .", "ner": [["UCM model", "Method"], ["UCM - captions", "Dataset"]], "rel": [["UCM model", "Trained-With", "UCM - captions"]], "rel_plus": [["UCM model:Method", "Trained-With", "UCM - captions:Dataset"]]}
{"doc_id": "4246700", "sentence": "From Table X , it can be found that the generalization capabilities of the model trained on RSICD is better than model trained on UCM - captions dataset evaluated on the test images of Sydney - captions dataset .", "ner": [["RSICD", "Dataset"], ["UCM - captions", "Dataset"], ["Sydney - captions", "Dataset"]], "rel": [["RSICD", "Compare-With", "UCM - captions"]], "rel_plus": [["RSICD:Dataset", "Compare-With", "UCM - captions:Dataset"]]}
{"doc_id": "4246700", "sentence": "And the generalization capabilities of the model trained on UCM - captions dataset is better than model trained on Sydney - captions dataset evaluated on the test images of RSICD .", "ner": [["UCM - captions", "Dataset"], ["Sydney - captions", "Dataset"], ["RSICD", "Dataset"]], "rel": [["UCM - captions", "Compare-With", "Sydney - captions"]], "rel_plus": [["UCM - captions:Dataset", "Compare-With", "Sydney - captions:Dataset"]]}
{"doc_id": "4246700", "sentence": "The results of models trained on different datasets evaluating on test images of different datasets are shown in Tables XI - XIII . that 4 0 % sentences generated by model trained on RSICD are unrelated to test images of Sydney - captions dataset .", "ner": [["RSICD", "Dataset"], ["Sydney - captions", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "4246700", "sentence": "In this paper , we give the instructions to describe remote sensing images comprehensively , and construct a remote sensing image captioning dataset ( RSICD ) .", "ner": [["remote sensing image captioning dataset", "Dataset"], ["RSICD", "Dataset"]], "rel": [["RSICD", "Synonym-Of", "remote sensing image captioning dataset"]], "rel_plus": [["RSICD:Dataset", "Synonym-Of", "remote sensing image captioning dataset:Dataset"]]}
{"doc_id": "4246700", "sentence": "Through extensive experiments , we give benchmarks on our dataset using the BLEU , METEOR , ROUGE L and CIDEr metric .", "ner": [["BLEU", "Method"], ["METEOR", "Method"], ["ROUGE L", "Method"], ["CIDEr", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "4246700", "sentence": "And we plan to apply some new techniques in image processing field and natural language processing field to remote sensing image caption generation task .", "ner": [["image processing", "Task"], ["natural language processing", "Task"], ["remote sensing image caption generation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "210839714", "sentence": "Specifically , we discuss that Transformer is a suitable basis model to learn the hidden EHR structure , and propose Graph Convolutional Transformer , which uses data statistics to guide the structure learning process .", "ner": [["Transformer", "Method"], ["Graph Convolutional Transformer", "Method"]], "rel": [["Graph Convolutional Transformer", "SubClass-Of", "Transformer"]], "rel_plus": [["Graph Convolutional Transformer:Method", "SubClass-Of", "Transformer:Method"]]}
{"doc_id": "210839714", "sentence": "The proposed model consistently outperformed previous approaches empirically , on both synthetic data and publicly available EHR data , for various prediction tasks such as graph reconstruction and readmission prediction , indicating that it can serve as an effective general - purpose representation learning algorithm for EHR data .", "ner": [["graph reconstruction", "Task"], ["readmission prediction", "Task"], ["representation learning", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210839714", "sentence": "Large medical records collected by electronic healthcare records ( EHR ) systems in healthcare organizations enabled deep learning methods to show impressive performance in diverse tasks such as predicting diagnosis [ 1 ] [ 2 ] [ 3 ] , learning medical concept representations [ 4 ] [ 5 ] [ 6 ] [ 7 ] , and making interpretable predictions [ 8 , 9 ] .", "ner": [["deep learning", "Method"], ["predicting diagnosis", "Task"], ["learning medical concept representations", "Task"], ["making interpretable predictions", "Task"]], "rel": [["deep learning", "Used-For", "predicting diagnosis"], ["deep learning", "Used-For", "learning medical concept representations"], ["deep learning", "Used-For", "making interpretable predictions"]], "rel_plus": [["deep learning:Method", "Used-For", "predicting diagnosis:Task"], ["deep learning:Method", "Used-For", "learning medical concept representations:Task"], ["deep learning:Method", "Used-For", "making interpretable predictions:Task"]]}
{"doc_id": "210839714", "sentence": "Then we propose the Graph Convolutional Transformer ( GCT ) to more effectively utilize the characteristics of EHR data while performing diverse prediction tasks .", "ner": [["Graph Convolutional Transformer", "Method"], ["GCT", "Method"]], "rel": [["GCT", "Synonym-Of", "Graph Convolutional Transformer"]], "rel_plus": [["GCT:Method", "Synonym-Of", "Graph Convolutional Transformer:Method"]]}
{"doc_id": "210839714", "sentence": "We test the Transformer and GCT on both synthetic data and real - world EHR records for encounter - based prediction tasks such as graph reconstruction and readmission prediction .", "ner": [["Transformer", "Method"], ["GCT", "Method"], ["encounter - based prediction tasks", "Task"], ["graph reconstruction", "Task"], ["readmission prediction", "Task"]], "rel": [["graph reconstruction", "SubTask-Of", "encounter - based prediction tasks"], ["readmission prediction", "SubTask-Of", "encounter - based prediction tasks"], ["GCT", "Used-For", "encounter - based prediction tasks"], ["Transformer", "Used-For", "encounter - based prediction tasks"], ["GCT", "Used-For", "graph reconstruction"], ["Transformer", "Used-For", "graph reconstruction"], ["Transformer", "Used-For", "readmission prediction"], ["GCT", "Used-For", "readmission prediction"]], "rel_plus": [["graph reconstruction:Task", "SubTask-Of", "encounter - based prediction tasks:Task"], ["readmission prediction:Task", "SubTask-Of", "encounter - based prediction tasks:Task"], ["GCT:Method", "Used-For", "encounter - based prediction tasks:Task"], ["Transformer:Method", "Used-For", "encounter - based prediction tasks:Task"], ["GCT:Method", "Used-For", "graph reconstruction:Task"], ["Transformer:Method", "Used-For", "graph reconstruction:Task"], ["Transformer:Method", "Used-For", "readmission prediction:Task"], ["GCT:Method", "Used-For", "readmission prediction:Task"]]}
{"doc_id": "210839714", "sentence": "In all tasks , GCT consistently outperformed baseline models , showing its potential to serve as an effective general - purpose representation learning algorithm for EHR data .", "ner": [["GCT", "Method"], ["representation learning", "Method"]], "rel": [["GCT", "SubClass-Of", "representation learning"]], "rel_plus": [["GCT:Method", "SubClass-Of", "representation learning:Method"]]}
{"doc_id": "210839714", "sentence": "By outperforming various bag - of - features models in heart failure prediction and general disease prediction , MiME demonstrated the usefulness of the structure information of encounter records .", "ner": [["heart failure prediction", "Task"], ["general disease prediction", "Task"], ["MiME", "Method"]], "rel": [["MiME", "Used-For", "heart failure prediction"], ["MiME", "Used-For", "general disease prediction"]], "rel_plus": [["MiME:Method", "Used-For", "heart failure prediction:Task"], ["MiME:Method", "Used-For", "general disease prediction:Task"]]}
{"doc_id": "210839714", "sentence": "The Transformer [ 1 1 ] was proposed for natural language processing , specifically machine translation .", "ner": [["Transformer", "Method"], ["natural language processing", "Task"], ["machine translation", "Task"]], "rel": [["machine translation", "SubTask-Of", "natural language processing"], ["Transformer", "Used-For", "natural language processing"], ["Transformer", "Used-For", "machine translation"]], "rel_plus": [["machine translation:Task", "SubTask-Of", "natural language processing:Task"], ["Transformer:Method", "Used-For", "natural language processing:Task"], ["Transformer:Method", "Used-For", "machine translation:Task"]]}
{"doc_id": "210839714", "sentence": "Given c i and A , we can use graph networks or MiME 2 to derive the visit representation v and use it for downstream tasks such as heart failure prediction .", "ner": [["graph networks", "Method"], ["MiME", "Method"], ["heart failure prediction", "Task"]], "rel": [["MiME", "Used-For", "heart failure prediction"], ["graph networks", "Used-For", "heart failure prediction"]], "rel_plus": [["MiME:Method", "Used-For", "heart failure prediction:Task"], ["graph networks:Method", "Used-For", "heart failure prediction:Task"]]}
{"doc_id": "210839714", "sentence": "To elaborate , we draw a comparison between two cases : \u2022 Case A : We know A , hence we can use Graph Convolutional Networks ( GCN ) .", "ner": [["Graph Convolutional Networks", "Method"], ["GCN", "Method"]], "rel": [["GCN", "Synonym-Of", "Graph Convolutional Networks"]], "rel_plus": [["GCN:Method", "Synonym-Of", "Graph Convolutional Networks:Method"]]}
{"doc_id": "210839714", "sentence": "MLP ( j ) is a multi - layer perceptron of the j - th convolution with its own trainable parameters . \u2022 Case B : We do not know A , hence we use Transformer , specifically the encoder with a single - head attention , which can be formulated as Figure 3 : Creating the conditional probability matrix P based on an example encounter .", "ner": [["MLP", "Method"], ["multi - layer perceptron", "Method"], ["convolution", "Method"], ["Transformer", "Method"]], "rel": [["multi - layer perceptron", "Synonym-Of", "MLP"]], "rel_plus": [["multi - layer perceptron:Method", "Synonym-Of", "MLP:Method"]]}
{"doc_id": "210839714", "sentence": "In fact , GCN can be seen as a special case of Transformer , where the attention mechanism is replaced with the known , fixed adjacency matrix .", "ner": [["GCN", "Method"], ["Transformer", "Method"], ["attention mechanism", "Method"]], "rel": [["attention mechanism", "Part-Of", "Transformer"], ["GCN", "SubClass-Of", "Transformer"]], "rel_plus": [["attention mechanism:Method", "Part-Of", "Transformer:Method"], ["GCN:Method", "SubClass-Of", "Transformer:Method"]]}
{"doc_id": "210839714", "sentence": "Therefore we propose Graph Convolutional Transformer ( GCT ) , which , based on data statistics , restricts the search to the space where it is likely to contain meaningful attention distribution .", "ner": [["Graph Convolutional Transformer", "Method"], ["GCT", "Method"]], "rel": [["GCT", "Synonym-Of", "Graph Convolutional Transformer"]], "rel_plus": [["GCT:Method", "Synonym-Of", "Graph Convolutional Transformer:Method"]]}
{"doc_id": "210839714", "sentence": "Note that GCT 's attention softmax ( Q ( j ) K ( j ) \u221a d ) , the mask M , and the conditional probabilities P are of the same size .", "ner": [["GCT", "Method"], ["attention softmax", "Method"]], "rel": [["attention softmax", "Part-Of", "GCT"]], "rel_plus": [["attention softmax:Method", "Part-Of", "GCT:Method"]]}
{"doc_id": "210839714", "sentence": "Therefore GCT uses the following formulation : Self - attention : In preliminary experiments , we noticed that attentions were often uniformly distributed in the first block of Transformer .", "ner": [["GCT", "Method"], ["Self - attention", "Method"], ["Transformer", "Method"]], "rel": [["Self - attention", "Part-Of", "GCT"]], "rel_plus": [["Self - attention:Method", "Part-Of", "GCT:Method"]]}
{"doc_id": "210839714", "sentence": "However , we do not want GCT to drastically deviate from the informative P , but rather gradually improve upon P. Therefore , based on the fact that attention is itself a probability distribution , and inspired by Trust Region Policy Optimization [ 2 4 ] , we sequentially penalize attention of j - th block if it deviates too much from the attention of j \u2212 1 - th block , using KL divergence .", "ner": [["GCT", "Method"], ["Trust Region Policy Optimization", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210839714", "sentence": "To test GCT on real - world EHR records , we use Philips eICU Collaborative Research Dataset 5 [ 2 5 ] . eICU consists of Intensive Care Unit ( ICU ) records filtered for remote caregivers , collected from multiple sites in the United States between 2 0 1 4 and 2 0 1 5 .", "ner": [["GCT", "Method"], ["Philips eICU Collaborative Research", "Dataset"], ["eICU", "Dataset"]], "rel": [["GCT", "Evaluated-With", "Philips eICU Collaborative Research"]], "rel_plus": [["GCT:Method", "Evaluated-With", "Philips eICU Collaborative Research:Dataset"]]}
{"doc_id": "210839714", "sentence": "Table 1 summarizes the data statistics . \u2022 GCN : Given the true adjacency matrix A , we follow Eq. ( 1 ) to learn the feature representations c i of each feature c i in a visit V. The visit embedding v ( i.e. graph - level representation ) is obtained from the placeholder visit node v. This model will serve as the optimal model during the experiments . \u2022 GCN P : Instead of the true adjacency matrix A , we use the conditional probability matrix P , and follow Eq. ( 1 ) . \u2022 GCN random : Instead of the true adjacency matrix A , we use a randomly generated normalized adjacency matrix where each element is indepdently sampled from a uniform distribution between 0 and 1 .", "ner": [["GCN", "Method"], ["GCN", "Method"], ["GCN random", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210839714", "sentence": "The visit representation v is obtained by simply summing all c i ' s. We use layer normalization [ 2 6 ] , drop - out [ 2 7 ] and residual connections [ 2 8 ] between layers . \u2022 Deep : We use multiple feedforward layers with ReLU activations ( including layer normalization , drop - out and residual connections ) on top of shallow to increase the expressivity .", "ner": [["layer normalization", "Method"], ["drop - out", "Method"], ["residual connections", "Method"], ["feedforward layers", "Method"], ["ReLU", "Method"], ["layer normalization", "Method"], ["drop - out", "Method"], ["residual connections", "Method"]], "rel": [["ReLU", "Part-Of", "feedforward layers"], ["layer normalization", "Part-Of", "feedforward layers"], ["drop - out", "Part-Of", "feedforward layers"], ["residual connections", "Part-Of", "feedforward layers"]], "rel_plus": [["ReLU:Method", "Part-Of", "feedforward layers:Method"], ["layer normalization:Method", "Part-Of", "feedforward layers:Method"], ["drop - out:Method", "Part-Of", "feedforward layers:Method"], ["residual connections:Method", "Part-Of", "feedforward layers:Method"]]}
{"doc_id": "210839714", "sentence": "Parentheses indicate which dataset is used for each task . \u2022 Graph reconstruction ( Synthetic ): Given an encounter with N features , we train models to learn N feature embeddings C , and predict whether there is an edge between every pair of features , by performing an inner - product between each feature embedding pairs c i and c j ( i.e. N 2 binary predictions ) .", "ner": [["Graph reconstruction", "Task"], ["Synthetic", "Dataset"]], "rel": [["Synthetic", "Benchmark-For", "Graph reconstruction"]], "rel_plus": [["Synthetic:Dataset", "Benchmark-For", "Graph reconstruction:Task"]]}
{"doc_id": "210839714", "sentence": "We do not use Deep baseline for this task , as we need individual embeddings for all features c i ' s. \u2022 Diagnosis - Treatment classification ( Synthetic ): We assign labels to an encounter if there are specific diagnosis ( d 1 and d 2 ) and treatment code ( m 1 ) connections .", "ner": [["Diagnosis - Treatment classification", "Task"], ["Synthetic", "Dataset"]], "rel": [["Synthetic", "Benchmark-For", "Diagnosis - Treatment classification"]], "rel_plus": [["Synthetic:Dataset", "Benchmark-For", "Diagnosis - Treatment classification:Task"]]}
{"doc_id": "210839714", "sentence": "Further details on the labels are provided in Appendix B. This is a multi - label prediction task using the visit representation v. \u2022 Masked diagnosis code prediction ( Synthetic , eICU ): Given an encounter record , we mask a random diagnosis code d i .", "ner": [["Masked diagnosis code prediction", "Task"], ["Synthetic", "Dataset"], ["eICU", "Dataset"]], "rel": [["Synthetic", "Benchmark-For", "Masked diagnosis code prediction"], ["eICU", "Benchmark-For", "Masked diagnosis code prediction"]], "rel_plus": [["Synthetic:Dataset", "Benchmark-For", "Masked diagnosis code prediction:Task"], ["eICU:Dataset", "Benchmark-For", "Masked diagnosis code prediction:Task"]]}
{"doc_id": "210839714", "sentence": "The row and the column of the conditional probability matrix P that correspond to the masked diagnosis were also masked to zeroes . \u2022 Readmission prediction ( eICU ): Given an encounter record , we train models to learn the visit embedding v to predict whether the patient will be admitted to the ICU again during the same hospital stay , i.e. , a binary prediction .", "ner": [["Readmission prediction", "Task"], ["eICU", "Dataset"]], "rel": [["eICU", "Benchmark-For", "Readmission prediction"]], "rel_plus": [["eICU:Dataset", "Benchmark-For", "Readmission prediction:Task"]]}
{"doc_id": "210839714", "sentence": "The prevalence is approximately 1 7 . 2 % . \u2022 Mortality prediction ( eICU ): Given an encounter record , we train models to learn the visit embedding v to predict patient death during the ICU admission , i.e. , a binary prediction .", "ner": [["Mortality prediction", "Task"], ["eICU", "Dataset"]], "rel": [["eICU", "Benchmark-For", "Mortality prediction"]], "rel_plus": [["eICU:Dataset", "Benchmark-For", "Mortality prediction:Task"]]}
{"doc_id": "210839714", "sentence": "Further training details and hyperparameter settings are described in Appendix C. Table 2 shows the graph reconstruction performance and the diagnosis - treatment classification performance of all models .", "ner": [["graph reconstruction", "Task"], ["diagnosis - treatment classification", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "210839714", "sentence": "Naturally , GCN shows the best performance since it uses the true adajcency matrix A. Given that GCN P 's performance is only marginally inferior to Transformer , we can infer that the conditional probability is indeed indicative of the true structure .", "ner": [["GCN", "Method"], ["GCN P", "Method"], ["Transformer", "Method"]], "rel": [["GCN P", "Compare-With", "Transformer"]], "rel_plus": [["GCN P:Method", "Compare-With", "Transformer:Method"]]}
{"doc_id": "210839714", "sentence": "GCT , which combines the strength of both GCN P and Transformer shows the best performance , besides GCN .", "ner": [["GCT", "Method"], ["GCN P", "Method"], ["Transformer", "Method"], ["GCN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210839714", "sentence": "Diagnosis - treatment classification , on the other hand , clearly penalizes randomly attending to the features , since GCN random shows the worst performance .", "ner": [["Diagnosis - treatment classification", "Task"], ["GCN random", "Method"]], "rel": [["GCN random", "Used-For", "Diagnosis - treatment classification"]], "rel_plus": [["GCN random:Method", "Used-For", "Diagnosis - treatment classification:Task"]]}
{"doc_id": "210839714", "sentence": "GCN could not be evaluated on eICU , since eICU does not have the true structure .", "ner": [["GCN", "Method"], ["eICU", "Dataset"], ["eICU", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "210839714", "sentence": "Table 4 shows the readmission prediction and mortality prediction performance of all models on eICU .", "ner": [["readmission prediction", "Task"], ["mortality prediction", "Task"], ["eICU", "Dataset"]], "rel": [["eICU", "Benchmark-For", "readmission prediction"], ["eICU", "Benchmark-For", "mortality prediction"]], "rel_plus": [["eICU:Dataset", "Benchmark-For", "readmission prediction:Task"], ["eICU:Dataset", "Benchmark-For", "mortality prediction:Task"]]}
{"doc_id": "210839714", "sentence": "As shown by GCN P and GCT 's superior performance , it is evident that readmission prediction heavily benefits from using the latent encounter structure .", "ner": [["GCN P", "Method"], ["GCT", "Method"], ["readmission prediction", "Task"]], "rel": [["GCT", "Used-For", "readmission prediction"], ["GCN P", "Used-For", "readmission prediction"]], "rel_plus": [["GCT:Method", "Used-For", "readmission prediction:Task"], ["GCN P:Method", "Used-For", "readmission prediction:Task"]]}
{"doc_id": "210839714", "sentence": "Mortality prediction , on the other hand , seems to rely little on the encounter structure , as can be seen from the marginally superior performance of GCT .", "ner": [["Mortality prediction", "Task"], ["GCT", "Method"]], "rel": [["GCT", "Used-For", "Mortality prediction"]], "rel_plus": [["GCT:Method", "Used-For", "Mortality prediction:Task"]]}
{"doc_id": "210839714", "sentence": "These two experiments indicate that not all prediction tasks require the true encounter structure , and it is our future work to apply GCT to various prediction tasks to evaluate its effectiveness .   In this section , we analyze the learned structure of both Transformer and GCT .", "ner": [["GCT", "Method"], ["Transformer", "Method"], ["GCT", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210839714", "sentence": "For Transformer and GCT , we calculated KL divergence betweenD \u2212 1 \u00c3 and the attention maps in each self - attention block , and averaged the results .", "ner": [["Transformer", "Method"], ["GCT", "Method"], ["KL divergence", "Method"], ["self - attention block", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210839714", "sentence": "GCT shows similar performance to GCN P in all tasks , and was even able to improve upon P in the graph reconstruction task .", "ner": [["GCT", "Method"], ["GCN P", "Method"], ["graph reconstruction", "Task"]], "rel": [["GCT", "Compare-With", "GCN P"], ["GCT", "Used-For", "graph reconstruction"], ["GCN P", "Used-For", "graph reconstruction"]], "rel_plus": [["GCT:Method", "Compare-With", "GCN P:Method"], ["GCT:Method", "Used-For", "graph reconstruction:Task"], ["GCN P:Method", "Used-For", "graph reconstruction:Task"]]}
{"doc_id": "210839714", "sentence": "Transformer showing strong performance in graph reconstruction , even with attentions significantly different from the true structure , again indicates the importance of just attending to other features in graph reconstruction , which was discussed in Section 4. 5 regarding the performance of GCN random .", "ner": [["Transformer", "Method"], ["graph reconstruction", "Task"], ["graph reconstruction", "Task"], ["GCN random", "Method"]], "rel": [["Transformer", "Used-For", "graph reconstruction"]], "rel_plus": [["Transformer:Method", "Used-For", "graph reconstruction:Task"]]}
{"doc_id": "210839714", "sentence": "We show visual examples of attention behavior of both Transformer and GCT in Appendix D. Learning effective patterns from raw EHR data is an essential step for improving the performance of many downstream prediction tasks .", "ner": [["Transformer", "Method"], ["GCT", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210839714", "sentence": "For GCN , GCN P , GCN random , Transformer , and GCT , we used undirected adjacency/attention matrix to enhance the message passing efficiency .", "ner": [["GCN", "Method"], ["GCN P", "Method"], ["GCN random", "Method"], ["Transformer", "Method"], ["GCT", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210839714", "sentence": "Tunable hyperparameters for models Shallow , Deep , GCN , GCN P , GCN random , and Transformer are as follows : \u2022 Adam learning rate ( 0.0 0 0 1 \u223c 0. 1 ) \u2022 Drop - out rate between layers ( 0.0 \u223c 0. 9 ) Shallow used 1 5 feedforward layers and Deep used 8 feedforward layers before , and 7 feedforward layers after summing the embeddings .", "ner": [["Shallow", "Method"], ["Deep", "Method"], ["GCN", "Method"], ["GCN P", "Method"], ["GCN random", "Method"], ["Transformer", "Method"], ["Adam", "Method"], ["Drop - out", "Method"], ["Shallow", "Method"], ["feedforward layers", "Method"], ["Deep", "Method"], ["feedforward layers", "Method"], ["feedforward layers", "Method"]], "rel": [["Adam", "Part-Of", "Shallow"], ["Drop - out", "Part-Of", "Shallow"], ["Adam", "Part-Of", "Deep"], ["Drop - out", "Part-Of", "Deep"], ["Adam", "Part-Of", "GCN"], ["Drop - out", "Part-Of", "GCN"], ["Adam", "Part-Of", "GCN P"], ["Drop - out", "Part-Of", "GCN P"], ["Adam", "Part-Of", "GCN random"], ["Drop - out", "Part-Of", "GCN random"], ["Adam", "Part-Of", "Transformer"], ["Drop - out", "Part-Of", "Transformer"], ["feedforward layers", "Part-Of", "Shallow"], ["feedforward layers", "Part-Of", "Deep"], ["feedforward layers", "Part-Of", "Deep"]], "rel_plus": [["Adam:Method", "Part-Of", "Shallow:Method"], ["Drop - out:Method", "Part-Of", "Shallow:Method"], ["Adam:Method", "Part-Of", "Deep:Method"], ["Drop - out:Method", "Part-Of", "Deep:Method"], ["Adam:Method", "Part-Of", "GCN:Method"], ["Drop - out:Method", "Part-Of", "GCN:Method"], ["Adam:Method", "Part-Of", "GCN P:Method"], ["Drop - out:Method", "Part-Of", "GCN P:Method"], ["Adam:Method", "Part-Of", "GCN random:Method"], ["Drop - out:Method", "Part-Of", "GCN random:Method"], ["Adam:Method", "Part-Of", "Transformer:Method"], ["Drop - out:Method", "Part-Of", "Transformer:Method"], ["feedforward layers:Method", "Part-Of", "Shallow:Method"], ["feedforward layers:Method", "Part-Of", "Deep:Method"], ["feedforward layers:Method", "Part-Of", "Deep:Method"]]}
{"doc_id": "210839714", "sentence": "The number of layers were chosen to match the number of trainable parameters of Transformer and GCT .", "ner": [["Transformer", "Method"], ["GCT", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210839714", "sentence": "GCN , GCN P , GCN random and Transformer three self - attention blocks , which was sufficient to cover the entire depth of EHR encounters .", "ner": [["GCN", "Method"], ["GCN P", "Method"], ["GCN random", "Method"], ["Transformer", "Method"], ["self - attention blocks", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210839714", "sentence": "Transformer used one attention head to match its representative power to GCN , GCN P , and GCN random , and so that we can accurately evaluate the effect of learning the correct encounter structure .", "ner": [["Transformer", "Method"], ["attention head", "Method"], ["GCN", "Method"], ["GCN P", "Method"], ["GCN random", "Method"]], "rel": [["attention head", "Part-Of", "Transformer"]], "rel_plus": [["attention head:Method", "Part-Of", "Transformer:Method"]]}
{"doc_id": "210839714", "sentence": "Tunable hyperparameters for GCT are as follows : \u2022 Adam learning rate ( 0.0 0 0 1 \u223c 0. 1 ) \u2022 Drop - out rate between layers ( 0.0 \u223c 0. 9 ) \u2022 Regularization coefficient ( 0.0 1 \u223c 1 0 0 . 0 ) GCT also used three self - attention blocks and one attention head .", "ner": [["GCT", "Method"], ["Adam", "Method"], ["Drop - out", "Method"], ["GCT", "Method"], ["self - attention blocks", "Method"], ["attention head", "Method"]], "rel": [["Adam", "Part-Of", "GCT"], ["Drop - out", "Part-Of", "GCT"], ["attention head", "Part-Of", "GCT"], ["self - attention blocks", "Part-Of", "GCT"]], "rel_plus": [["Adam:Method", "Part-Of", "GCT:Method"], ["Drop - out:Method", "Part-Of", "GCT:Method"], ["attention head:Method", "Part-Of", "GCT:Method"], ["self - attention blocks:Method", "Part-Of", "GCT:Method"]]}
{"doc_id": "210839714", "sentence": "In this section , we compare the attention behavior of Transformer and GCT in two different context ; graph reconstruction and masked diagnosis code prediction .", "ner": [["Transformer", "Method"], ["GCT", "Method"], ["graph reconstruction", "Task"], ["masked diagnosis code prediction", "Task"]], "rel": [["Transformer", "Used-For", "graph reconstruction"], ["GCT", "Used-For", "graph reconstruction"], ["Transformer", "Used-For", "masked diagnosis code prediction"], ["GCT", "Used-For", "masked diagnosis code prediction"]], "rel_plus": [["Transformer:Method", "Used-For", "graph reconstruction:Task"], ["GCT:Method", "Used-For", "graph reconstruction:Task"], ["Transformer:Method", "Used-For", "masked diagnosis code prediction:Task"], ["GCT:Method", "Used-For", "masked diagnosis code prediction:Task"]]}
{"doc_id": "210839714", "sentence": "Figure 4 shows Transformer 's attentions in each self - attention block when performing graph reconstruction .", "ner": [["Transformer", "Method"], ["self - attention block", "Method"], ["graph reconstruction", "Task"]], "rel": [["self - attention block", "Part-Of", "Transformer"], ["Transformer", "Used-For", "graph reconstruction"]], "rel_plus": [["self - attention block:Method", "Part-Of", "Transformer:Method"], ["Transformer:Method", "Used-For", "graph reconstruction:Task"]]}
{"doc_id": "210839714", "sentence": "Figure 5 shows GCT 's attention in each self - attention blcok when performing graph reconstruction .", "ner": [["GCT", "Method"], ["self - attention blcok", "Method"], ["graph reconstruction", "Task"]], "rel": [["self - attention blcok", "Part-Of", "GCT"], ["GCT", "Used-For", "graph reconstruction"]], "rel_plus": [["self - attention blcok:Method", "Part-Of", "GCT:Method"], ["GCT:Method", "Used-For", "graph reconstruction:Task"]]}
{"doc_id": "210839714", "sentence": "Contrary to Transformer , GCT starts with a very specific attention distribution .", "ner": [["Transformer", "Method"], ["GCT", "Method"]], "rel": [["Transformer", "Compare-With", "GCT"]], "rel_plus": [["Transformer:Method", "Compare-With", "GCT:Method"]]}
{"doc_id": "210839714", "sentence": "Since the goal of the graph reconstruction task is to predict the edges between nodes , it may be an obvious result that both Transformer and GCT 's attentions mimic the true adjacency matrix .", "ner": [["graph reconstruction", "Task"], ["Transformer", "Method"], ["GCT", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210839714", "sentence": "Therefore , we show another set of attentions from Transformer and GCT trained for the masked diagnosis code prediction task .", "ner": [["Transformer", "Method"], ["GCT", "Method"], ["diagnosis code prediction task", "Task"]], "rel": [["Transformer", "Used-For", "diagnosis code prediction task"], ["GCT", "Used-For", "diagnosis code prediction task"]], "rel_plus": [["Transformer:Method", "Used-For", "diagnosis code prediction task:Task"], ["GCT:Method", "Used-For", "diagnosis code prediction task:Task"]]}
{"doc_id": "210839714", "sentence": "Similar to graph reconstruction , Transformer starts with an evenly distributed attentions , and develops its own structure .", "ner": [["graph reconstruction", "Task"], ["Transformer", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210839714", "sentence": "Therefore , unlike in the graph reconstruction task , GCT puts heavy attention to the Visit node in this task , in order to learn the co - occurring diagnosis codes .", "ner": [["graph reconstruction", "Task"], ["GCT", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "6116678", "sentence": "Salient object detection has recently witnessed substantial progress due to powerful features extracted using deep convolutional neural networks ( CNNs ) .", "ner": [["Salient object detection", "Task"], ["deep convolutional neural networks", "Method"], ["CNNs", "Method"]], "rel": [["deep convolutional neural networks", "Used-For", "Salient object detection"], ["CNNs", "Synonym-Of", "deep convolutional neural networks"]], "rel_plus": [["deep convolutional neural networks:Method", "Used-For", "Salient object detection:Task"], ["CNNs:Method", "Synonym-Of", "deep convolutional neural networks:Method"]]}
{"doc_id": "6116678", "sentence": "Finally , a fully connected CRF model can be optionally incorporated to improve spatial coherence and contour localization in the fused result from these two streams .", "ner": [["fully connected CRF", "Method"], ["improve spatial coherence", "Task"], ["contour localization", "Task"]], "rel": [["fully connected CRF", "Used-For", "improve spatial coherence"], ["fully connected CRF", "Used-For", "contour localization"]], "rel_plus": [["fully connected CRF:Method", "Used-For", "improve spatial coherence:Task"], ["fully connected CRF:Method", "Used-For", "contour localization:Task"]]}
{"doc_id": "6116678", "sentence": "Though early work primarily focused on predicting eye - fixations in images , research has shown that salient object detection , which emphasizes object - level integrity of saliency prediction results , is more useful and can serve as a pre - processing step for a variety of computer vision and image processing tasks including content - aware image editing [ 3 ] , object detection [ 3 7 ] , image classification [ 4 6 ] , person re - identification [ 4 ] and video summarization [ 3 3 ] .", "ner": [["salient object detection", "Task"], ["saliency prediction", "Task"], ["computer vision", "Task"], ["image processing", "Task"], ["content - aware image editing", "Task"], ["object detection", "Task"], ["image classification", "Task"], ["person re - identification", "Task"], ["video summarization", "Task"]], "rel": [["salient object detection", "SubTask-Of", "saliency prediction"], ["salient object detection", "SubTask-Of", "computer vision"], ["content - aware image editing", "SubTask-Of", "image processing"], ["object detection", "SubTask-Of", "image processing"], ["image classification", "SubTask-Of", "image processing"], ["person re - identification", "SubTask-Of", "image processing"], ["video summarization", "SubTask-Of", "image processing"]], "rel_plus": [["salient object detection:Task", "SubTask-Of", "saliency prediction:Task"], ["salient object detection:Task", "SubTask-Of", "computer vision:Task"], ["content - aware image editing:Task", "SubTask-Of", "image processing:Task"], ["object detection:Task", "SubTask-Of", "image processing:Task"], ["image classification:Task", "SubTask-Of", "image processing:Task"], ["person re - identification:Task", "SubTask-Of", "image processing:Task"], ["video summarization:Task", "SubTask-Of", "image processing:Task"]]}
{"doc_id": "6116678", "sentence": "To obtain more robust features than handcrafted ones for salient object detection , deep convolutional neural networks ( CNNs ) have recently been employed , achieving substantially better results than previous state of the art [ 2 6 , 5 0 , 4 4 ] .", "ner": [["salient object detection", "Task"], ["deep convolutional neural networks", "Method"], ["CNNs", "Method"]], "rel": [["deep convolutional neural networks", "Used-For", "salient object detection"], ["CNNs", "Synonym-Of", "deep convolutional neural networks"]], "rel_plus": [["deep convolutional neural networks:Method", "Used-For", "salient object detection:Task"], ["CNNs:Method", "Synonym-Of", "deep convolutional neural networks:Method"]]}
{"doc_id": "6116678", "sentence": "In addition to improved robustness , features extracted using CNNs contain more high - level semantic information since those CNNs were typically pre - trained on datasets for visual recognition tasks .", "ner": [["CNNs", "Method"], ["CNNs", "Method"], ["visual recognition", "Task"]], "rel": [["CNNs", "Used-For", "visual recognition"]], "rel_plus": [["CNNs:Method", "Used-For", "visual recognition:Task"]]}
{"doc_id": "6116678", "sentence": "For example , training a patch - oriented CNN model for saliency detection takes over 2 GPU days and requires hundreds of gigabytes of storage for the 5 0 0 0 images in the MSRA - B dataset .", "ner": [["CNN", "Method"], ["saliency detection", "Task"], ["MSRA - B", "Dataset"]], "rel": [["CNN", "Used-For", "saliency detection"], ["MSRA - B", "Benchmark-For", "saliency detection"], ["CNN", "Trained-With", "MSRA - B"]], "rel_plus": [["CNN:Method", "Used-For", "saliency detection:Task"], ["MSRA - B:Dataset", "Benchmark-For", "saliency detection:Task"], ["CNN:Method", "Trained-With", "MSRA - B:Dataset"]]}
{"doc_id": "6116678", "sentence": "In this paper , inspired by a recent trend of developing fully convolutional neural networks for pixel labeling prob - lems [ 3 1 , 6 , 4 7 ] , we propose an end - to - end deep contrast network to overcome the aforementioned limitations of recent CNN - based saliency detection methods .", "ner": [["fully convolutional neural networks", "Method"], ["pixel labeling prob - lems", "Task"], ["end - to - end deep contrast network", "Method"], ["CNN", "Method"], ["saliency detection", "Task"]], "rel": [["fully convolutional neural networks", "Used-For", "pixel labeling prob - lems"], ["CNN", "Used-For", "saliency detection"], ["end - to - end deep contrast network", "Used-For", "saliency detection"]], "rel_plus": [["fully convolutional neural networks:Method", "Used-For", "pixel labeling prob - lems:Task"], ["CNN:Method", "Used-For", "saliency detection:Task"], ["end - to - end deep contrast network:Method", "Used-For", "saliency detection:Task"]]}
{"doc_id": "6116678", "sentence": "In the fully convolutional stream , we design a multi - scale fully convolutional network ( MS - FCN ) , which takes the raw image as input and directly produces a saliency map with pixellevel accuracy .", "ner": [["multi - scale fully convolutional network", "Method"], ["MS - FCN", "Method"]], "rel": [["MS - FCN", "Synonym-Of", "multi - scale fully convolutional network"]], "rel_plus": [["MS - FCN:Method", "Synonym-Of", "multi - scale fully convolutional network:Method"]]}
{"doc_id": "6116678", "sentence": "The segment - level spatial pooling stream generates another saliency map at the superpixel level by performing spatial pooling and saliency estimation over superpixels .", "ner": [["segment - level spatial pooling stream", "Method"], ["spatial pooling", "Task"], ["saliency estimation", "Task"]], "rel": [["segment - level spatial pooling stream", "Used-For", "spatial pooling"], ["segment - level spatial pooling stream", "Used-For", "saliency estimation"]], "rel_plus": [["segment - level spatial pooling stream:Method", "Used-For", "spatial pooling:Task"], ["segment - level spatial pooling stream:Method", "Used-For", "saliency estimation:Task"]]}
{"doc_id": "6116678", "sentence": "The fused saliency map from these two streams is further refined with a fully connected CRF for better spatial coherence and contour localization . \u2022 We propose a multi - scale fully convolutional network as the first stream in our deep contrast network to infer a pixel - level saliency map directly from the raw input image .", "ner": [["fused saliency map", "Method"], ["fully connected CRF", "Method"], ["spatial coherence", "Task"], ["contour localization", "Task"], ["multi - scale fully convolutional network", "Method"], ["deep contrast network", "Method"], ["pixel - level saliency map", "Task"]], "rel": [["fully connected CRF", "Part-Of", "fused saliency map"], ["fused saliency map", "Used-For", "spatial coherence"], ["fused saliency map", "Used-For", "contour localization"], ["multi - scale fully convolutional network", "Part-Of", "deep contrast network"], ["multi - scale fully convolutional network", "Used-For", "pixel - level saliency map"]], "rel_plus": [["fully connected CRF:Method", "Part-Of", "fused saliency map:Method"], ["fused saliency map:Method", "Used-For", "spatial coherence:Task"], ["fused saliency map:Method", "Used-For", "contour localization:Task"], ["multi - scale fully convolutional network:Method", "Part-Of", "deep contrast network:Method"], ["multi - scale fully convolutional network:Method", "Used-For", "pixel - level saliency map:Task"]]}
{"doc_id": "6116678", "sentence": "Recently , machine learning and artificial intelligence have been revolutionized by deep convolutional neural networks , which have set new state of the art on a number of visual recognition tasks , including image classification [ 2 5 ] , object detection [ 1 6 ] , scene classification [ 4 8 ] and scene parsing [ 1 3 ] , closing the gap to human - level performance .", "ner": [["deep convolutional neural networks", "Method"], ["visual recognition", "Task"], ["image classification", "Task"], ["object detection", "Task"], ["scene classification", "Task"], ["scene parsing", "Task"]], "rel": [["deep convolutional neural networks", "Used-For", "visual recognition"], ["image classification", "SubTask-Of", "visual recognition"], ["object detection", "SubTask-Of", "visual recognition"], ["scene classification", "SubTask-Of", "visual recognition"], ["scene parsing", "SubTask-Of", "visual recognition"], ["deep convolutional neural networks", "Used-For", "image classification"], ["deep convolutional neural networks", "Used-For", "object detection"], ["deep convolutional neural networks", "Used-For", "scene classification"], ["deep convolutional neural networks", "Used-For", "scene parsing"]], "rel_plus": [["deep convolutional neural networks:Method", "Used-For", "visual recognition:Task"], ["image classification:Task", "SubTask-Of", "visual recognition:Task"], ["object detection:Task", "SubTask-Of", "visual recognition:Task"], ["scene classification:Task", "SubTask-Of", "visual recognition:Task"], ["scene parsing:Task", "SubTask-Of", "visual recognition:Task"], ["deep convolutional neural networks:Method", "Used-For", "image classification:Task"], ["deep convolutional neural networks:Method", "Used-For", "object detection:Task"], ["deep convolutional neural networks:Method", "Used-For", "scene classification:Task"], ["deep convolutional neural networks:Method", "Used-For", "scene parsing:Task"]]}
{"doc_id": "6116678", "sentence": "However , directly applying existing fully convolutional network architecture to salient object detection would not be most appropriate because a standard fully convolutional model is not particularly good at capturing subtle visual contrast in an image .", "ner": [["fully convolutional network", "Method"], ["salient object detection", "Task"]], "rel": [["fully convolutional network", "Used-For", "salient object detection"]], "rel_plus": [["fully convolutional network:Method", "Used-For", "salient object detection:Task"]]}
{"doc_id": "6116678", "sentence": "This paper can be viewed as the first piece of work that aims to discover visual contrast information inside an image using end - to - end convolutional neural networks .   As shown in Fig. 1 , the architecture of our deep contrast network for salient object detection consists of two complementary components , a fully convolutional stream and a segment - wise spatial pooling stream .", "ner": [["convolutional neural networks", "Method"], ["deep contrast network", "Method"], ["salient object detection", "Task"], ["fully convolutional stream", "Method"], ["segment - wise spatial pooling stream", "Method"]], "rel": [["fully convolutional stream", "Part-Of", "deep contrast network"], ["segment - wise spatial pooling stream", "Part-Of", "deep contrast network"], ["deep contrast network", "Used-For", "salient object detection"]], "rel_plus": [["fully convolutional stream:Method", "Part-Of", "deep contrast network:Method"], ["segment - wise spatial pooling stream:Method", "Part-Of", "deep contrast network:Method"], ["deep contrast network:Method", "Used-For", "salient object detection:Task"]]}
{"doc_id": "6116678", "sentence": "The fully convolutional stream is a multi - scale fully convolutional network ( MS - FCN ) , which generates a saliency map S 1 with one eighth resolution of the raw input image by exploiting visual contrast across multiscale convolutional layers .", "ner": [["multi - scale fully convolutional network", "Method"], ["MS - FCN", "Method"]], "rel": [["MS - FCN", "Synonym-Of", "multi - scale fully convolutional network"]], "rel_plus": [["MS - FCN:Method", "Synonym-Of", "multi - scale fully convolutional network:Method"]]}
{"doc_id": "6116678", "sentence": "The segment - wise spatial pooling stream generates a saliency map at the superpixel level by performing spatial pooling and saliency estimation over individual superpixels .", "ner": [["segment - wise spatial pooling stream", "Method"], ["spatial pooling", "Task"], ["saliency estimation", "Task"]], "rel": [["segment - wise spatial pooling stream", "Used-For", "spatial pooling"], ["segment - wise spatial pooling stream", "Used-For", "saliency estimation"]], "rel_plus": [["segment - wise spatial pooling stream:Method", "Used-For", "spatial pooling:Task"], ["segment - wise spatial pooling stream:Method", "Used-For", "saliency estimation:Task"]]}
{"doc_id": "6116678", "sentence": "To re - purpose it into a dense image saliency prediction network , the two fully connected layers of VGG 1 6 are first converted into convolutional ones with 1 \u00d7 1 kernel as described in [ 3 1 ] .", "ner": [["dense image saliency prediction network", "Method"], ["fully connected layers", "Method"], ["VGG 1 6", "Method"], ["1 \u00d7 1 kernel", "Method"]], "rel": [["VGG 1 6", "Part-Of", "dense image saliency prediction network"], ["1 \u00d7 1 kernel", "Part-Of", "VGG 1 6"], ["fully connected layers", "Part-Of", "VGG 1 6"]], "rel_plus": [["VGG 1 6:Method", "Part-Of", "dense image saliency prediction network:Method"], ["1 \u00d7 1 kernel:Method", "Part-Of", "VGG 1 6:Method"], ["fully connected layers:Method", "Part-Of", "VGG 1 6:Method"]]}
{"doc_id": "6116678", "sentence": "As shown in Fig. 2 , we connect three extra convolutional layers to each of the first four max - pooling layers of VGG 1 6 .", "ner": [["convolutional layers", "Method"], ["max - pooling", "Method"], ["VGG 1 6", "Method"]], "rel": [["convolutional layers", "Part-Of", "VGG 1 6"], ["max - pooling", "Part-Of", "VGG 1 6"]], "rel_plus": [["convolutional layers:Method", "Part-Of", "VGG 1 6:Method"], ["max - pooling:Method", "Part-Of", "VGG 1 6:Method"]]}
{"doc_id": "6116678", "sentence": "To better model visual contrast between regions and visual saliency along region boundaries , we design a segment - wise spatial pooling stream in our network .", "ner": [["segment - wise spatial pooling", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "6116678", "sentence": "We first decompose the raw input image into a set of superpixels , and call each superpixel a segment .", "ner": [], "rel": [], "rel_plus": []}
{"doc_id": "6116678", "sentence": "A mask is computed for every segment in the feature map generated from the last true convolutional layer ( Conv 5 3 ) of MS - FCN as follows .", "ner": [["convolutional layer", "Method"], ["Conv 5 3", "Method"], ["MS - FCN", "Method"]], "rel": [["Conv 5 3", "Synonym-Of", "convolutional layer"], ["convolutional layer", "Part-Of", "MS - FCN"]], "rel_plus": [["Conv 5 3:Method", "Synonym-Of", "convolutional layer:Method"], ["convolutional layer:Method", "Part-Of", "MS - FCN:Method"]]}
{"doc_id": "6116678", "sentence": "Note that feature maps generated from Conv 5 3 have 8 - pixel strides in our MS - FCN instead of 3 2 pixel ones in the original VGG 1 6 network since subsampling was skipped in the last two max - pooling layers as described in Section 3. 1 .", "ner": [["Conv 5 3", "Method"], ["MS - FCN", "Method"], ["VGG 1 6", "Method"], ["max - pooling", "Method"]], "rel": [["Conv 5 3", "Part-Of", "MS - FCN"], ["MS - FCN", "Compare-With", "VGG 1 6"]], "rel_plus": [["Conv 5 3:Method", "Part-Of", "MS - FCN:Method"], ["MS - FCN:Method", "Compare-With", "VGG 1 6:Method"]]}
{"doc_id": "6116678", "sentence": "Since segments on Conv 5 3 have variable size , to produce a fixed - length feature vector , we further perform spatial pooling ( SP ) over a fixed grid as with [ 1 8 ] .", "ner": [["spatial pooling", "Method"], ["SP", "Method"]], "rel": [["SP", "Synonym-Of", "spatial pooling"]], "rel_plus": [["SP:Method", "Synonym-Of", "spatial pooling:Method"]]}
{"doc_id": "6116678", "sentence": "The output of the second fully connected layer is fed into the output layer , which uses the sigmoid function to perform logistic regression to produce a distribution over binary saliency labels .", "ner": [["fully connected layer", "Method"], ["sigmoid", "Method"], ["logistic regression", "Task"]], "rel": [["sigmoid", "Used-For", "logistic regression"]], "rel_plus": [["sigmoid:Method", "Used-For", "logistic regression:Task"]]}
{"doc_id": "6116678", "sentence": "Moreover , our model also achieves better results as segment features are extracted from our multiscale fully convolutional network , which has been fine - tuned for salient object detection , instead of from the original VGG 1 6 model for image classification .", "ner": [["fully convolutional network", "Method"], ["salient object detection", "Task"], ["VGG 1 6", "Method"], ["image classification", "Task"]], "rel": [["fully convolutional network", "Used-For", "salient object detection"], ["VGG 1 6", "Used-For", "image classification"]], "rel_plus": [["fully convolutional network:Method", "Used-For", "salient object detection:Task"], ["VGG 1 6:Method", "Used-For", "image classification:Task"]]}
{"doc_id": "6116678", "sentence": "Segment features are extracted using the original VGG 1 6 network pre - trained over the ImageNet dataset [ 1 1 ] .", "ner": [["VGG 1 6", "Method"], ["ImageNet", "Dataset"]], "rel": [["VGG 1 6", "Trained-With", "ImageNet"]], "rel_plus": [["VGG 1 6:Method", "Trained-With", "ImageNet:Dataset"]]}
{"doc_id": "6116678", "sentence": "During this process , the weights for fusing the saliency maps ( S 1 and S 2 ) from the two streams as well as the parameters in the multiscale fully convolutional network are updated using stochastic gradient descent .", "ner": [["fully convolutional network", "Method"], ["stochastic gradient descent", "Method"]], "rel": [["stochastic gradient descent", "Part-Of", "fully convolutional network"]], "rel_plus": [["stochastic gradient descent:Method", "Part-Of", "fully convolutional network:Method"]]}
{"doc_id": "6116678", "sentence": "The loss function for fine - tuning the deep contrast network ( the first stream ) and the fusing weights is the cross entropy between the ground truth and the fused saliency map ( S ): where G is the groundtruth label , W denotes the collection of all network parameters in MS - FCN and the fusion layer , \u03b2 i is a weight balancing the number of salient pixels and unsalient ones , and |I| , |I| and |I| + denote the total number of pixels , unsalient pixels and salient pixels in image I , respectively .", "ner": [["deep contrast network", "Method"], ["cross entropy", "Method"], ["fused saliency map", "Method"], ["MS - FCN", "Method"], ["fusion layer", "Method"]], "rel": [["cross entropy", "Part-Of", "deep contrast network"], ["fused saliency map", "Part-Of", "deep contrast network"]], "rel_plus": [["cross entropy:Method", "Part-Of", "deep contrast network:Method"], ["fused saliency map:Method", "Part-Of", "deep contrast network:Method"]]}
{"doc_id": "6116678", "sentence": "In this paper , we use a slightly modified version of the SLIC algorithm [ 2 ] , which uses geodesic image distance [ 9 ] during K - means clustering in the CIELab color space .", "ner": [["SLIC", "Method"], ["geodesic image distance", "Method"], ["K - means clustering", "Method"]], "rel": [["geodesic image distance", "Part-Of", "K - means clustering"]], "rel_plus": [["geodesic image distance:Method", "Part-Of", "K - means clustering:Method"]]}
{"doc_id": "6116678", "sentence": "Since both streams in our deep contrast network assign saliency scores to individual pixels or segments without considering the consistency of saliency scores among neighboring pixels and segments , we propose a pixelwise saliency refinement model based on a fully connected CRF [ 2 4 ] to improve spatial coherence .", "ner": [["deep contrast network", "Method"], ["fully connected CRF", "Method"], ["improve spatial coherence", "Task"]], "rel": [["fully connected CRF", "Used-For", "improve spatial coherence"]], "rel_plus": [["fully connected CRF:Method", "Used-For", "improve spatial coherence:Task"]]}
{"doc_id": "6116678", "sentence": "A quantitative study of the effectiveness of the saliency refinement model can be found in Section 5. 3 . 2 .    We evaluate the performance of our method on five public datasets : MSRA - B [ 3 0 ] , PASCAL - S [ 2 8 ] , DUT - OMRON [ 4 9 ] , HKU - IS [ 2 6 ] and SOD [ 3 6 ] .", "ner": [["MSRA - B", "Dataset"], ["PASCAL - S", "Dataset"], ["DUT - OMRON", "Dataset"], ["HKU - IS", "Dataset"], ["SOD", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "6116678", "sentence": "PASCAL - S was built using the validation set of the PAS - CAL VOC 2 0 1 0 segmentation challenge .", "ner": [["PASCAL - S", "Dataset"], ["PAS - CAL VOC 2 0 1 0 segmentation", "Dataset"]], "rel": [["PASCAL - S", "SubClass-Of", "PAS - CAL VOC 2 0 1 0 segmentation"]], "rel_plus": [["PASCAL - S:Dataset", "SubClass-Of", "PAS - CAL VOC 2 0 1 0 segmentation:Dataset"]]}
{"doc_id": "6116678", "sentence": "The SOD dataset contains 3 0 0 images and it was originally designed for image segmentation .", "ner": [["SOD", "Dataset"], ["image segmentation", "Task"]], "rel": [["SOD", "Benchmark-For", "image segmentation"]], "rel_plus": [["SOD:Dataset", "Benchmark-For", "image segmentation:Task"]]}
{"doc_id": "6116678", "sentence": "Our DCL and DCL + ( DCL with CRF ) consistently outperform other methods across all the testing datasets .", "ner": [["DCL", "Method"], ["DCL +", "Method"], ["DCL with CRF", "Method"]], "rel": [["DCL with CRF", "Synonym-Of", "DCL +"]], "rel_plus": [["DCL with CRF:Method", "Synonym-Of", "DCL +:Method"]]}
{"doc_id": "6116678", "sentence": "Note that MC [ 5 0 ] and LEGS [ 4 4 ] are overrated on the MSRA - B dataset and LEGS [ 4 4 ] is also overrated on the PASCAL - S dataset . where \u03b2 2 is set to 0. 3 to weigh precision more than recall as suggested in [ 1 ] .", "ner": [["MC", "Method"], ["LEGS", "Method"], ["MSRA - B", "Dataset"], ["LEGS", "Method"], ["PASCAL - S", "Dataset"]], "rel": [["LEGS", "Evaluated-With", "MSRA - B"], ["MC", "Evaluated-With", "MSRA - B"], ["LEGS", "Evaluated-With", "PASCAL - S"]], "rel_plus": [["LEGS:Method", "Evaluated-With", "MSRA - B:Dataset"], ["MC:Method", "Evaluated-With", "MSRA - B:Dataset"], ["LEGS:Method", "Evaluated-With", "PASCAL - S:Dataset"]]}
{"doc_id": "6116678", "sentence": "We use DCL to denote our saliency model based on deep contrast learning only without CRF - based post - processing , and DCL + to denote the saliency model that includes CRFbased refinement .", "ner": [["DCL", "Method"], ["deep contrast learning", "Method"], ["CRF - based post - processing", "Method"], ["DCL +", "Method"], ["CRFbased refinement", "Method"]], "rel": [["deep contrast learning", "Part-Of", "DCL"], ["CRFbased refinement", "Part-Of", "DCL +"]], "rel_plus": [["deep contrast learning:Method", "Part-Of", "DCL:Method"], ["CRFbased refinement:Method", "Part-Of", "DCL +:Method"]]}
{"doc_id": "6116678", "sentence": "While it takes around 2 5 hours to train our deep contrast network using the MSRA - B dataset , it only takes 1. 5 seconds for the trained model ( DCL ) to detect salient objects in a testing image with 4 0 0 x 3 0 0 pixels on a PC with an NVIDIA Titan Black GPU and a 3. 4 GHz Intel processor .", "ner": [["deep contrast network", "Method"], ["MSRA - B", "Dataset"], ["DCL", "Method"]], "rel": [["deep contrast network", "Trained-With", "MSRA - B"]], "rel_plus": [["deep contrast network:Method", "Trained-With", "MSRA - B:Dataset"]]}
{"doc_id": "6116678", "sentence": "Experimental results will show that DCL alone without CRF - based post - processing already outperforms existing state - of - the - art methods .", "ner": [["DCL", "Method"], ["CRF - based post - processing", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "6116678", "sentence": "We compare our saliency models ( DCL and DCL + ) against eight recent state - of - the - art methods , including SF [ 3 9 ] , GC [ 8 ] , DRFI [ 2 1 ] , PISA [ 4 3 ] , BSCA [ 4 0 ] , LEGS [ 4 4 ] , MC [ 5 0 ] and MDF [ 2 6 ] .", "ner": [["DCL", "Method"], ["DCL +", "Method"], ["SF", "Method"], ["GC", "Method"], ["DRFI", "Method"], ["PISA", "Method"], ["BSCA", "Method"], ["LEGS", "Method"], ["MC", "Method"], ["MDF", "Method"]], "rel": [["DCL +", "Compare-With", "SF"], ["DCL", "Compare-With", "SF"], ["DCL +", "Compare-With", "GC"], ["DCL", "Compare-With", "GC"], ["DCL +", "Compare-With", "DRFI"], ["DCL", "Compare-With", "DRFI"], ["DCL +", "Compare-With", "PISA"], ["DCL", "Compare-With", "PISA"], ["DCL +", "Compare-With", "BSCA"], ["DCL", "Compare-With", "BSCA"], ["DCL +", "Compare-With", "LEGS"], ["DCL", "Compare-With", "LEGS"], ["DCL +", "Compare-With", "MC"], ["DCL", "Compare-With", "MC"], ["DCL +", "Compare-With", "MDF"], ["DCL", "Compare-With", "MDF"]], "rel_plus": [["DCL +:Method", "Compare-With", "SF:Method"], ["DCL:Method", "Compare-With", "SF:Method"], ["DCL +:Method", "Compare-With", "GC:Method"], ["DCL:Method", "Compare-With", "GC:Method"], ["DCL +:Method", "Compare-With", "DRFI:Method"], ["DCL:Method", "Compare-With", "DRFI:Method"], ["DCL +:Method", "Compare-With", "PISA:Method"], ["DCL:Method", "Compare-With", "PISA:Method"], ["DCL +:Method", "Compare-With", "BSCA:Method"], ["DCL:Method", "Compare-With", "BSCA:Method"], ["DCL +:Method", "Compare-With", "LEGS:Method"], ["DCL:Method", "Compare-With", "LEGS:Method"], ["DCL +:Method", "Compare-With", "MC:Method"], ["DCL:Method", "Compare-With", "MC:Method"], ["DCL +:Method", "Compare-With", "MDF:Method"], ["DCL:Method", "Compare-With", "MDF:Method"]]}
{"doc_id": "6116678", "sentence": "In addition , we also train a fully convolutional neural network ( FCN ) ( the FCN - 8 s network proposed in [ 3 1 ] ) for comparison .", "ner": [["convolutional neural network", "Method"], ["FCN", "Method"], ["FCN - 8 s", "Method"]], "rel": [["FCN", "Synonym-Of", "convolutional neural network"], ["convolutional neural network", "Compare-With", "FCN - 8 s"]], "rel_plus": [["FCN:Method", "Synonym-Of", "convolutional neural network:Method"], ["convolutional neural network:Method", "Compare-With", "FCN - 8 s:Method"]]}
{"doc_id": "6116678", "sentence": "To train the FCN saliency model , we simply replace its last softmax layer with a sigmoid cross - entropy layer for saliency inference , and finetune the revised model using the training sets in the aforementioned saliency datasets .", "ner": [["FCN saliency model", "Method"], ["softmax", "Method"], ["sigmoid cross - entropy layer", "Method"]], "rel": [["sigmoid cross - entropy layer", "Part-Of", "FCN saliency model"]], "rel_plus": [["sigmoid cross - entropy layer:Method", "Part-Of", "FCN saliency model:Method"]]}
{"doc_id": "6116678", "sentence": "It is necessary to point out that the performance of MC [ 5 0 ] is overrated on the MSRA - B dataset and the performance of LEGS [ 4 4 ] is overrated on both the MSRA - B dataset and the PASCAL - S dataset because most testing images in the corresponding datasets were used as training samples for the publicly released trained models of MC and LEGS used in our comparison .", "ner": [["MC", "Method"], ["MSRA - B", "Dataset"], ["LEGS", "Method"], ["MSRA - B", "Dataset"], ["PASCAL - S", "Dataset"], ["MC", "Method"], ["LEGS", "Method"]], "rel": [["MC", "Evaluated-With", "MSRA - B"], ["LEGS", "Evaluated-With", "MSRA - B"], ["LEGS", "Trained-With", "MSRA - B"], ["MC", "Trained-With", "MSRA - B"], ["LEGS", "Evaluated-With", "PASCAL - S"], ["MC", "Trained-With", "PASCAL - S"], ["LEGS", "Trained-With", "PASCAL - S"]], "rel_plus": [["MC:Method", "Evaluated-With", "MSRA - B:Dataset"], ["LEGS:Method", "Evaluated-With", "MSRA - B:Dataset"], ["LEGS:Method", "Trained-With", "MSRA - B:Dataset"], ["MC:Method", "Trained-With", "MSRA - B:Dataset"], ["LEGS:Method", "Evaluated-With", "PASCAL - S:Dataset"], ["MC:Method", "Trained-With", "PASCAL - S:Dataset"], ["LEGS:Method", "Trained-With", "PASCAL - S:Dataset"]]}
{"doc_id": "6116678", "sentence": "Refer to the supplemental materials for the results on the PASCAL - S and SOD datasets .", "ner": [["PASCAL - S", "Dataset"], ["SOD", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "6116678", "sentence": "Our complete model ( DCL + ) improves the maximum Fmeasure achieved by the best - performing existing algorithm by 3. 5 % , 5. 0 % , 7. 7 % , 7. 6 % and 6. 0 % respectively on MSRA - B ( skipping MC and LEGS on this dataset ) , HKU - IS , DUT - OMRON , PASCAL - S ( skipping LEGS on this dataset ) and SOD .", "ner": [["DCL +", "Method"], ["MSRA - B", "Dataset"], ["MC", "Method"], ["LEGS", "Method"], ["HKU - IS", "Dataset"], ["DUT - OMRON", "Dataset"], ["PASCAL - S", "Dataset"], ["LEGS", "Method"], ["SOD", "Dataset"]], "rel": [["DCL +", "Evaluated-With", "MSRA - B"], ["DCL +", "Evaluated-With", "HKU - IS"], ["DCL +", "Evaluated-With", "DUT - OMRON"], ["DCL +", "Evaluated-With", "PASCAL - S"], ["DCL +", "Evaluated-With", "SOD"]], "rel_plus": [["DCL +:Method", "Evaluated-With", "MSRA - B:Dataset"], ["DCL +:Method", "Evaluated-With", "HKU - IS:Dataset"], ["DCL +:Method", "Evaluated-With", "DUT - OMRON:Dataset"], ["DCL +:Method", "Evaluated-With", "PASCAL - S:Dataset"], ["DCL +:Method", "Evaluated-With", "SOD:Dataset"]]}
{"doc_id": "6116678", "sentence": "And at the same time , our model lowers the MAE by 2 8 . 8 % , 3 5 . 5 % , 9. 1 % , 2 5 . 5 % and 1 8 . 7 % respectively on MSRA - B ( skipping MC and LEGS on this dataset ) , HKU - IS , DUT - OMRON , PASCAL - S ( skipping LEGS on this dataset ) and SOD .", "ner": [["MAE", "Method"], ["MSRA - B", "Dataset"], ["MC", "Method"], ["LEGS", "Method"], ["HKU - IS", "Dataset"], ["DUT - OMRON", "Dataset"], ["PASCAL - S", "Dataset"], ["LEGS", "Method"], ["SOD", "Dataset"]], "rel": [["MAE", "Evaluated-With", "MSRA - B"], ["MAE", "Evaluated-With", "HKU - IS"], ["MAE", "Evaluated-With", "DUT - OMRON"], ["MAE", "Evaluated-With", "PASCAL - S"], ["MAE", "Evaluated-With", "SOD"]], "rel_plus": [["MAE:Method", "Evaluated-With", "MSRA - B:Dataset"], ["MAE:Method", "Evaluated-With", "HKU - IS:Dataset"], ["MAE:Method", "Evaluated-With", "DUT - OMRON:Dataset"], ["MAE:Method", "Evaluated-With", "PASCAL - S:Dataset"], ["MAE:Method", "Evaluated-With", "SOD:Dataset"]]}
{"doc_id": "6116678", "sentence": "We can also see that our model without CRF ( DCL ) significantly outperforms all evaluated salient object detection algorithms across all the considered datasets .", "ner": [["CRF", "Method"], ["DCL", "Method"], ["salient object detection", "Task"]], "rel": [["DCL", "Used-For", "salient object detection"]], "rel_plus": [["DCL:Method", "Used-For", "salient object detection:Task"]]}
{"doc_id": "6116678", "sentence": "Our model also significantly outperforms the FCN adapted from a model originally designed for semantic segmentation [ 3 1 ] because we explicitly perform deep contrast learning , which is critical for saliency detection .    Our deep contrast network consists of a fully convolutional stream and a segment - wise spatial pooling stream .", "ner": [["FCN", "Method"], ["semantic segmentation", "Task"], ["deep contrast learning", "Method"], ["saliency detection", "Task"], ["deep contrast network", "Method"], ["fully convolutional stream", "Method"], ["segment - wise spatial pooling", "Method"]], "rel": [["FCN", "Used-For", "semantic segmentation"], ["deep contrast learning", "Used-For", "saliency detection"], ["fully convolutional stream", "Part-Of", "deep contrast network"], ["segment - wise spatial pooling", "Part-Of", "deep contrast network"]], "rel_plus": [["FCN:Method", "Used-For", "semantic segmentation:Task"], ["deep contrast learning:Method", "Used-For", "saliency detection:Task"], ["fully convolutional stream:Method", "Part-Of", "deep contrast network:Method"], ["segment - wise spatial pooling:Method", "Part-Of", "deep contrast network:Method"]]}
{"doc_id": "6116678", "sentence": "To show the effectiveness and necessity of these two components , we compare the saliency map S 1 generated from the first stream ( MS - FCN ) , the saliency map S 2 from the second segment - level stream and the fused saliency map from S 1 and S 2 ( DCL ) using testing images in the MSRA - B dataset .", "ner": [["MS - FCN", "Method"], ["segment - level stream", "Method"], ["fused saliency map", "Method"], ["DCL", "Method"], ["MSRA - B", "Dataset"]], "rel": [["segment - level stream", "Part-Of", "DCL"], ["fused saliency map", "Part-Of", "DCL"], ["MS - FCN", "Evaluated-With", "MSRA - B"], ["DCL", "Evaluated-With", "MSRA - B"]], "rel_plus": [["segment - level stream:Method", "Part-Of", "DCL:Method"], ["fused saliency map:Method", "Part-Of", "DCL:Method"], ["MS - FCN:Method", "Evaluated-With", "MSRA - B:Dataset"], ["DCL:Method", "Evaluated-With", "MSRA - B:Dataset"]]}
{"doc_id": "6116678", "sentence": "As shown in Fig. 7 , the fused saliency map ( DCL ) consistently achieves the best performance on average precision , recall and F - measure , and the fully convolutional stream ( MS - FCN ) has more contribution to the fused result than the segment - wise spatial pooling stream .", "ner": [["fused saliency map", "Method"], ["DCL", "Method"], ["fully convolutional stream", "Method"], ["MS - FCN", "Method"], ["segment - wise spatial pooling", "Method"]], "rel": [["DCL", "Synonym-Of", "fused saliency map"], ["fully convolutional stream", "Part-Of", "MS - FCN"], ["fully convolutional stream", "Compare-With", "segment - wise spatial pooling"]], "rel_plus": [["DCL:Method", "Synonym-Of", "fused saliency map:Method"], ["fully convolutional stream:Method", "Part-Of", "MS - FCN:Method"], ["fully convolutional stream:Method", "Compare-With", "segment - wise spatial pooling:Method"]]}
{"doc_id": "6116678", "sentence": "To demonstrate the effectiveness of MS - FCN , we also generate saliency maps from the last scale of MS - FCN ( the best performing scale ) for comparison .", "ner": [["MS - FCN", "Method"], ["MS - FCN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "6116678", "sentence": "The last scale of MS - FCN is in fact the fully convolutional version of the original VGG 1 6 network .", "ner": [["MS - FCN", "Method"], ["VGG 1 6", "Method"]], "rel": [["MS - FCN", "SubClass-Of", "VGG 1 6"]], "rel_plus": [["MS - FCN:Method", "SubClass-Of", "VGG 1 6:Method"]]}
{"doc_id": "6116678", "sentence": "As shown in Fig. 7 , this single scale of MS - FCN ( called SC MSFCN ) performs much worse than the complete version of MS - FCN in terms of the PR curve as well as the average precision , recall and F - measure .", "ner": [["single scale of MS - FCN", "Method"], ["SC MSFCN", "Method"], ["MS - FCN", "Method"]], "rel": [["SC MSFCN", "Synonym-Of", "single scale of MS - FCN"], ["single scale of MS - FCN", "Compare-With", "MS - FCN"]], "rel_plus": [["SC MSFCN:Method", "Synonym-Of", "single scale of MS - FCN:Method"], ["single scale of MS - FCN:Method", "Compare-With", "MS - FCN:Method"]]}
{"doc_id": "6116678", "sentence": "To validate its effectiveness , we have also evaluated the performance of our final saliency model with and without the CRF using the testing images in the MSRA - B dataset .", "ner": [["CRF", "Method"], ["MSRA - B", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "6116678", "sentence": "A fully connected CRF model can be optionally incorporated to further improve spatial coherence and contour localization in the fused result from these two streams .", "ner": [["fully connected CRF", "Method"], ["improve spatial coherence", "Task"], ["contour localization", "Task"]], "rel": [["fully connected CRF", "Used-For", "improve spatial coherence"], ["fully connected CRF", "Used-For", "contour localization"]], "rel_plus": [["fully connected CRF:Method", "Used-For", "improve spatial coherence:Task"], ["fully connected CRF:Method", "Used-For", "contour localization:Task"]]}
{"doc_id": "210860962", "sentence": "The great majority of existing domain adaptation models rely on image translation networks , which often contain a huge amount of domain - specific parameters .", "ner": [["domain adaptation models", "Method"], ["image translation networks", "Method"]], "rel": [["image translation networks", "Used-For", "domain adaptation models"]], "rel_plus": [["image translation networks:Method", "Used-For", "domain adaptation models:Method"]]}
{"doc_id": "210860962", "sentence": "Moreover , following recent advances in image translation , we adopt the recently introduced StarGAN architecture as image translation backbone , since it is able to perform translations across multiple domains by means of a single generator network .", "ner": [["image translation", "Task"], ["StarGAN", "Method"], ["image translation", "Task"]], "rel": [["StarGAN", "Used-For", "image translation"]], "rel_plus": [["StarGAN:Method", "Used-For", "image translation:Task"]]}
{"doc_id": "210860962", "sentence": "More specifically , in computer vision , the success of AlexNet [ 2 9 ] in the ImageNet Large Scale Visual Recognition Challenge ( ILSVRC ) in 2 0 1 2 changed the research landscape .", "ner": [["computer vision", "Task"], ["AlexNet", "Method"], ["ImageNet Large Scale Visual Recognition Challenge", "Dataset"], ["ILSVRC", "Dataset"]], "rel": [["AlexNet", "Used-For", "computer vision"], ["ImageNet Large Scale Visual Recognition Challenge", "Benchmark-For", "computer vision"], ["ILSVRC", "Synonym-Of", "ImageNet Large Scale Visual Recognition Challenge"], ["AlexNet", "Evaluated-With", "ImageNet Large Scale Visual Recognition Challenge"]], "rel_plus": [["AlexNet:Method", "Used-For", "computer vision:Task"], ["ImageNet Large Scale Visual Recognition Challenge:Dataset", "Benchmark-For", "computer vision:Task"], ["ILSVRC:Dataset", "Synonym-Of", "ImageNet Large Scale Visual Recognition Challenge:Dataset"], ["AlexNet:Method", "Evaluated-With", "ImageNet Large Scale Visual Recognition Challenge:Dataset"]]}
{"doc_id": "210860962", "sentence": "Since then , deep learning architectures have been quickly spreading and have shown impressive results in tasks such as image classification [ 2 1 ] , [ 2 3 ] , [ 2 9 ] , [ 4 8 ] , [ 5 0 ] , semantic segmentation [ 3 ] , [ 7 ] , [ 8 ] , [ 1 3 ] , [ 2 6 ] , [ 4 3 ] , [ 4 6 ] , [ 5 6 ] , [ 5 9 ] , or object detection [ 1 6 ] , [ 4 0 ] , [ 4 1 ] , among many others .", "ner": [["deep learning", "Method"], ["image classification", "Task"], ["semantic segmentation", "Task"], ["object detection", "Task"]], "rel": [["deep learning", "Used-For", "image classification"], ["deep learning", "Used-For", "semantic segmentation"], ["deep learning", "Used-For", "object detection"]], "rel_plus": [["deep learning:Method", "Used-For", "image classification:Task"], ["deep learning:Method", "Used-For", "semantic segmentation:Task"], ["deep learning:Method", "Used-For", "object detection:Task"]]}
{"doc_id": "210860962", "sentence": "To mitigate labeling efforts , many works suggest exploiting datasets built from data generated by computer simulated environment , such as GTA 5 [ 4 2 ] and SYNTHIA [ 4 4 ] , where infinite Corresponding author : mikel.menta@cvc.uab.es Code : https://github.com/mkmenta/domain adapt segm/ amounts of data can be easily collected .", "ner": [["GTA 5", "Dataset"], ["SYNTHIA", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "210860962", "sentence": "Recent advances in unsupervised domain adaptation mostly target the task of image classification [ 1 4 ] , [ 1 5 ] , [ 3 4 ] , [ 4 7 ] , [ 5 2 ] , with several extensions to other tasks such as semantic segmentation [ 2 2 ] , [ 3 7 ] , [ 5 1 ] and object detection [ 9 ] , [ 2 4 ] .", "ner": [["unsupervised domain adaptation", "Method"], ["image classification", "Task"], ["semantic segmentation", "Task"], ["object detection", "Task"]], "rel": [["unsupervised domain adaptation", "Used-For", "image classification"], ["unsupervised domain adaptation", "Used-For", "semantic segmentation"], ["unsupervised domain adaptation", "Used-For", "object detection"]], "rel_plus": [["unsupervised domain adaptation:Method", "Used-For", "image classification:Task"], ["unsupervised domain adaptation:Method", "Used-For", "semantic segmentation:Task"], ["unsupervised domain adaptation:Method", "Used-For", "object detection:Task"]]}
{"doc_id": "210860962", "sentence": "Additionally , many approaches incorporate image translation or reconstruction objective to obtain more general features , which may be composed of separate networks to process each domain .", "ner": [["image translation", "Task"], ["reconstruction objective", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "210860962", "sentence": "In this thesis , we present a novel unsupervised domain adaptation model for semantic segmentation , which addresses the previous concerns .", "ner": [["unsupervised domain adaptation", "Method"], ["semantic segmentation", "Task"]], "rel": [["unsupervised domain adaptation", "Used-For", "semantic segmentation"]], "rel_plus": [["unsupervised domain adaptation:Method", "Used-For", "semantic segmentation:Task"]]}
{"doc_id": "210860962", "sentence": "The contributions are twofold : 1 ) We successfully reduce the amount of parameters of our domain adaptation network to roughly the half w.r.t . many other methods , by following the idea of StarGAN of providing an additional channel to the input of the network with the corresponding domain label . 2 ) We present a novel pixel - wise discriminator training procedure that is able to perform a local discrimination of the feature representations by taking into account per class information .", "ner": [["domain adaptation network", "Method"], ["StarGAN", "Method"], ["pixel - wise discriminator", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210860962", "sentence": "This section aims to give the reader an overview of recent state - ofthe - art approaches that tackle problems relevant to this thesis , namely image translation , semantic segmentation and unsupervised domain adaptation , with special focus on the latter .", "ner": [["image translation", "Task"], ["semantic segmentation", "Task"], ["unsupervised domain adaptation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "210860962", "sentence": "Image translation and style transfer consist on transferring the appearance of a certain reference image or domain of images to a target image .", "ner": [["Image translation", "Task"], ["style transfer", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "210860962", "sentence": "Nevertheless , with the introduction of Generative Adversarial Networks ( GANs ) [ 1 8 ] and conditional GANs [ 3 6 ] , new research directions exploiting and extending these architectures to perform image translation have emerged [ 5 ] , [ 2 5 ] , [ 6 0 ] .", "ner": [["Generative Adversarial Networks", "Method"], ["GANs", "Method"], ["conditional GANs", "Method"], ["image translation", "Task"]], "rel": [["GANs", "Synonym-Of", "Generative Adversarial Networks"], ["Generative Adversarial Networks", "Used-For", "image translation"], ["conditional GANs", "Used-For", "image translation"]], "rel_plus": [["GANs:Method", "Synonym-Of", "Generative Adversarial Networks:Method"], ["Generative Adversarial Networks:Method", "Used-For", "image translation:Task"], ["conditional GANs:Method", "Used-For", "image translation:Task"]]}
{"doc_id": "210860962", "sentence": "Finally , in [ 3 0 ] the strategy is focused on obtaining representative features that are agnostic to the domains and that , as a result , can be decoded to any target domain The task of semantic segmentation , which consists on classifying each pixel of an image according to some known labels , is currently mainly addressed by Deep Neural Networks ( DNN ) in the state - ofthe - art .", "ner": [["semantic segmentation", "Task"], ["Deep Neural Networks", "Method"], ["DNN", "Method"]], "rel": [["Deep Neural Networks", "Used-For", "semantic segmentation"], ["DNN", "Synonym-Of", "Deep Neural Networks"]], "rel_plus": [["Deep Neural Networks:Method", "Used-For", "semantic segmentation:Task"], ["DNN:Method", "Synonym-Of", "Deep Neural Networks:Method"]]}
{"doc_id": "210860962", "sentence": "Current DNN models to tackle pixel - prediction problems are based on Fully Convolutional Networks ( FCNs ) [ 4 6 ] .", "ner": [["DNN", "Method"], ["pixel - prediction problems", "Task"], ["Fully Convolutional Networks", "Method"], ["FCNs", "Method"]], "rel": [["Fully Convolutional Networks", "SubClass-Of", "DNN"], ["DNN", "Used-For", "pixel - prediction problems"], ["Fully Convolutional Networks", "Used-For", "pixel - prediction problems"], ["FCNs", "Synonym-Of", "Fully Convolutional Networks"]], "rel_plus": [["Fully Convolutional Networks:Method", "SubClass-Of", "DNN:Method"], ["DNN:Method", "Used-For", "pixel - prediction problems:Task"], ["Fully Convolutional Networks:Method", "Used-For", "pixel - prediction problems:Task"], ["FCNs:Method", "Synonym-Of", "Fully Convolutional Networks:Method"]]}
{"doc_id": "210860962", "sentence": "FCNs endow Convolutional Neural Networks ( CNNs ) with an upsampling path to recover the input resolution .", "ner": [["FCNs", "Method"], ["Convolutional Neural Networks", "Method"], ["CNNs", "Method"], ["upsampling path", "Method"]], "rel": [["CNNs", "Synonym-Of", "Convolutional Neural Networks"], ["FCNs", "Part-Of", "Convolutional Neural Networks"], ["upsampling path", "Part-Of", "Convolutional Neural Networks"]], "rel_plus": [["CNNs:Method", "Synonym-Of", "Convolutional Neural Networks:Method"], ["FCNs:Method", "Part-Of", "Convolutional Neural Networks:Method"], ["upsampling path:Method", "Part-Of", "Convolutional Neural Networks:Method"]]}
{"doc_id": "210860962", "sentence": "Other proposed alternatives involve training the models on synthetic datasets such as SYNTHIA [ 4 4 ] or GTA 5 [ 4 2 ] , where labels can be extracted automatically .", "ner": [["SYNTHIA", "Dataset"], ["GTA 5", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "210860962", "sentence": "But we can also find a significant variety of approaches in semantic segmentation [ 2 2 ] , [ 3 7 ] , [ 5 1 ] , object detection [ 9 ] , [ 2 4 ] and depth estimation [ 2 ] , [ 5 ] , among others .", "ner": [["semantic segmentation", "Task"], ["object detection", "Task"], ["depth estimation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "210860962", "sentence": "However , a significant number of works rely on state - of - the - art image translation methods ( reviewed in subsection II - A ) or simple image reconstruction with different purposes .", "ner": [["image translation", "Task"], ["image reconstruction", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "210860962", "sentence": "Generally , in many cases , the image translation and/or reconstruction tasks are used as additional signal to learn more representative features for the target domain [ 6 ] , [ 3 7 ] .", "ner": [["image translation", "Task"], ["reconstruction", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "210860962", "sentence": "Finally , alternative research directions include ( 1 ) predicting the weights of the target model [ 4 5 ] ; ( 2 ) addressing the well known mode collapse problem in adversarial training with conditioning [ 3 3 ] ; ( 3 ) dealing with a partial unsupervised domain adaptation formulation [ 5 7 ] ; or ( 4 ) addressing and analyzing less popular tasks in the domain adaptation literature , such as object detection [ 9 ] .", "ner": [["partial unsupervised domain adaptation formulation", "Method"], ["domain adaptation", "Method"], ["object detection", "Task"]], "rel": [["domain adaptation", "Used-For", "object detection"]], "rel_plus": [["domain adaptation:Method", "Used-For", "object detection:Task"]]}
{"doc_id": "210860962", "sentence": "In the following subsections , we will explain fully convolutional network architectures for semantic segmentation [ 1 3 ] , [ 2 6 ] , [ 4 3 ] , [ 4 6 ] as well as the CycleGAN [ 6 0 ] and StarGAN [ 1 0 ] architectures for unpaired image translation .", "ner": [["fully convolutional network", "Method"], ["semantic segmentation", "Task"], ["CycleGAN", "Method"], ["StarGAN", "Method"], ["unpaired image translation", "Task"]], "rel": [["fully convolutional network", "Used-For", "semantic segmentation"], ["StarGAN", "Used-For", "unpaired image translation"], ["CycleGAN", "Used-For", "unpaired image translation"]], "rel_plus": [["fully convolutional network:Method", "Used-For", "semantic segmentation:Task"], ["StarGAN:Method", "Used-For", "unpaired image translation:Task"], ["CycleGAN:Method", "Used-For", "unpaired image translation:Task"]]}
{"doc_id": "210860962", "sentence": "As mentioned in Section II - B , a common approach for tackling the semantic segmentation problem is the use of FCNs that allow to obtain a segmentation mask directly from an image in an end - to - end way .", "ner": [["semantic segmentation", "Task"], ["FCNs", "Method"]], "rel": [["FCNs", "Used-For", "semantic segmentation"]], "rel_plus": [["FCNs:Method", "Used-For", "semantic segmentation:Task"]]}
{"doc_id": "210860962", "sentence": "FCNs are composed of a downsampling path ( with convolutional and subsampling operations ) followed by an upsampling path ( with transposed convolutions ) , which recovers the input resolution .", "ner": [["FCNs", "Method"], ["downsampling path", "Method"], ["convolutional and subsampling operations", "Method"], ["upsampling path", "Method"], ["transposed convolutions", "Method"]], "rel": [["downsampling path", "Part-Of", "FCNs"], ["upsampling path", "Part-Of", "FCNs"], ["convolutional and subsampling operations", "Part-Of", "downsampling path"], ["transposed convolutions", "Part-Of", "upsampling path"]], "rel_plus": [["downsampling path:Method", "Part-Of", "FCNs:Method"], ["upsampling path:Method", "Part-Of", "FCNs:Method"], ["convolutional and subsampling operations:Method", "Part-Of", "downsampling path:Method"], ["transposed convolutions:Method", "Part-Of", "upsampling path:Method"]]}
{"doc_id": "210860962", "sentence": "Given where ( x ( i ) , y ( i ) ) are pairs of image and segmentation masks respectively , we define a FCN S which approximates as well as possible the mapping of an input sample x , potentially not included in D , to its corresponding label y. Figure 1 depicts an FCN architecture with an initial convolutional block , 2 downsampling blocks , followed by 2 upsampling blocks and a final convolution that performs the segmentation .", "ner": [["FCN", "Method"], ["FCN", "Method"], ["convolutional block", "Method"], ["downsampling blocks", "Method"], ["upsampling blocks", "Method"], ["convolution", "Method"], ["segmentation", "Task"]], "rel": [["convolutional block", "Part-Of", "FCN"], ["downsampling blocks", "Part-Of", "FCN"], ["upsampling blocks", "Part-Of", "FCN"], ["convolution", "Part-Of", "FCN"], ["FCN", "Used-For", "segmentation"]], "rel_plus": [["convolutional block:Method", "Part-Of", "FCN:Method"], ["downsampling blocks:Method", "Part-Of", "FCN:Method"], ["upsampling blocks:Method", "Part-Of", "FCN:Method"], ["convolution:Method", "Part-Of", "FCN:Method"], ["FCN:Method", "Used-For", "segmentation:Task"]]}
{"doc_id": "210860962", "sentence": "Considering the segmentations in a one - hot encoding , where y c , p is 1 if pixel p in segmentation y corresponds to class c and 0 otherwise , the cross - entropy loss is defined as follows : One of the most popular image translation architectures based in conditional adversarial models is CycleGAN [ 6 0 ] .", "ner": [["image translation", "Task"], ["CycleGAN", "Method"]], "rel": [["CycleGAN", "Used-For", "image translation"]], "rel_plus": [["CycleGAN:Method", "Used-For", "image translation:Task"]]}
{"doc_id": "210860962", "sentence": "CycleGAN is designed to perform unpaired image translations between two domains of images by training two FCN generators and two discriminators .", "ner": [["CycleGAN", "Method"], ["unpaired image translations", "Task"], ["FCN generators", "Method"], ["two discriminators", "Method"]], "rel": [["two discriminators", "Part-Of", "CycleGAN"], ["FCN generators", "Part-Of", "CycleGAN"], ["CycleGAN", "Used-For", "unpaired image translations"]], "rel_plus": [["two discriminators:Method", "Part-Of", "CycleGAN:Method"], ["FCN generators:Method", "Part-Of", "CycleGAN:Method"], ["CycleGAN:Method", "Used-For", "unpaired image translations:Task"]]}
{"doc_id": "210860962", "sentence": "The generator networks are FCNs which , in our implementation , have an initial convolutional block , followed by 2 downsampling blocks , 2 upsampling blocks and a final convolution that maps back to the image domain .", "ner": [["generator networks", "Method"], ["FCNs", "Method"], ["convolutional block", "Method"], ["downsampling blocks", "Method"], ["upsampling blocks", "Method"], ["convolution", "Method"]], "rel": [["FCNs", "Part-Of", "generator networks"], ["convolutional block", "Part-Of", "FCNs"], ["downsampling blocks", "Part-Of", "FCNs"], ["convolution", "Part-Of", "FCNs"], ["upsampling blocks", "Part-Of", "FCNs"]], "rel_plus": [["FCNs:Method", "Part-Of", "generator networks:Method"], ["convolutional block:Method", "Part-Of", "FCNs:Method"], ["downsampling blocks:Method", "Part-Of", "FCNs:Method"], ["convolution:Method", "Part-Of", "FCNs:Method"], ["upsampling blocks:Method", "Part-Of", "FCNs:Method"]]}
{"doc_id": "210860962", "sentence": "The discriminators D follow the PatchGAN architecture [ 2 5 ] with 4 downsampling blocks and a fully connected layer with a single output .", "ner": [["discriminators D", "Method"], ["PatchGAN", "Method"], ["downsampling blocks", "Method"], ["fully connected layer", "Method"]], "rel": [["downsampling blocks", "Part-Of", "discriminators D"], ["fully connected layer", "Part-Of", "discriminators D"], ["discriminators D", "Part-Of", "PatchGAN"]], "rel_plus": [["downsampling blocks:Method", "Part-Of", "discriminators D:Method"], ["fully connected layer:Method", "Part-Of", "discriminators D:Method"], ["discriminators D:Method", "Part-Of", "PatchGAN:Method"]]}
{"doc_id": "210860962", "sentence": "Finally , the model is trained by making the generators G ab and G ba minimize the objective where \u03bb is an hyper - parameter controlling the relative importance of the two objectives , and making the discriminators Da and D b minimize the objective C. StarGAN StarGAN [ 1 0 ] is a recently proposed alternative to CycleGAN to address image translation , and is also the model that we choose as backbone for the unsupervised domain adaptation pipeline introduced in Section IV .", "ner": [["generators G", "Method"], ["discriminators Da", "Method"], ["StarGAN", "Method"], ["StarGAN", "Method"], ["CycleGAN", "Method"], ["image translation", "Task"], ["unsupervised domain adaptation", "Method"]], "rel": [["CycleGAN", "Used-For", "image translation"], ["CycleGAN", "Part-Of", "unsupervised domain adaptation"]], "rel_plus": [["CycleGAN:Method", "Used-For", "image translation:Task"], ["CycleGAN:Method", "Part-Of", "unsupervised domain adaptation:Method"]]}
{"doc_id": "210860962", "sentence": "Similarly to the CycleGAN model explained in Section III - B , the generator G is a FCN , which in our implementation has an initial convolutional block , followed by 2 downsampling blocks , 2 upsampling blocks and a final convolution that maps back to the image domain .", "ner": [["CycleGAN", "Method"], ["generator G", "Method"], ["FCN", "Method"], ["convolutional block", "Method"], ["downsampling blocks", "Method"], ["upsampling blocks", "Method"], ["convolution", "Method"]], "rel": [["generator G", "Part-Of", "CycleGAN"], ["FCN", "Part-Of", "generator G"], ["convolutional block", "Part-Of", "FCN"], ["downsampling blocks", "Part-Of", "FCN"], ["upsampling blocks", "Part-Of", "FCN"], ["downsampling blocks", "Part-Of", "FCN"], ["upsampling blocks", "Part-Of", "FCN"], ["convolution", "Part-Of", "FCN"]], "rel_plus": [["generator G:Method", "Part-Of", "CycleGAN:Method"], ["FCN:Method", "Part-Of", "generator G:Method"], ["convolutional block:Method", "Part-Of", "FCN:Method"], ["downsampling blocks:Method", "Part-Of", "FCN:Method"], ["upsampling blocks:Method", "Part-Of", "FCN:Method"], ["downsampling blocks:Method", "Part-Of", "FCN:Method"], ["upsampling blocks:Method", "Part-Of", "FCN:Method"], ["convolution:Method", "Part-Of", "FCN:Method"]]}
{"doc_id": "210860962", "sentence": "Again , as in CycleGAN , the discriminator D follows the PatchGAN [ 2 5 ] discriminator architecture with 4 downsampling blocks and two ouputs D rf and D dom .", "ner": [["CycleGAN", "Method"], ["discriminator D", "Method"], ["PatchGAN", "Method"], ["downsampling blocks", "Method"]], "rel": [["discriminator D", "Part-Of", "CycleGAN"], ["downsampling blocks", "Part-Of", "discriminator D"], ["discriminator D", "Part-Of", "PatchGAN"]], "rel_plus": [["discriminator D:Method", "Part-Of", "CycleGAN:Method"], ["downsampling blocks:Method", "Part-Of", "discriminator D:Method"], ["discriminator D:Method", "Part-Of", "PatchGAN:Method"]]}
{"doc_id": "210860962", "sentence": "In order to preserve the content of the input image in the translations , a cycle consistency loss is added to the generator 's set of losses , following CycleGAN : An example of one of the two cycle settings is represented in Figure 5 .", "ner": [["cycle consistency loss", "Method"], ["generator", "Method"], ["CycleGAN", "Method"]], "rel": [["cycle consistency loss", "Part-Of", "generator"], ["generator", "Part-Of", "CycleGAN"]], "rel_plus": [["cycle consistency loss:Method", "Part-Of", "generator:Method"], ["generator:Method", "Part-Of", "CycleGAN:Method"]]}
{"doc_id": "210860962", "sentence": "In the setting of unsupervised domain adaptation , we assume access to a dataset Ds = { ( x drawn from a source domain distribution , where in the case of image segmentation , x denote images and y ground - truth segmentation masks .", "ner": [["unsupervised domain adaptation", "Method"], ["image segmentation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "210860962", "sentence": "In order to tackle the unsupervised domain adaptation problem , we introduce a model based on an image translation backbone that we use to translate images xs and xt to their opposite domain .", "ner": [["unsupervised domain adaptation problem", "Task"], ["image translation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "210860962", "sentence": "As mentioned in Section II - C , many domain adaptation architectures use image translation or image reconstruction as an auxiliary task to obtain more general features , in order to boost the classification or segmentation performance on the target domain .", "ner": [["domain adaptation", "Method"], ["image translation", "Task"], ["image reconstruction", "Task"], ["classification", "Task"], ["segmentation", "Task"]], "rel": [["image translation", "Used-For", "domain adaptation"], ["image reconstruction", "Used-For", "domain adaptation"], ["image reconstruction", "Used-For", "classification"], ["image translation", "Used-For", "classification"], ["image translation", "Used-For", "segmentation"], ["image reconstruction", "Used-For", "segmentation"]], "rel_plus": [["image translation:Task", "Used-For", "domain adaptation:Method"], ["image reconstruction:Task", "Used-For", "domain adaptation:Method"], ["image reconstruction:Task", "Used-For", "classification:Task"], ["image translation:Task", "Used-For", "classification:Task"], ["image translation:Task", "Used-For", "segmentation:Task"], ["image reconstruction:Task", "Used-For", "segmentation:Task"]]}
{"doc_id": "210860962", "sentence": "In our model , we use image translation as auxiliary task and we rely on the state - of - the - art StarGAN [ 1 0 ] model as the backbone module of the proposed network .", "ner": [["image translation", "Task"], ["StarGAN", "Method"]], "rel": [["StarGAN", "Used-For", "image translation"]], "rel_plus": [["StarGAN:Method", "Used-For", "image translation:Task"]]}
{"doc_id": "210860962", "sentence": "The main advantage of using StarGAN architecture is that it allows us to have domain specific parameters in a single network , drastically reducing the number of parameters required to perform image translation w.r.t . related approaches in the literature such as [ 2 ] , [ 6 ] , [ 3 4 ] , [ 4 7 ] , [ 5 2 ] .", "ner": [["StarGAN", "Method"], ["image translation", "Task"]], "rel": [["StarGAN", "Used-For", "image translation"]], "rel_plus": [["StarGAN:Method", "Used-For", "image translation:Task"]]}
{"doc_id": "210860962", "sentence": "D f is implemented by two upsampling convolution and a final convolutional layer and its role is to predict , whether each pixel in the image space belongs to the source domain or the target domain .", "ner": [["upsampling convolution", "Method"], ["convolutional layer", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210860962", "sentence": "We use the modification of MNIST presented in [ 1 4 ] , which adds texture and color to the original version of MNIST , using random patches from BSDS 5 0 0 dataset [ 1 ] and inverting the color in the pixels belonging to the digit .", "ner": [["MNIST", "Dataset"], ["MNIST", "Dataset"], ["BSDS 5 0 0", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "210860962", "sentence": "Particulary , considering I M an MNIST image and I B a BSDS 5 0 0 random patch ( both in range The interesting characteristic about this dataset is that , when using it as a target domain in the domain adaptation problem , given a source domain as MNIST in simple black and white , it simulates the knowledge transfer from simulated environments to more complex real ones .", "ner": [["MNIST", "Dataset"], ["BSDS 5 0 0", "Dataset"], ["domain adaptation problem", "Task"], ["MNIST", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "210860962", "sentence": "To obtain masks with different per class pixel distributions with respect to MNIST and MNIST - M , we generate another modification of the MNIST dataset , which is based on eroding the original digits .", "ner": [["MNIST", "Dataset"], ["MNIST - M", "Dataset"], ["MNIST", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "210860962", "sentence": "We train these baselines with MNIST - thin as source domain and MNIST - M as target domain , highlighting the difference in appearance and per class distribution among the two domains , and providing an insightful proof of concept .", "ner": [["MNIST - thin", "Dataset"], ["MNIST - M", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "210860962", "sentence": "In particular , we train the network on MNIST - thin and we test it on MNIST - M images .", "ner": [["MNIST - thin", "Dataset"], ["MNIST - M", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "210860962", "sentence": "In order to exploit both Ds and Dt simultaneously , one could resort to training with additional auxiliary losses , e.g. for image reconstruction or image translation between domains .", "ner": [["image reconstruction", "Task"], ["image translation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "210860962", "sentence": "Note that this model does not have any domain adaptation component , but should still obtain more general features than the previous single FCN baseline .", "ner": [["domain adaptation", "Method"], ["FCN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210860962", "sentence": "By means of this study , we aim to assess the impact of the contributions of this thesis : the convenience of using a single image translation network with domain specific parameters and the advantage of the proposed class - conditional feature matching discriminator .", "ner": [["image translation network", "Method"], ["class - conditional feature matching discriminator", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210860962", "sentence": "For this particular reason and to highlight the benefit of having domain specific encoder parameters in a single image translation network , we define this experiment to compare and contrast the results achieved by CycleGAN ( Section III - B ) w.r.t . the ones achieved by StarGAN ( Section III - C ) .", "ner": [["image translation network", "Method"], ["CycleGAN", "Method"], ["StarGAN", "Method"]], "rel": [["image translation network", "Compare-With", "CycleGAN"]], "rel_plus": [["image translation network:Method", "Compare-With", "CycleGAN:Method"]]}
{"doc_id": "210860962", "sentence": "Our aim is to determine whether , despite the significant reduction of number of parameters in StarGAN and given a similar training strategy , the resulting translations are qualitatively comparable . 2 ) Class conditional discriminator : In order to measure the benefits of conditioning the feature adversarial matching per classes ( as described in Section IV ) , we define additional experiments using different modifications of D f .", "ner": [["StarGAN", "Method"], ["Class conditional discriminator", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210860962", "sentence": "The discriminator network is implemented by 2 downsampling convolutions , an average pooling and a fully connected classification layer .", "ner": [["discriminator network", "Method"], ["downsampling convolutions", "Method"], ["average pooling", "Method"], ["fully connected classification layer", "Method"]], "rel": [["downsampling convolutions", "Part-Of", "discriminator network"], ["average pooling", "Part-Of", "discriminator network"], ["fully connected classification layer", "Part-Of", "discriminator network"]], "rel_plus": [["downsampling convolutions:Method", "Part-Of", "discriminator network:Method"], ["average pooling:Method", "Part-Of", "discriminator network:Method"], ["fully connected classification layer:Method", "Part-Of", "discriminator network:Method"]]}
{"doc_id": "210860962", "sentence": "This is achieved by concatenating the segmentation prediction to the features extracted by the encoder , following the same idea as StarGAN with the translation labels .", "ner": [["StarGAN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210860962", "sentence": "After concatenation , the data is processed by 4 downsampling convolutional blocks followed by a fully connected layer .", "ner": [["downsampling convolutional blocks", "Method"], ["fully connected layer", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210860962", "sentence": "First , we will show qualitative results comparing CycleGAN and StarGAN as outlined in Section V - C 1 .", "ner": [["CycleGAN", "Method"], ["StarGAN", "Method"]], "rel": [["CycleGAN", "Compare-With", "StarGAN"]], "rel_plus": [["CycleGAN:Method", "Compare-With", "StarGAN:Method"]]}
{"doc_id": "210860962", "sentence": "The goal of this experiment is to assess the image translation quality of StarGAN vs CycleGAN and argue the choice of the image translation backbone in the proposed unsupervised domain adaptation model .", "ner": [["image translation", "Task"], ["StarGAN", "Method"], ["CycleGAN", "Method"], ["image translation", "Task"], ["unsupervised domain adaptation", "Method"]], "rel": [["StarGAN", "Used-For", "image translation"], ["CycleGAN", "Used-For", "image translation"], ["StarGAN", "Compare-With", "CycleGAN"]], "rel_plus": [["StarGAN:Method", "Used-For", "image translation:Task"], ["CycleGAN:Method", "Used-For", "image translation:Task"], ["StarGAN:Method", "Compare-With", "CycleGAN:Method"]]}
{"doc_id": "210860962", "sentence": "To do so , we train both StarGAN and CycleGAN models to perform image translation between MNIST and MNIST - M images .", "ner": [["StarGAN", "Method"], ["CycleGAN", "Method"], ["image translation", "Task"], ["MNIST", "Dataset"], ["MNIST - M", "Dataset"]], "rel": [["StarGAN", "Used-For", "image translation"], ["CycleGAN", "Used-For", "image translation"], ["MNIST", "Benchmark-For", "image translation"], ["MNIST - M", "Benchmark-For", "image translation"], ["StarGAN", "Trained-With", "MNIST"], ["CycleGAN", "Trained-With", "MNIST"], ["StarGAN", "Trained-With", "MNIST - M"], ["CycleGAN", "Trained-With", "MNIST - M"]], "rel_plus": [["StarGAN:Method", "Used-For", "image translation:Task"], ["CycleGAN:Method", "Used-For", "image translation:Task"], ["MNIST:Dataset", "Benchmark-For", "image translation:Task"], ["MNIST - M:Dataset", "Benchmark-For", "image translation:Task"], ["StarGAN:Method", "Trained-With", "MNIST:Dataset"], ["CycleGAN:Method", "Trained-With", "MNIST:Dataset"], ["StarGAN:Method", "Trained-With", "MNIST - M:Dataset"], ["CycleGAN:Method", "Trained-With", "MNIST - M:Dataset"]]}
{"doc_id": "210860962", "sentence": "In order to compare both models , we provide a small set of qualitative examples in Figure 1 0 Along with the translation samples , we detail the number of parameters required by a standard implementation of both models , see Table VI -A. As shown in the table , CycleGAN and StarGAN use similar architectures for their generators and discriminators .", "ner": [["CycleGAN", "Method"], ["StarGAN", "Method"], ["generators", "Method"], ["discriminators", "Method"]], "rel": [["generators", "Part-Of", "CycleGAN"], ["discriminators", "Part-Of", "CycleGAN"], ["CycleGAN", "Compare-With", "StarGAN"], ["discriminators", "Part-Of", "StarGAN"], ["generators", "Part-Of", "StarGAN"]], "rel_plus": [["generators:Method", "Part-Of", "CycleGAN:Method"], ["discriminators:Method", "Part-Of", "CycleGAN:Method"], ["CycleGAN:Method", "Compare-With", "StarGAN:Method"], ["discriminators:Method", "Part-Of", "StarGAN:Method"], ["generators:Method", "Part-Of", "StarGAN:Method"]]}
{"doc_id": "210860962", "sentence": "Recall that CycleGAN requires two generators and two discriminators , whereas StarGAN only requires a single generator and a single discriminator .", "ner": [["CycleGAN", "Method"], ["generators", "Method"], ["discriminators", "Method"], ["StarGAN", "Method"], ["generator", "Method"], ["discriminator", "Method"]], "rel": [["generators", "Part-Of", "CycleGAN"], ["discriminators", "Part-Of", "CycleGAN"], ["generator", "Part-Of", "StarGAN"], ["discriminator", "Part-Of", "StarGAN"], ["CycleGAN", "Compare-With", "StarGAN"]], "rel_plus": [["generators:Method", "Part-Of", "CycleGAN:Method"], ["discriminators:Method", "Part-Of", "CycleGAN:Method"], ["generator:Method", "Part-Of", "StarGAN:Method"], ["discriminator:Method", "Part-Of", "StarGAN:Method"], ["CycleGAN:Method", "Compare-With", "StarGAN:Method"]]}
{"doc_id": "210860962", "sentence": "Therefore , StarGAN exhibits a reduction of roughly 5 0 % in number of parameters when compared to CycleGAN .", "ner": [["StarGAN", "Method"], ["CycleGAN", "Method"]], "rel": [["StarGAN", "Compare-With", "CycleGAN"]], "rel_plus": [["StarGAN:Method", "Compare-With", "CycleGAN:Method"]]}
{"doc_id": "210860962", "sentence": "The extra parameters in the StarGAN generator and discriminator networks ( w.r.t . the CycleGAN single domain networks ) are caused by the domain input conditioning of the generator and the domain classification output of the discriminator .", "ner": [["StarGAN", "Method"], ["generator", "Method"], ["discriminator", "Method"], ["CycleGAN", "Method"], ["generator", "Method"], ["discriminator", "Method"]], "rel": [["generator", "Part-Of", "StarGAN"], ["discriminator", "Part-Of", "StarGAN"]], "rel_plus": [["generator:Method", "Part-Of", "StarGAN:Method"], ["discriminator:Method", "Part-Of", "StarGAN:Method"]]}
{"doc_id": "210860962", "sentence": "All models are trained to perform unsupervised domain adaption from MNIST - thin to MNIST - M , i.e. only MNIST - thin segmentation masks are used for training .", "ner": [["unsupervised domain adaption", "Task"], ["MNIST - thin", "Dataset"], ["MNIST - M", "Dataset"], ["MNIST - thin", "Dataset"]], "rel": [["MNIST - thin", "Benchmark-For", "unsupervised domain adaption"], ["MNIST - M", "Benchmark-For", "unsupervised domain adaption"], ["MNIST - thin", "Benchmark-For", "unsupervised domain adaption"]], "rel_plus": [["MNIST - thin:Dataset", "Benchmark-For", "unsupervised domain adaption:Task"], ["MNIST - M:Dataset", "Benchmark-For", "unsupervised domain adaption:Task"], ["MNIST - thin:Dataset", "Benchmark-For", "unsupervised domain adaption:Task"]]}
{"doc_id": "210860962", "sentence": "We first stress the results from the FCN segmenter and StarGAN with segmenter ( SGAN - S ) baselines described in Section V - B. None of those baselines include any domain adaptation step , and their performance on the target domain serves solely as lower bound to the unsupervised domain adaptation models .", "ner": [["FCN", "Method"], ["StarGAN with segmenter", "Method"], ["SGAN - S", "Method"], ["domain adaptation", "Method"], ["unsupervised domain adaptation", "Method"]], "rel": [["SGAN - S", "Synonym-Of", "StarGAN with segmenter"]], "rel_plus": [["SGAN - S:Method", "Synonym-Of", "StarGAN with segmenter:Method"]]}
{"doc_id": "210860962", "sentence": "As expected , both baselines FCN segmenter and SGAN - S exhibit the lowest results , especially when it comes to segmenting the target domain digits , outlining the need of including domain adaptation constraints to the model .", "ner": [["FCN", "Method"], ["SGAN - S", "Method"], ["domain adaptation", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210860962", "sentence": "However , adding an image translation backbone significantly boosts the results adding 0. 1 2 7 points of digit IoU and 0.0 8 points of mean IoU. The differences among FCN segmenter and SGAN - S can also be visually perceived in the qualitative results shown in Figure 1 1 , where some digits that were missed by the FCN segmenter , appear in the SGAN - S segmentations .", "ner": [["image translation", "Task"], ["FCN", "Method"], ["SGAN - S", "Method"], ["FCN", "Method"], ["SGAN - S", "Method"]], "rel": [["FCN", "Compare-With", "SGAN - S"]], "rel_plus": [["FCN:Method", "Compare-With", "SGAN - S:Method"]]}
{"doc_id": "210860962", "sentence": "Second , we analyze the results of the often adopted domain adaptation strategy denoted as SGAN - S Uncond . ( for unconditioned discriminator ) in Table VI - B and Figure 1 1 .", "ner": [["domain adaptation", "Method"], ["SGAN - S Uncond .", "Method"], ["unconditioned discriminator", "Method"]], "rel": [["unconditioned discriminator", "Part-Of", "SGAN - S Uncond ."], ["domain adaptation", "Used-For", "SGAN - S Uncond ."]], "rel_plus": [["unconditioned discriminator:Method", "Part-Of", "SGAN - S Uncond .:Method"], ["domain adaptation:Method", "Used-For", "SGAN - S Uncond .:Method"]]}
{"doc_id": "210860962", "sentence": "The is SGAN - S Uncond . model is a natural alternative to our model , which uses a global discriminator to match features from both domains at the image translation bottleneck ( see Section V - C 2 ) .", "ner": [["SGAN - S Uncond .", "Method"], ["global discriminator", "Method"], ["image translation", "Task"]], "rel": [["global discriminator", "Part-Of", "SGAN - S Uncond ."], ["SGAN - S Uncond .", "Used-For", "image translation"]], "rel_plus": [["global discriminator:Method", "Part-Of", "SGAN - S Uncond .:Method"], ["SGAN - S Uncond .:Method", "Used-For", "image translation:Task"]]}
{"doc_id": "210860962", "sentence": "We can also see SGAN - S Uncond . as an extension of SGAN - S , which includes a global feature matching as domain adaptation step .", "ner": [["SGAN - S Uncond .", "Method"], ["SGAN - S", "Method"], ["global feature matching", "Method"], ["domain adaptation", "Method"]], "rel": [["global feature matching", "Used-For", "SGAN - S Uncond ."], ["SGAN - S Uncond .", "SubClass-Of", "SGAN - S"], ["global feature matching", "Used-For", "domain adaptation"]], "rel_plus": [["global feature matching:Method", "Used-For", "SGAN - S Uncond .:Method"], ["SGAN - S Uncond .:Method", "SubClass-Of", "SGAN - S:Method"], ["global feature matching:Method", "Used-For", "domain adaptation:Method"]]}
{"doc_id": "210860962", "sentence": "SGAN - S , which results in a 0. 2 2 6 points target digit boost and 0. 1 4 7 mIoU points boost over the FCN segmenter baseline .", "ner": [["SGAN - S", "Method"], ["FCN", "Method"]], "rel": [["SGAN - S", "Compare-With", "FCN"]], "rel_plus": [["SGAN - S:Method", "Compare-With", "FCN:Method"]]}
{"doc_id": "210860962", "sentence": "We remind that in this version , the feature discriminator D f takes the segmentation predictions as input , by concatenating them to the image translation bottleneck features .", "ner": [["discriminator D", "Method"], ["image translation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "210860962", "sentence": "In this thesis , we have tackled the unsupervised domain adaptation problem for semantic segmentation .", "ner": [["unsupervised domain adaptation", "Method"], ["semantic segmentation", "Task"]], "rel": [["unsupervised domain adaptation", "Used-For", "semantic segmentation"]], "rel_plus": [["unsupervised domain adaptation:Method", "Used-For", "semantic segmentation:Task"]]}
{"doc_id": "210860962", "sentence": "A natural next step would be to test our method in larger scale datasets , such as SYNTHIA [ 4 4 ] or GTA 5 [ 4 2 ] vs. Cityscapes [ 1 1 ] , with the intention of analyzing the performance on more realistic environments .", "ner": [["SYNTHIA", "Dataset"], ["GTA 5", "Dataset"], ["Cityscapes", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "210860962", "sentence": "Fully convolutional network for semantic segmentation .", "ner": [["Fully convolutional network", "Method"], ["semantic segmentation", "Task"]], "rel": [["Fully convolutional network", "Used-For", "semantic segmentation"]], "rel_plus": [["Fully convolutional network:Method", "Used-For", "semantic segmentation:Task"]]}
{"doc_id": "210860962", "sentence": "The network is built with several convolutional blocks composed of a convolution ( or transposed convolution when upsampling ) , a dropout layer [ 4 9 ] , an instance normalization layer ( without computation of running statistics ) [ 5 5 ] and a rectified linear unit ( ReLU ) activation .", "ner": [["convolutional blocks", "Method"], ["convolution", "Method"], ["transposed convolution", "Method"], ["dropout", "Method"], ["instance normalization", "Method"], ["rectified linear unit", "Method"], ["ReLU", "Method"]], "rel": [["convolution", "Part-Of", "convolutional blocks"], ["transposed convolution", "Part-Of", "convolutional blocks"], ["dropout", "Part-Of", "convolutional blocks"], ["instance normalization", "Part-Of", "convolutional blocks"], ["rectified linear unit", "Part-Of", "convolutional blocks"], ["ReLU", "Synonym-Of", "rectified linear unit"]], "rel_plus": [["convolution:Method", "Part-Of", "convolutional blocks:Method"], ["transposed convolution:Method", "Part-Of", "convolutional blocks:Method"], ["dropout:Method", "Part-Of", "convolutional blocks:Method"], ["instance normalization:Method", "Part-Of", "convolutional blocks:Method"], ["rectified linear unit:Method", "Part-Of", "convolutional blocks:Method"], ["ReLU:Method", "Synonym-Of", "rectified linear unit:Method"]]}
{"doc_id": "210860962", "sentence": "More specifically , it is composed of a first convolutional block of 3 2 channels and 3 \u00d7 3 kernel size , followed by two downsampling blocks with stride 2 and 4 \u00d7 4 kernel size that duplicate the input channels , two upsampling blocks to recover the input resolution ( dividing by 2 the number of channels ) and a final 3 \u00d7 3 convolutional layer followed by a sigmoid non - linearity .", "ner": [["convolutional block", "Method"], ["downsampling blocks", "Method"], ["upsampling blocks", "Method"], ["3 \u00d7 3 convolutional layer", "Method"], ["sigmoid", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210860962", "sentence": "We apply a 0. 2 dropout in all convolutional blocks along the network during training .", "ner": [["dropout", "Method"], ["convolutional blocks", "Method"]], "rel": [["dropout", "Part-Of", "convolutional blocks"]], "rel_plus": [["dropout:Method", "Part-Of", "convolutional blocks:Method"]]}
{"doc_id": "210860962", "sentence": "The network is trained using an Adam optimizer [ 2 8 ] with an initial learning rate of 0.0 0 1 and an exponential decay of 0. 9 9 5 after each epoch .", "ner": [["Adam optimizer", "Method"], ["exponential decay", "Method"]], "rel": [["exponential decay", "Part-Of", "Adam optimizer"]], "rel_plus": [["exponential decay:Method", "Part-Of", "Adam optimizer:Method"]]}
{"doc_id": "210860962", "sentence": "CycleGAN and StarGAN .", "ner": [["CycleGAN", "Method"], ["StarGAN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210860962", "sentence": "For the CycleGAN [ 6 0 ] and StarGAN [ 1 0 ] architectures presented in Sections III - B and III - C respectively , we mostly use the same implementation hyper - parameters published by the authors with slight modifications .", "ner": [["CycleGAN", "Method"], ["StarGAN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210860962", "sentence": "In the case of StarGAN , we also reduce the kernel size of the generator 's convolutions to 3 \u00d7 3 .", "ner": [["StarGAN", "Method"], ["generator", "Method"], ["convolutions", "Method"]], "rel": [["generator", "Part-Of", "StarGAN"], ["convolutions", "Part-Of", "generator"]], "rel_plus": [["generator:Method", "Part-Of", "StarGAN:Method"], ["convolutions:Method", "Part-Of", "generator:Method"]]}
{"doc_id": "210860962", "sentence": "The models have been depicted in Figure 2 ( CycleGAN ) and Figure 4 ( StarGAN ) .", "ner": [["CycleGAN", "Method"], ["StarGAN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210860962", "sentence": "The segmentation decoder S attached to Ge consists of an upsampling path and a final convolutional layer following the FCN for segmentation model , detailed in this appendix .", "ner": [["decoder S", "Method"], ["upsampling path", "Method"], ["convolutional layer", "Method"], ["FCN", "Method"]], "rel": [["upsampling path", "Part-Of", "decoder S"], ["convolutional layer", "Part-Of", "decoder S"], ["FCN", "Part-Of", "decoder S"]], "rel_plus": [["upsampling path:Method", "Part-Of", "decoder S:Method"], ["convolutional layer:Method", "Part-Of", "decoder S:Method"], ["FCN:Method", "Part-Of", "decoder S:Method"]]}
{"doc_id": "210860962", "sentence": "It is depicted in detail in Figure 7 and consits of two upsampling convolutional blocks and a final 3 \u00d7 3 convolutional layer with a single output channel .", "ner": [["upsampling convolutional blocks", "Method"], ["convolutional layer", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210860962", "sentence": "The upsampling convolutional blocks include a 4 \u00d7 4 transposed convolution that upsamples the spatial resolution by 2 and outputs half of its input channels , followed by a 0.0 1 slope leaky ReLU .", "ner": [["upsampling convolutional blocks", "Method"], ["4 \u00d7 4 transposed convolution", "Method"], ["leaky ReLU", "Method"]], "rel": [["4 \u00d7 4 transposed convolution", "Part-Of", "upsampling convolutional blocks"], ["leaky ReLU", "Part-Of", "upsampling convolutional blocks"]], "rel_plus": [["4 \u00d7 4 transposed convolution:Method", "Part-Of", "upsampling convolutional blocks:Method"], ["leaky ReLU:Method", "Part-Of", "upsampling convolutional blocks:Method"]]}
{"doc_id": "210860962", "sentence": "All modules have a 0. 2 dropout layer in their convolutional blocks .", "ner": [["dropout", "Method"], ["convolutional blocks", "Method"]], "rel": [["dropout", "Part-Of", "convolutional blocks"]], "rel_plus": [["dropout:Method", "Part-Of", "convolutional blocks:Method"]]}
{"doc_id": "210860962", "sentence": "All the modules are trained with an Adam optimizer [ 2 8 ] with an initial learning rate of 0.0 0 0 1 and an exponential decay of 0. 9 9 5 every 1 5 0 0 training iterations .", "ner": [["Adam optimizer", "Method"], ["exponential decay", "Method"]], "rel": [["exponential decay", "Part-Of", "Adam optimizer"]], "rel_plus": [["exponential decay:Method", "Part-Of", "Adam optimizer:Method"]]}
{"doc_id": "210860962", "sentence": "This discriminator is depicted in Figure 9a and is composed of two downsampling blocks , an average pooling and a fully connected layer .", "ner": [["discriminator", "Method"], ["downsampling blocks", "Method"], ["average pooling", "Method"], ["fully connected layer", "Method"]], "rel": [["downsampling blocks", "Part-Of", "discriminator"], ["average pooling", "Part-Of", "discriminator"], ["fully connected layer", "Part-Of", "discriminator"]], "rel_plus": [["downsampling blocks:Method", "Part-Of", "discriminator:Method"], ["average pooling:Method", "Part-Of", "discriminator:Method"], ["fully connected layer:Method", "Part-Of", "discriminator:Method"]]}
{"doc_id": "210860962", "sentence": "The downsampling blocks consist of a 4 \u00d7 4 convolution with stride 2 , duplicating the number of channels at its input .", "ner": [["downsampling blocks", "Method"], ["4 \u00d7 4 convolution", "Method"]], "rel": [["4 \u00d7 4 convolution", "Part-Of", "downsampling blocks"]], "rel_plus": [["4 \u00d7 4 convolution:Method", "Part-Of", "downsampling blocks:Method"]]}
{"doc_id": "210860962", "sentence": "The convolution is followed by a leaky ReLU of slope 0.0 1 .", "ner": [["convolution", "Method"], ["leaky ReLU", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210860962", "sentence": "The architecture of this discriminator is as follows : convolutional blocks of 4 \u00d7 4 convolutions and leaky ReLU of 0.0 1 slope .", "ner": [["convolutional blocks", "Method"], ["4 \u00d7 4 convolutions", "Method"], ["leaky ReLU", "Method"]], "rel": [["leaky ReLU", "Part-Of", "convolutional blocks"], ["4 \u00d7 4 convolutions", "Part-Of", "convolutional blocks"]], "rel_plus": [["leaky ReLU:Method", "Part-Of", "convolutional blocks:Method"], ["4 \u00d7 4 convolutions:Method", "Part-Of", "convolutional blocks:Method"]]}
{"doc_id": "210860962", "sentence": "Then , the segmentation mask is concatenated to the upsampled features and 4 downsampling blocks are applied to reduce by a factor of 2 each axis of the spatial resolution , while duplicating the number of channels at each step ( without taking into account the extra channels from the concatenation of the segmentation prediction ) .", "ner": [["downsampling blocks", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210860962", "sentence": "These features are finally processed by an average pooling , which is followed by a fully connected layer with a single output .", "ner": [["average pooling", "Method"], ["fully connected layer", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "53731879", "sentence": "Concretely , we propose a Self - similarity Grouping ( SSG ) approach , which exploits the potential similarity ( from global body to local parts ) of unlabeled samples to automatically build multiple clusters from different views .", "ner": [["Self - similarity Grouping", "Method"], ["SSG", "Method"]], "rel": [["SSG", "Synonym-Of", "Self - similarity Grouping"]], "rel_plus": [["SSG:Method", "Synonym-Of", "Self - similarity Grouping:Method"]]}
{"doc_id": "53731879", "sentence": "Despite the apparent simplify , our SSG outperforms the state - of - the - arts by more than 4. 6 % ( DukeMTMC to Market 1 5 0 1 ) and 4. 4 % ( Market 1 5 0 1 to DukeMTMC ) in mAP , respectively .", "ner": [["SSG", "Method"], ["DukeMTMC", "Dataset"], ["Market 1 5 0 1", "Dataset"], ["Market 1 5 0 1", "Dataset"], ["DukeMTMC", "Dataset"]], "rel": [["SSG", "Evaluated-With", "DukeMTMC"], ["SSG", "Evaluated-With", "Market 1 5 0 1"], ["SSG", "Evaluated-With", "Market 1 5 0 1"], ["SSG", "Evaluated-With", "DukeMTMC"]], "rel_plus": [["SSG:Method", "Evaluated-With", "DukeMTMC:Dataset"], ["SSG:Method", "Evaluated-With", "Market 1 5 0 1:Dataset"], ["SSG:Method", "Evaluated-With", "Market 1 5 0 1:Dataset"], ["SSG:Method", "Evaluated-With", "DukeMTMC:Dataset"]]}
{"doc_id": "53731879", "sentence": "Upon our SSG , we further introduce a clustering - guided semisupervised approach named SSG + + to conduct the one - shot domain adaption in an open set setting ( i.e. the number of independent identities from the target domain is unknown ) .", "ner": [["SSG", "Method"], ["SSG + +", "Method"]], "rel": [["SSG + +", "SubClass-Of", "SSG"]], "rel_plus": [["SSG + +:Method", "SubClass-Of", "SSG:Method"]]}
{"doc_id": "53731879", "sentence": "Without spending much effort on labeling , our SSG + + can further promote the mAP upon SSG by 1 0 . 7 % and 6. 9 % , respectively .", "ner": [["SSG + +", "Method"], ["SSG", "Method"]], "rel": [["SSG + +", "Compare-With", "SSG"]], "rel_plus": [["SSG + +:Method", "Compare-With", "SSG:Method"]]}
{"doc_id": "53731879", "sentence": "Since it is extremely expensive to label all images in target dataset , one of the most popular solutions for that problem is unsupervised domain adaptation(UDA ) .", "ner": [["unsupervised domain adaptation(UDA )", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "53731879", "sentence": "The common UDA has been studied extensively in image classification , object detection , face recognition and semantic segmentation . [ 4 , 5 , 2 7 ] .", "ner": [["UDA", "Method"], ["image classification", "Task"], ["object detection", "Task"], ["face recognition", "Task"], ["semantic segmentation", "Task"]], "rel": [["UDA", "Used-For", "image classification"], ["UDA", "Used-For", "object detection"], ["UDA", "Used-For", "face recognition"], ["UDA", "Used-For", "semantic segmentation"]], "rel_plus": [["UDA:Method", "Used-For", "image classification:Task"], ["UDA:Method", "Used-For", "object detection:Task"], ["UDA:Method", "Used-For", "face recognition:Task"], ["UDA:Method", "Used-For", "semantic segmentation:Task"]]}
{"doc_id": "53731879", "sentence": "Comparison of performance with fully supervised learning , directly transfer and state - of - art unsupervised domain adaptation method and our proposed one shot domain adaptation UDA approaches always have an assumption that the source and target domain share the same set of classes , which does not hold for person re - ID problem .", "ner": [["unsupervised domain adaptation method", "Method"], ["one shot domain adaptation UDA approaches", "Method"], ["person re - ID", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "53731879", "sentence": "Recently , several unsupervised domain adaptation approaches for person re - ID have been proposed and achieve some promising improvements .", "ner": [["unsupervised domain adaptation approaches", "Method"], ["person re - ID", "Task"]], "rel": [["unsupervised domain adaptation approaches", "Used-For", "person re - ID"]], "rel_plus": [["unsupervised domain adaptation approaches:Method", "Used-For", "person re - ID:Task"]]}
{"doc_id": "53731879", "sentence": "Specifically , the state - of - art UDA methods are about 2 0 % to 3 0 % lower than the corresponding fully supervised baselines , making person re - ID limited in real world scenarios .", "ner": [["UDA methods", "Method"], ["person re - ID", "Task"]], "rel": [["UDA methods", "Used-For", "person re - ID"]], "rel_plus": [["UDA methods:Method", "Used-For", "person re - ID:Task"]]}
{"doc_id": "53731879", "sentence": "Moreover , most existing person re - ID UDA methods need to generate lots of images with target domain style as the first step , which is time - consuming , especially , for the dataset with many cameras , and this kind of pre - processing step makes the model can not be trained end - to - end .", "ner": [["person re - ID UDA methods", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "53731879", "sentence": "In particular , one shot learning is based on the setting that only one sample from each category is labeled , which does not require much more human effort compared with the UDA and is also cheaper and feasible compared with its fullysupervised counterpart .", "ner": [["one shot learning", "Method"], ["UDA", "Method"]], "rel": [["one shot learning", "Compare-With", "UDA"]], "rel_plus": [["one shot learning:Method", "Compare-With", "UDA:Method"]]}
{"doc_id": "53731879", "sentence": "By taking the advantages of both one shot mining and self - mining , we achieve the mAP scores of 7 1 . 5 % and 5 5 . 9 % on Market 1 5 0 1 and DukeMTMC - reID , which outperforms the state - of - the - arts more than 1 7 . 8 % and 6. 9 % respectively .", "ner": [["one shot mining", "Method"], ["self - mining", "Method"], ["Market 1 5 0 1", "Dataset"], ["DukeMTMC - reID", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "53731879", "sentence": "More importantly , under five shots setting , we recover more than 9 8 % and 9 5 % performance of fully supervised method on Market 1 5 0 1 and DukeMTMC - reID respectively , as shown in Fig 1 .", "ner": [["Market 1 5 0 1", "Dataset"], ["DukeMTMC - reID", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "53731879", "sentence": "The contribution of this work can be summarized as following . \u2022 We propose a simple yet effective one shot domain adaptation framework for person re - ID , which can recover the performance of its fully supervised counterpart with few annotations . \u2022 We introduce a novel similarity - guided strategy for person re - ID one shot mining and integrate it into UDA framework , so that we can train two branches of unsupervised domain adaptation and one shot domain adaptation jointly and effectively boost the process of domain adaption . \u2022 We conduct extensive experiments and ablation study on Market 1 5 0 1 [ 4 4 ] and DukeMTMC - ReID [ 3 1 , 4 6 ] to demonstrate the effectiveness of one shot domain adaptation and each component .", "ner": [["one shot domain adaptation framework", "Method"], ["person re - ID", "Task"], ["similarity - guided strategy", "Method"], ["person re - ID one shot mining", "Task"], ["UDA framework", "Method"], ["unsupervised domain adaptation", "Method"], ["one shot domain adaptation", "Method"], ["domain adaption", "Method"], ["Market 1 5 0 1", "Dataset"], ["DukeMTMC - ReID", "Dataset"], ["one shot domain adaptation", "Method"]], "rel": [["one shot domain adaptation framework", "Used-For", "person re - ID"], ["similarity - guided strategy", "Used-For", "person re - ID one shot mining"], ["similarity - guided strategy", "Used-For", "UDA framework"], ["unsupervised domain adaptation", "SubClass-Of", "domain adaption"], ["one shot domain adaptation", "SubClass-Of", "domain adaption"], ["one shot domain adaptation", "Evaluated-With", "Market 1 5 0 1"], ["one shot domain adaptation", "Evaluated-With", "DukeMTMC - ReID"]], "rel_plus": [["one shot domain adaptation framework:Method", "Used-For", "person re - ID:Task"], ["similarity - guided strategy:Method", "Used-For", "person re - ID one shot mining:Task"], ["similarity - guided strategy:Method", "Used-For", "UDA framework:Method"], ["unsupervised domain adaptation:Method", "SubClass-Of", "domain adaption:Method"], ["one shot domain adaptation:Method", "SubClass-Of", "domain adaption:Method"], ["one shot domain adaptation:Method", "Evaluated-With", "Market 1 5 0 1:Dataset"], ["one shot domain adaptation:Method", "Evaluated-With", "DukeMTMC - ReID:Dataset"]]}
{"doc_id": "53731879", "sentence": "Our work is closely related to unsupervised domain adaptation(UDA ) where no data in target domain are labeled during training .", "ner": [["unsupervised domain adaptation(UDA", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "53731879", "sentence": "For instance , Ganin et al. [ 1 6 ] propose a gradient reversal layer ( GRL ) and integrate it into standard deep neural network for minimizing the classification loss while maximizing domain confusion loss .", "ner": [["gradient reversal layer", "Method"], ["GRL", "Method"], ["deep neural network", "Method"], ["classification loss", "Method"], ["domain confusion loss", "Method"]], "rel": [["GRL", "Synonym-Of", "gradient reversal layer"], ["gradient reversal layer", "Part-Of", "deep neural network"], ["gradient reversal layer", "Used-For", "classification loss"], ["gradient reversal layer", "Used-For", "domain confusion loss"]], "rel_plus": [["GRL:Method", "Synonym-Of", "gradient reversal layer:Method"], ["gradient reversal layer:Method", "Part-Of", "deep neural network:Method"], ["gradient reversal layer:Method", "Used-For", "classification loss:Method"], ["gradient reversal layer:Method", "Used-For", "domain confusion loss:Method"]]}
{"doc_id": "53731879", "sentence": "Deng et al. [ 8 ] aim to translate images from source domain to target domain by proposed similarity preserving generative adversarial network(SPGAN ) .", "ner": [["similarity preserving generative adversarial network(SPGAN )", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "53731879", "sentence": "In [ 3 9 ] , a Transferable Joint Attribute - Identity Deep Learning ( TJ - AIDL ) is proposed to learn an attribute - semantic and identity discriminative feature representation space for target domain without using additional labeled data in target domain .", "ner": [["Transferable Joint Attribute - Identity Deep Learning", "Method"], ["TJ - AIDL", "Method"]], "rel": [["TJ - AIDL", "Synonym-Of", "Transferable Joint Attribute - Identity Deep Learning"]], "rel_plus": [["TJ - AIDL:Method", "Synonym-Of", "Transferable Joint Attribute - Identity Deep Learning:Method"]]}
{"doc_id": "53731879", "sentence": "In [ 4 9 ] , Zhong et al. introduce a Hetero - Homogeneous Learning ( HHL ) method , which aims to improve the generalization ability of re - ID models on the target set by achieving camera invariance and domain connectedness simultaneously .", "ner": [["Hetero - Homogeneous Learning", "Method"], ["HHL", "Method"], ["re - ID", "Task"]], "rel": [["HHL", "Synonym-Of", "Hetero - Homogeneous Learning"], ["Hetero - Homogeneous Learning", "Used-For", "re - ID"]], "rel_plus": [["HHL:Method", "Synonym-Of", "Hetero - Homogeneous Learning:Method"], ["Hetero - Homogeneous Learning:Method", "Used-For", "re - ID:Task"]]}
{"doc_id": "53731879", "sentence": "One - shot learning aims at learning a task from one or very few training examples [ 1 1 ] and there are some works of one shot person re - ID [ 1 , 1 3 , 2 6 , 4 1 ] .", "ner": [["One - shot learning", "Method"], ["one shot person re - ID", "Task"]], "rel": [["one shot person re - ID", "Used-For", "One - shot learning"]], "rel_plus": [["one shot person re - ID:Task", "Used-For", "One - shot learning:Method"]]}
{"doc_id": "53731879", "sentence": "In [ 1 ] , Bak et al. utilize a metric learning approach for a pair of cameras which can be split into texture and color components for one shot image - based re - ID .", "ner": [["metric learning", "Method"], ["one shot image - based re - ID", "Task"]], "rel": [["metric learning", "Used-For", "one shot image - based re - ID"]], "rel_plus": [["metric learning:Method", "Used-For", "one shot image - based re - ID:Task"]]}
{"doc_id": "53731879", "sentence": "To the best of our knowledge , there are no previous works on one shot domain adaptation for per - son re - ID and existing one shot re - ID methods can hardly apply to the domain adaptation directly .", "ner": [["one shot domain adaptation", "Method"], ["per - son re - ID", "Task"], ["one shot re - ID", "Task"], ["domain adaptation", "Method"]], "rel": [["one shot domain adaptation", "Used-For", "per - son re - ID"], ["one shot domain adaptation", "Used-For", "one shot re - ID"], ["one shot domain adaptation", "SubClass-Of", "domain adaptation"]], "rel_plus": [["one shot domain adaptation:Method", "Used-For", "per - son re - ID:Task"], ["one shot domain adaptation:Method", "Used-For", "one shot re - ID:Task"], ["one shot domain adaptation:Method", "SubClass-Of", "domain adaptation:Method"]]}
{"doc_id": "53731879", "sentence": "Based on the above analysis , in this paper , we aim to address person re - ID domain adaptation with similarity - guided one shot learning approach .", "ner": [["person re - ID domain adaptation", "Method"], ["similarity - guided one shot learning approach", "Method"]], "rel": [["similarity - guided one shot learning approach", "Used-For", "person re - ID domain adaptation"]], "rel_plus": [["similarity - guided one shot learning approach:Method", "Used-For", "person re - ID domain adaptation:Method"]]}
{"doc_id": "53731879", "sentence": "Problem Definition For one shot domain adaptation in person re - ID , we have a labeled source dataset S : { X S , Y S } , which contains N s person images and each image x s has a corresponding label y s , where y s \u2208 { 1 , 2 , ... , P s } , P s is the number of identities in source dataset .", "ner": [["one shot domain adaptation", "Method"], ["person re - ID", "Task"]], "rel": [["one shot domain adaptation", "Used-For", "person re - ID"]], "rel_plus": [["one shot domain adaptation:Method", "Used-For", "person re - ID:Task"]]}
{"doc_id": "53731879", "sentence": "Our proposed one shot domain adaptation framework is based on a model pre - trained on source dataset S. In order to obtain the baseline model , we utilize ResNet 5 0 [ 1 9 ] pretrained on ImageNet [ 7 ] as backbone network .", "ner": [["one shot domain adaptation framework", "Method"], ["ResNet 5 0", "Method"], ["ImageNet", "Dataset"]], "rel": [["ResNet 5 0", "Part-Of", "ImageNet"]], "rel_plus": [["ResNet 5 0:Method", "Part-Of", "ImageNet:Dataset"]]}
{"doc_id": "53731879", "sentence": "The fully connected ( FC ) layer is named as FC and the number of output channels is changed from 1 0 0 0 to P s , where P s is the number of identifies in S. And the outputs of global average pooling ( GAP ) and FC are noted as f 1 and f 2 .", "ner": [["fully connected", "Method"], ["FC", "Method"], ["FC", "Method"], ["global average pooling", "Method"], ["GAP", "Method"], ["FC", "Method"]], "rel": [["FC", "Synonym-Of", "fully connected"], ["GAP", "Synonym-Of", "global average pooling"]], "rel_plus": [["FC:Method", "Synonym-Of", "fully connected:Method"], ["GAP:Method", "Synonym-Of", "global average pooling:Method"]]}
{"doc_id": "53731879", "sentence": "With these feature vectors , we conduct domain adaptation by self - mining and similarityguided one shot mining .", "ner": [["domain adaptation", "Method"], ["self - mining", "Method"], ["similarityguided one shot mining", "Method"]], "rel": [["self - mining", "Used-For", "domain adaptation"], ["similarityguided one shot mining", "Used-For", "domain adaptation"]], "rel_plus": [["self - mining:Method", "Used-For", "domain adaptation:Method"], ["similarityguided one shot mining:Method", "Used-For", "domain adaptation:Method"]]}
{"doc_id": "53731879", "sentence": "For one shot mining , instead of some step - wised approaches which exploit the unlabeled data in target dataset gradually , we propose a novel similarity - guided one shot mining approach , which can effectively boost the domain adaption process .", "ner": [["one shot mining", "Method"], ["similarity - guided one shot mining", "Method"], ["domain adaption process", "Method"]], "rel": [["similarity - guided one shot mining", "SubClass-Of", "one shot mining"], ["similarity - guided one shot mining", "Used-For", "domain adaption process"]], "rel_plus": [["similarity - guided one shot mining:Method", "SubClass-Of", "one shot mining:Method"], ["similarity - guided one shot mining:Method", "Used-For", "domain adaption process:Method"]]}
{"doc_id": "53731879", "sentence": "Although the re - ID performance drops dramatically when directly adopted to another dataset , it is still much better than the performance of directly applying the ResNet 5 0 pretrained on ImageNet , which is almost zero .", "ner": [["re - ID", "Task"], ["ResNet 5 0", "Method"], ["ImageNet", "Dataset"]], "rel": [["ResNet 5 0", "Trained-With", "ImageNet"]], "rel_plus": [["ResNet 5 0:Method", "Trained-With", "ImageNet:Dataset"]]}
{"doc_id": "53731879", "sentence": "As mentioned in [ 3 3 ] , this self - mining approach is simple yet effective for unsupervised re - ID domain adaptation , which provides a good starting point for our following one shot mining .", "ner": [["self - mining", "Method"], ["unsupervised re - ID domain adaptation", "Method"], ["one shot mining", "Method"]], "rel": [["self - mining", "Used-For", "unsupervised re - ID domain adaptation"]], "rel_plus": [["self - mining:Method", "Used-For", "unsupervised re - ID domain adaptation:Method"]]}
{"doc_id": "53731879", "sentence": "Although unsupervised domain adaptation for person re - ID has been studied extensively [ 8 , 4 0 , 3 9 , 4 9 ] recently , there is still more than 2 5 % and 1 5 % performance drop in mAP and Rank - 1 accuracy comparing to fully supervised baseline , as shown in Fig 1 .", "ner": [["unsupervised domain adaptation", "Method"], ["person re - ID", "Task"]], "rel": [["unsupervised domain adaptation", "Used-For", "person re - ID"]], "rel_plus": [["unsupervised domain adaptation:Method", "Used-For", "person re - ID:Task"]]}
{"doc_id": "53731879", "sentence": "As shown in Fig 2 , after feature extraction on each unlabeled image , f 1 is feed into two branch : self - mining and one shot mining .", "ner": [["feature extraction", "Task"], ["self - mining", "Method"], ["one shot mining", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "53731879", "sentence": "After assigning self pseudo labels and target pseudo labels , the self mining branch uses the hard - batch triplet loss [ 2 0 ] as object function and one shot mining branch employs the same triplet loss with confidence term .", "ner": [["self mining branch", "Method"], ["hard - batch triplet loss", "Method"], ["one shot mining branch", "Method"], ["triplet loss with confidence term", "Method"]], "rel": [["hard - batch triplet loss", "Part-Of", "self mining branch"], ["triplet loss with confidence term", "Part-Of", "one shot mining branch"]], "rel_plus": [["hard - batch triplet loss:Method", "Part-Of", "self mining branch:Method"], ["triplet loss with confidence term:Method", "Part-Of", "one shot mining branch:Method"]]}
{"doc_id": "53731879", "sentence": "Our loss function for optimization is the combination of softmax loss and batch - hard triplet loss as follows : One Shot Learning Domain Adaptation For the one shot mining and self - mining , we just leverage the hardbatch triplet loss for metric learning and the only difference is that we introduce a confidence term to original hard - batch triplet loss for one shot mining , which can be formulated as follows : ( 7 ) Hence , our object function for one shot domain adaption framework is 4 .", "ner": [["softmax loss", "Method"], ["batch - hard triplet loss", "Method"], ["One Shot Learning Domain Adaptation", "Method"], ["one shot mining", "Method"], ["self - mining", "Method"], ["hardbatch triplet loss", "Method"], ["metric learning", "Method"], ["confidence term", "Method"], ["hard - batch triplet loss", "Method"], ["one shot mining", "Method"], ["one shot domain adaption framework", "Method"]], "rel": [["hardbatch triplet loss", "Part-Of", "metric learning"], ["confidence term", "Part-Of", "hard - batch triplet loss"], ["hard - batch triplet loss", "Part-Of", "one shot mining"]], "rel_plus": [["hardbatch triplet loss:Method", "Part-Of", "metric learning:Method"], ["confidence term:Method", "Part-Of", "hard - batch triplet loss:Method"], ["hard - batch triplet loss:Method", "Part-Of", "one shot mining:Method"]]}
{"doc_id": "53731879", "sentence": "Experiments In this section , we evaluate the proposed method on three re - ID datasets which are considered as large scale in the community , i.e. Market 1 5 0 1 [ 4 4 ] and DukeMTMCReID [ 3 1 , 4 6 ] and MSMT 1 7 [ 4 0 ] .", "ner": [["re - ID", "Task"], ["Market 1 5 0 1", "Dataset"], ["DukeMTMCReID", "Dataset"], ["MSMT 1 7", "Dataset"]], "rel": [["Market 1 5 0 1", "Benchmark-For", "re - ID"], ["DukeMTMCReID", "Benchmark-For", "re - ID"], ["MSMT 1 7", "Benchmark-For", "re - ID"]], "rel_plus": [["Market 1 5 0 1:Dataset", "Benchmark-For", "re - ID:Task"], ["DukeMTMCReID:Dataset", "Benchmark-For", "re - ID:Task"], ["MSMT 1 7:Dataset", "Benchmark-For", "re - ID:Task"]]}
{"doc_id": "53731879", "sentence": "DukeMTMC - ReID [ 4 6 ] is a subset of the DukeMTMC dataset [ 3 1 ] .", "ner": [["DukeMTMC - ReID", "Dataset"], ["DukeMTMC", "Dataset"]], "rel": [["DukeMTMC - ReID", "SubClass-Of", "DukeMTMC"]], "rel_plus": [["DukeMTMC - ReID:Dataset", "SubClass-Of", "DukeMTMC:Dataset"]]}
{"doc_id": "53731879", "sentence": "For Market - 1 5 0 1 and DukeMTMC - ReID , we use the evaluation packages provided by [ 4 4 ] and [ 4 6 ] , respectively .", "ner": [["Market - 1 5 0 1", "Dataset"], ["DukeMTMC - ReID", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "53731879", "sentence": "For data augmentation , we employ random cropping , random flipping and random erasing [ 4 8 ] .", "ner": [["data augmentation", "Method"], ["random cropping", "Method"], ["random flipping", "Method"], ["random erasing", "Method"]], "rel": [["random cropping", "SubClass-Of", "data augmentation"], ["random flipping", "SubClass-Of", "data augmentation"], ["random erasing", "SubClass-Of", "data augmentation"]], "rel_plus": [["random cropping:Method", "SubClass-Of", "data augmentation:Method"], ["random flipping:Method", "SubClass-Of", "data augmentation:Method"], ["random erasing:Method", "SubClass-Of", "data augmentation:Method"]]}
{"doc_id": "53731879", "sentence": "During training , we use the Adam [ 2 2 ] with weight decay 0.0 0 0 5 to optimize the parameters for 1 5 0 epochs .", "ner": [["Adam", "Method"], ["weight decay", "Method"]], "rel": [["weight decay", "Part-Of", "Adam"]], "rel_plus": [["weight decay:Method", "Part-Of", "Adam:Method"]]}
{"doc_id": "53731879", "sentence": "During training , we follow the same settings of data augmentation and triplet loss to train the one shot adaptation framework .", "ner": [["data augmentation", "Method"], ["triplet loss", "Method"], ["one shot adaptation framework", "Method"]], "rel": [["one shot adaptation framework", "Trained-With", "data augmentation"], ["one shot adaptation framework", "Trained-With", "triplet loss"]], "rel_plus": [["one shot adaptation framework:Method", "Trained-With", "data augmentation:Method"], ["one shot adaptation framework:Method", "Trained-With", "triplet loss:Method"]]}
{"doc_id": "53731879", "sentence": "For instance , the baseline model trained on Market 1 5 0 1 tested on Market 1 5 0 1 achieves 9 2 . 5 % in rank - 1 accuracy and 8 0 . 8 % in mAP , but it drops to 2 6 . 9 % and 1 3 . 7 % when tested on DukeMTMC - reID , where the performance gap is more than 6 0 % .", "ner": [["Market 1 5 0 1", "Dataset"], ["Market 1 5 0 1", "Dataset"], ["DukeMTMC - reID", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "53731879", "sentence": "And the similar drop can be observed when DukeMTMC - reID is used as training set and tested on Market 1 5 0 1 .", "ner": [["DukeMTMC - reID", "Dataset"], ["Market 1 5 0 1", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "53731879", "sentence": "For example , when training set is DukeMTMC - reID and testing set is Market 1 5 0 1 , to our best knowledge , the best UDA approach achieves 5 3 . 7 % and 7 5 . 8 % on mAP and rank - 1 accuracy , which is lower than the fully supervised method by about 2 0 % , and it is described in Table 2 [ 8 ] as well .", "ner": [["DukeMTMC - reID", "Dataset"], ["Market 1 5 0 1", "Dataset"], ["UDA approach", "Method"]], "rel": [["UDA approach", "Evaluated-With", "DukeMTMC - reID"], ["UDA approach", "Evaluated-With", "Market 1 5 0 1"]], "rel_plus": [["UDA approach:Method", "Evaluated-With", "DukeMTMC - reID:Dataset"], ["UDA approach:Method", "Evaluated-With", "Market 1 5 0 1:Dataset"]]}
{"doc_id": "53731879", "sentence": "By our proposed one shot domain adaptation framework , we achieve 7 1 . 5 % and 8 7 . 5 % on mAP and rank - 1 accuracy respectively when trained on DukeMTMC - reID while tested on Market 1 5 0 1 , which is only 9% and 5% lower than baseline model .", "ner": [["one shot domain adaptation framework", "Method"], ["DukeMTMC - reID", "Dataset"], ["Market 1 5 0 1", "Dataset"]], "rel": [["one shot domain adaptation framework", "Evaluated-With", "DukeMTMC - reID"], ["one shot domain adaptation framework", "Evaluated-With", "Market 1 5 0 1"]], "rel_plus": [["one shot domain adaptation framework:Method", "Evaluated-With", "DukeMTMC - reID:Dataset"], ["one shot domain adaptation framework:Method", "Evaluated-With", "Market 1 5 0 1:Dataset"]]}
{"doc_id": "53731879", "sentence": "As shown in Table 1 , only with self mining , we improve the performance by + 1 9 . 3 % and + 2 4 . 1 in mAP and rank - 1 accuracy when trained on DukeMTMCreID and tested on Market 1 5 0 1 .", "ner": [["self mining", "Method"], ["DukeMTMCreID", "Dataset"], ["Market 1 5 0 1", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "53731879", "sentence": "When trained on Market 1 5 0 1 and tested on DukeMTMCreID , the performance gain is + 2 6 . 8 % and + 4 2 . 4 % in rank - 1 accuracy and mAP , respectively .", "ner": [["Market 1 5 0 1", "Dataset"], ["DukeMTMCreID", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "53731879", "sentence": "When tested on DukeMTMC - reID , Market - 1 5 0 1 is used as source , and vice versa . \" Baseline denotes using the full identity labels on the corresponding target dataset(See Section 3. 1 ) . \" Direct Transfer means directly applying the source - trained model on the target domain.\"UDA \" stands for the state - of - art unsupervised domain adaptation approach . \" SSM \" means self similarity mining as described in Section 3. 2 . 1 . \" w SG \" is our proposed similarity - guided one shot mining strategy . \" w/o SG \" means training the one shot domain framework only by one shot data . \" Jointly \" stands for proposed joint training strategy in Section 3. 2 . 2 .", "ner": [["DukeMTMC - reID", "Dataset"], ["Market - 1 5 0 1", "Dataset"], ["domain.\"UDA \"", "Method"], ["unsupervised domain adaptation approach", "Method"], ["SSM", "Method"], ["self similarity mining", "Method"], ["w SG", "Method"], ["similarity - guided one shot mining", "Method"]], "rel": [["domain.\"UDA \"", "Synonym-Of", "unsupervised domain adaptation approach"], ["SSM", "Synonym-Of", "self similarity mining"], ["w SG", "Synonym-Of", "similarity - guided one shot mining"]], "rel_plus": [["domain.\"UDA \":Method", "Synonym-Of", "unsupervised domain adaptation approach:Method"], ["SSM:Method", "Synonym-Of", "self similarity mining:Method"], ["w SG:Method", "Synonym-Of", "similarity - guided one shot mining:Method"]]}
{"doc_id": "53731879", "sentence": "From Table 1 , we gain + 7. 7 % and + 4. 5 % in mAP and rank - 1 accuracy , respectively , when trained on DukeMTMC - reID and tested on Market 1 5 0 1 .", "ner": [["DukeMTMC - reID", "Dataset"], ["Market 1 5 0 1", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "53731879", "sentence": "And at the end of training stage , it has a great precision on both of Market 1 5 0 1 and DukeMTMC - reID .", "ner": [["Market 1 5 0 1", "Dataset"], ["DukeMTMC - reID", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "53731879", "sentence": "Specifically , the precision is more than 8 0 % and 9 0 % on Market 1 5 0 1 and DukeMTMC - reID , respectively .", "ner": [["Market 1 5 0 1", "Dataset"], ["DukeMTMC - reID", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "53731879", "sentence": "So far all components in one shot domain adaptation framework have been evaluated and validated , we achieve promising performances on both Market 1 5 0 1 and DukeMTMC - reID .", "ner": [["one shot domain adaptation framework", "Method"], ["Market 1 5 0 1", "Dataset"], ["DukeMTMC - reID", "Dataset"]], "rel": [["one shot domain adaptation framework", "Evaluated-With", "Market 1 5 0 1"], ["one shot domain adaptation framework", "Evaluated-With", "DukeMTMC - reID"]], "rel_plus": [["one shot domain adaptation framework:Method", "Evaluated-With", "Market 1 5 0 1:Dataset"], ["one shot domain adaptation framework:Method", "Evaluated-With", "DukeMTMC - reID:Dataset"]]}
{"doc_id": "53731879", "sentence": "For instance , we achieve 4 6 . 8 % and 3 6 . 9 % improvements in mAP and rank - 1 accuracy when trained on DukeMTMC - reID and tested on Market 1 5 0 1 .", "ner": [["DukeMTMC - reID", "Dataset"], ["Market 1 5 0 1", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "53731879", "sentence": "Specif -   To our best knowledge , there are no previous works on one shot domain adaptation for person re - ID , so we compare the proposed method with the state - of - the - art unsupervised learning methods on Market 1 5 0 1 , DukeMTMC - reID and MSMT 1 7 in Table 2 , Table 3 and Table 4 respectively .", "ner": [["one shot domain adaptation", "Method"], ["person re - ID", "Task"], ["unsupervised learning", "Task"], ["Market 1 5 0 1", "Dataset"], ["DukeMTMC - reID", "Dataset"], ["MSMT 1 7", "Dataset"]], "rel": [["one shot domain adaptation", "Used-For", "person re - ID"], ["Market 1 5 0 1", "Benchmark-For", "unsupervised learning"], ["DukeMTMC - reID", "Benchmark-For", "unsupervised learning"], ["MSMT 1 7", "Benchmark-For", "unsupervised learning"]], "rel_plus": [["one shot domain adaptation:Method", "Used-For", "person re - ID:Task"], ["Market 1 5 0 1:Dataset", "Benchmark-For", "unsupervised learning:Task"], ["DukeMTMC - reID:Dataset", "Benchmark-For", "unsupervised learning:Task"], ["MSMT 1 7:Dataset", "Benchmark-For", "unsupervised learning:Task"]]}
{"doc_id": "53731879", "sentence": "Results on Market 1 5 0 1 On Market - 1 5 0 1 , we compare our results with two hand - crafted features , i.e. Bagof - Words ( BoW ) [ 4 4 ] and local maximal occurrence ( LOMO ) [ 2 4 ] , three unsupervised methods , including UMDL [ 2 9 ] , PUL [ 1 0 ] and CAMEL [ 4 3 ] , and five unsupervised domain adaptation methods , including PTGAN [ 4 0 ] , SPGAN [ 8 ] , TJ - AIDL [ 3 9 ] , ARN [ 2 3 ] and UDAP [ 3 3 ] .", "ner": [["Market 1 5 0 1", "Dataset"], ["Market - 1 5 0 1", "Dataset"], ["Bagof - Words", "Method"], ["BoW", "Method"], ["local maximal occurrence", "Method"], ["LOMO", "Method"], ["unsupervised methods", "Method"], ["UMDL", "Method"], ["PUL", "Method"], ["CAMEL", "Method"], ["unsupervised domain adaptation methods", "Method"], ["PTGAN", "Method"], ["SPGAN", "Method"], ["TJ - AIDL", "Method"], ["ARN", "Method"], ["UDAP", "Method"]], "rel": [["Bagof - Words", "Evaluated-With", "Market - 1 5 0 1"], ["local maximal occurrence", "Evaluated-With", "Market - 1 5 0 1"], ["UMDL", "Evaluated-With", "Market - 1 5 0 1"], ["PUL", "Evaluated-With", "Market - 1 5 0 1"], ["CAMEL", "Evaluated-With", "Market - 1 5 0 1"], ["PTGAN", "Evaluated-With", "Market - 1 5 0 1"], ["SPGAN", "Evaluated-With", "Market - 1 5 0 1"], ["TJ - AIDL", "Evaluated-With", "Market - 1 5 0 1"], ["ARN", "Evaluated-With", "Market - 1 5 0 1"], ["UDAP", "Evaluated-With", "Market - 1 5 0 1"], ["BoW", "Synonym-Of", "Bagof - Words"], ["LOMO", "Synonym-Of", "local maximal occurrence"], ["UMDL", "SubClass-Of", "unsupervised methods"], ["PUL", "SubClass-Of", "unsupervised methods"], ["CAMEL", "SubClass-Of", "unsupervised methods"], ["PTGAN", "SubClass-Of", "unsupervised domain adaptation methods"], ["SPGAN", "SubClass-Of", "unsupervised domain adaptation methods"], ["TJ - AIDL", "SubClass-Of", "unsupervised domain adaptation methods"], ["ARN", "SubClass-Of", "unsupervised domain adaptation methods"], ["UDAP", "SubClass-Of", "unsupervised domain adaptation methods"]], "rel_plus": [["Bagof - Words:Method", "Evaluated-With", "Market - 1 5 0 1:Dataset"], ["local maximal occurrence:Method", "Evaluated-With", "Market - 1 5 0 1:Dataset"], ["UMDL:Method", "Evaluated-With", "Market - 1 5 0 1:Dataset"], ["PUL:Method", "Evaluated-With", "Market - 1 5 0 1:Dataset"], ["CAMEL:Method", "Evaluated-With", "Market - 1 5 0 1:Dataset"], ["PTGAN:Method", "Evaluated-With", "Market - 1 5 0 1:Dataset"], ["SPGAN:Method", "Evaluated-With", "Market - 1 5 0 1:Dataset"], ["TJ - AIDL:Method", "Evaluated-With", "Market - 1 5 0 1:Dataset"], ["ARN:Method", "Evaluated-With", "Market - 1 5 0 1:Dataset"], ["UDAP:Method", "Evaluated-With", "Market - 1 5 0 1:Dataset"], ["BoW:Method", "Synonym-Of", "Bagof - Words:Method"], ["LOMO:Method", "Synonym-Of", "local maximal occurrence:Method"], ["UMDL:Method", "SubClass-Of", "unsupervised methods:Method"], ["PUL:Method", "SubClass-Of", "unsupervised methods:Method"], ["CAMEL:Method", "SubClass-Of", "unsupervised methods:Method"], ["PTGAN:Method", "SubClass-Of", "unsupervised domain adaptation methods:Method"], ["SPGAN:Method", "SubClass-Of", "unsupervised domain adaptation methods:Method"], ["TJ - AIDL:Method", "SubClass-Of", "unsupervised domain adaptation methods:Method"], ["ARN:Method", "SubClass-Of", "unsupervised domain adaptation methods:Method"], ["UDAP:Method", "SubClass-Of", "unsupervised domain adaptation methods:Method"]]}
{"doc_id": "53731879", "sentence": "The comparisons indicate the competitiveness and effectiveness of the proposed method on Market - 1 5 0 1 Results on DukeMTMC - reID The similar improvement can also be observed when we tested on DukeMTMCreID dataset .", "ner": [["Market - 1 5 0 1", "Dataset"], ["DukeMTMC - reID", "Dataset"], ["DukeMTMCreID", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "53731879", "sentence": "Therefore , the superiority of proposed one shot domain adaptation for person re - ID can be concluded .", "ner": [["one shot domain adaptation", "Method"], ["person re - ID", "Task"]], "rel": [["one shot domain adaptation", "Used-For", "person re - ID"]], "rel_plus": [["one shot domain adaptation:Method", "Used-For", "person re - ID:Task"]]}
{"doc_id": "53731879", "sentence": "Results on MSMT 1 7 In addition , we further evaluate proposed one shot domain adaptation approach on MSMT 1 7 dataset , which is the largest and most challenging re - ID dataset .", "ner": [["MSMT 1 7", "Dataset"], ["one shot domain adaptation approach", "Method"], ["MSMT 1 7", "Dataset"], ["re - ID", "Task"]], "rel": [["one shot domain adaptation approach", "Evaluated-With", "MSMT 1 7"], ["MSMT 1 7", "Benchmark-For", "re - ID"], ["one shot domain adaptation approach", "Used-For", "re - ID"]], "rel_plus": [["one shot domain adaptation approach:Method", "Evaluated-With", "MSMT 1 7:Dataset"], ["MSMT 1 7:Dataset", "Benchmark-For", "re - ID:Task"], ["one shot domain adaptation approach:Method", "Used-For", "re - ID:Task"]]}
{"doc_id": "53731879", "sentence": "In this work , we made the first endeavour to tackle the challenging domain adaption person re - ID by one ( few ) shot learning .", "ner": [["domain adaption person re - ID", "Task"], ["one ( few ) shot learning", "Method"]], "rel": [["one ( few ) shot learning", "Used-For", "domain adaption person re - ID"]], "rel_plus": [["one ( few ) shot learning:Method", "Used-For", "domain adaption person re - ID:Task"]]}
{"doc_id": "23569888", "sentence": "To avoid the biases in currently available datasets , we consider a natural human - robot interaction setting to design a data - acquisition protocol for visual object recognition on the iCub humanoid robot .", "ner": [["human - robot interaction", "Task"], ["data - acquisition protocol", "Method"], ["visual object recognition", "Task"]], "rel": [["data - acquisition protocol", "Used-For", "visual object recognition"]], "rel_plus": [["data - acquisition protocol:Method", "Used-For", "visual object recognition:Task"]]}
{"doc_id": "23569888", "sentence": "By studying both object categorization and identification problems , we highlight key differences between object recognition in robotics applications and in image retrieval tasks , for which the considered deep learning approaches have been originally designed .", "ner": [["object categorization", "Task"], ["object recognition", "Task"], ["image retrieval", "Task"], ["deep learning", "Method"]], "rel": [["deep learning", "Used-For", "object recognition"], ["object recognition", "Compare-With", "image retrieval"], ["deep learning", "Used-For", "image retrieval"]], "rel_plus": [["deep learning:Method", "Used-For", "object recognition:Task"], ["object recognition:Task", "Compare-With", "image retrieval:Task"], ["deep learning:Method", "Used-For", "image retrieval:Task"]]}
{"doc_id": "23569888", "sentence": "Clearly , visual perception is only one of the possible sensory modalities enabling object recognition in robotics ( Montesano et al 2 0 0 8 ; Chitta et al 2 0 1 1 ; Dahiya et al 2 0 1 0 ; Natale et al 2 0 0 4 ; Gorges et al 2 0 1 0 ; Sinapov et al 2 0 1 4 a ; Hosoda and Iwase 2 0 1 0 ; Moldovan et al 2 0 1 2 ) and indeed , the comparison with human intelligence suggests there is more than \" just \" vision to object recognition ( Metta et al 2 0 0 6 ; Fitzpatrick et al 2 0 0 8 ; .", "ner": [["visual perception", "Task"], ["object recognition", "Task"], ["robotics", "Task"], ["object recognition", "Task"]], "rel": [["object recognition", "SubTask-Of", "robotics"]], "rel_plus": [["object recognition:Task", "SubTask-Of", "robotics:Task"]]}
{"doc_id": "23569888", "sentence": "To this end , in this work we mainly focus on object recognition tasks in robotics using only visual cues .", "ner": [["object recognition", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "23569888", "sentence": "RO ] 2 8 Sep 2 0 1 7 primarily reported on computer vision benchmarks such as ( Griffin et al 2 0 0 7 ; Everingham et al 2 0 1 0 Everingham et al , 2 0 1 5 Russakovsky et al 2 0 1 5 ) , which have been essentially designed for image retrieval tasks , and hardly are representative of a robotics scenario .", "ner": [["computer vision", "Task"], ["image retrieval", "Task"]], "rel": [["image retrieval", "SubTask-Of", "computer vision"]], "rel_plus": [["image retrieval:Task", "SubTask-Of", "computer vision:Task"]]}
{"doc_id": "23569888", "sentence": "Using the iCub robot , we devised a human - robot interaction framework to acquire a corresponding dataset , named iCWT ( iCubWorld Transformations ) .", "ner": [["human - robot interaction", "Task"], ["iCWT", "Dataset"], ["iCubWorld Transformations", "Dataset"]], "rel": [["human - robot interaction", "Used-For", "iCWT"], ["iCWT", "Synonym-Of", "iCubWorld Transformations"]], "rel_plus": [["human - robot interaction:Task", "Used-For", "iCWT:Dataset"], ["iCWT:Dataset", "Synonym-Of", "iCubWorld Transformations:Dataset"]]}
{"doc_id": "23569888", "sentence": "Provided with the iCWT dataset , we performed extensive empirical investigation using different state of the art Convolutional Neural Networks ( CNN ) architectures ( Krizhevsky et al 2 0 1 2 a ; Simonyan and Zisserman 2 0 1 5 ; Szegedy et al 2 0 1 5 ; He et al 2 0 1 6 ) .", "ner": [["iCWT", "Dataset"], ["Convolutional Neural Networks", "Method"], ["CNN", "Method"]], "rel": [["Convolutional Neural Networks", "Evaluated-With", "iCWT"], ["CNN", "Synonym-Of", "Convolutional Neural Networks"]], "rel_plus": [["Convolutional Neural Networks:Method", "Evaluated-With", "iCWT:Dataset"], ["CNN:Method", "Synonym-Of", "Convolutional Neural Networks:Method"]]}
{"doc_id": "23569888", "sentence": "Indeed , CNNs are known to need massive amount of data to work and data - augmentation is often used to improve results .", "ner": [["CNNs", "Method"], ["data - augmentation", "Method"]], "rel": [["data - augmentation", "Used-For", "CNNs"]], "rel_plus": [["data - augmentation:Method", "Used-For", "CNNs:Method"]]}
{"doc_id": "23569888", "sentence": "As we discuss in detail later in the paper , investigating this latter question highlighted some differences between iCWT and other datasets such as ImageNet ( Russakovsky et al 2 0 1 5 ) .", "ner": [["iCWT", "Dataset"], ["ImageNet", "Dataset"]], "rel": [["iCWT", "Compare-With", "ImageNet"]], "rel_plus": [["iCWT:Dataset", "Compare-With", "ImageNet:Dataset"]]}
{"doc_id": "23569888", "sentence": "More generally , we identified clear differences between the object recognition task in robotics with respect to scenarios typically considered in learning and vision .", "ner": [["object recognition", "Task"], ["robotics", "Task"]], "rel": [["object recognition", "Used-For", "robotics"]], "rel_plus": [["object recognition:Task", "Used-For", "robotics:Task"]]}
{"doc_id": "23569888", "sentence": "Sec. 6 concludes our study with the review of possible directions of improvement for visual recognition in robotics .", "ner": [["visual recognition", "Task"], ["robotics", "Task"]], "rel": [["visual recognition", "SubTask-Of", "robotics"]], "rel_plus": [["visual recognition:Task", "SubTask-Of", "robotics:Task"]]}
{"doc_id": "23569888", "sentence": "Deep Learning methods are receiving growing attention in robotics , and are being adopted for a variety of problems such as object recognition ( Schwarz et al 2 0 1 5 ; Held et al 2 0 1 6 ; Pasquale et al 2 0 1 6 a ) , place recognition and mapping ( S\u00fcnderhauf et al 2 0 1 6 ( S\u00fcnderhauf et al , 2 0 1 5 , object affordances ( Nguyen et al 2 0 1 6 ) , grasping ( Redmon and Angelova 2 0 1 4 ; Pinto and Gupta 2 0 1 5 ; Levine et al 2 0 1 6 ) and tactile perception ( Baishya and B\u00e4uml 2 0 1 6 ) .", "ner": [["robotics", "Task"], ["object recognition", "Task"]], "rel": [["object recognition", "SubTask-Of", "robotics"]], "rel_plus": [["object recognition:Task", "SubTask-Of", "robotics:Task"]]}
{"doc_id": "23569888", "sentence": "In ( Schwarz et al 2 0 1 5 ) the authors demonstrate transfer learning from pre - trained deep Convolutional Neural Networks ( CNNs ) and propose a way to include depth information from an RGB - D camera .", "ner": [["transfer learning", "Task"], ["Convolutional Neural Networks", "Method"], ["CNNs", "Method"]], "rel": [["Convolutional Neural Networks", "Used-For", "transfer learning"], ["CNNs", "Synonym-Of", "Convolutional Neural Networks"]], "rel_plus": [["Convolutional Neural Networks:Method", "Used-For", "transfer learning:Task"], ["CNNs:Method", "Synonym-Of", "Convolutional Neural Networks:Method"]]}
{"doc_id": "23569888", "sentence": "In the literature , several datasets for visual recognition in robotics have been proposed : COIL ( Nene et al 1 9 9 6 ) , ALOI ( Geusebroek et al 2 0 0 5 ) , Washington RGB - D ( Lai et al 2 0 1 1 ) , KIT ( Kasper et al 2 0 1 2 ) , SHORT - 1 0 0 ( RiveraRubio et al 2 0 1 4 ) , BigBIRD ( Singh et al 2 0 1 4 ) , Rutgers Amazon Picking Challenge RGB - D Dataset ( Rennie et al 2 0 1 6 ) .", "ner": [["visual recognition", "Task"], ["robotics", "Task"], ["COIL", "Dataset"], ["ALOI", "Dataset"], ["Washington RGB - D", "Dataset"], ["KIT", "Dataset"], ["SHORT - 1 0 0", "Dataset"], ["BigBIRD", "Dataset"], ["Rutgers Amazon Picking Challenge RGB - D Dataset", "Dataset"]], "rel": [["COIL", "Benchmark-For", "visual recognition"], ["ALOI", "Benchmark-For", "visual recognition"], ["Washington RGB - D", "Benchmark-For", "visual recognition"], ["KIT", "Benchmark-For", "visual recognition"], ["SHORT - 1 0 0", "Benchmark-For", "visual recognition"], ["BigBIRD", "Benchmark-For", "visual recognition"], ["Rutgers Amazon Picking Challenge RGB - D Dataset", "Benchmark-For", "visual recognition"], ["visual recognition", "SubTask-Of", "robotics"]], "rel_plus": [["COIL:Dataset", "Benchmark-For", "visual recognition:Task"], ["ALOI:Dataset", "Benchmark-For", "visual recognition:Task"], ["Washington RGB - D:Dataset", "Benchmark-For", "visual recognition:Task"], ["KIT:Dataset", "Benchmark-For", "visual recognition:Task"], ["SHORT - 1 0 0:Dataset", "Benchmark-For", "visual recognition:Task"], ["BigBIRD:Dataset", "Benchmark-For", "visual recognition:Task"], ["Rutgers Amazon Picking Challenge RGB - D Dataset:Dataset", "Benchmark-For", "visual recognition:Task"], ["visual recognition:Task", "SubTask-Of", "robotics:Task"]]}
{"doc_id": "23569888", "sentence": "For instance , ( Bakry et al 2 0 1 5 ) use the Washington RGB - D and the Pascal 3D+ ( Xiang et al 2 0 1 4 ) datasets in order to evaluate the invariance of deep learning methods , and their analysis consequently focuses mainly to 3D viewpoint changes .", "ner": [["Washington RGB - D", "Dataset"], ["Pascal 3D+", "Dataset"], ["deep learning methods", "Method"]], "rel": [["deep learning methods", "Evaluated-With", "Washington RGB - D"], ["deep learning methods", "Evaluated-With", "Pascal 3D+"]], "rel_plus": [["deep learning methods:Method", "Evaluated-With", "Washington RGB - D:Dataset"], ["deep learning methods:Method", "Evaluated-With", "Pascal 3D+:Dataset"]]}
{"doc_id": "23569888", "sentence": "Recently , ( Borji et al 2 0 1 6 ) , motivated by similar considerations , presented the iLab - 2 0 M dataset : specifically , the authors aim to create a large - scale visual recognition benchmark which , beyond representing a high number of object instances , provides also a sufficient number of images per object , in order to study invariance properties of deep learning methods .", "ner": [["iLab - 2 0 M", "Dataset"], ["visual recognition", "Task"], ["deep learning", "Method"]], "rel": [["iLab - 2 0 M", "Benchmark-For", "visual recognition"]], "rel_plus": [["iLab - 2 0 M:Dataset", "Benchmark-For", "visual recognition:Task"]]}
{"doc_id": "23569888", "sentence": "In this section we present a novel dataset for visual recognition , iCubWorld Transformations ( iCWT ) , which is used for the empirical analysis in this work . iCWT is the latest release of the iCubWorld 1 project , whose goal is to benchmark and improve visual recognition systems for robotics . iCubWorld datasets ( Ciliberto et al 2 0 1 3 ; Fanello et al 2 0 1 3 b ; Pasquale et al 2 0 1 5 ) are designed to record a prototypical visual \" experience \" of a robot , the humanoid iCub , while it is performing vision - based tasks .", "ner": [["visual recognition", "Task"], ["iCubWorld Transformations", "Dataset"], ["iCWT", "Dataset"], ["iCWT", "Dataset"], ["iCubWorld", "Dataset"], ["visual recognition systems", "Method"], ["robotics", "Task"], ["iCubWorld", "Dataset"]], "rel": [["iCubWorld Transformations", "Benchmark-For", "visual recognition"], ["iCWT", "Synonym-Of", "iCubWorld Transformations"], ["iCWT", "SubClass-Of", "iCubWorld"], ["visual recognition systems", "Used-For", "robotics"]], "rel_plus": [["iCubWorld Transformations:Dataset", "Benchmark-For", "visual recognition:Task"], ["iCWT:Dataset", "Synonym-Of", "iCubWorld Transformations:Dataset"], ["iCWT:Dataset", "SubClass-Of", "iCubWorld:Dataset"], ["visual recognition systems:Method", "Used-For", "robotics:Task"]]}
{"doc_id": "23569888", "sentence": "Indeed , the resulting dataset offers a natural testbed for visual recognition in robotics , which is as close as possible to the real application .", "ner": [["visual recognition", "Task"], ["robotics", "Task"]], "rel": [["visual recognition", "SubTask-Of", "robotics"]], "rel_plus": [["visual recognition:Task", "SubTask-Of", "robotics:Task"]]}
{"doc_id": "23569888", "sentence": "Data acquisition for iCWT followed a protocol similar to the one in ( Fanello et al 2 0 1 3 a ) , which was adopted for previous iCubWorld releases ( Ciliberto et al 2 0 1 3 ; Fanello et al 2 0 1 3 b ; Pasquale et al 2 0 1 5 ) : a human \" teacher \" shows an object to the robot and pronounces the associated label to annotate subsequent images .", "ner": [["iCWT", "Dataset"], ["iCubWorld", "Dataset"]], "rel": [["iCWT", "SubClass-Of", "iCubWorld"]], "rel_plus": [["iCWT:Dataset", "SubClass-Of", "iCubWorld:Dataset"]]}
{"doc_id": "23569888", "sentence": "We developed an application to scale this acquisition procedure to hundreds of objects , which were collected during multiple interactive sessions . iCWT is available on - line and we plan to make also this application publicly available in order for other laboratories to use the same protocol to collect their own ( or possibly contribute to ) iCubWorld .", "ner": [["iCWT", "Dataset"], ["iCubWorld", "Dataset"]], "rel": [["iCWT", "SubClass-Of", "iCubWorld"]], "rel_plus": [["iCWT:Dataset", "SubClass-Of", "iCubWorld:Dataset"]]}
{"doc_id": "23569888", "sentence": "Note that , while we used an initial subset of iCWT in ( Pasquale et al 2 0 1 6 a ) , in this paper we present the dataset for the first time in its entirety . iCWT is the largest iCubWorld release so far , comprising 2 0 0 objects evenly organized into 2 0 categories that can be typically found in a domestic environment .", "ner": [["iCWT", "Dataset"], ["iCWT", "Dataset"], ["iCubWorld", "Dataset"]], "rel": [["iCWT", "SubClass-Of", "iCubWorld"]], "rel_plus": [["iCWT:Dataset", "SubClass-Of", "iCubWorld:Dataset"]]}
{"doc_id": "23569888", "sentence": "Fig. 1 reports a sample image for each category in iCWT : 1 1 categories ( in red in the figure ) are also in the ImageNet Large - Scale Visual Recognition Challenge ( ILSVRC ) 2 0 1 2 ( Russakovsky et al 2 0 1 5 ) , i.e. we found semantically and visually similar classes among the 1 0 0 0 of the classification challenge .", "ner": [["iCWT", "Dataset"], ["ImageNet Large - Scale Visual Recognition Challenge", "Dataset"], ["ILSVRC", "Dataset"], ["classification", "Task"]], "rel": [["ILSVRC", "Synonym-Of", "ImageNet Large - Scale Visual Recognition Challenge"], ["ImageNet Large - Scale Visual Recognition Challenge", "Benchmark-For", "classification"]], "rel_plus": [["ILSVRC:Dataset", "Synonym-Of", "ImageNet Large - Scale Visual Recognition Challenge:Dataset"], ["ImageNet Large - Scale Visual Recognition Challenge:Dataset", "Benchmark-For", "classification:Task"]]}
{"doc_id": "23569888", "sentence": "The remaining 9 categories do not appear ILSVRC but belong ( or are similar ) to a synset in the larger ImageNet dataset ( Deng et al 2 0 0 9 ) .", "ner": [["ILSVRC", "Dataset"], ["ImageNet", "Dataset"]], "rel": [["ILSVRC", "Compare-With", "ImageNet"]], "rel_plus": [["ILSVRC:Dataset", "Compare-With", "ImageNet:Dataset"]]}
{"doc_id": "23569888", "sentence": "To our knowledge , iCWT is the first dataset to address invariancerelated questions in robotics and accounts a much wider range of visual transformations with respect to previous datasets .", "ner": [["iCWT", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "23569888", "sentence": "In particular we briefly introduce the principal concepts behind deep Convolutional Neural Networks ( CNNs ) , describe the architectures used in our analysis and the algorithms adopted to train and apply them .", "ner": [["Convolutional Neural Networks", "Method"], ["CNNs", "Method"]], "rel": [["CNNs", "Synonym-Of", "Convolutional Neural Networks"]], "rel_plus": [["CNNs:Method", "Synonym-Of", "Convolutional Neural Networks:Method"]]}
{"doc_id": "23569888", "sentence": "Deep Convolutional Neural Networks ( CNNs ) are hierarchical models organized as the concatenation of multiple processing layers .", "ner": [["Convolutional Neural Networks", "Method"], ["CNNs", "Method"]], "rel": [["CNNs", "Synonym-Of", "Convolutional Neural Networks"]], "rel_plus": [["CNNs:Method", "Synonym-Of", "Convolutional Neural Networks:Method"]]}
{"doc_id": "23569888", "sentence": "The prototypical structure of a CNN ( see Fig. 4 ) performs at each layer ( usually referred to as convolution layer ) the following set of operations : \u2022 Convolutions : local convolution with respect to a bank of ( learned ) linear filters . \u2022 Spatial Downsampling for instance using strided convolutions . \u2022 Element - wise Non Linearity such as sigmoid functions ( Bishop 2 0 0 6 ) or , more recently Rectifying Linear Units ( He et al 2 0 1 5 ) . \u2022 Spatial Pooling to aggregate local responses to the signal in a single component , for instance by taking the maximum value observed ( max - pooling ) .", "ner": [["CNN", "Method"], ["convolution layer", "Method"], ["Convolutions", "Method"], ["convolution", "Method"], ["Spatial Downsampling", "Method"], ["strided convolutions", "Method"], ["Element - wise Non Linearity", "Method"], ["sigmoid", "Method"], ["Rectifying Linear Units", "Method"], ["Spatial Pooling", "Method"], ["max - pooling", "Method"]], "rel": [["convolution layer", "Part-Of", "CNN"], ["Convolutions", "Part-Of", "CNN"], ["strided convolutions", "Part-Of", "Spatial Downsampling"], ["sigmoid", "SubClass-Of", "Element - wise Non Linearity"], ["Rectifying Linear Units", "SubClass-Of", "Element - wise Non Linearity"], ["max - pooling", "SubClass-Of", "Spatial Pooling"]], "rel_plus": [["convolution layer:Method", "Part-Of", "CNN:Method"], ["Convolutions:Method", "Part-Of", "CNN:Method"], ["strided convolutions:Method", "Part-Of", "Spatial Downsampling:Method"], ["sigmoid:Method", "SubClass-Of", "Element - wise Non Linearity:Method"], ["Rectifying Linear Units:Method", "SubClass-Of", "Element - wise Non Linearity:Method"], ["max - pooling:Method", "SubClass-Of", "Spatial Pooling:Method"]]}
{"doc_id": "23569888", "sentence": "The filters , locally processing the input signal at each layer , are typically learned by minimizing a desired loss function , such as the overall classification accuracy in supervised settings or the reconstruction error in unsupervised ones .", "ner": [["classification", "Task"], ["reconstruction", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "23569888", "sentence": "Until recently , the most common strategy in image classification settings was to follow convolution layers ( C ) with a set of fully connected layers ( FC ) , namely a standard multi - layer Neural Network .", "ner": [["image classification", "Task"], ["convolution layers", "Method"], ["C", "Method"], ["fully connected layers", "Method"], ["FC", "Method"], ["multi - layer Neural Network", "Method"]], "rel": [["convolution layers", "Used-For", "image classification"], ["fully connected layers", "Used-For", "image classification"], ["multi - layer Neural Network", "Used-For", "image classification"], ["C", "Synonym-Of", "convolution layers"], ["FC", "Synonym-Of", "fully connected layers"]], "rel_plus": [["convolution layers:Method", "Used-For", "image classification:Task"], ["fully connected layers:Method", "Used-For", "image classification:Task"], ["multi - layer Neural Network:Method", "Used-For", "image classification:Task"], ["C:Method", "Synonym-Of", "convolution layers:Method"], ["FC:Method", "Synonym-Of", "fully connected layers:Method"]]}
{"doc_id": "23569888", "sentence": "In classification settings , the final layer of a network is typically a softmax function that maps the CNN output into individual class likelihood scores .", "ner": [["classification", "Task"], ["softmax", "Method"], ["CNN", "Method"]], "rel": [["softmax", "Used-For", "classification"], ["CNN", "Used-For", "classification"], ["softmax", "Part-Of", "CNN"]], "rel_plus": [["softmax:Method", "Used-For", "classification:Task"], ["CNN:Method", "Used-For", "classification:Task"], ["softmax:Method", "Part-Of", "CNN:Method"]]}
{"doc_id": "23569888", "sentence": "To further mitigate the risk of overfitting , regularization techniques such as weights L 2 regularization , dropout ( Hinton et al 2 0 1 2 ; Srivastava et al 2 0 1 4 ) or , more recently , batch normalization ( Ioffe and Szegedy 2 0 1 5 ) , have proved helpful .", "ner": [["regularization techniques", "Method"], ["weights L 2 regularization", "Method"], ["dropout", "Method"], ["batch normalization", "Method"]], "rel": [["weights L 2 regularization", "SubClass-Of", "regularization techniques"], ["dropout", "SubClass-Of", "regularization techniques"], ["batch normalization", "SubClass-Of", "regularization techniques"]], "rel_plus": [["weights L 2 regularization:Method", "SubClass-Of", "regularization techniques:Method"], ["dropout:Method", "SubClass-Of", "regularization techniques:Method"], ["batch normalization:Method", "SubClass-Of", "regularization techniques:Method"]]}
{"doc_id": "23569888", "sentence": "In this work we investigate the performance of modern CNNs on the robotic setting of iCubWorld .", "ner": [["CNNs", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "23569888", "sentence": "For this analysis we selected recent architectures achieving the highest accuracy on the ImageNet Large - Scale Visual Recognition Challenge ( ILSVRC ) ( Russakovsky et al 2 0 1 5 ) between 2 0 1 2 and 2 0 1 5 .", "ner": [["ImageNet Large - Scale Visual Recognition Challenge", "Dataset"], ["ILSVRC", "Dataset"]], "rel": [["ILSVRC", "Synonym-Of", "ImageNet Large - Scale Visual Recognition Challenge"]], "rel_plus": [["ILSVRC:Dataset", "Synonym-Of", "ImageNet Large - Scale Visual Recognition Challenge:Dataset"]]}
{"doc_id": "23569888", "sentence": "Below we summarize their structures : 4 A small variation of the AlexNet model , winner of ILSVRC 2 0 1 2 ( Krizhevsky et al 2 0 1 2 a ) .", "ner": [["AlexNet", "Method"], ["ILSVRC 2 0 1 2", "Dataset"]], "rel": [["AlexNet", "Evaluated-With", "ILSVRC 2 0 1 2"]], "rel_plus": [["AlexNet:Method", "Evaluated-With", "ILSVRC 2 0 1 2:Dataset"]]}
{"doc_id": "23569888", "sentence": "GoogLeNet 6 Winner of ILSVRC 2 0 1 4 .", "ner": [["GoogLeNet", "Method"], ["ILSVRC 2 0 1 4", "Dataset"]], "rel": [["GoogLeNet", "Evaluated-With", "ILSVRC 2 0 1 4"]], "rel_plus": [["GoogLeNet:Method", "Evaluated-With", "ILSVRC 2 0 1 4:Dataset"]]}
{"doc_id": "23569888", "sentence": "It diverges from previous architectures in that it concatenates so called inception modules and uses just one FC layer at the very end , reducing the parameters number to \u223c 4M for 2 2 layers . 7 The name is short for residual networks , which won the ILSVRC 2 0 1 5 ( He et al 2 0 1 6 ) , of which ResNet - 5 0 is a smaller version stacking 5 0 layers in \u223c 2 0 M parameters .", "ner": [["FC layer", "Method"], ["residual networks", "Method"], ["ILSVRC 2 0 1 5", "Dataset"], ["ResNet - 5 0", "Method"]], "rel": [["residual networks", "Used-For", "ILSVRC 2 0 1 5"]], "rel_plus": [["residual networks:Method", "Used-For", "ILSVRC 2 0 1 5:Dataset"]]}
{"doc_id": "23569888", "sentence": "Fig. 4 Example of a Convolutional Neural Network ( Sec. 3. 1 ) and of the two knowledge transfer approaches considered in this work ( Sec. 3. 2 ) . ( Blue pipeline ) feature extraction : in this case the response of one of the layers is used as a feature vector for a \" shallow \" predictor like RLSCs , SVMs ( see Sec. 3. 2 . 1 ) , which is trained on the new task . ( Red pipeline ) fine - tuning : in this case the network is trained end - to - end to the new task by replacing the final layer and using the original model as a \" warm - restart \" ( see Sec. 3. 2 . 2 ) .", "ner": [["Convolutional Neural Network", "Method"], ["feature extraction", "Method"], ["\" shallow \" predictor", "Method"], ["RLSCs", "Method"], ["SVMs", "Method"]], "rel": [["RLSCs", "SubClass-Of", "\" shallow \" predictor"], ["SVMs", "SubClass-Of", "\" shallow \" predictor"]], "rel_plus": [["RLSCs:Method", "SubClass-Of", "\" shallow \" predictor:Method"], ["SVMs:Method", "SubClass-Of", "\" shallow \" predictor:Method"]]}
{"doc_id": "23569888", "sentence": "This is typically done by training a standard classifier such as a Support Vector Machine ( SVM ) or a Regularized Least Squares Classifier ( RLSC ) ( Bishop 2 0 0 6 ) on top of feature vectors obtained from one or multiple layer responses .", "ner": [["Support Vector Machine", "Method"], ["SVM", "Method"], ["Regularized Least Squares Classifier", "Method"], ["RLSC", "Method"]], "rel": [["SVM", "Synonym-Of", "Support Vector Machine"], ["RLSC", "Synonym-Of", "Regularized Least Squares Classifier"]], "rel_plus": [["SVM:Method", "Synonym-Of", "Support Vector Machine:Method"], ["RLSC:Method", "Synonym-Of", "Regularized Least Squares Classifier:Method"]]}
{"doc_id": "23569888", "sentence": "In the empirical analysis of this work we used feature extraction as a strategy to transfer knowledge from CNNs trained on ImageNet to iCWT .", "ner": [["feature extraction", "Method"], ["CNNs", "Method"], ["ImageNet", "Dataset"], ["iCWT", "Dataset"]], "rel": [["feature extraction", "Part-Of", "CNNs"], ["CNNs", "Trained-With", "ImageNet"], ["CNNs", "Trained-With", "iCWT"]], "rel_plus": [["feature extraction:Method", "Part-Of", "CNNs:Method"], ["CNNs:Method", "Trained-With", "ImageNet:Dataset"], ["CNNs:Method", "Trained-With", "iCWT:Dataset"]]}
{"doc_id": "23569888", "sentence": "CaffeNet fc 6 or fc 7 GoogLeNet pool 5 / 7 x 7 s 1 VGG - 1 6 fc 6 or fc 7 ResNet - 5 0 pool 5 Table 3 Fine - tuning protocols for CaffeNet and GoogLeNet .", "ner": [["CaffeNet", "Method"], ["GoogLeNet", "Method"], ["VGG - 1 6", "Method"], ["ResNet - 5 0", "Method"], ["CaffeNet", "Method"], ["GoogLeNet", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "23569888", "sentence": "Fine - tuning has been recently used to adapt network models learned on ImageNet to robotic tasks ( see , e.g. , ( Eitel et al 2 0 1 5 ; Redmon and Angelova 2 0 1 5 a ; Pasquale et al 2 0 1 6 a ; Nguyen et al 2 0 1 6 ) ) .", "ner": [["ImageNet", "Dataset"], ["robotic", "Task"]], "rel": [["ImageNet", "Benchmark-For", "robotic"]], "rel_plus": [["ImageNet:Dataset", "Benchmark-For", "robotic:Task"]]}
{"doc_id": "23569888", "sentence": "In our experiments we performed fine - tuning only for CaffeNet and GoogLeNet , which are representative of most recent architectures and were significantly faster to fine - tune than VGG - 1 6 and ResNet - 5 0 .", "ner": [["CaffeNet", "Method"], ["GoogLeNet", "Method"], ["VGG - 1 6", "Method"], ["ResNet - 5 0", "Method"]], "rel": [["CaffeNet", "Compare-With", "VGG - 1 6"], ["GoogLeNet", "Compare-With", "VGG - 1 6"], ["GoogLeNet", "Compare-With", "ResNet - 5 0"], ["CaffeNet", "Compare-With", "ResNet - 5 0"]], "rel_plus": [["CaffeNet:Method", "Compare-With", "VGG - 1 6:Method"], ["GoogLeNet:Method", "Compare-With", "VGG - 1 6:Method"], ["GoogLeNet:Method", "Compare-With", "ResNet - 5 0:Method"], ["CaffeNet:Method", "Compare-With", "ResNet - 5 0:Method"]]}
{"doc_id": "23569888", "sentence": "In this section we present our empirical investigation of Deep Learning methods in the robotic setting of iCubWorld .", "ner": [["Deep Learning methods", "Method"], ["robotic", "Task"], ["iCubWorld", "Dataset"]], "rel": [["iCubWorld", "Benchmark-For", "robotic"], ["Deep Learning methods", "Used-For", "robotic"]], "rel_plus": [["iCubWorld:Dataset", "Benchmark-For", "robotic:Task"], ["Deep Learning methods:Method", "Used-For", "robotic:Task"]]}
{"doc_id": "23569888", "sentence": "Modern datasets for visual recognition comprise an extremely large number of images ( e.g. 1 Million for the ILSVRC challenge ) depicting objects in a wide range of natural scenes .", "ner": [["visual recognition", "Task"], ["ILSVRC", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "23569888", "sentence": "If this was the case , deep learning models trained on ImageNet would achieve high recognition performance on iCubWorld as well , without any re - training or adaptation .", "ner": [["deep learning models", "Method"], ["ImageNet", "Dataset"], ["iCubWorld", "Dataset"]], "rel": [["deep learning models", "Trained-With", "ImageNet"], ["deep learning models", "Evaluated-With", "iCubWorld"]], "rel_plus": [["deep learning models:Method", "Trained-With", "ImageNet:Dataset"], ["deep learning models:Method", "Evaluated-With", "iCubWorld:Dataset"]]}
{"doc_id": "23569888", "sentence": "Similarly , it was recently observed that conventional \" non - deep \" models trained on ImageNet perform poorly when applied to robotic settings ( Goehring et al 2 0 1 4 ) .", "ner": [["non - deep \" models", "Method"], ["ImageNet", "Dataset"]], "rel": [["non - deep \" models", "Trained-With", "ImageNet"]], "rel_plus": [["non - deep \" models:Method", "Trained-With", "ImageNet:Dataset"]]}
{"doc_id": "23569888", "sentence": "To address this question , we evaluated four off - theshelf CNNs ( the ones reviewed in Sec. 3. 1 ) for the task of image classification on iCWT .", "ner": [["CNNs", "Method"], ["image classification", "Task"], ["iCWT", "Dataset"]], "rel": [["iCWT", "Benchmark-For", "image classification"], ["CNNs", "Used-For", "image classification"]], "rel_plus": [["iCWT:Dataset", "Benchmark-For", "image classification:Task"], ["CNNs:Method", "Used-For", "image classification:Task"]]}
{"doc_id": "23569888", "sentence": "For these experiments we restricted the test set to the 1 1 categories of iCWT that appear also in the ILSVRC challenge ( see Sec. 2 and Fig. 1 ) .", "ner": [["iCWT", "Dataset"], ["ILSVRC", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "23569888", "sentence": "Fig . 5 reports the average classification accuracy on iCWT ( Dark Blue ) and ImageNet ( Gray ) .", "ner": [["classification", "Task"], ["iCWT", "Dataset"], ["ImageNet", "Dataset"]], "rel": [["iCWT", "Benchmark-For", "classification"], ["ImageNet", "Benchmark-For", "classification"]], "rel_plus": [["iCWT:Dataset", "Benchmark-For", "classification:Task"], ["ImageNet:Dataset", "Benchmark-For", "classification:Task"]]}
{"doc_id": "23569888", "sentence": "It can be immediately observed that there is a substantial drop in performance of \u223c 5 0 \u2212 6 0 % when testing on iCWT rather than ImageNet , suggesting that differences between the two datasets exist , and in particular that iCubWorld is not a sub - domain of ImageNet .", "ner": [["iCWT", "Dataset"], ["ImageNet", "Dataset"], ["iCubWorld", "Dataset"], ["ImageNet", "Dataset"]], "rel": [["iCWT", "Compare-With", "ImageNet"], ["iCubWorld", "Compare-With", "ImageNet"]], "rel_plus": [["iCWT:Dataset", "Compare-With", "ImageNet:Dataset"], ["iCubWorld:Dataset", "Compare-With", "ImageNet:Dataset"]]}
{"doc_id": "23569888", "sentence": "Indeed , as mentioned in Sec. 3 , a common practice when training a CNN is to perform data augmentation , namely to artificially increase the dataset size by applying synthetic transformations to the original images ( e.g. rotations , reflections , crops , illumination changes , etc . ) .", "ner": [["CNN", "Method"], ["data augmentation", "Method"], ["rotations", "Method"], ["reflections", "Method"], ["crops", "Method"], ["illumination changes", "Method"]], "rel": [["data augmentation", "Used-For", "CNN"], ["rotations", "SubClass-Of", "data augmentation"], ["reflections", "SubClass-Of", "data augmentation"], ["crops", "SubClass-Of", "data augmentation"], ["illumination changes", "SubClass-Of", "data augmentation"]], "rel_plus": [["data augmentation:Method", "Used-For", "CNN:Method"], ["rotations:Method", "SubClass-Of", "data augmentation:Method"], ["reflections:Method", "SubClass-Of", "data augmentation:Method"], ["crops:Method", "SubClass-Of", "data augmentation:Method"], ["illumination changes:Method", "SubClass-Of", "data augmentation:Method"]]}
{"doc_id": "23569888", "sentence": "Secondary observations : \u2022 Fine - tuning and RLSC achieve comparable accuracy ( both for CaffeNet and GoogLeNet ) . \u2022 We confirm the ILSVRC trends , with more recent networks outperforming older versions but with VGG - 1 6 features being better than those of GoogLeNet when using feature extraction with RLSC . \u2022 Note that CaffeNet performs worse when training data is scarce because of the high number of parameters to be learned in the 3 FC layers ( see Sec. 3 and the supplementary material ) . \u2022 To further support these findings , in the supplementary material we report results for the same experiment performed using less example instances per category .", "ner": [["RLSC", "Method"], ["CaffeNet", "Method"], ["GoogLeNet", "Method"], ["ILSVRC", "Dataset"], ["VGG - 1 6", "Method"], ["GoogLeNet", "Method"], ["feature extraction", "Method"], ["RLSC", "Method"], ["CaffeNet", "Method"], ["FC layers", "Method"]], "rel": [["RLSC", "Part-Of", "CaffeNet"], ["RLSC", "Part-Of", "GoogLeNet"], ["VGG - 1 6", "Evaluated-With", "ILSVRC"], ["GoogLeNet", "Evaluated-With", "ILSVRC"], ["feature extraction", "Part-Of", "VGG - 1 6"], ["VGG - 1 6", "Compare-With", "GoogLeNet"], ["feature extraction", "Part-Of", "GoogLeNet"], ["RLSC", "Part-Of", "GoogLeNet"], ["RLSC", "Part-Of", "feature extraction"]], "rel_plus": [["RLSC:Method", "Part-Of", "CaffeNet:Method"], ["RLSC:Method", "Part-Of", "GoogLeNet:Method"], ["VGG - 1 6:Method", "Evaluated-With", "ILSVRC:Dataset"], ["GoogLeNet:Method", "Evaluated-With", "ILSVRC:Dataset"], ["feature extraction:Method", "Part-Of", "VGG - 1 6:Method"], ["VGG - 1 6:Method", "Compare-With", "GoogLeNet:Method"], ["feature extraction:Method", "Part-Of", "GoogLeNet:Method"], ["RLSC:Method", "Part-Of", "GoogLeNet:Method"], ["RLSC:Method", "Part-Of", "feature extraction:Method"]]}
{"doc_id": "23569888", "sentence": "Secondary observations : \u2022 Fine - tuning the networks seems to lead to worse performance than RLSC on feature extraction both for CaffeNet and GoogLeNet . \u2022 Also in this setting we confirm ILSVRC results , with more recent networks outperforming previous ones but with VGG - 1 6 outperforming GoogLeNet when using feature extraction with RLSC . \u2022 To further support our findings , in the supplementary material we report results for a similar experiment but discriminating only between 1 0 or 5 categories .", "ner": [["RLSC", "Method"], ["feature extraction", "Method"], ["CaffeNet", "Method"], ["GoogLeNet", "Method"], ["ILSVRC", "Dataset"], ["VGG - 1 6", "Method"], ["GoogLeNet", "Method"], ["feature extraction", "Method"], ["RLSC", "Method"]], "rel": [["RLSC", "Part-Of", "feature extraction"], ["RLSC", "Part-Of", "CaffeNet"], ["RLSC", "Part-Of", "GoogLeNet"], ["VGG - 1 6", "Evaluated-With", "ILSVRC"], ["GoogLeNet", "Evaluated-With", "ILSVRC"], ["RLSC", "Part-Of", "VGG - 1 6"], ["VGG - 1 6", "Compare-With", "GoogLeNet"], ["feature extraction", "Part-Of", "GoogLeNet"], ["RLSC", "Part-Of", "feature extraction"]], "rel_plus": [["RLSC:Method", "Part-Of", "feature extraction:Method"], ["RLSC:Method", "Part-Of", "CaffeNet:Method"], ["RLSC:Method", "Part-Of", "GoogLeNet:Method"], ["VGG - 1 6:Method", "Evaluated-With", "ILSVRC:Dataset"], ["GoogLeNet:Method", "Evaluated-With", "ILSVRC:Dataset"], ["RLSC:Method", "Part-Of", "VGG - 1 6:Method"], ["VGG - 1 6:Method", "Compare-With", "GoogLeNet:Method"], ["feature extraction:Method", "Part-Of", "GoogLeNet:Method"], ["RLSC:Method", "Part-Of", "feature extraction:Method"]]}
{"doc_id": "23569888", "sentence": "While both robot vision and image retrieval address the same visual recognition problem , they are cast within two very different training regimes .", "ner": [["robot vision", "Task"], ["image retrieval", "Task"], ["visual recognition", "Task"]], "rel": [["robot vision", "Used-For", "visual recognition"], ["image retrieval", "Used-For", "visual recognition"]], "rel_plus": [["robot vision:Task", "Used-For", "visual recognition:Task"], ["image retrieval:Task", "Used-For", "visual recognition:Task"]]}
{"doc_id": "23569888", "sentence": "To this end , in the following we deepen our analysis on the performance of CNNs on iCWT , with particular focus on the concept of invariance .", "ner": [["CNNs", "Method"], ["iCWT", "Dataset"]], "rel": [["CNNs", "Evaluated-With", "iCWT"]], "rel_plus": [["CNNs:Method", "Evaluated-With", "iCWT:Dataset"]]}
{"doc_id": "23569888", "sentence": "While this can be expected for transformations involving 3D rotations of the object , it is quite surprising for affine ones , namely Scale , 2D Rotation and Background to which the convolutional structure of the CNN should be invariant \" by design \" , or learned during the training on ImageNet .", "ner": [["CNN", "Method"], ["ImageNet", "Dataset"]], "rel": [["CNN", "Trained-With", "ImageNet"]], "rel_plus": [["CNN:Method", "Trained-With", "ImageNet:Dataset"]]}
{"doc_id": "23569888", "sentence": "Note that until recently , this problem has been approached with methods based on keypoints extraction and template matching ( Lowe 2 0 0 4 ; Philbin et al 2 0 0 8 ; Collet et al 2 0 1 1 b ; Crowley and Zisserman 2 0 1 4 ; Collet et al 2 0 1 1 a Collet et al , 2 0 0 9 Muja et al 2 0 1 1 ) , however it has been recently observed that appraoches relying on more holistic visual representations perform typically better in low/mid - resolution settings such as iCubWorld ( Ciliberto et al 2 0 1 3 ) .", "ner": [["iCubWorld", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "23569888", "sentence": "In Sec. 4. 1 we have seen how knowledge acquired on ImageNet can be transferred to the iCubWorld domain to successfully tackle categorization tasks .", "ner": [["ImageNet", "Dataset"], ["iCubWorld", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "23569888", "sentence": "We addressed this question by considering an object identification task on iCWT , where we compared CNN models trained with an increasing number of examples .", "ner": [["object identification", "Task"], ["iCWT", "Dataset"], ["CNN", "Method"]], "rel": [["iCWT", "Benchmark-For", "object identification"], ["CNN", "Used-For", "object identification"], ["CNN", "Trained-With", "iCWT"]], "rel_plus": [["iCWT:Dataset", "Benchmark-For", "object identification:Task"], ["CNN:Method", "Used-For", "object identification:Task"], ["CNN:Method", "Trained-With", "iCWT:Dataset"]]}
{"doc_id": "23569888", "sentence": "To this end , here we investigate whether fine - tuning a CNN on the iCubWorld domain can indeed improve/adapt the invariance properties of the network to the identification task .", "ner": [["CNN", "Method"], ["iCubWorld", "Dataset"]], "rel": [["CNN", "Trained-With", "iCubWorld"]], "rel_plus": [["CNN:Method", "Trained-With", "iCubWorld:Dataset"]]}
{"doc_id": "23569888", "sentence": "We consider the same learning setting of the experiments reported in Fig. 1 1 , but we evaluate learning models that are first \" pre - fine - tuned \" on one of the following datasets to improve their invariance : \u2022 iCubWorld identification ( iCWT i d ) .", "ner": [["iCubWorld identification", "Dataset"], ["iCWT i d", "Dataset"]], "rel": [["iCubWorld identification", "Synonym-Of", "iCWT i d"]], "rel_plus": [["iCubWorld identification:Dataset", "Synonym-Of", "iCWT i d:Dataset"]]}
{"doc_id": "23569888", "sentence": "This dataset contains images of all instances of the 5 object categories of iCWT that were not used in the previous experiments , namely oven glove . squeezer , sprayer , body lotion and soda bottle .", "ner": [["iCWT", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "23569888", "sentence": "Training and validation sets were obtained following the same protocol of Sec. 5. 1 . \u2022 iCubWorld categorization ( iCWT cat ) .", "ner": [["iCubWorld categorization", "Dataset"], ["iCWT cat", "Dataset"]], "rel": [["iCWT cat", "Synonym-Of", "iCubWorld categorization"]], "rel_plus": [["iCWT cat:Dataset", "Synonym-Of", "iCubWorld categorization:Dataset"]]}
{"doc_id": "23569888", "sentence": "This dataset contains images of the 5 categories above but sampled from both iCWT and ImageNet .", "ner": [["iCWT", "Dataset"], ["ImageNet", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "23569888", "sentence": "Note that most iCWT categories do not appear in ILSVRC but are in synsets in the larger ImageNet dataset ( see the supplementary material for a list of the corresponding synsets ) .", "ner": [["iCWT", "Dataset"], ["ILSVRC", "Dataset"], ["ImageNet", "Dataset"]], "rel": [["ILSVRC", "Compare-With", "ImageNet"]], "rel_plus": [["ILSVRC:Dataset", "Compare-With", "ImageNet:Dataset"]]}
{"doc_id": "23569888", "sentence": "This dataset is conceived to test the possibility to have the CNN directly learn the relation between ImageNet examples on which it has been originally trained and iCWT images .", "ner": [["CNN", "Method"], ["ImageNet", "Dataset"], ["iCWT", "Dataset"]], "rel": [["CNN", "Trained-With", "ImageNet"], ["CNN", "Trained-With", "iCWT"]], "rel_plus": [["CNN:Method", "Trained-With", "ImageNet:Dataset"], ["CNN:Method", "Trained-With", "iCWT:Dataset"]]}
{"doc_id": "23569888", "sentence": "ImageNet synsets corresponding to the 5 object categories on which the CNNs will be later trained and tested for invariance , namely book , flower , glass , hairbrush and hairclip .", "ner": [["ImageNet", "Dataset"], ["CNNs", "Method"]], "rel": [["CNNs", "Trained-With", "ImageNet"]], "rel_plus": [["CNNs:Method", "Trained-With", "ImageNet:Dataset"]]}
{"doc_id": "23569888", "sentence": "Moreover , the performance of ( iCWT i d ) are in general more stable across different viewpoint transformations , suggesting that the preliminary fine - tuning could have indeed allowed the CNNs to become partially invariant to transformations in iCubWorld .", "ner": [["iCWT i d", "Dataset"], ["CNNs", "Method"], ["iCubWorld", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "23569888", "sentence": "We hypothesize this to be due to two possible reasons : 1 ) networks trained on the ILSVRC are already highly optimized for categorization and there is no gain in adapting the visual representation or 2 ) , the negative effect of the limited semantic variability in iCubWorld with respect to ImageNet may \" overcome \" the potential benefits of increasing the invariance to viewpoint transformations .", "ner": [["ILSVRC", "Dataset"], ["iCubWorld", "Dataset"], ["ImageNet", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "23569888", "sentence": "In this work we studied the application of modern Deep Learning methods to visual recognition tasks in robotics .", "ner": [["Deep Learning methods", "Method"], ["visual recognition", "Task"], ["robotics", "Task"]], "rel": [["Deep Learning methods", "Used-For", "visual recognition"], ["Deep Learning methods", "Used-For", "robotics"], ["visual recognition", "SubTask-Of", "robotics"]], "rel_plus": [["Deep Learning methods:Method", "Used-For", "visual recognition:Task"], ["Deep Learning methods:Method", "Used-For", "robotics:Task"], ["visual recognition:Task", "SubTask-Of", "robotics:Task"]]}
{"doc_id": "23569888", "sentence": "We challenged Deep Learning methods on an object recognition task that was specifically designed to represent a prototypical visual recognition problem in a real robotics application .", "ner": [["Deep Learning methods", "Method"], ["object recognition", "Task"], ["visual recognition", "Task"], ["real robotics application", "Task"]], "rel": [["Deep Learning methods", "Used-For", "object recognition"], ["visual recognition", "SubTask-Of", "real robotics application"]], "rel_plus": [["Deep Learning methods:Method", "Used-For", "object recognition:Task"], ["visual recognition:Task", "SubTask-Of", "real robotics application:Task"]]}
{"doc_id": "23569888", "sentence": "Indeed , in real - world robotics applications , failures due to object mis - detection or mis - classification can potentially lead to dramatically more critical and harmful consequences than in standard image retrieval scenarios .", "ner": [["real - world robotics applications", "Task"], ["image retrieval", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "23569888", "sentence": "In this section we consider possible directions for future research , aimed at mitigating the impact of such differences when performing visual recognition in robotics .", "ner": [["visual recognition", "Task"], ["robotics", "Task"]], "rel": [["visual recognition", "SubTask-Of", "robotics"]], "rel_plus": [["visual recognition:Task", "SubTask-Of", "robotics:Task"]]}
{"doc_id": "23569888", "sentence": "Exploiting these correlations can be done in many ways ranging from trivial solutions ( like temporal averaging of the classification results ) to more complex ones that rely on scene reconstruction and object tracking ( Song et al 2 0 1 5 ) ) .", "ner": [["classification", "Task"], ["scene reconstruction", "Task"], ["object tracking", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "23569888", "sentence": "For our tests we have devised a prototypical vision task for a humanoid robot in which human - robot interaction is exploited to obtain realistic supervision and train an object recognition system .", "ner": [["human - robot interaction", "Task"], ["object recognition system", "Method"]], "rel": [["human - robot interaction", "Used-For", "object recognition system"]], "rel_plus": [["human - robot interaction:Task", "Used-For", "object recognition system:Method"]]}
{"doc_id": "23569888", "sentence": "Supplementary Material Giulia Pasquale , Carlo Ciliberto , Francesca Odone , Lorenzo Rosasco and Lorenzo Natale A iCubWorld Transformations Fig. 1 3 reports one image for each object instance in the iCWT dataset .", "ner": [["iCubWorld", "Dataset"], ["iCWT", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "23569888", "sentence": "Note that not all categories in iCWT are part of the ILSVRC , but they belong ( or at least are similar ) to synsets in the larger ImageNet dataset ( Deng et al 2 0 0 9 ) .", "ner": [["iCWT", "Dataset"], ["ILSVRC", "Dataset"], ["ImageNet", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "23569888", "sentence": "In Sec. 4. 1 we observed that direct application of off - the - shelf CNNs to iCubWorld leads to poor performance .", "ner": [["CNNs", "Method"], ["iCubWorld", "Dataset"]], "rel": [["CNNs", "Evaluated-With", "iCubWorld"]], "rel_plus": [["CNNs:Method", "Evaluated-With", "iCubWorld:Dataset"]]}
{"doc_id": "23569888", "sentence": "Indeed , each image in iCWT is annotated with the coordinates of the object 's centroid and with a bounding box provided by segmentation of the depth map ( see Sec. 2. 1 ) .", "ner": [["iCWT", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "23569888", "sentence": "We finally chose to apply CNNs by extracting a square region of 2 5 6 \u00d7 2 5 6 from iCWT images , and then considering only the central crop ( Light Green in Fig. 1 4 ) .", "ner": [["CNNs", "Method"], ["iCWT", "Dataset"]], "rel": [["CNNs", "Evaluated-With", "iCWT"]], "rel_plus": [["CNNs:Method", "Evaluated-With", "iCWT:Dataset"]]}
{"doc_id": "23569888", "sentence": "In Fig. 5 we reported the accuracy of the off - the - shelf CNNs applied on iCWT and ImageNet .", "ner": [["CNNs", "Method"], ["iCWT", "Dataset"], ["ImageNet", "Dataset"]], "rel": [["CNNs", "Evaluated-With", "iCWT"], ["CNNs", "Evaluated-With", "ImageNet"]], "rel_plus": [["CNNs:Method", "Evaluated-With", "iCWT:Dataset"], ["CNNs:Method", "Evaluated-With", "ImageNet:Dataset"]]}
{"doc_id": "23569888", "sentence": "However , since the task was reduced to a 1 1 - class categorization setting , we considered the prediction of the networks limited to the 1 1 corresponding labels rather than the full vector of 1 0 0 0 scores normally produced by a CNN trained on the ILSVRC .", "ner": [["CNN", "Method"], ["ILSVRC", "Dataset"]], "rel": [["CNN", "Trained-With", "ILSVRC"]], "rel_plus": [["CNN:Method", "Trained-With", "ILSVRC:Dataset"]]}
{"doc_id": "23569888", "sentence": "For completeness , in Fig. 1 5 we report the resulting accuracy of the same CNNs but taking as prediction the class achieving maximum score over all available 1 0 0 0 , even if not present in the considered test sets ( the one of iCWT and the reduced ImageNet ) .", "ner": [["CNNs", "Method"], ["iCWT", "Dataset"], ["ImageNet", "Dataset"]], "rel": [["CNNs", "Evaluated-With", "iCWT"], ["CNNs", "Evaluated-With", "ImageNet"]], "rel_plus": [["CNNs:Method", "Evaluated-With", "iCWT:Dataset"], ["CNNs:Method", "Evaluated-With", "ImageNet:Dataset"]]}
{"doc_id": "23569888", "sentence": "In this section we follow - up the preliminary analysis reported in Sec. 4. 1 discussing the potential biases in ImageNet , pre - venting off - the - shelf models to generalize well when tested on iCubWorld without applying knowledge transfer techniques .", "ner": [["ImageNet", "Dataset"], ["iCubWorld", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "23569888", "sentence": "To this end , we report a series of excerpts showing the frameby - frame predictions of some of the CNNs considered when tested on iCWT sequences .", "ner": [["CNNs", "Method"], ["iCWT", "Dataset"]], "rel": [["CNNs", "Evaluated-With", "iCWT"]], "rel_plus": [["CNNs:Method", "Evaluated-With", "iCWT:Dataset"]]}
{"doc_id": "23569888", "sentence": "This qualitative analysis allows to better understand what frames of iCubWorld are actually \" harder \" to recognize for the CNNs and compare them with prototypical examples in ImageNet .", "ner": [["iCubWorld", "Dataset"], ["CNNs", "Method"], ["ImageNet", "Dataset"]], "rel": [["CNNs", "Evaluated-With", "iCubWorld"], ["CNNs", "Evaluated-With", "ImageNet"]], "rel_plus": [["CNNs:Method", "Evaluated-With", "iCubWorld:Dataset"], ["CNNs:Method", "Evaluated-With", "ImageNet:Dataset"]]}
{"doc_id": "23569888", "sentence": "This qualitative observation confirms recent studies ( Herranz et al 2 0 1 6 ) that specifically highlight the problem of the scale bias in CNNs , preventing models trained on object - centric datasets as ImageNet to generalize to scene - centric settings , where objects mostly appear in a small part of the image ( like in the second part of our sequences ) .", "ner": [["CNNs", "Method"], ["ImageNet", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "23569888", "sentence": "Indeed , it can be noticed that on many sequences there are periodic time intervals when the CNN does ( or does not ) recognize the object , corresponding to configurations in which that category appears more or less frequently in the ImageNet dataset .", "ner": [["CNN", "Method"], ["ImageNet", "Dataset"]], "rel": [["CNN", "Evaluated-With", "ImageNet"]], "rel_plus": [["CNN:Method", "Evaluated-With", "ImageNet:Dataset"]]}
{"doc_id": "23569888", "sentence": "In order to evaluate the best output layer for CaffeNet and VGG - 1 6 , and also to assign a reasonable value to m , we considered one small and one large categorization tasks on iCWT , representative of the smallest and larger task that we expected to run in our analysis , and compared the RLCS accuracy on them when varying these factors .", "ner": [["CaffeNet", "Method"], ["VGG - 1 6", "Method"], ["iCWT", "Dataset"], ["RLCS", "Method"]], "rel": [["CaffeNet", "Evaluated-With", "iCWT"], ["VGG - 1 6", "Evaluated-With", "iCWT"], ["RLCS", "Evaluated-With", "iCWT"], ["CaffeNet", "Compare-With", "RLCS"], ["VGG - 1 6", "Compare-With", "RLCS"]], "rel_plus": [["CaffeNet:Method", "Evaluated-With", "iCWT:Dataset"], ["VGG - 1 6:Method", "Evaluated-With", "iCWT:Dataset"], ["RLCS:Method", "Evaluated-With", "iCWT:Dataset"], ["CaffeNet:Method", "Compare-With", "RLCS:Method"], ["VGG - 1 6:Method", "Compare-With", "RLCS:Method"]]}
{"doc_id": "23569888", "sentence": "We first observed that fc 6 features consistently performed better than fc 7 , hence we decided to use this layer when extracting representations from off - the - shelf CaffeNet or VGG - 1 6 on images of iCWT .", "ner": [["CaffeNet", "Method"], ["VGG - 1 6", "Method"], ["iCWT", "Dataset"]], "rel": [["CaffeNet", "Evaluated-With", "iCWT"], ["VGG - 1 6", "Evaluated-With", "iCWT"]], "rel_plus": [["CaffeNet:Method", "Evaluated-With", "iCWT:Dataset"], ["VGG - 1 6:Method", "Evaluated-With", "iCWT:Dataset"]]}
{"doc_id": "23569888", "sentence": "While model selection is per se an open problem when dealing with deep networks , in our setting this issue is even more complicated by the fact that we do not have a fixed reference task on which to optimize the training ( as e.g. , can be the ILSVRC ) , but we instead plan to span over wide range of tasks , comprising small or large training sets .", "ner": [["model selection", "Task"], ["ILSVRC", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "23569888", "sentence": "Dropout % : percentage of dropout in FC layers .", "ner": [["Dropout", "Method"], ["dropout", "Method"], ["FC layers", "Method"]], "rel": [["dropout", "Part-Of", "FC layers"]], "rel_plus": [["dropout:Method", "Part-Of", "FC layers:Method"]]}
{"doc_id": "23569888", "sentence": "Solver : we tried either SGD or Adam ( Kingma and Ba 2 0 1 5 ) solvers in Caffe .", "ner": [["Solver", "Method"], ["SGD", "Method"], ["Adam", "Method"]], "rel": [["SGD", "SubClass-Of", "Solver"], ["Adam", "SubClass-Of", "Solver"]], "rel_plus": [["SGD:Method", "SubClass-Of", "Solver:Method"], ["Adam:Method", "SubClass-Of", "Solver:Method"]]}
{"doc_id": "23569888", "sentence": "LR Decay Policy : when using SGD , we used polynomial decay with exponent 0. 5 or \u2212 3 ; when using Adam , we maintained the learning rate constant .", "ner": [["LR Decay Policy", "Method"], ["SGD", "Method"], ["Adam", "Method"]], "rel": [["LR Decay Policy", "Part-Of", "SGD"], ["LR Decay Policy", "Part-Of", "Adam"]], "rel_plus": [["LR Decay Policy:Method", "Part-Of", "SGD:Method"], ["LR Decay Policy:Method", "Part-Of", "Adam:Method"]]}
{"doc_id": "23569888", "sentence": "To this end , in Fig. 2 1 we report the accuracy obtained respectively when using the Adam solver ( Left ) or SGD ( Right ) .", "ner": [["Adam", "Method"], ["SGD", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "23569888", "sentence": "It can be observed that , while the SGD solver is more robust to different choices of base LR , Adam provides slightly better accuracies for mid - range values of base LR , both for the small and the large experiment .", "ner": [["SGD", "Method"], ["Adam", "Method"]], "rel": [["SGD", "Compare-With", "Adam"]], "rel_plus": [["SGD:Method", "Compare-With", "Adam:Method"]]}
{"doc_id": "23569888", "sentence": "Specifically , we considered exactly the same fine - tuned models as in Sec 5. 2 . 1 for the first three strategies ( namely , iCWT i d , iCWT cat , iCWT + ImNet ) and , for the last one ( ImNet ) , we fine - tuned over the 1 5 ImageNet synsets corresponding to the 1 5 categories that will be later involved in the categorization task .", "ner": [["iCWT i d", "Dataset"], ["iCWT cat", "Dataset"], ["iCWT + ImNet", "Dataset"], ["ImageNet", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "23569888", "sentence": "In the experiments reported below , we report this loss as the difference between the average classification accuracy observed when testing a model on the same day it was trained on and on the other available day in iCWT .", "ner": [["classification", "Task"], ["iCWT", "Dataset"]], "rel": [["iCWT", "Benchmark-For", "classification"]], "rel_plus": [["iCWT:Dataset", "Benchmark-For", "classification:Task"]]}
{"doc_id": "23569888", "sentence": "Interestingly however , when using less aggressive strategies such as the conservative fine - tuning or the feature extraction based classifier , modern networks such as GoogleNet and ResNet - 5 0 seem in general quite robust .", "ner": [["feature extraction based classifier", "Method"], ["GoogleNet", "Method"], ["ResNet - 5 0", "Method"]], "rel": [["ResNet - 5 0", "SubClass-Of", "feature extraction based classifier"], ["GoogleNet", "SubClass-Of", "feature extraction based classifier"]], "rel_plus": [["ResNet - 5 0:Method", "SubClass-Of", "feature extraction based classifier:Method"], ["GoogleNet:Method", "SubClass-Of", "feature extraction based classifier:Method"]]}
{"doc_id": "199543700", "sentence": "Image recognition is an important topic in computer vision and image processing , and has been mainly addressed by supervised deep learning methods , which need a large set of labeled images to achieve promising performance .", "ner": [["Image recognition", "Task"], ["computer vision", "Task"], ["image processing", "Task"], ["supervised deep learning", "Method"]], "rel": [["supervised deep learning", "Used-For", "Image recognition"], ["Image recognition", "SubTask-Of", "computer vision"], ["supervised deep learning", "Used-For", "computer vision"], ["supervised deep learning", "Used-For", "image processing"]], "rel_plus": [["supervised deep learning:Method", "Used-For", "Image recognition:Task"], ["Image recognition:Task", "SubTask-Of", "computer vision:Task"], ["supervised deep learning:Method", "Used-For", "computer vision:Task"], ["supervised deep learning:Method", "Used-For", "image processing:Task"]]}
{"doc_id": "199543700", "sentence": "To better exploit the power of unlabeled data for image recognition , in this paper , we propose a semi - supervised and generative approach , namely the semi - supervised self - growing generative adversarial network ( SGGAN ) .", "ner": [["image recognition", "Task"], ["semi - supervised self - growing generative adversarial network", "Method"], ["SGGAN", "Method"]], "rel": [["semi - supervised self - growing generative adversarial network", "Used-For", "image recognition"], ["SGGAN", "Synonym-Of", "semi - supervised self - growing generative adversarial network"]], "rel_plus": [["semi - supervised self - growing generative adversarial network:Method", "Used-For", "image recognition:Task"], ["SGGAN:Method", "Synonym-Of", "semi - supervised self - growing generative adversarial network:Method"]]}
{"doc_id": "199543700", "sentence": "To stabilize and speed up the training process of SGGAN , we employ the metric Maximum Mean Discrepancy as the feature matching objective function and achieve larger gain than the standard semi - supervised GANs ( SSGANs ) , narrowing the gap to the supervised methods .", "ner": [["SGGAN", "Method"], ["metric Maximum Mean Discrepancy", "Method"], ["semi - supervised GANs", "Method"], ["SSGANs", "Method"]], "rel": [["metric Maximum Mean Discrepancy", "Part-Of", "SGGAN"], ["SSGANs", "Synonym-Of", "semi - supervised GANs"]], "rel_plus": [["metric Maximum Mean Discrepancy:Method", "Part-Of", "SGGAN:Method"], ["SSGANs:Method", "Synonym-Of", "semi - supervised GANs:Method"]]}
{"doc_id": "199543700", "sentence": "Experiments on several benchmark datasets show the effectiveness of the proposed SGGAN on image recognition and facial attribute recognition tasks .", "ner": [["SGGAN", "Method"], ["image recognition", "Task"], ["facial attribute recognition", "Task"]], "rel": [["SGGAN", "Used-For", "image recognition"], ["SGGAN", "Used-For", "facial attribute recognition"]], "rel_plus": [["SGGAN:Method", "Used-For", "image recognition:Task"], ["SGGAN:Method", "Used-For", "facial attribute recognition:Task"]]}
{"doc_id": "199543700", "sentence": "On the other hand , with the successes of Deep Convolutional Generative Adversarial Networks ( DCGAN ) [ 4 7 ] on general pattern recognition tasks , Generative Adversarial Networks ( GANs ) have been widely applied into unsupervised learning problems [ 5 2 ] .", "ner": [["Deep Convolutional Generative Adversarial Networks", "Method"], ["DCGAN", "Method"], ["general pattern recognition", "Task"], ["Generative Adversarial Networks", "Method"], ["GANs", "Method"], ["unsupervised learning", "Task"]], "rel": [["DCGAN", "Synonym-Of", "Deep Convolutional Generative Adversarial Networks"], ["Deep Convolutional Generative Adversarial Networks", "Used-For", "general pattern recognition"], ["GANs", "Synonym-Of", "Generative Adversarial Networks"], ["Generative Adversarial Networks", "Used-For", "unsupervised learning"]], "rel_plus": [["DCGAN:Method", "Synonym-Of", "Deep Convolutional Generative Adversarial Networks:Method"], ["Deep Convolutional Generative Adversarial Networks:Method", "Used-For", "general pattern recognition:Task"], ["GANs:Method", "Synonym-Of", "Generative Adversarial Networks:Method"], ["Generative Adversarial Networks:Method", "Used-For", "unsupervised learning:Task"]]}
{"doc_id": "199543700", "sentence": "It is well known that the GANs can hardly be trained deeply enough when compared to the other concurrently networks such as ResNet [ 2 1 ] .", "ner": [["GANs", "Method"], ["ResNet", "Method"]], "rel": [["GANs", "Compare-With", "ResNet"]], "rel_plus": [["GANs:Method", "Compare-With", "ResNet:Method"]]}
{"doc_id": "199543700", "sentence": "This is because that the generator of the GANs are usually very shallow and can often drift to \" model collapse \" ( a parameter setting where it always emits the same point ) , restricting the GANs to grow up to achieve promising performance on large scale datasets such as ImageNet [ 1 2 ] .", "ner": [["generator", "Method"], ["GANs", "Method"], ["GANs", "Method"], ["ImageNet", "Dataset"]], "rel": [["generator", "Part-Of", "GANs"], ["GANs", "Evaluated-With", "ImageNet"]], "rel_plus": [["generator:Method", "Part-Of", "GANs:Method"], ["GANs:Method", "Evaluated-With", "ImageNet:Dataset"]]}
{"doc_id": "199543700", "sentence": "The proposed SGGAN is a united semi - supervised GAN containing three self - growing groups .", "ner": [["SGGAN", "Method"], ["united semi - supervised GAN", "Method"], ["self - growing groups", "Method"]], "rel": [["self - growing groups", "Part-Of", "SGGAN"], ["SGGAN", "SubClass-Of", "united semi - supervised GAN"]], "rel_plus": [["self - growing groups:Method", "Part-Of", "SGGAN:Method"], ["SGGAN:Method", "SubClass-Of", "united semi - supervised GAN:Method"]]}
{"doc_id": "199543700", "sentence": "In semi - supervised learning ( SSL ) framework , label inferring is a major challenging to its success .", "ner": [["semi - supervised learning", "Method"], ["SSL", "Method"]], "rel": [["SSL", "Synonym-Of", "semi - supervised learning"]], "rel_plus": [["SSL:Method", "Synonym-Of", "semi - supervised learning:Method"]]}
{"doc_id": "199543700", "sentence": "We solve the second problem by proposing a novel technique named convolutionblock - transformation ( CBT ) proposed by us .", "ner": [["convolutionblock - transformation", "Method"], ["CBT", "Method"]], "rel": [["CBT", "Synonym-Of", "convolutionblock - transformation"]], "rel_plus": [["CBT:Method", "Synonym-Of", "convolutionblock - transformation:Method"]]}
{"doc_id": "199543700", "sentence": "It is difficult to directly train a deep network in our case , so we propose a simple yet effective convolution block transformation ( CBT ) technique to transfer weights from a shallower network to a deep one by shortcut and an adaptive scaling layer following the shallower convolution block .", "ner": [["convolution block transformation", "Method"], ["CBT", "Method"]], "rel": [["CBT", "Synonym-Of", "convolution block transformation"]], "rel_plus": [["CBT:Method", "Synonym-Of", "convolution block transformation:Method"]]}
{"doc_id": "199543700", "sentence": "We evaluate our method on CIFAR 1 0 , SVHN and face attribute recognition dataset , which is more challenging due to complex face variations .", "ner": [["CIFAR 1 0", "Dataset"], ["SVHN", "Dataset"], ["face attribute recognition dataset", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "199543700", "sentence": "In summary , the major contributions of this paper are summarized as follows : \u2022 We propose an semi - supervised self - growing generative adversarial network ( SGGAN ) for image recognition problem .", "ner": [["semi - supervised self - growing generative adversarial network", "Method"], ["SGGAN", "Method"], ["image recognition", "Task"]], "rel": [["SGGAN", "Synonym-Of", "semi - supervised self - growing generative adversarial network"], ["semi - supervised self - growing generative adversarial network", "Used-For", "image recognition"]], "rel_plus": [["SGGAN:Method", "Synonym-Of", "semi - supervised self - growing generative adversarial network:Method"], ["semi - supervised self - growing generative adversarial network:Method", "Used-For", "image recognition:Task"]]}
{"doc_id": "199543700", "sentence": "The employed MMD can help to stabilize the training of the proposed SGGAN model , and thus avoid the model collapse pitfall of traditional GANs . \u2022 We propose a novel convolution block transformation ( CBT ) technique to harmonize the self - growing process of the proposed SGGAN model to address the generalization of its classifier .", "ner": [["MMD", "Method"], ["SGGAN", "Method"], ["GANs", "Method"], ["convolution block transformation", "Method"], ["CBT", "Method"], ["self - growing", "Method"], ["SGGAN", "Method"]], "rel": [["MMD", "Part-Of", "SGGAN"], ["CBT", "Synonym-Of", "convolution block transformation"], ["self - growing", "Used-For", "SGGAN"]], "rel_plus": [["MMD:Method", "Part-Of", "SGGAN:Method"], ["CBT:Method", "Synonym-Of", "convolution block transformation:Method"], ["self - growing:Method", "Used-For", "SGGAN:Method"]]}
{"doc_id": "199543700", "sentence": "We prove it is easier to train a model growing from a shallow network to a deep one , and thus achieving better performance . \u2022 We conduct extensive experiments on image and face attribute recognition problems to systematically evaluate our proposed SGGAN model .", "ner": [["image and face attribute recognition", "Task"], ["SGGAN", "Method"]], "rel": [["SGGAN", "Used-For", "image and face attribute recognition"]], "rel_plus": [["SGGAN:Method", "Used-For", "image and face attribute recognition:Task"]]}
{"doc_id": "199543700", "sentence": "We demonstrate that MMD and CBT can separately and simultaneously stabilize the training of the proposed SGGAN .", "ner": [["MMD", "Method"], ["CBT", "Method"], ["SGGAN", "Method"]], "rel": [["MMD", "Part-Of", "SGGAN"], ["CBT", "Part-Of", "SGGAN"]], "rel_plus": [["MMD:Method", "Part-Of", "SGGAN:Method"], ["CBT:Method", "Part-Of", "SGGAN:Method"]]}
{"doc_id": "199543700", "sentence": "When compared with supervised methods , SGGAN can achieves competitive or even higher accuracies on various benchmark datasets when compared with state - of - the - art GAN based approaches such as the Improved GAN [ 5 2 ] and supervised learning networks such as VGG - 1 6 [ 5 3 ] and ResNet - 5 0 [ 2 1 ] .", "ner": [["SGGAN", "Method"], ["GAN", "Method"], ["Improved GAN", "Method"], ["VGG - 1 6", "Method"], ["ResNet - 5 0", "Method"]], "rel": [["SGGAN", "Compare-With", "GAN"], ["Improved GAN", "SubClass-Of", "GAN"], ["SGGAN", "Compare-With", "Improved GAN"], ["SGGAN", "Compare-With", "VGG - 1 6"], ["SGGAN", "Compare-With", "ResNet - 5 0"]], "rel_plus": [["SGGAN:Method", "Compare-With", "GAN:Method"], ["Improved GAN:Method", "SubClass-Of", "GAN:Method"], ["SGGAN:Method", "Compare-With", "Improved GAN:Method"], ["SGGAN:Method", "Compare-With", "VGG - 1 6:Method"], ["SGGAN:Method", "Compare-With", "ResNet - 5 0:Method"]]}
{"doc_id": "199543700", "sentence": "The rest of this paper is organized as follows : In Section II , we briefly reviews related work on semi - supervised learning , generative adversarial networks , the optimization of GAN and face attribute recognition .", "ner": [["semi - supervised learning", "Method"], ["generative adversarial networks", "Method"], ["GAN", "Method"], ["face attribute recognition", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "199543700", "sentence": "Denote by the generative network in GAN by G and the discriminative network in GAN by D. The purpose of the G network is to generate virtually realistic images and the purpose of the D network is to distinguish between the virtually generated and realistic unlabeled images through the min - max optimization problem .", "ner": [["generative network", "Method"], ["GAN", "Method"], ["G", "Method"], ["discriminative network", "Method"], ["GAN", "Method"], ["D.", "Method"], ["G network", "Method"], ["D network", "Method"]], "rel": [["G", "Synonym-Of", "generative network"], ["generative network", "Part-Of", "GAN"], ["D.", "Synonym-Of", "discriminative network"], ["discriminative network", "Part-Of", "GAN"]], "rel_plus": [["G:Method", "Synonym-Of", "generative network:Method"], ["generative network:Method", "Part-Of", "GAN:Method"], ["D.:Method", "Synonym-Of", "discriminative network:Method"], ["discriminative network:Method", "Part-Of", "GAN:Method"]]}
{"doc_id": "199543700", "sentence": "In the work of DC - GAN [ 4 7 ] , there are several techniques proposed to stabilize the training of the GANs , i.e. , using leaky - ReLUs and batch normalization for the training of the discriminator network , and convolution with stride 2 instead of max - pooling layers for the training of the generator network .", "ner": [["DC - GAN", "Method"], ["GANs", "Method"], ["leaky - ReLUs", "Method"], ["batch normalization", "Method"], ["discriminator network", "Method"], ["convolution with stride 2", "Method"], ["max - pooling", "Method"], ["generator network", "Method"]], "rel": [["leaky - ReLUs", "Part-Of", "discriminator network"], ["batch normalization", "Part-Of", "discriminator network"], ["convolution with stride 2", "Part-Of", "generator network"], ["max - pooling", "Part-Of", "generator network"]], "rel_plus": [["leaky - ReLUs:Method", "Part-Of", "discriminator network:Method"], ["batch normalization:Method", "Part-Of", "discriminator network:Method"], ["convolution with stride 2:Method", "Part-Of", "generator network:Method"], ["max - pooling:Method", "Part-Of", "generator network:Method"]]}
{"doc_id": "199543700", "sentence": "SGGAN starts from the basic baby generator and discriminator , in which the junior and senior generator/discriminator are self - grown from the baby counterparts via our proposed CBT technique . is introduced with theoretically proved effectiveness as the objective of the generative model to stabilize the training process of GAN .", "ner": [["SGGAN", "Method"], ["generator", "Method"], ["discriminator", "Method"], ["generator/discriminator", "Method"], ["CBT", "Method"], ["generative model", "Method"], ["GAN", "Method"]], "rel": [["generator", "Part-Of", "SGGAN"], ["discriminator", "Part-Of", "SGGAN"], ["CBT", "Part-Of", "SGGAN"]], "rel_plus": [["generator:Method", "Part-Of", "SGGAN:Method"], ["discriminator:Method", "Part-Of", "SGGAN:Method"], ["CBT:Method", "Part-Of", "SGGAN:Method"]]}
{"doc_id": "199543700", "sentence": "The main advantage of Wasserstein distance based GAN frameworks is that this distance can guarantee great stability for training the generative model , which is not limited to the DCGAN approach .", "ner": [["Wasserstein distance", "Method"], ["GAN", "Method"], ["generative model", "Method"], ["DCGAN", "Method"]], "rel": [["Wasserstein distance", "Part-Of", "GAN"]], "rel_plus": [["Wasserstein distance:Method", "Part-Of", "GAN:Method"]]}
{"doc_id": "199543700", "sentence": "C. GAN based semi - supervised learning Donahue et al. [ 1 3 ] introduced an adversarial formulation with a third component , which they call the \" encoder \" .", "ner": [["GAN based semi - supervised learning", "Method"], ["adversarial formulation with a third component", "Method"], ["encoder", "Method"]], "rel": [["encoder", "Synonym-Of", "adversarial formulation with a third component"]], "rel_plus": [["encoder:Method", "Synonym-Of", "adversarial formulation with a third component:Method"]]}
{"doc_id": "199543700", "sentence": "Radford et al. [ 4 6 ] trained an mLSTM RNN on Amazon reviews to learn a language model and then used its internal cell state from the last time step as features for the subsequent supervised task of sentiment analysis of Amazon reviews .", "ner": [["mLSTM RNN", "Method"], ["Amazon reviews", "Dataset"], ["sentiment analysis", "Task"], ["Amazon reviews", "Dataset"]], "rel": [["mLSTM RNN", "Trained-With", "Amazon reviews"], ["Amazon reviews", "Benchmark-For", "sentiment analysis"], ["mLSTM RNN", "Used-For", "sentiment analysis"]], "rel_plus": [["mLSTM RNN:Method", "Trained-With", "Amazon reviews:Dataset"], ["Amazon reviews:Dataset", "Benchmark-For", "sentiment analysis:Task"], ["mLSTM RNN:Method", "Used-For", "sentiment analysis:Task"]]}
{"doc_id": "199543700", "sentence": "Recently , Salimans , et al. [ 5 2 ] proposed a way to utilize GANs for a classification task with k classes .", "ner": [["GANs", "Method"], ["classification", "Task"]], "rel": [["GANs", "Used-For", "classification"]], "rel_plus": [["GANs:Method", "Used-For", "classification:Task"]]}
{"doc_id": "199543700", "sentence": "Then the extracted features are fed into a standard classifier , such as SVM [ 5 5 ] and random forest [ 3 7 ] .", "ner": [["SVM", "Method"], ["random forest", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "199543700", "sentence": "For instance , the authors of FaceTracer [ 3 0 ] split the whole face region into multiple sub - regions , extracted multiple types of features for each region , and train a SVM classifier on the concatenated features .   Recently , deep learning ( especially CNN based ) methods [ 2 9 ] have achieved great success in face attribute recognition due to their ability to learn discriminative features from huge amount of labeled data .", "ner": [["SVM", "Method"], ["deep learning", "Method"], ["CNN", "Method"], ["face attribute recognition", "Task"]], "rel": [["CNN", "SubClass-Of", "deep learning"], ["CNN", "Used-For", "face attribute recognition"], ["deep learning", "Used-For", "face attribute recognition"]], "rel_plus": [["CNN:Method", "SubClass-Of", "deep learning:Method"], ["CNN:Method", "Used-For", "face attribute recognition:Task"], ["deep learning:Method", "Used-For", "face attribute recognition:Task"]]}
{"doc_id": "199543700", "sentence": "The authors in [ 4 0 ] applied two CNNs ( ANet and LNet ) to the face attribute recognition task , on which the LNet is trained to locate the entire face region and the ANet is trained to extract high - level face representation .", "ner": [["CNNs", "Method"], ["ANet", "Method"], ["LNet", "Method"], ["face attribute recognition", "Task"], ["LNet", "Method"], ["locate the entire face region", "Task"], ["ANet", "Method"], ["extract high - level face representation", "Task"]], "rel": [["ANet", "SubClass-Of", "CNNs"], ["LNet", "SubClass-Of", "CNNs"], ["CNNs", "Used-For", "face attribute recognition"], ["ANet", "Used-For", "face attribute recognition"], ["LNet", "Used-For", "face attribute recognition"], ["LNet", "Used-For", "locate the entire face region"], ["ANet", "Used-For", "extract high - level face representation"]], "rel_plus": [["ANet:Method", "SubClass-Of", "CNNs:Method"], ["LNet:Method", "SubClass-Of", "CNNs:Method"], ["CNNs:Method", "Used-For", "face attribute recognition:Task"], ["ANet:Method", "Used-For", "face attribute recognition:Task"], ["LNet:Method", "Used-For", "face attribute recognition:Task"], ["LNet:Method", "Used-For", "locate the entire face region:Task"], ["ANet:Method", "Used-For", "extract high - level face representation:Task"]]}
{"doc_id": "199543700", "sentence": "Our SGGAN network includes a group of GANs , in which the junior generator or discriminator grows from corresponding baby counterpart , and the senior generator or discriminator grows from corresponding junior counterpart .", "ner": [["SGGAN", "Method"], ["GANs", "Method"], ["generator", "Method"], ["discriminator", "Method"]], "rel": [["GANs", "Part-Of", "SGGAN"], ["generator", "Part-Of", "GANs"], ["discriminator", "Part-Of", "GANs"]], "rel_plus": [["GANs:Method", "Part-Of", "SGGAN:Method"], ["generator:Method", "Part-Of", "GANs:Method"], ["discriminator:Method", "Part-Of", "GANs:Method"]]}
{"doc_id": "199543700", "sentence": "The activation functions we employed for the generator and discriminator are ReLU and Leaky - ReLU , respectively .", "ner": [["generator", "Method"], ["discriminator", "Method"], ["ReLU", "Method"], ["Leaky - ReLU", "Method"]], "rel": [["ReLU", "Part-Of", "generator"], ["Leaky - ReLU", "Part-Of", "generator"], ["Leaky - ReLU", "Part-Of", "discriminator"], ["ReLU", "Part-Of", "discriminator"]], "rel_plus": [["ReLU:Method", "Part-Of", "generator:Method"], ["Leaky - ReLU:Method", "Part-Of", "generator:Method"], ["Leaky - ReLU:Method", "Part-Of", "discriminator:Method"], ["ReLU:Method", "Part-Of", "discriminator:Method"]]}
{"doc_id": "199543700", "sentence": "Specifically , if we train the generator on the SVHN dataset [ 4 5 ] , it will produce an image of size 3 2 \u00d7 3 2 \u00d7 3 . 2 ) Discriminator : The baby discriminator has 9 CNN layers with Batch Normalization [ 2 5 ] , followed by Leaky - ReLU activation function .", "ner": [["generator", "Method"], ["SVHN", "Dataset"], ["Discriminator", "Method"], ["discriminator", "Method"], ["CNN layers", "Method"], ["Batch Normalization", "Method"], ["Leaky - ReLU", "Method"]], "rel": [["generator", "Trained-With", "SVHN"], ["CNN layers", "Part-Of", "discriminator"], ["Leaky - ReLU", "Part-Of", "discriminator"], ["Batch Normalization", "Part-Of", "CNN layers"]], "rel_plus": [["generator:Method", "Trained-With", "SVHN:Dataset"], ["CNN layers:Method", "Part-Of", "discriminator:Method"], ["Leaky - ReLU:Method", "Part-Of", "discriminator:Method"], ["Batch Normalization:Method", "Part-Of", "CNN layers:Method"]]}
{"doc_id": "199543700", "sentence": "It is the same with the deep neural networks used for image recognition [ 5 7 ] , object detection [ 5 8 ] , and image segmentation [ 4 4 ] , etc .", "ner": [["deep neural networks", "Method"], ["image recognition", "Task"], ["object detection", "Task"], ["image segmentation", "Task"]], "rel": [["deep neural networks", "Used-For", "image recognition"], ["deep neural networks", "Used-For", "object detection"], ["deep neural networks", "Used-For", "image segmentation"]], "rel_plus": [["deep neural networks:Method", "Used-For", "image recognition:Task"], ["deep neural networks:Method", "Used-For", "object detection:Task"], ["deep neural networks:Method", "Used-For", "image segmentation:Task"]]}
{"doc_id": "199543700", "sentence": "The difference is that the Leaky - ReLU [ 6 0 ] is used in our discriminator instead of the regular ReLU [ 2 0 ] .", "ner": [["Leaky - ReLU", "Method"], ["discriminator", "Method"], ["ReLU", "Method"]], "rel": [["Leaky - ReLU", "Part-Of", "discriminator"]], "rel_plus": [["Leaky - ReLU:Method", "Part-Of", "discriminator:Method"]]}
{"doc_id": "199543700", "sentence": "The reason we employ Leaky - ReLU instead of the regular ReLU is that , the regular ReLU function will truncate the negative values to 0 , which will block the gradients to flow through the generative networks .", "ner": [["Leaky - ReLU", "Method"], ["ReLU", "Method"], ["ReLU", "Method"]], "rel": [["Leaky - ReLU", "Compare-With", "ReLU"]], "rel_plus": [["Leaky - ReLU:Method", "Compare-With", "ReLU:Method"]]}
{"doc_id": "199543700", "sentence": "Theoretically , Leaky - ReLU represents an attempt to solve the dying ReLU problem [ 4 2 ] .", "ner": [["Leaky - ReLU", "Method"], ["ReLU", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "199543700", "sentence": "In this section , we propose a convolution block transformation ( CBT ) technique to transform an existing network into a deeper one .", "ner": [["convolution block transformation", "Method"], ["CBT", "Method"]], "rel": [["CBT", "Synonym-Of", "convolution block transformation"]], "rel_plus": [["CBT:Method", "Synonym-Of", "convolution block transformation:Method"]]}
{"doc_id": "199543700", "sentence": "However , they only initialize the weights of one layer in each cycle , and this operation has difficulties with the batch normalization ( BN ) layer .", "ner": [["batch normalization", "Method"], ["BN", "Method"]], "rel": [["BN", "Synonym-Of", "batch normalization"]], "rel_plus": [["BN:Method", "Synonym-Of", "batch normalization:Method"]]}
{"doc_id": "199543700", "sentence": "This is because that the BN layer requires running forward inference on the training data to calculate the mean and variance of activation function , which are then used to set the output scale and bias of the BN layer to disentangle the normalization of the statistics of this layer .", "ner": [["BN", "Method"], ["BN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "199543700", "sentence": "Finally , the added results are fed into a global average pooling ( GAP ) layer ( for more details about GAP , please refer to the Section III - E ) .", "ner": [["global average pooling", "Method"], ["GAP", "Method"], ["GAP", "Method"]], "rel": [["GAP", "Synonym-Of", "global average pooling"]], "rel_plus": [["GAP:Method", "Synonym-Of", "global average pooling:Method"]]}
{"doc_id": "199543700", "sentence": "Here , we call the up - described operator as the convolution block transformation ( CBT ) .", "ner": [["convolution block transformation", "Method"], ["CBT", "Method"]], "rel": [["CBT", "Synonym-Of", "convolution block transformation"]], "rel_plus": [["CBT:Method", "Synonym-Of", "convolution block transformation:Method"]]}
{"doc_id": "199543700", "sentence": "Consequently , the generator updates its parameters by matching the expectation of the features on the next of the final layer of the discriminator network , which is the output of Global Average Pooling ( GAP ) layer in our case .", "ner": [["Global Average Pooling", "Method"], ["GAP", "Method"]], "rel": [["GAP", "Synonym-Of", "Global Average Pooling"]], "rel_plus": [["GAP:Method", "Synonym-Of", "Global Average Pooling:Method"]]}
{"doc_id": "199543700", "sentence": "The overall procedures of training the proposed SGGAN is summarized in Algorithm 1 . 1 ) Pre - Training : The purpose of pre - training is to train the initial baby GAN cell .", "ner": [["SGGAN", "Method"], ["GAN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "199543700", "sentence": "The improving performance of the Alexnet [ 2 9 ] to the VGG [ 5 3 ] , and finally to the ResNet [ 2 1 ] , all demonstrates the great successes in the ILSVRC [ 5 1 ] challenge on the Imagenet Dataset [ 1 2 ] .", "ner": [["Alexnet", "Method"], ["VGG", "Method"], ["ResNet", "Method"], ["ILSVRC", "Dataset"], ["Imagenet", "Dataset"]], "rel": [["ResNet", "Evaluated-With", "ILSVRC"], ["VGG", "Evaluated-With", "ILSVRC"], ["Alexnet", "Evaluated-With", "ILSVRC"], ["Alexnet", "Evaluated-With", "Imagenet"], ["VGG", "Evaluated-With", "Imagenet"], ["ResNet", "Evaluated-With", "Imagenet"]], "rel_plus": [["ResNet:Method", "Evaluated-With", "ILSVRC:Dataset"], ["VGG:Method", "Evaluated-With", "ILSVRC:Dataset"], ["Alexnet:Method", "Evaluated-With", "ILSVRC:Dataset"], ["Alexnet:Method", "Evaluated-With", "Imagenet:Dataset"], ["VGG:Method", "Evaluated-With", "Imagenet:Dataset"], ["ResNet:Method", "Evaluated-With", "Imagenet:Dataset"]]}
{"doc_id": "199543700", "sentence": "For example , VGG [ 5 3 ] uses 3 \u00d7 3 convolution to achieve deeper architecture and ResNet [ 2 1 ] treats convolution added with shortcut as a basic unit and repeats that unit until the depth limit of the network is reached .", "ner": [["VGG", "Method"], ["3 \u00d7 3 convolution", "Method"], ["ResNet", "Method"], ["convolution", "Method"], ["shortcut", "Method"]], "rel": [["3 \u00d7 3 convolution", "Part-Of", "VGG"], ["convolution", "Part-Of", "ResNet"], ["shortcut", "Part-Of", "convolution"]], "rel_plus": [["3 \u00d7 3 convolution:Method", "Part-Of", "VGG:Method"], ["convolution:Method", "Part-Of", "ResNet:Method"], ["shortcut:Method", "Part-Of", "convolution:Method"]]}
{"doc_id": "199543700", "sentence": "In this section , we first evaluate the proposed semisupervised self - growing GAN ( SGGAN ) approach and justify the effectiveness of each component in the SGGAN approach .", "ner": [["semisupervised self - growing GAN", "Method"], ["SGGAN", "Method"], ["SGGAN", "Method"]], "rel": [["SGGAN", "Synonym-Of", "semisupervised self - growing GAN"]], "rel_plus": [["SGGAN:Method", "Synonym-Of", "semisupervised self - growing GAN:Method"]]}
{"doc_id": "199543700", "sentence": "Then we compare SGGAN with other state - of - the - art semisupervised GAN based approaches on image recognition problem on two widely employed datasets .", "ner": [["SGGAN", "Method"], ["semisupervised GAN based approaches", "Method"], ["image recognition", "Task"]], "rel": [["SGGAN", "Compare-With", "semisupervised GAN based approaches"], ["SGGAN", "Used-For", "image recognition"], ["semisupervised GAN based approaches", "Used-For", "image recognition"]], "rel_plus": [["SGGAN:Method", "Compare-With", "semisupervised GAN based approaches:Method"], ["SGGAN:Method", "Used-For", "image recognition:Task"], ["semisupervised GAN based approaches:Method", "Used-For", "image recognition:Task"]]}
{"doc_id": "199543700", "sentence": "To demonstrate the broad applicability of the proposed SGGAN approach , we also compare it with the leading supervised deep learning approaches on two commonly used datasets for face attribute recognition .", "ner": [["SGGAN", "Method"], ["supervised deep learning", "Method"], ["face attribute recognition", "Task"]], "rel": [["SGGAN", "Compare-With", "supervised deep learning"], ["SGGAN", "Used-For", "face attribute recognition"], ["supervised deep learning", "Used-For", "face attribute recognition"]], "rel_plus": [["SGGAN:Method", "Compare-With", "supervised deep learning:Method"], ["SGGAN:Method", "Used-For", "face attribute recognition:Task"], ["supervised deep learning:Method", "Used-For", "face attribute recognition:Task"]]}
{"doc_id": "199543700", "sentence": "In this section , we compare the proposed SGGAN approach with state - of - the - art semisupervised GAN based methods by using the widely used CIFAR - 1 0 dataset [ 2 8 ] and the Street View House Numbers ( SVHN ) dataset [ 4 5 ] .", "ner": [["SGGAN", "Method"], ["semisupervised GAN based methods", "Method"], ["CIFAR - 1 0", "Dataset"], ["Street View House Numbers", "Dataset"], ["SVHN", "Dataset"]], "rel": [["SGGAN", "Compare-With", "semisupervised GAN based methods"], ["SGGAN", "Evaluated-With", "CIFAR - 1 0"], ["semisupervised GAN based methods", "Evaluated-With", "CIFAR - 1 0"], ["SVHN", "Synonym-Of", "Street View House Numbers"], ["SGGAN", "Evaluated-With", "Street View House Numbers"], ["semisupervised GAN based methods", "Evaluated-With", "Street View House Numbers"]], "rel_plus": [["SGGAN:Method", "Compare-With", "semisupervised GAN based methods:Method"], ["SGGAN:Method", "Evaluated-With", "CIFAR - 1 0:Dataset"], ["semisupervised GAN based methods:Method", "Evaluated-With", "CIFAR - 1 0:Dataset"], ["SVHN:Dataset", "Synonym-Of", "Street View House Numbers:Dataset"], ["SGGAN:Method", "Evaluated-With", "Street View House Numbers:Dataset"], ["semisupervised GAN based methods:Method", "Evaluated-With", "Street View House Numbers:Dataset"]]}
{"doc_id": "199543700", "sentence": "The CIFAR 1 0 dataset [ 2 8 ] is introduced by A. Krizhevsky and G. Hinton in 2 0 0 9 , and has been a benchmark dataset for image classification problem ever since .", "ner": [["CIFAR 1 0", "Dataset"], ["image classification", "Task"]], "rel": [["CIFAR 1 0", "Benchmark-For", "image classification"]], "rel_plus": [["CIFAR 1 0:Dataset", "Benchmark-For", "image classification:Task"]]}
{"doc_id": "199543700", "sentence": "The SVHN dataset [ 4 5 ] is a real - world image dataset for digit recognition problem .", "ner": [["SVHN", "Dataset"], ["digit recognition", "Task"]], "rel": [["SVHN", "Benchmark-For", "digit recognition"]], "rel_plus": [["SVHN:Dataset", "Benchmark-For", "digit recognition:Task"]]}
{"doc_id": "199543700", "sentence": "We also compare the proposed SGGAN approach with the leading supervised deep learning methods on facial attribute recognition problem with the CelebFaces Attributes Dataset ( CelebA ) dataset [ 4 0 ] and the Labeled Faces in the Wild - a ( LFW - a ) dataset [ 2 4 ] .", "ner": [["SGGAN", "Method"], ["supervised deep learning", "Method"], ["facial attribute recognition", "Task"], ["CelebFaces Attributes Dataset", "Dataset"], ["CelebA", "Dataset"], ["Labeled Faces in the Wild - a", "Dataset"], ["LFW - a", "Dataset"]], "rel": [["SGGAN", "Compare-With", "supervised deep learning"], ["SGGAN", "Used-For", "facial attribute recognition"], ["CelebFaces Attributes Dataset", "Benchmark-For", "facial attribute recognition"], ["Labeled Faces in the Wild - a", "Benchmark-For", "facial attribute recognition"], ["supervised deep learning", "Used-For", "facial attribute recognition"], ["CelebA", "Synonym-Of", "CelebFaces Attributes Dataset"], ["SGGAN", "Evaluated-With", "CelebFaces Attributes Dataset"], ["supervised deep learning", "Evaluated-With", "CelebFaces Attributes Dataset"], ["LFW - a", "Synonym-Of", "Labeled Faces in the Wild - a"], ["SGGAN", "Evaluated-With", "Labeled Faces in the Wild - a"], ["supervised deep learning", "Evaluated-With", "Labeled Faces in the Wild - a"]], "rel_plus": [["SGGAN:Method", "Compare-With", "supervised deep learning:Method"], ["SGGAN:Method", "Used-For", "facial attribute recognition:Task"], ["CelebFaces Attributes Dataset:Dataset", "Benchmark-For", "facial attribute recognition:Task"], ["Labeled Faces in the Wild - a:Dataset", "Benchmark-For", "facial attribute recognition:Task"], ["supervised deep learning:Method", "Used-For", "facial attribute recognition:Task"], ["CelebA:Dataset", "Synonym-Of", "CelebFaces Attributes Dataset:Dataset"], ["SGGAN:Method", "Evaluated-With", "CelebFaces Attributes Dataset:Dataset"], ["supervised deep learning:Method", "Evaluated-With", "CelebFaces Attributes Dataset:Dataset"], ["LFW - a:Dataset", "Synonym-Of", "Labeled Faces in the Wild - a:Dataset"], ["SGGAN:Method", "Evaluated-With", "Labeled Faces in the Wild - a:Dataset"], ["supervised deep learning:Method", "Evaluated-With", "Labeled Faces in the Wild - a:Dataset"]]}
{"doc_id": "199543700", "sentence": "For the LFWA dataset [ 2 4 ] , we follow the experimental settings as described in [ 4 0 ] .   In this section , we justify the influence of different components in our proposed SGGAN approach on the performance of recognition errors .", "ner": [["LFWA", "Dataset"], ["SGGAN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "199543700", "sentence": "We compare the proposed SGGAN model with different routes of \" grow up \" on the CelebA dataset [ 4 0 ] .", "ner": [["SGGAN", "Method"], ["\" grow up \"", "Method"], ["CelebA", "Dataset"]], "rel": [["\" grow up \"", "Used-For", "SGGAN"], ["SGGAN", "Evaluated-With", "CelebA"]], "rel_plus": [["\" grow up \":Method", "Used-For", "SGGAN:Method"], ["SGGAN:Method", "Evaluated-With", "CelebA:Dataset"]]}
{"doc_id": "199543700", "sentence": "Besides , the SGGAN model \" grow up \" with two models can achieve better performance than the SG - GAN model with only one baby/junior/senior model .", "ner": [["SGGAN", "Method"], ["SG - GAN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "199543700", "sentence": "Similar findings can be found in the experiments on other attributes of the CelebA dataset [ 4 0 ] as well as on other datasets such as LFWA [ 2 4 ] .", "ner": [["CelebA", "Dataset"], ["LFWA", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "199543700", "sentence": "Table III : The accuracy ( % ) of the proposed SGGAN network with different self - growing routes by using the \" gender \" attribute in the CelebA dataset [ 4 0 ] .", "ner": [["SGGAN", "Method"], ["CelebA", "Dataset"]], "rel": [["SGGAN", "Evaluated-With", "CelebA"]], "rel_plus": [["SGGAN:Method", "Evaluated-With", "CelebA:Dataset"]]}
{"doc_id": "199543700", "sentence": "Since the model collapse is a fundamental problem in the training of GAN , we use MMD to stabilize the GAN .", "ner": [["GAN", "Method"], ["MMD", "Method"], ["GAN", "Method"]], "rel": [["MMD", "Part-Of", "GAN"]], "rel_plus": [["MMD:Method", "Part-Of", "GAN:Method"]]}
{"doc_id": "199543700", "sentence": "Convolution Block Transformation ( CBT ) .", "ner": [["Convolution Block Transformation", "Method"], ["CBT", "Method"]], "rel": [["CBT", "Synonym-Of", "Convolution Block Transformation"]], "rel_plus": [["CBT:Method", "Synonym-Of", "Convolution Block Transformation:Method"]]}
{"doc_id": "199543700", "sentence": "Figure 1 0 shows that the recognition accuracy ( % ) of the SGGAN model trained with the CBT technique are consistently higher than the model trained without CBT in different epochs .", "ner": [["SGGAN", "Method"], ["CBT", "Method"], ["CBT", "Method"]], "rel": [["CBT", "Part-Of", "SGGAN"]], "rel_plus": [["CBT:Method", "Part-Of", "SGGAN:Method"]]}
{"doc_id": "199543700", "sentence": "Comparisons with Fine - tuned VGG and ResNet Networks .", "ner": [["VGG", "Method"], ["ResNet", "Method"]], "rel": [["VGG", "Compare-With", "ResNet"]], "rel_plus": [["VGG:Method", "Compare-With", "ResNet:Method"]]}
{"doc_id": "199543700", "sentence": "In order to show the advantages of our algorithm on label effectiveness , we compare our SGGAN model with the stateof - the - art networks such as the VGG - 1 6 network [ 5 3 ] and the ResNet network [ 2 1 ] in the deep learning field .", "ner": [["SGGAN", "Method"], ["VGG - 1 6", "Method"], ["ResNet", "Method"], ["deep learning", "Method"]], "rel": [["ResNet", "Part-Of", "SGGAN"], ["SGGAN", "Compare-With", "VGG - 1 6"], ["ResNet", "SubClass-Of", "deep learning"], ["VGG - 1 6", "SubClass-Of", "deep learning"]], "rel_plus": [["ResNet:Method", "Part-Of", "SGGAN:Method"], ["SGGAN:Method", "Compare-With", "VGG - 1 6:Method"], ["ResNet:Method", "SubClass-Of", "deep learning:Method"], ["VGG - 1 6:Method", "SubClass-Of", "deep learning:Method"]]}
{"doc_id": "199543700", "sentence": "For the two networks , we load the model provided by corresponding authors pre - trained on the ImageNet dataset [ 1 2 ] ( which contains 1 0 0 0 classes with 1. 2 million images ) , and then carefully fine - tune these networks on the training set of the CelebA dataset [ 4 0 ] .", "ner": [["ImageNet", "Dataset"], ["CelebA", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "199543700", "sentence": "The proposed SGGAN model , the pre - trained VGG - 1 6 and Resnet - 5 0 networks are all fine - tuned with different numbers ( i.e. , 8 0 0 , 1 6 0 0 , 3 2 0 0 , 4 8 0 0 , 6 4 0 0 , 7 2 0 0 ) of labeled images in the CelebA dataset [ 4 0 ] with \" gender \" attribute in the comparison experiments .", "ner": [["SGGAN", "Method"], ["VGG - 1 6", "Method"], ["Resnet - 5 0", "Method"], ["CelebA", "Dataset"]], "rel": [["VGG - 1 6", "Trained-With", "CelebA"], ["Resnet - 5 0", "Trained-With", "CelebA"], ["SGGAN", "Trained-With", "CelebA"]], "rel_plus": [["VGG - 1 6:Method", "Trained-With", "CelebA:Dataset"], ["Resnet - 5 0:Method", "Trained-With", "CelebA:Dataset"], ["SGGAN:Method", "Trained-With", "CelebA:Dataset"]]}
{"doc_id": "199543700", "sentence": "We fine - tune the VGG - 1 6 and ResNet - 5 0 networks in a standard manner as described in corresponding paper .", "ner": [["VGG - 1 6", "Method"], ["ResNet - 5 0", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "199543700", "sentence": "We compare the proposed SGGAN approach with the fine - tuned VGG - 1 6 and ResNet - 5 0 networks with different numbers of labeled training images .", "ner": [["SGGAN", "Method"], ["VGG - 1 6", "Method"], ["ResNet - 5 0", "Method"]], "rel": [["SGGAN", "Compare-With", "VGG - 1 6"], ["SGGAN", "Compare-With", "ResNet - 5 0"]], "rel_plus": [["SGGAN:Method", "Compare-With", "VGG - 1 6:Method"], ["SGGAN:Method", "Compare-With", "ResNet - 5 0:Method"]]}
{"doc_id": "199543700", "sentence": "The results on accuracy ( % ) are listed in Table IV , from which one can see that when the numbers of labeled training images are 8 0 0 , 1 , 6 0 0 , 3 , 2 0 0 , and 4 , 8 0 0 , the proposed SGGAN approach can achieve higher recognition accuracies than the fine - tuned VGG - 1 6 and ResNet - 5 0 networks on the CelebA dataset with the \" gender \" attribute .", "ner": [["SGGAN", "Method"], ["VGG - 1 6", "Method"], ["ResNet - 5 0", "Method"], ["CelebA", "Dataset"]], "rel": [["SGGAN", "Compare-With", "VGG - 1 6"], ["SGGAN", "Compare-With", "ResNet - 5 0"], ["SGGAN", "Evaluated-With", "CelebA"], ["ResNet - 5 0", "Evaluated-With", "CelebA"], ["VGG - 1 6", "Evaluated-With", "CelebA"]], "rel_plus": [["SGGAN:Method", "Compare-With", "VGG - 1 6:Method"], ["SGGAN:Method", "Compare-With", "ResNet - 5 0:Method"], ["SGGAN:Method", "Evaluated-With", "CelebA:Dataset"], ["ResNet - 5 0:Method", "Evaluated-With", "CelebA:Dataset"], ["VGG - 1 6:Method", "Evaluated-With", "CelebA:Dataset"]]}
{"doc_id": "199543700", "sentence": "When the numbers of the training samples increase to 6 , 4 0 0 and 7 , 2 0 0 , the proposed SGGAN approach obtains slightly inferior ( but still comparable ) performance to the VGG - 1 6 and ResNet - 5 0 networks .", "ner": [["SGGAN", "Method"], ["VGG - 1 6", "Method"], ["ResNet - 5 0", "Method"]], "rel": [["SGGAN", "Compare-With", "VGG - 1 6"], ["SGGAN", "Compare-With", "ResNet - 5 0"]], "rel_plus": [["SGGAN:Method", "Compare-With", "VGG - 1 6:Method"], ["SGGAN:Method", "Compare-With", "ResNet - 5 0:Method"]]}
{"doc_id": "199543700", "sentence": "All these results demonstrate the competing ability of the proposed SGGAN approach as a whole system over the leading VGG and ResNet networks on image recognition tasks such as face attribute recognition .", "ner": [["SGGAN", "Method"], ["VGG", "Method"], ["ResNet", "Method"], ["image recognition", "Task"], ["face attribute recognition", "Task"]], "rel": [["SGGAN", "Compare-With", "VGG"], ["SGGAN", "Compare-With", "ResNet"], ["SGGAN", "Used-For", "image recognition"], ["VGG", "Used-For", "image recognition"], ["ResNet", "Used-For", "image recognition"], ["SGGAN", "Used-For", "face attribute recognition"], ["VGG", "Used-For", "face attribute recognition"], ["ResNet", "Used-For", "face attribute recognition"]], "rel_plus": [["SGGAN:Method", "Compare-With", "VGG:Method"], ["SGGAN:Method", "Compare-With", "ResNet:Method"], ["SGGAN:Method", "Used-For", "image recognition:Task"], ["VGG:Method", "Used-For", "image recognition:Task"], ["ResNet:Method", "Used-For", "image recognition:Task"], ["SGGAN:Method", "Used-For", "face attribute recognition:Task"], ["VGG:Method", "Used-For", "face attribute recognition:Task"], ["ResNet:Method", "Used-For", "face attribute recognition:Task"]]}
{"doc_id": "199543700", "sentence": "C. Comparison with state - of - the - art semi - supervised learning approaches on image recognition 1 ) Problem Description : Image recognition problem is the task of assigning one label to an input image from a fixed set of categories .", "ner": [["semi - supervised learning", "Method"], ["image recognition", "Task"], ["Image recognition", "Task"]], "rel": [["semi - supervised learning", "Used-For", "image recognition"]], "rel_plus": [["semi - supervised learning:Method", "Used-For", "image recognition:Task"]]}
{"doc_id": "199543700", "sentence": "Image recognition has a large variety of practical applications , and is related to many other computer vision tasks such as object detection and segmentation . 2 ) Comparison Methods : We compare the proposed SG - GAN approach with other competing semi - supervised learning approaches such as the Ladder Network [ 4 8 ] , which proposed to train the ladder network simultaneously minimize the sum of supervised and unsupervised cost functions by back - propagation , avoiding the need for layer - wise pretraining .", "ner": [["Image recognition", "Task"], ["computer vision", "Task"], ["object detection", "Task"], ["segmentation", "Task"], ["SG - GAN", "Method"], ["semi - supervised learning", "Method"], ["Ladder Network", "Method"], ["ladder network", "Method"], ["back - propagation", "Method"]], "rel": [["Image recognition", "SubTask-Of", "computer vision"], ["object detection", "SubTask-Of", "computer vision"], ["segmentation", "SubTask-Of", "computer vision"], ["Ladder Network", "SubClass-Of", "semi - supervised learning"], ["SG - GAN", "Compare-With", "Ladder Network"], ["ladder network", "Part-Of", "Ladder Network"]], "rel_plus": [["Image recognition:Task", "SubTask-Of", "computer vision:Task"], ["object detection:Task", "SubTask-Of", "computer vision:Task"], ["segmentation:Task", "SubTask-Of", "computer vision:Task"], ["Ladder Network:Method", "SubClass-Of", "semi - supervised learning:Method"], ["SG - GAN:Method", "Compare-With", "Ladder Network:Method"], ["ladder network:Method", "Part-Of", "Ladder Network:Method"]]}
{"doc_id": "199543700", "sentence": "And some leading GANs based approaches such as CatGAN [ 5 4 ] , which is based on an objective function that trades - off mutual information between unlabeled examples and their predicted categorical class distribution , against robustness of the classifier to an adversarial generative model .", "ner": [["GANs", "Method"], ["CatGAN", "Method"]], "rel": [["CatGAN", "SubClass-Of", "GANs"]], "rel_plus": [["CatGAN:Method", "SubClass-Of", "GANs:Method"]]}
{"doc_id": "199543700", "sentence": "And the Improved GAN [ 5 2 ] , which propose a technique called feature matching to address the instability of GANs by specifying a new objective for the generator to prevents it from overtraining on the current discriminator .", "ner": [["GAN", "Method"], ["GANs", "Method"], ["generator", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "199543700", "sentence": "We compare these competing methods on the CIFAR 1 0 dataset and the SVHN dataset [ 4 5 ] . 3 ) Results and Discussions : The experimental results are shown in Table V and Table VI .", "ner": [["CIFAR 1 0", "Dataset"], ["SVHN", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "199543700", "sentence": "As the CIFAR 1 0 dataset [ 2 8 ] , the SVHN dataset [ 4 5 ] is used for validating semi - supervised learning methods .", "ner": [["CIFAR 1 0", "Dataset"], ["SVHN", "Dataset"], ["semi - supervised learning", "Method"]], "rel": [["semi - supervised learning", "Evaluated-With", "CIFAR 1 0"], ["semi - supervised learning", "Evaluated-With", "SVHN"]], "rel_plus": [["semi - supervised learning:Method", "Evaluated-With", "CIFAR 1 0:Dataset"], ["semi - supervised learning:Method", "Evaluated-With", "SVHN:Dataset"]]}
{"doc_id": "199543700", "sentence": "The applications of it include suspect identification [ 2 7 ] , face verification [ 3 2 ] and face retrieval [ 3 1 ] .", "ner": [["suspect identification", "Task"], ["face verification", "Task"], ["face retrieval", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "199543700", "sentence": "Consequently , the large number of unlabeled face images available on the Internet have attracted increasing interests of researchers to tackle facial attribute recognition problem by semi - supervised learning ( SSL ) [ 6 ] methods . 2 ) Comparisons methods : The proposed method is compared with four competitive fully - supervised approaches including FaceTracer [ 3 0 ] , PANDA - w [ 7 4 ] , LNet+ANet(w/o ) and LNet+ANet [ 7 4 ] on the two datasets mentioned above .", "ner": [["facial attribute recognition", "Task"], ["semi - supervised learning", "Method"], ["SSL", "Method"], ["fully - supervised approaches", "Method"], ["FaceTracer", "Method"], ["PANDA - w", "Method"], ["LNet+ANet(w/o", "Method"], ["LNet+ANet", "Method"]], "rel": [["semi - supervised learning", "Used-For", "facial attribute recognition"], ["SSL", "Synonym-Of", "semi - supervised learning"], ["FaceTracer", "SubClass-Of", "fully - supervised approaches"], ["PANDA - w", "SubClass-Of", "fully - supervised approaches"], ["LNet+ANet(w/o", "SubClass-Of", "fully - supervised approaches"], ["LNet+ANet", "SubClass-Of", "fully - supervised approaches"]], "rel_plus": [["semi - supervised learning:Method", "Used-For", "facial attribute recognition:Task"], ["SSL:Method", "Synonym-Of", "semi - supervised learning:Method"], ["FaceTracer:Method", "SubClass-Of", "fully - supervised approaches:Method"], ["PANDA - w:Method", "SubClass-Of", "fully - supervised approaches:Method"], ["LNet+ANet(w/o:Method", "SubClass-Of", "fully - supervised approaches:Method"], ["LNet+ANet:Method", "SubClass-Of", "fully - supervised approaches:Method"]]}
{"doc_id": "199543700", "sentence": "The first one uses all the training/validation data in the LFWA dataset and the other uses the data of CelebA as the unlabeled data pool .", "ner": [["LFWA", "Dataset"], ["CelebA", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "199543700", "sentence": "Our algorithm runs ten times , and we report the average result . 3 ) Results and Discussions : The comparison results on CelebA and LFWA datasets are shown in Table VII , from which one can see that the proposed SGGAN approach achieve comparable performance on the recognition accuracy when compared with the state - of - the - art supervised learning based deep learning methods .", "ner": [["CelebA", "Dataset"], ["LFWA", "Dataset"], ["SGGAN", "Method"], ["supervised learning based deep learning", "Method"]], "rel": [["SGGAN", "Evaluated-With", "CelebA"], ["supervised learning based deep learning", "Evaluated-With", "CelebA"], ["SGGAN", "Evaluated-With", "LFWA"], ["supervised learning based deep learning", "Evaluated-With", "LFWA"], ["SGGAN", "Compare-With", "supervised learning based deep learning"]], "rel_plus": [["SGGAN:Method", "Evaluated-With", "CelebA:Dataset"], ["supervised learning based deep learning:Method", "Evaluated-With", "CelebA:Dataset"], ["SGGAN:Method", "Evaluated-With", "LFWA:Dataset"], ["supervised learning based deep learning:Method", "Evaluated-With", "LFWA:Dataset"], ["SGGAN:Method", "Compare-With", "supervised learning based deep learning:Method"]]}
{"doc_id": "199543700", "sentence": "For example , the proposed SGGAN trained with the MMD objective and the CBT technique ( i.e. , SGGAN - MMD - CBT ) achieves an accuracy of 8 6 . 2 2 % , which is only slightly inferior to the LNet+ANet methods , but still superior to all the other methods .", "ner": [["SGGAN", "Method"], ["MMD", "Method"], ["CBT", "Method"], ["SGGAN - MMD - CBT", "Method"], ["LNet+ANet", "Method"]], "rel": [["CBT", "Part-Of", "SGGAN"], ["MMD", "Part-Of", "SGGAN"], ["SGGAN - MMD - CBT", "Compare-With", "LNet+ANet"]], "rel_plus": [["CBT:Method", "Part-Of", "SGGAN:Method"], ["MMD:Method", "Part-Of", "SGGAN:Method"], ["SGGAN - MMD - CBT:Method", "Compare-With", "LNet+ANet:Method"]]}
{"doc_id": "199543700", "sentence": "Note that the proposed SGGAN - MMD - CBT approach achieves such promising performance with only 4% labeled training images .", "ner": [["SGGAN - MMD - CBT", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "199543700", "sentence": "The LFWA dataset is a standard benchmark for face attribute recognition .", "ner": [["LFWA", "Dataset"], ["face attribute recognition", "Task"]], "rel": [["LFWA", "Benchmark-For", "face attribute recognition"]], "rel_plus": [["LFWA:Dataset", "Benchmark-For", "face attribute recognition:Task"]]}
{"doc_id": "199543700", "sentence": "We use all training images in CelebA dataset as the unlabeled pool for our algorithm to train a SGGAN on LFWA dataset .", "ner": [["CelebA", "Dataset"], ["SGGAN", "Method"], ["LFWA", "Dataset"]], "rel": [["CelebA", "Used-For", "SGGAN"], ["SGGAN", "Trained-With", "LFWA"]], "rel_plus": [["CelebA:Dataset", "Used-For", "SGGAN:Method"], ["SGGAN:Method", "Trained-With", "LFWA:Dataset"]]}
{"doc_id": "199543700", "sentence": "Table VIII show the results , in the first row of the table , the \" LFWA ( Outer data ) \" shows Junior generator Senior generator Figure 1 1 : The generated samples of the baby , junior , and senior generators of the proposed SGGAN approach . the result of SGGAN using CelebA as the unlabeled pool and \" LFWA \" is the result of SGGAN only use the images in LFWA .", "ner": [["LFWA", "Dataset"], ["Junior generator", "Method"], ["Senior generator", "Method"], ["senior generators", "Method"], ["SGGAN", "Method"], ["SGGAN", "Method"], ["CelebA", "Dataset"], ["LFWA", "Dataset"], ["SGGAN", "Method"], ["LFWA", "Dataset"]], "rel": [["senior generators", "Part-Of", "SGGAN"], ["SGGAN", "Evaluated-With", "CelebA"], ["SGGAN", "Evaluated-With", "LFWA"]], "rel_plus": [["senior generators:Method", "Part-Of", "SGGAN:Method"], ["SGGAN:Method", "Evaluated-With", "CelebA:Dataset"], ["SGGAN:Method", "Evaluated-With", "LFWA:Dataset"]]}
{"doc_id": "199543700", "sentence": "Which also demonstrate the effectiveness of our algorithm for the semi - supervised image recognition tasks .   Feature matching is proved to help the GANs work much better if the goal is to obtain a strong classifier using the approach to semi - supervised learning [ 5 2 ] .", "ner": [["image recognition", "Task"], ["Feature matching", "Method"], ["GANs", "Method"], ["semi - supervised learning", "Method"]], "rel": [["Feature matching", "Part-Of", "GANs"]], "rel_plus": [["Feature matching:Method", "Part-Of", "GANs:Method"]]}
{"doc_id": "199543700", "sentence": "This demonstrate that the proposed SGGAN network with \" grow up \" strategy can indeed make better generation during the training process , and hence implicitly help improve the performance of the GAN model on the recognition tasks .", "ner": [["SGGAN", "Method"], ["\" grow up \" strategy", "Method"], ["GAN", "Method"], ["recognition", "Task"]], "rel": [["\" grow up \" strategy", "Used-For", "SGGAN"], ["GAN", "Used-For", "recognition"]], "rel_plus": [["\" grow up \" strategy:Method", "Used-For", "SGGAN:Method"], ["GAN:Method", "Used-For", "recognition:Task"]]}
{"doc_id": "199543700", "sentence": "In this paper , we propose a simple yet effective semisupervised self - growing generative adversarial network ( SG - GAN ) for image recognition .", "ner": [["semisupervised self - growing generative adversarial network", "Method"], ["SG - GAN", "Method"], ["image recognition", "Task"]], "rel": [["SG - GAN", "Synonym-Of", "semisupervised self - growing generative adversarial network"], ["semisupervised self - growing generative adversarial network", "Used-For", "image recognition"]], "rel_plus": [["SG - GAN:Method", "Synonym-Of", "semisupervised self - growing generative adversarial network:Method"], ["semisupervised self - growing generative adversarial network:Method", "Used-For", "image recognition:Task"]]}
{"doc_id": "199543700", "sentence": "We propose a convolution - blocktransformation ( CBT ) preservation technique to promote the network self - growing and obtain deeper network .", "ner": [["convolution - blocktransformation", "Method"], ["CBT", "Method"]], "rel": [["CBT", "Synonym-Of", "convolution - blocktransformation"]], "rel_plus": [["CBT:Method", "Synonym-Of", "convolution - blocktransformation:Method"]]}
{"doc_id": "199543700", "sentence": "The experiments on CIFAR 1 0 and SVHN dataset demonstrate effectiveness our methods .", "ner": [["CIFAR 1 0", "Dataset"], ["SVHN", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "199543700", "sentence": "Extensive experiments on the CelebA and LFWA demonstrate the generalization of our method .", "ner": [["CelebA", "Dataset"], ["LFWA", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "56657874", "sentence": "Domain adaption ( DA ) and domain generalization ( DG ) are two closely related methods which are both concerned with the task of assigning labels to an unlabeled data set .", "ner": [["Domain adaption", "Method"], ["DA", "Method"], ["domain generalization", "Method"], ["DG", "Method"]], "rel": [["DA", "Synonym-Of", "Domain adaption"], ["DG", "Synonym-Of", "domain generalization"]], "rel_plus": [["DA:Method", "Synonym-Of", "Domain adaption:Method"], ["DG:Method", "Synonym-Of", "domain generalization:Method"]]}
{"doc_id": "56657874", "sentence": "The only dissimilarity between these approaches is that DA can access the target data during the training phase , while the target data is totally unseen during the training phase in DG .", "ner": [["DA", "Method"], ["DG", "Method"]], "rel": [["DA", "Compare-With", "DG"]], "rel_plus": [["DA:Method", "Compare-With", "DG:Method"]]}
{"doc_id": "56657874", "sentence": "If DA methods are applied directly to DG by a simple exclusion of the target data from training , poor performance will result for a given task .", "ner": [["DA", "Method"], ["DG", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "56657874", "sentence": "In our first approach , we propose a novel deep domain generalization architecture utilizing synthetic data generated by a Generative Adversarial Network ( GAN ) .", "ner": [["deep domain generalization", "Method"], ["Generative Adversarial Network", "Method"], ["GAN", "Method"]], "rel": [["Generative Adversarial Network", "Used-For", "deep domain generalization"], ["GAN", "Synonym-Of", "Generative Adversarial Network"]], "rel_plus": [["Generative Adversarial Network:Method", "Used-For", "deep domain generalization:Method"], ["GAN:Method", "Synonym-Of", "Generative Adversarial Network:Method"]]}
{"doc_id": "56657874", "sentence": "In our second approach , we introduce a protocol for applying DA methods to a DG scenario by excluding the target data from the training phase , splitting the source data to training and validation parts , and treating the validation data as target data for DA .", "ner": [["DA", "Method"], ["DG", "Method"], ["DA", "Method"]], "rel": [["DA", "Used-For", "DG"]], "rel_plus": [["DA:Method", "Used-For", "DG:Method"]]}
{"doc_id": "56657874", "sentence": "The success of Deep Neural Networks ( DNNs ) or Deep Learning ( DL ) largely depends on the availability of large sets of labeled data .", "ner": [["Deep Neural Networks", "Method"], ["DNNs", "Method"], ["Deep Learning", "Method"], ["DL", "Method"]], "rel": [["DNNs", "Synonym-Of", "Deep Neural Networks"], ["DL", "Synonym-Of", "Deep Learning"]], "rel_plus": [["DNNs:Method", "Synonym-Of", "Deep Neural Networks:Method"], ["DL:Method", "Synonym-Of", "Deep Learning:Method"]]}
{"doc_id": "56657874", "sentence": "Domain adaption ( DA ) and domain generalization ( DG ) methods have been proposed to address the poor performance due to such domain shift .", "ner": [["Domain adaption", "Method"], ["DA", "Method"], ["domain generalization", "Method"], ["DG", "Method"]], "rel": [["DA", "Synonym-Of", "Domain adaption"], ["DG", "Synonym-Of", "domain generalization"]], "rel_plus": [["DA:Method", "Synonym-Of", "Domain adaption:Method"], ["DG:Method", "Synonym-Of", "domain generalization:Method"]]}
{"doc_id": "56657874", "sentence": "DA and DG are similar frameworks , but the key difference between DA and DG is the availability of the target data in training phase .", "ner": [["DA", "Method"], ["DG", "Method"], ["DA", "Method"], ["DG", "Method"]], "rel": [["DA", "Compare-With", "DG"]], "rel_plus": [["DA:Method", "Compare-With", "DG:Method"]]}
{"doc_id": "56657874", "sentence": "DA methods access the target data dur - ing training while DG approaches do not have access to the target data .", "ner": [["DA", "Method"], ["DG", "Method"]], "rel": [["DA", "Compare-With", "DG"]], "rel_plus": [["DA:Method", "Compare-With", "DG:Method"]]}
{"doc_id": "56657874", "sentence": "DA is an attractive area of research in computer vision and pattern recognition fields because it handles the task properly with limited target samples .", "ner": [["DA", "Method"], ["computer vision", "Task"], ["pattern recognition", "Task"]], "rel": [["DA", "Used-For", "computer vision"], ["DA", "Used-For", "pattern recognition"]], "rel_plus": [["DA:Method", "Used-For", "computer vision:Task"], ["DA:Method", "Used-For", "pattern recognition:Task"]]}
{"doc_id": "56657874", "sentence": "Domain adaptation can be classified into three groups : Unsupervised Domain Adaptation ( UDA ) , Semi - Supervised Domain Adaptation ( SSDA ) and Supervised Domain Adaptation ( SDA ) .", "ner": [["Domain adaptation", "Method"], ["Unsupervised Domain Adaptation", "Method"], ["UDA", "Method"], ["Semi - Supervised Domain Adaptation", "Method"], ["SSDA", "Method"], ["Supervised Domain Adaptation", "Method"], ["SDA", "Method"]], "rel": [["Unsupervised Domain Adaptation", "SubClass-Of", "Domain adaptation"], ["Semi - Supervised Domain Adaptation", "SubClass-Of", "Domain adaptation"], ["Supervised Domain Adaptation", "SubClass-Of", "Domain adaptation"], ["UDA", "Synonym-Of", "Unsupervised Domain Adaptation"], ["SSDA", "Synonym-Of", "Semi - Supervised Domain Adaptation"], ["SDA", "Synonym-Of", "Supervised Domain Adaptation"]], "rel_plus": [["Unsupervised Domain Adaptation:Method", "SubClass-Of", "Domain adaptation:Method"], ["Semi - Supervised Domain Adaptation:Method", "SubClass-Of", "Domain adaptation:Method"], ["Supervised Domain Adaptation:Method", "SubClass-Of", "Domain adaptation:Method"], ["UDA:Method", "Synonym-Of", "Unsupervised Domain Adaptation:Method"], ["SSDA:Method", "Synonym-Of", "Semi - Supervised Domain Adaptation:Method"], ["SDA:Method", "Synonym-Of", "Supervised Domain Adaptation:Method"]]}
{"doc_id": "56657874", "sentence": "UDA does not require any labeled target data whereas SDA needs all labeled target samples .", "ner": [["UDA", "Method"], ["SDA", "Method"]], "rel": [["UDA", "Compare-With", "SDA"]], "rel_plus": [["UDA:Method", "Compare-With", "SDA:Method"]]}
{"doc_id": "56657874", "sentence": "Unsupervised deep domain adaptation ( UDDA ) methods require large sets of target data in order to be more successful in producing a desired output .", "ner": [["Unsupervised deep domain adaptation", "Method"], ["UDDA", "Method"]], "rel": [["UDDA", "Synonym-Of", "Unsupervised deep domain adaptation"]], "rel_plus": [["UDDA:Method", "Synonym-Of", "Unsupervised deep domain adaptation:Method"]]}
{"doc_id": "56657874", "sentence": "For an example , we have the images from ImageNet [ 6 ] and Caltech - 2 5 6 [ 1 5 ] datasets and we want to classify images in the LabelMe dataset [ 3 5 ] .", "ner": [["ImageNet", "Dataset"], ["Caltech - 2 5 6", "Dataset"], ["LabelMe", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "56657874", "sentence": "Although DA and DG frameworks are very close and both have similar goals ( producing a strong classifier on target data ) , the existing DA techniques do not perform well when directly applied in DG .", "ner": [["DA", "Method"], ["DG", "Method"], ["DA", "Method"], ["DG", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "56657874", "sentence": "In this paper , we propose a novel deep domain generalization framework by utilizing synthetic data that are generated by a GAN during the training stage where the discrep - ancy between the real and synthetic data is minimized using the existing domain adaptation metrics such as maximum mean discrepancy or correlation alignment .", "ner": [["domain generalization framework", "Method"], ["GAN", "Method"], ["domain adaptation", "Method"]], "rel": [["GAN", "Part-Of", "domain generalization framework"]], "rel_plus": [["GAN:Method", "Part-Of", "domain generalization framework:Method"]]}
{"doc_id": "56657874", "sentence": "Our approach takes advantage of the natural image correspondence built by CycleGAN [ 4 5 ] and ComboGAN [ 1 ] .", "ner": [["CycleGAN", "Method"], ["ComboGAN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "56657874", "sentence": "The contributions of this paper are two - fold : \u2022 We implement a novel deep domain generalization framework utilizing synthetic data that is generated by a GAN .", "ner": [["deep domain generalization framework", "Method"], ["GAN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "56657874", "sentence": "The discrepancy between the real data and synthetic data is decreased using existing domain discrepancy metrics in DG settings . \u2022 We introduce a protocol for applying DA methods on DG scenarios where the source datasets are split into training and validation sets , and the validation data acts as target data for DG .", "ner": [["DG", "Method"], ["DA", "Method"], ["DG", "Method"], ["DG", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "56657874", "sentence": "We conduct extensive experiments to evaluate the image classification accuracy of our proposed method across a large set of alternatives in DG settings .", "ner": [["image classification", "Task"], ["DG", "Method"]], "rel": [["DG", "Used-For", "image classification"]], "rel_plus": [["DG:Method", "Used-For", "image classification:Task"]]}
{"doc_id": "56657874", "sentence": "This section reviews existing research on DA and DG , especially in the avenue of object classification .", "ner": [["DA", "Method"], ["DG", "Method"], ["object classification", "Task"]], "rel": [["DA", "Used-For", "object classification"], ["DG", "Used-For", "object classification"]], "rel_plus": [["DA:Method", "Used-For", "object classification:Task"], ["DG:Method", "Used-For", "object classification:Task"]]}
{"doc_id": "56657874", "sentence": "All the DA techniques can be divided into two main categories : Conventional Domain Adaptation methods [ 1 3 , 3 1 , 3 7 , 4 4 ] and Deep Domain Adaptation methods [ 2 3 ] [ 2 4 ] [ 2 5 ] [ 4 0 ] [ 4 1 ] [ 4 2 ] .", "ner": [["DA", "Method"], ["Conventional Domain Adaptation methods", "Method"], ["Domain Adaptation methods", "Method"]], "rel": [["Domain Adaptation methods", "SubClass-Of", "DA"], ["Conventional Domain Adaptation methods", "SubClass-Of", "DA"]], "rel_plus": [["Domain Adaptation methods:Method", "SubClass-Of", "DA:Method"], ["Conventional Domain Adaptation methods:Method", "SubClass-Of", "DA:Method"]]}
{"doc_id": "56657874", "sentence": "The image classification accuracy obtained with Deep Convolutional Activation Features ( DeCAF ) [ 8 ] even without using any adaptation algorithm is remarkably better than any conventional domain adaptation methods [ 1 3 , 3 1 , 3 7 , 4 4 ] due to the capacity of a DNN to extract more robust features using nonlinear function .", "ner": [["image classification", "Task"], ["Deep Convolutional Activation Features", "Method"], ["DeCAF", "Method"], ["domain adaptation methods", "Method"], ["DNN", "Method"]], "rel": [["Deep Convolutional Activation Features", "Used-For", "image classification"], ["domain adaptation methods", "Used-For", "image classification"], ["DeCAF", "Synonym-Of", "Deep Convolutional Activation Features"], ["Deep Convolutional Activation Features", "Compare-With", "domain adaptation methods"]], "rel_plus": [["Deep Convolutional Activation Features:Method", "Used-For", "image classification:Task"], ["domain adaptation methods:Method", "Used-For", "image classification:Task"], ["DeCAF:Method", "Synonym-Of", "Deep Convolutional Activation Features:Method"], ["Deep Convolutional Activation Features:Method", "Compare-With", "domain adaptation methods:Method"]]}
{"doc_id": "56657874", "sentence": "Eric Tzeng et al. [ 4 1 ] introduced the Deep Domain Confusion ( DDC ) domain adaptation method where the discrepancy is reduced by introducing a confusion layer .", "ner": [["Deep Domain Confusion", "Method"], ["DDC", "Method"], ["domain adaptation", "Method"]], "rel": [["DDC", "Synonym-Of", "Deep Domain Confusion"], ["Deep Domain Confusion", "SubClass-Of", "domain adaptation"]], "rel_plus": [["DDC:Method", "Synonym-Of", "Deep Domain Confusion:Method"], ["Deep Domain Confusion:Method", "SubClass-Of", "domain adaptation:Method"]]}
{"doc_id": "56657874", "sentence": "Long et al. proposed the Domain Adaptation Network ( DAN ) [ 2 3 ] that introduced the sum of MMDs defined between several layers which are used to mitigate the domain discrepancy problem .", "ner": [["Domain Adaptation Network", "Method"], ["DAN", "Method"]], "rel": [["DAN", "Synonym-Of", "Domain Adaptation Network"]], "rel_plus": [["DAN:Method", "Synonym-Of", "Domain Adaptation Network:Method"]]}
{"doc_id": "56657874", "sentence": "This idea was further boosted by the Joint Adaptation Networks [ 2 5 ] and Residual Transfer Networks [ 2 4 ] .", "ner": [["Joint Adaptation Networks", "Method"], ["Residual Transfer Networks", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "56657874", "sentence": "Hemanth et al. [ 4 2 ] proposed a new Deep Hasing Network for UDA where hash codes are used to address the DA problem .", "ner": [["Deep Hasing Network", "Method"], ["UDA", "Method"], ["DA", "Method"]], "rel": [["Deep Hasing Network", "Used-For", "UDA"]], "rel_plus": [["Deep Hasing Network:Method", "Used-For", "UDA:Method"]]}
{"doc_id": "56657874", "sentence": "The idea of [ 2 6 , 3 8 , 3 9 ] is similar to Deep Domain Confusion ( DDC ) [ 4 1 ] and Deep Adaptation Network ( DAN ) [ 2 3 ] except that instead of MMD , they adopted CORAL loss to minimize the discrepancy .", "ner": [["Deep Domain Confusion", "Method"], ["DDC", "Method"], ["Deep Adaptation Network", "Method"], ["DAN", "Method"], ["MMD", "Method"], ["CORAL loss", "Method"]], "rel": [["DDC", "Synonym-Of", "Deep Domain Confusion"], ["DAN", "Synonym-Of", "Deep Adaptation Network"]], "rel_plus": [["DDC:Method", "Synonym-Of", "Deep Domain Confusion:Method"], ["DAN:Method", "Synonym-Of", "Deep Adaptation Network:Method"]]}
{"doc_id": "56657874", "sentence": "The aforementioned methods utilized two streams of Convolutional Neural Networks ( CNN ) where the source and target networks are fused at the classifier level .", "ner": [["Convolutional Neural Networks", "Method"], ["CNN", "Method"]], "rel": [["CNN", "Synonym-Of", "Convolutional Neural Networks"]], "rel_plus": [["CNN:Method", "Synonym-Of", "Convolutional Neural Networks:Method"]]}
{"doc_id": "56657874", "sentence": "Domain - Adversarial Neural Networks ( DANN ) [ 1 0 ] introduced a deep domain adaptation approach by integrating a gradient reversal layer into the traditional architecture .", "ner": [["Domain - Adversarial Neural Networks", "Method"], ["DANN", "Method"], ["deep domain adaptation", "Method"], ["gradient reversal layer", "Method"]], "rel": [["DANN", "Synonym-Of", "Domain - Adversarial Neural Networks"], ["gradient reversal layer", "Part-Of", "Domain - Adversarial Neural Networks"], ["Domain - Adversarial Neural Networks", "SubClass-Of", "deep domain adaptation"]], "rel_plus": [["DANN:Method", "Synonym-Of", "Domain - Adversarial Neural Networks:Method"], ["gradient reversal layer:Method", "Part-Of", "Domain - Adversarial Neural Networks:Method"], ["Domain - Adversarial Neural Networks:Method", "SubClass-Of", "deep domain adaptation:Method"]]}
{"doc_id": "56657874", "sentence": "Current DA approaches still suffer from DG problems when the target data is unavailable during training .", "ner": [["DA", "Method"], ["DG", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "56657874", "sentence": "In this paper , we explore how the DA approaches can be applied more efficiently on DG scenarios to improve their generalization capability .", "ner": [["DA", "Method"], ["DG", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "56657874", "sentence": "In recent research , DG is a less explored issue than DA .", "ner": [["DG", "Method"], ["DA", "Method"]], "rel": [["DG", "Compare-With", "DA"]], "rel_plus": [["DG:Method", "Compare-With", "DA:Method"]]}
{"doc_id": "56657874", "sentence": "The aim of DG is to acquire knowledge from the multiple source domains available to obtain a domain - independent model that can be used for a particular task , such as classification , to an unseen domain .", "ner": [["DG", "Method"], ["classification", "Task"]], "rel": [["DG", "Used-For", "classification"]], "rel_plus": [["DG:Method", "Used-For", "classification:Task"]]}
{"doc_id": "56657874", "sentence": "Blanchard et al. [ 2 ] first introduced an augmented Support Vector Machine ( SVM ) based model that solved automatic gating of flow cytometry by encoding empirical marginal distributions into the kernel .", "ner": [["Support Vector Machine", "Method"], ["SVM", "Method"]], "rel": [["SVM", "Synonym-Of", "Support Vector Machine"]], "rel_plus": [["SVM:Method", "Synonym-Of", "Support Vector Machine:Method"]]}
{"doc_id": "56657874", "sentence": "Ghifary et al. [ 1 2 ] proposed an autoencoder based technique to extract domain - invariant information through multi - task learning .", "ner": [["autoencoder", "Method"], ["multi - task learning", "Method"]], "rel": [["multi - task learning", "Used-For", "autoencoder"]], "rel_plus": [["multi - task learning:Method", "Used-For", "autoencoder:Method"]]}
{"doc_id": "56657874", "sentence": "For multi - view domain generalization , [ 2 9 , 3 0 ] proposed another DG approach where multiple types of features of the source samples were used to learn a robust classifier .", "ner": [["multi - view domain generalization", "Task"], ["DG", "Method"]], "rel": [["DG", "Used-For", "multi - view domain generalization"]], "rel_plus": [["DG:Method", "Used-For", "multi - view domain generalization:Task"]]}
{"doc_id": "56657874", "sentence": "Li et al. [ 2 2 ] proposed another DG method based on a low - rank parameterized CNN .", "ner": [["DG", "Method"], ["low - rank parameterized CNN", "Method"]], "rel": [["low - rank parameterized CNN", "Part-Of", "DG"]], "rel_plus": [["low - rank parameterized CNN:Method", "Part-Of", "DG:Method"]]}
{"doc_id": "56657874", "sentence": "A deep domain generalization architecture with a structured low - rank constraint to mitigate the domain generalization issue was proposed in [ 7 ] where consistent information across multiple related source domains were captured .", "ner": [["deep domain generalization", "Method"], ["domain generalization issue", "Task"]], "rel": [["deep domain generalization", "Used-For", "domain generalization issue"]], "rel_plus": [["deep domain generalization:Method", "Used-For", "domain generalization issue:Task"]]}
{"doc_id": "56657874", "sentence": "Although many DA techniques based on deep architectures are proposed in recent years , very few DG approaches based on deep architecture are introduced .", "ner": [["DA", "Method"], ["DG", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "56657874", "sentence": "In this paper , we explore GAN along with a deep architecture to address the DG challenges .", "ner": [["GAN", "Method"], ["DG challenges", "Task"]], "rel": [["GAN", "Used-For", "DG challenges"]], "rel_plus": [["GAN:Method", "Used-For", "DG challenges:Task"]]}
{"doc_id": "56657874", "sentence": "We propose a deep domain generalization framework using synthetic data generated by a GAN where the styles are transferred from one domain into another domain .", "ner": [["deep domain generalization", "Method"], ["GAN", "Method"]], "rel": [["GAN", "Part-Of", "deep domain generalization"]], "rel_plus": [["GAN:Method", "Part-Of", "deep domain generalization:Method"]]}
{"doc_id": "56657874", "sentence": "We formalize domain adaptation and domain generalization as follows : Let us consider that the source domain data samples are The data distribution of the source and target data are different , i.e. , where Y t is the label of target samples .", "ner": [["domain adaptation", "Method"], ["domain generalization", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "56657874", "sentence": "Generative Adversarial Networks ( GANs ) have accomplished great outcomes in different applications , for example , image generation [ 1 4 ] , text 2 image [ 3 4 ] , image - toimage translation [ 1 7 , 4 5 ] , person re - identification [ 1 8 ] and image editing [ 3 3 ] .", "ner": [["Generative Adversarial Networks", "Method"], ["GANs", "Method"], ["image generation", "Task"], ["text 2 image", "Task"], ["image - toimage translation", "Task"], ["person re - identification", "Task"], ["image editing", "Task"]], "rel": [["GANs", "Synonym-Of", "Generative Adversarial Networks"], ["Generative Adversarial Networks", "Used-For", "image generation"], ["Generative Adversarial Networks", "Used-For", "text 2 image"], ["Generative Adversarial Networks", "Used-For", "image - toimage translation"], ["Generative Adversarial Networks", "Used-For", "person re - identification"], ["Generative Adversarial Networks", "Used-For", "image editing"]], "rel_plus": [["GANs:Method", "Synonym-Of", "Generative Adversarial Networks:Method"], ["Generative Adversarial Networks:Method", "Used-For", "image generation:Task"], ["Generative Adversarial Networks:Method", "Used-For", "text 2 image:Task"], ["Generative Adversarial Networks:Method", "Used-For", "image - toimage translation:Task"], ["Generative Adversarial Networks:Method", "Used-For", "person re - identification:Task"], ["Generative Adversarial Networks:Method", "Used-For", "image editing:Task"]]}
{"doc_id": "56657874", "sentence": "Zhu et al. [ 4 5 ] proposed CycleGAN that learned a mapping between an input image and an output image using both adversarial and cycle consistency loss where unpaired data were used for image generation .", "ner": [["CycleGAN", "Method"], ["cycle consistency loss", "Method"], ["image generation", "Task"]], "rel": [["cycle consistency loss", "Part-Of", "CycleGAN"], ["CycleGAN", "Used-For", "image generation"]], "rel_plus": [["cycle consistency loss:Method", "Part-Of", "CycleGAN:Method"], ["CycleGAN:Method", "Used-For", "image generation:Task"]]}
{"doc_id": "56657874", "sentence": "Choi et al. [ 4 ] proposed Stargan for image translation from one domain to another domain .", "ner": [["Stargan", "Method"], ["image translation", "Task"]], "rel": [["Stargan", "Used-For", "image translation"]], "rel_plus": [["Stargan:Method", "Used-For", "image translation:Task"]]}
{"doc_id": "56657874", "sentence": "In addition to decrease the discrepancy among mapping functions , we use cycle consistency loss , To transfer the style of one domain into another domain , for an example , to generate images in the style in domain S 2 D from the image of domain S 1 D , the full objective of the GAN is , The previous domain generalization method [ 1 1 ] divided the source domain datasets into a training set and a test set by random selection from the source datasets .", "ner": [["cycle consistency loss", "Method"], ["GAN", "Method"], ["domain generalization", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "56657874", "sentence": "If there is no domain mismatch between the source and target data , the classifying function f is trained by minimizing the classification loss , where E[. ] and l represent mathematical expectation and any loss functions ( such as , categorical cross - entropy for multi - class classification ) respectively .", "ner": [["classification", "Task"], ["multi - class classification", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "56657874", "sentence": "Overall , in most of the deep domain adaptation algorithms , the objective is , We use GAN to generate images from one source domain into the distributions of other available source domains .", "ner": [["deep domain adaptation", "Method"], ["GAN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "56657874", "sentence": "Most of the DA methods use Siamese based architecture where two stream of CNN is used .", "ner": [["DA", "Method"], ["Siamese based architecture", "Method"], ["CNN", "Method"]], "rel": [["Siamese based architecture", "Part-Of", "DA"], ["CNN", "Part-Of", "Siamese based architecture"]], "rel_plus": [["Siamese based architecture:Method", "Part-Of", "DA:Method"], ["CNN:Method", "Part-Of", "Siamese based architecture:Method"]]}
{"doc_id": "56657874", "sentence": "This 7 0 % ( from all source domain ) is fed into one stream of CNN and the other 3 0 % ( from all source domain ) data is fed into the second stream of CNN during the training phase .", "ner": [["CNN", "Method"], ["CNN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "56657874", "sentence": "In this section , we conduct substantial experiments to evaluate the proposed method and compare with state - ofthe - art UDDA and DG techniques .   We evaluate all the methods on four standard domain adaptation and generalization benchmark datasets : Office - 3 1 [ 3 6 ] , Office - Caltech [ 1 3 ] , Office - Home [ 4 2 ] and PACS [ 2 2 ] .", "ner": [["UDDA", "Method"], ["DG", "Method"], ["domain adaptation", "Method"], ["Office - 3 1", "Dataset"], ["Office - Caltech", "Dataset"], ["Office - Home", "Dataset"], ["PACS", "Dataset"]], "rel": [["domain adaptation", "Evaluated-With", "Office - 3 1"], ["domain adaptation", "Evaluated-With", "Office - Caltech"], ["domain adaptation", "Evaluated-With", "Office - Home"], ["domain adaptation", "Evaluated-With", "PACS"]], "rel_plus": [["domain adaptation:Method", "Evaluated-With", "Office - 3 1:Dataset"], ["domain adaptation:Method", "Evaluated-With", "Office - Caltech:Dataset"], ["domain adaptation:Method", "Evaluated-With", "Office - Home:Dataset"], ["domain adaptation:Method", "Evaluated-With", "PACS:Dataset"]]}
{"doc_id": "56657874", "sentence": "Office - 3 1 [ 3 6 ] is the most prominent benchmark dataset for domain adaptation .", "ner": [["Office - 3 1", "Dataset"], ["domain adaptation", "Task"]], "rel": [["domain adaptation", "Benchmark-For", "Office - 3 1"]], "rel_plus": [["domain adaptation:Task", "Benchmark-For", "Office - 3 1:Dataset"]]}
{"doc_id": "56657874", "sentence": "Office - Caltech [ 1 3 ] dataset is formed by taking the 1 0 common classes of two datasets : Office PACS [ 2 2 ] is also a recently released benchmark dataset for domain generalization which is created by considering the common classes among Caltech 2 5 6 , Sketchy , TU - Berlin and Google Images .", "ner": [["Office - Caltech", "Dataset"], ["Office PACS", "Dataset"], ["domain generalization", "Method"], ["Caltech 2 5 6", "Dataset"], ["Sketchy", "Dataset"], ["TU - Berlin", "Dataset"], ["Google Images", "Dataset"]], "rel": [["Office PACS", "Evaluated-With", "domain generalization"]], "rel_plus": [["Office PACS:Dataset", "Evaluated-With", "domain generalization:Method"]]}
{"doc_id": "56657874", "sentence": "The GAN network consists of two convolution layers with two stride - 2 , several residual blocks and two fractionally strided convolution layers with stride 0. 5 .", "ner": [["GAN", "Method"], ["convolution layers", "Method"], ["residual blocks", "Method"]], "rel": [["convolution layers", "Part-Of", "GAN"], ["residual blocks", "Part-Of", "GAN"]], "rel_plus": [["convolution layers:Method", "Part-Of", "GAN:Method"], ["residual blocks:Method", "Part-Of", "GAN:Method"]]}
{"doc_id": "56657874", "sentence": "For domain generalization , we use DAN [ 2 3 ] , D - CORAL [ 3 9 ] , RTN [ 2 4 ] and JAN [ 2 5 ] architecture where Alexnet [ 2 1 ] is used , comprising of five convolution layers and three fully connected layers .", "ner": [["domain generalization", "Method"], ["DAN", "Method"], ["D - CORAL", "Method"], ["RTN", "Method"], ["JAN", "Method"], ["Alexnet", "Method"], ["convolution layers", "Method"], ["fully connected layers", "Method"]], "rel": [["DAN", "Part-Of", "domain generalization"], ["D - CORAL", "Part-Of", "domain generalization"], ["RTN", "Part-Of", "domain generalization"], ["JAN", "Part-Of", "domain generalization"], ["convolution layers", "Part-Of", "Alexnet"], ["fully connected layers", "Part-Of", "Alexnet"]], "rel_plus": [["DAN:Method", "Part-Of", "domain generalization:Method"], ["D - CORAL:Method", "Part-Of", "domain generalization:Method"], ["RTN:Method", "Part-Of", "domain generalization:Method"], ["JAN:Method", "Part-Of", "domain generalization:Method"], ["convolution layers:Method", "Part-Of", "Alexnet:Method"], ["fully connected layers:Method", "Part-Of", "Alexnet:Method"]]}
{"doc_id": "56657874", "sentence": "For synthetic image generation , we use ComboGAN [ 1 ] where we set the value of \u03bb as 1 0 in Equation 4 .", "ner": [["image generation", "Task"], ["ComboGAN", "Method"]], "rel": [["ComboGAN", "Used-For", "image generation"]], "rel_plus": [["ComboGAN:Method", "Used-For", "image generation:Task"]]}
{"doc_id": "56657874", "sentence": "For domain generalization , we use two streams of CNN .", "ner": [["domain generalization", "Method"], ["CNN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "56657874", "sentence": "In each stream , we extend AlexNet [ 2 1 ] model which is pretrained on the ImageNet [ 6 ] dataset .", "ner": [["AlexNet", "Method"], ["ImageNet", "Dataset"]], "rel": [["AlexNet", "Trained-With", "ImageNet"]], "rel_plus": [["AlexNet:Method", "Trained-With", "ImageNet:Dataset"]]}
{"doc_id": "56657874", "sentence": "We set the dimension of the last fully connected layer ( fc 8) to the number of categories ( for instance , 6 5 for Office - Home dataset ) .", "ner": [["fully connected layer", "Method"], ["fc 8)", "Method"], ["Office - Home", "Dataset"]], "rel": [["fc 8)", "Synonym-Of", "fully connected layer"]], "rel_plus": [["fc 8):Method", "Synonym-Of", "fully connected layer:Method"]]}
{"doc_id": "56657874", "sentence": "For fair comparison , we use the same net - work architecture ( AlexNet [ 2 1 ] ) that are used in the existing domain adaptation methods such as DAN [ 2 3 ] , D - CORAL [ 3 9 ] , JAN [ 2 5 ] and RTN [ 2 4 ] in domain generalization settings .", "ner": [["AlexNet", "Method"], ["domain adaptation", "Method"], ["DAN", "Method"], ["D - CORAL", "Method"], ["JAN", "Method"], ["RTN", "Method"], ["domain generalization", "Method"]], "rel": [["DAN", "SubClass-Of", "domain adaptation"], ["D - CORAL", "SubClass-Of", "domain adaptation"], ["JAN", "SubClass-Of", "domain adaptation"], ["RTN", "SubClass-Of", "domain adaptation"], ["domain generalization", "SubClass-Of", "domain adaptation"]], "rel_plus": [["DAN:Method", "SubClass-Of", "domain adaptation:Method"], ["D - CORAL:Method", "SubClass-Of", "domain adaptation:Method"], ["JAN:Method", "SubClass-Of", "domain adaptation:Method"], ["RTN:Method", "SubClass-Of", "domain adaptation:Method"], ["domain generalization:Method", "SubClass-Of", "domain adaptation:Method"]]}
{"doc_id": "56657874", "sentence": "DAN [ 2 3 ] is a deep domain adaptation model where the discrepancy between the source and target data is minimized using MMD .", "ner": [["DAN", "Method"], ["deep domain adaptation", "Method"]], "rel": [["DAN", "SubClass-Of", "deep domain adaptation"]], "rel_plus": [["DAN:Method", "SubClass-Of", "deep domain adaptation:Method"]]}
{"doc_id": "56657874", "sentence": "JAN [ 2 5 ] is also a deep domain adaptation method where the discrepancy between the source and target data is mitigated using Joint Maximum Mean Discrepancy ( JMMD ) criterion .", "ner": [["JAN", "Method"], ["deep domain adaptation", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "56657874", "sentence": "Table 3 : Recognition accuracies for domain generalization on the Office - Home dataset [ 4 2 ] with synthetic images that are generated using MUNIT .", "ner": [["domain generalization", "Method"], ["Office - Home", "Dataset"], ["MUNIT", "Method"]], "rel": [["domain generalization", "Evaluated-With", "Office - Home"]], "rel_plus": [["domain generalization:Method", "Evaluated-With", "Office - Home:Dataset"]]}
{"doc_id": "56657874", "sentence": "Table 4 : Recognition accuracies for domain generalization on the Office - Home dataset [ 4 2 ] with synthetic images that are generated using Stargan .", "ner": [["Recognition", "Task"], ["domain generalization", "Method"], ["Office - Home", "Dataset"]], "rel": [["domain generalization", "Evaluated-With", "Office - Home"], ["Recognition", "Benchmark-For", "Office - Home"]], "rel_plus": [["domain generalization:Method", "Evaluated-With", "Office - Home:Dataset"], ["Recognition:Task", "Benchmark-For", "Office - Home:Dataset"]]}
{"doc_id": "56657874", "sentence": "The 7 0 % data is fed into one stream of CNN and 3 0 % data is fed into the second stream of CNN .", "ner": [["CNN", "Method"], ["CNN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "56657874", "sentence": "In the above setting , we evaluate 4 existing domain adaptation ( DAN [ 2 3 ] , D - CORAL [ 3 9 ] , JAN [ 2 5 ] and RTN [ 2 4 ] ) methods for domain generalization settings .", "ner": [["domain adaptation", "Method"], ["DAN", "Method"], ["D - CORAL", "Method"], ["JAN", "Method"], ["RTN", "Method"], ["domain generalization", "Method"]], "rel": [["DAN", "SubClass-Of", "domain adaptation"], ["D - CORAL", "SubClass-Of", "domain adaptation"], ["JAN", "SubClass-Of", "domain adaptation"], ["RTN", "SubClass-Of", "domain adaptation"], ["domain adaptation", "Used-For", "domain generalization"]], "rel_plus": [["DAN:Method", "SubClass-Of", "domain adaptation:Method"], ["D - CORAL:Method", "SubClass-Of", "domain adaptation:Method"], ["JAN:Method", "SubClass-Of", "domain adaptation:Method"], ["RTN:Method", "SubClass-Of", "domain adaptation:Method"], ["domain adaptation:Method", "Used-For", "domain generalization:Method"]]}
{"doc_id": "56657874", "sentence": "The real images are fed into one stream of CNN and synthetic images are fed into another stream of CNN .", "ner": [["CNN", "Method"], ["CNN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "56657874", "sentence": "To further increase the justification of adopting Com - boGAN in our framework , we also experimentally evaluate our domain generalization model using synthetic images that are generated by MUNIT [ 1 6 ] and Stargan [ 4 ] on Office - Home dataset .", "ner": [["Com - boGAN", "Method"], ["domain generalization", "Method"], ["MUNIT", "Method"], ["Stargan", "Method"], ["Office - Home", "Dataset"]], "rel": [["MUNIT", "Part-Of", "domain generalization"], ["Stargan", "Part-Of", "domain generalization"], ["domain generalization", "Evaluated-With", "Office - Home"]], "rel_plus": [["MUNIT:Method", "Part-Of", "domain generalization:Method"], ["Stargan:Method", "Part-Of", "domain generalization:Method"], ["domain generalization:Method", "Evaluated-With", "Office - Home:Dataset"]]}
{"doc_id": "56657874", "sentence": "The domain generalization on different tasks are reported in Table 3 and Table 4 using MUNIT and Stargan respectively .", "ner": [["domain generalization", "Method"], ["MUNIT", "Method"]], "rel": [["MUNIT", "Part-Of", "domain generalization"]], "rel_plus": [["MUNIT:Method", "Part-Of", "domain generalization:Method"]]}
{"doc_id": "56657874", "sentence": "From the results ( Tables 1 , 2 , 3 and 4 ) , we make two important observations : ( 1 ) The multi - component synthetic data generation using GAN boosts the domain generalization performance ; and ( 2 ) As ComboGAN can handle more than two domains at a time compared to MUNIT and Stargan , it can generate more multi - component images which are more effective for domain generalization .", "ner": [["GAN", "Method"], ["domain generalization", "Method"], ["ComboGAN", "Method"], ["MUNIT", "Method"], ["Stargan", "Method"], ["domain generalization", "Method"]], "rel": [["GAN", "Used-For", "domain generalization"], ["ComboGAN", "Compare-With", "MUNIT"], ["ComboGAN", "Compare-With", "Stargan"]], "rel_plus": [["GAN:Method", "Used-For", "domain generalization:Method"], ["ComboGAN:Method", "Compare-With", "MUNIT:Method"], ["ComboGAN:Method", "Compare-With", "Stargan:Method"]]}
{"doc_id": "56657874", "sentence": "We further evaluate and compare our proposed approach with both shallow and deep domain generalization stateof - the - art methods : Undoing the Damage of Dataset Bias ( Undo - Bias ) [ 1 9 ] , Unbiased Metric Learning ( UML ) [ 9 ] , Low - Rank Structure from Latent Domains for Domain Generalization ( LRE - SVM ) [ 4 3 ] , Multi - Task Autoencoders ( MTAE ) [ 1 2 ] , Domain Separation Network ( DSN ) [ 3 ] , Deep Domain Generalization with Structured Low - Rank Constraint(DGLRC ) [ 7 ] , Domain Generalization via Invari - Table 7 : Recognition accuracies for domain generalization on the PACS dataset [ 2 2 ] using synthetic images that are generated by ComboGAN .", "ner": [["deep domain generalization", "Method"], ["Undoing the Damage of Dataset Bias", "Method"], ["Undo - Bias", "Method"], ["Unbiased Metric Learning", "Method"], ["UML", "Method"], ["Low - Rank Structure from Latent Domains for Domain Generalization", "Method"], ["LRE - SVM", "Method"], ["Multi - Task Autoencoders", "Method"], ["MTAE", "Method"], ["Domain Separation Network", "Method"], ["DSN", "Method"], ["Deep Domain Generalization with Structured Low - Rank Constraint(DGLRC )", "Method"], ["Domain Generalization", "Method"], ["Recognition", "Task"], ["domain generalization", "Method"], ["PACS", "Dataset"], ["ComboGAN", "Method"]], "rel": [["Undo - Bias", "Synonym-Of", "Undoing the Damage of Dataset Bias"], ["UML", "Synonym-Of", "Unbiased Metric Learning"], ["LRE - SVM", "Synonym-Of", "Low - Rank Structure from Latent Domains for Domain Generalization"], ["MTAE", "Synonym-Of", "Multi - Task Autoencoders"], ["DSN", "Synonym-Of", "Domain Separation Network"], ["PACS", "Benchmark-For", "Recognition"], ["ComboGAN", "Used-For", "PACS"]], "rel_plus": [["Undo - Bias:Method", "Synonym-Of", "Undoing the Damage of Dataset Bias:Method"], ["UML:Method", "Synonym-Of", "Unbiased Metric Learning:Method"], ["LRE - SVM:Method", "Synonym-Of", "Low - Rank Structure from Latent Domains for Domain Generalization:Method"], ["MTAE:Method", "Synonym-Of", "Multi - Task Autoencoders:Method"], ["DSN:Method", "Synonym-Of", "Domain Separation Network:Method"], ["PACS:Dataset", "Benchmark-For", "Recognition:Task"], ["ComboGAN:Method", "Used-For", "PACS:Dataset"]]}
{"doc_id": "56657874", "sentence": "The subscript S represents synthetic data . ant Feature Representation ( uDICA ) [ 2 8 ] , Deeper , Broader and Artier Domain Generalization ( DBADG ) [ 2 2 ] .", "ner": [["uDICA", "Method"], ["Deeper , Broader and Artier Domain Generalization", "Method"], ["DBADG", "Method"]], "rel": [["DBADG", "Synonym-Of", "Deeper , Broader and Artier Domain Generalization"]], "rel_plus": [["DBADG:Method", "Synonym-Of", "Deeper , Broader and Artier Domain Generalization:Method"]]}
{"doc_id": "56657874", "sentence": "We report comparative results in Table 5 , 6 and 7 on Office 3 1 , Office - Caltech , and PACS datasets respectively .", "ner": [["Office 3 1", "Dataset"], ["Office - Caltech", "Dataset"], ["PACS", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "56657874", "sentence": "In contrast , our domain generalization method using synthetic data with D - CORAL [ 3 9 ] , JAN [ 2 5 ] , and RTN [ 2 4 ] network architectures achieves 8 0 . 3 3 % , 8 1 . 2 1 % and 8 0 . 8 7 % average accuracies respectively which outperforms the state - of - the - art methods .", "ner": [["domain generalization", "Method"], ["D - CORAL", "Method"], ["JAN", "Method"], ["RTN", "Method"]], "rel": [["D - CORAL", "Part-Of", "domain generalization"], ["JAN", "Part-Of", "domain generalization"], ["RTN", "Part-Of", "domain generalization"]], "rel_plus": [["D - CORAL:Method", "Part-Of", "domain generalization:Method"], ["JAN:Method", "Part-Of", "domain generalization:Method"], ["RTN:Method", "Part-Of", "domain generalization:Method"]]}
{"doc_id": "56657874", "sentence": "For Office - Caltech dataset ( see Table 6 ) , although DGLRC [ 7 ] achieved best performance , our proposed method is different in network architecture as we use generative adversarial network to generate synthetic data .", "ner": [["Office - Caltech", "Dataset"], ["DGLRC", "Method"]], "rel": [["DGLRC", "Evaluated-With", "Office - Caltech"]], "rel_plus": [["DGLRC:Method", "Evaluated-With", "Office - Caltech:Dataset"]]}
{"doc_id": "56657874", "sentence": "Using synthetic data with DAN [ 2 3 ] , D - CORAL [ 3 9 ] , JAN [ 2 5 ] and RTN [ 2 4 ] network architecture , we achieve 8 5 . 5 2 % , 8 5 . 2 9 % , 8 6 . 8 7 % and 8 6 . 0 4 % average accuracies respectively .", "ner": [["DAN", "Method"], ["D - CORAL", "Method"], ["JAN", "Method"], ["RTN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "56657874", "sentence": "For PACS dataset , our proposed method achieves state - of - the - art performance using synthetic data and JAN architecture [ 2 5 ] .", "ner": [["PACS", "Dataset"], ["JAN", "Method"]], "rel": [["JAN", "Evaluated-With", "PACS"]], "rel_plus": [["JAN:Method", "Evaluated-With", "PACS:Dataset"]]}
{"doc_id": "56657874", "sentence": "Our image translation model is unsupervised , and capable of translating images from one domain to another in open - set domain adaptation/generalization scenario .", "ner": [["image translation", "Task"], ["domain adaptation/generalization", "Method"]], "rel": [["domain adaptation/generalization", "Used-For", "image translation"]], "rel_plus": [["domain adaptation/generalization:Method", "Used-For", "image translation:Task"]]}
{"doc_id": "56657874", "sentence": "In this paper , we developed a novel deep domain generalization architecture using synthetic images which were generated by a GAN and existing domain discrepancy minimizing metrics which aims to learn a domain agnostic model from the real and synthetic data that can be applied for unseen datasets .", "ner": [["deep domain generalization", "Method"], ["GAN", "Method"]], "rel": [["GAN", "Part-Of", "deep domain generalization"]], "rel_plus": [["GAN:Method", "Part-Of", "deep domain generalization:Method"]]}
{"doc_id": "204901567", "sentence": "More concretely , we first train a transformer - based masked language model on one language , and transfer it to a new language by learning a new embedding matrix with the same masked language modeling objective , freezing parameters of all other layers .", "ner": [["transformer - based masked language model", "Method"], ["masked language modeling objective", "Method"]], "rel": [["masked language modeling objective", "Part-Of", "transformer - based masked language model"]], "rel_plus": [["masked language modeling objective:Method", "Part-Of", "transformer - based masked language model:Method"]]}
{"doc_id": "204901567", "sentence": "However , we show that it is competitive with multilingual BERT on standard cross - lingual classification benchmarks and on a new Cross - lingual Question Answering Dataset ( XQuAD ) .", "ner": [["multilingual BERT", "Method"], ["cross - lingual classification", "Task"], ["Cross - lingual Question Answering Dataset", "Dataset"], ["XQuAD", "Dataset"]], "rel": [["multilingual BERT", "Used-For", "cross - lingual classification"], ["Cross - lingual Question Answering Dataset", "Benchmark-For", "cross - lingual classification"], ["XQuAD", "Synonym-Of", "Cross - lingual Question Answering Dataset"], ["multilingual BERT", "Evaluated-With", "Cross - lingual Question Answering Dataset"]], "rel_plus": [["multilingual BERT:Method", "Used-For", "cross - lingual classification:Task"], ["Cross - lingual Question Answering Dataset:Dataset", "Benchmark-For", "cross - lingual classification:Task"], ["XQuAD:Dataset", "Synonym-Of", "Cross - lingual Question Answering Dataset:Dataset"], ["multilingual BERT:Method", "Evaluated-With", "Cross - lingual Question Answering Dataset:Dataset"]]}
{"doc_id": "204901567", "sentence": "We also release XQuAD as a more comprehensive cross - lingual benchmark , which comprises 2 4 0 paragraphs and 1 1 9 0 question - answer pairs from SQuAD v 1 . 1 translated into ten languages by professional translators .", "ner": [["XQuAD", "Dataset"], ["SQuAD v 1 . 1", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "204901567", "sentence": "Multilingual pre - training methods such as multilingual BERT ( mBERT , Devlin et al. , 2 0 1 9 ) have been successfully used for zero - shot cross - lingual transfer ( Pires et al. , 2 0 1 9 ; Lample and Conneau , 2 0 1 9 ) .", "ner": [["multilingual BERT", "Method"], ["mBERT", "Method"], ["zero - shot cross - lingual transfer", "Task"]], "rel": [["mBERT", "Synonym-Of", "multilingual BERT"], ["multilingual BERT", "Used-For", "zero - shot cross - lingual transfer"]], "rel_plus": [["mBERT:Method", "Synonym-Of", "multilingual BERT:Method"], ["multilingual BERT:Method", "Used-For", "zero - shot cross - lingual transfer:Task"]]}
{"doc_id": "204901567", "sentence": "These methods work by jointly training a * Work done as an intern at DeepMind . transformer model ( Vaswani et al. , 2 0 1 7 ) to perform masked language modeling ( MLM ) in multiple languages , which is then fine - tuned on a downstream task using labeled data in a single language - typically English .", "ner": [["transformer", "Method"], ["masked language modeling", "Task"], ["MLM", "Task"]], "rel": [["MLM", "Used-For", "masked language modeling"], ["transformer", "Used-For", "masked language modeling"]], "rel_plus": [["MLM:Task", "Used-For", "masked language modeling:Task"], ["transformer:Method", "Used-For", "masked language modeling:Task"]]}
{"doc_id": "204901567", "sentence": "As illustrated in Figure 1 , our method starts with a monolingual transformer trained with MLM , which we transfer to a new language by learning a new embedding matrix through MLM in the new language Figure 1 : Four steps for zero - shot cross - lingual transfer : ( i ) pre - train a monolingual transformer model in English akin to BERT ; ( ii ) freeze the transformer body and learn new token embeddings from scratch for a second language using the same training objective over its monolingual corpus ; ( iii ) fine - tune the model on English while keeping the embeddings frozen ; and ( iv ) zero - shot transfer it to the new language by swapping the token embeddings . while freezing parameters of all other layers .", "ner": [["monolingual transformer", "Method"], ["MLM", "Task"], ["MLM", "Task"], ["zero - shot cross - lingual transfer", "Task"], ["monolingual transformer", "Method"], ["BERT", "Method"], ["transformer", "Method"]], "rel": [["monolingual transformer", "Trained-With", "MLM"], ["BERT", "SubClass-Of", "monolingual transformer"]], "rel_plus": [["monolingual transformer:Method", "Trained-With", "MLM:Task"], ["BERT:Method", "SubClass-Of", "monolingual transformer:Method"]]}
{"doc_id": "204901567", "sentence": "However , we show that it is competitive with joint multilingual pre - training across standard zero - shot cross - lingual transfer benchmarks ( XNLI , MLDoc , and PAWS - X ) .", "ner": [["zero - shot cross - lingual transfer", "Task"], ["XNLI", "Dataset"], ["MLDoc", "Dataset"], ["PAWS - X", "Dataset"]], "rel": [["XNLI", "Benchmark-For", "zero - shot cross - lingual transfer"], ["PAWS - X", "Benchmark-For", "zero - shot cross - lingual transfer"], ["MLDoc", "Benchmark-For", "zero - shot cross - lingual transfer"]], "rel_plus": [["XNLI:Dataset", "Benchmark-For", "zero - shot cross - lingual transfer:Task"], ["PAWS - X:Dataset", "Benchmark-For", "zero - shot cross - lingual transfer:Task"], ["MLDoc:Dataset", "Benchmark-For", "zero - shot cross - lingual transfer:Task"]]}
{"doc_id": "204901567", "sentence": "We also experiment with a new Cross - lingual Question Answering Dataset ( XQuAD ) , which consists of 2 4 0 paragraphs and 1 1 9 0 questionanswer pairs from SQuAD v 1 . 1 ( Rajpurkar et al. , 2 0 1 6 ) translated into ten languages by professional translators .", "ner": [["Cross - lingual Question Answering Dataset", "Dataset"], ["XQuAD", "Dataset"], ["SQuAD v 1 . 1", "Dataset"]], "rel": [["XQuAD", "Synonym-Of", "Cross - lingual Question Answering Dataset"]], "rel_plus": [["XQuAD:Dataset", "Synonym-Of", "Cross - lingual Question Answering Dataset:Dataset"]]}
{"doc_id": "204901567", "sentence": "Question answering as a task is a classic probe for language understanding .", "ner": [["Question answering", "Task"], ["language understanding", "Task"]], "rel": [["Question answering", "SubTask-Of", "language understanding"]], "rel_plus": [["Question answering:Task", "SubTask-Of", "language understanding:Task"]]}
{"doc_id": "204901567", "sentence": "We believe that XQuAD can serve as a more comprehensive benchmark to evaluate cross - lingual models and make this dataset publicly available at https://github.com/ deepmind/XQuAD .", "ner": [["XQuAD", "Dataset"], ["cross - lingual models", "Method"]], "rel": [["cross - lingual models", "Evaluated-With", "XQuAD"]], "rel_plus": [["cross - lingual models:Method", "Evaluated-With", "XQuAD:Dataset"]]}
{"doc_id": "204901567", "sentence": "Our results on XQuAD demonstrate that the monolingual transfer approach can be made competitive with jointly trained multilingual models by learning second language - specific transformations via adapter modules ( Rebuffi et al. , 2 0 1 7 ) .", "ner": [["XQuAD", "Dataset"], ["monolingual transfer approach", "Method"], ["multilingual models", "Method"], ["adapter modules", "Method"]], "rel": [["monolingual transfer approach", "Evaluated-With", "XQuAD"], ["adapter modules", "Part-Of", "multilingual models"]], "rel_plus": [["monolingual transfer approach:Method", "Evaluated-With", "XQuAD:Dataset"], ["adapter modules:Method", "Part-Of", "multilingual models:Method"]]}
{"doc_id": "204901567", "sentence": "Pre - train a monolingual BERT ( i.e. a transformer ) in L 1 with masked language modeling ( MLM ) and next sentence prediction ( NSP ) objectives on an unlabeled L 1 corpus . 2 .", "ner": [["monolingual BERT", "Method"], ["transformer", "Method"], ["masked language modeling", "Task"], ["MLM", "Task"], ["next sentence prediction", "Task"], ["NSP", "Task"]], "rel": [["monolingual BERT", "SubClass-Of", "transformer"], ["MLM", "Synonym-Of", "masked language modeling"], ["monolingual BERT", "Trained-With", "masked language modeling"], ["NSP", "Synonym-Of", "next sentence prediction"], ["monolingual BERT", "Trained-With", "next sentence prediction"]], "rel_plus": [["monolingual BERT:Method", "SubClass-Of", "transformer:Method"], ["MLM:Task", "Synonym-Of", "masked language modeling:Task"], ["monolingual BERT:Method", "Trained-With", "masked language modeling:Task"], ["NSP:Task", "Synonym-Of", "next sentence prediction:Task"], ["monolingual BERT:Method", "Trained-With", "next sentence prediction:Task"]]}
{"doc_id": "204901567", "sentence": "Transfer the model to a new language by learning new token embeddings while freezing the transformer body with the same training objectives ( MLM and NSP ) on an unlabeled L 2 corpus . 3 .", "ner": [["transformer", "Method"], ["MLM", "Task"], ["NSP", "Task"]], "rel": [["transformer", "Trained-With", "MLM"], ["transformer", "Trained-With", "NSP"]], "rel_plus": [["transformer:Method", "Trained-With", "MLM:Task"], ["transformer:Method", "Trained-With", "NSP:Task"]]}
{"doc_id": "204901567", "sentence": "In Step 2 , when we transfer the L 1 transformer to L 2 , we add a feed - forward adapter module after the projection following multi - headed attention and after the two feed - forward layers in each transformer layer , similar to Houlsby et al. ( 2 0 1 9 ) .", "ner": [["transformer", "Method"], ["feed - forward adapter module", "Method"], ["multi - headed attention", "Method"], ["feed - forward layers", "Method"], ["transformer layer", "Method"]], "rel": [["feed - forward layers", "Part-Of", "transformer layer"], ["multi - headed attention", "Part-Of", "transformer layer"], ["feed - forward adapter module", "Part-Of", "transformer layer"]], "rel_plus": [["feed - forward layers:Method", "Part-Of", "transformer layer:Method"], ["multi - headed attention:Method", "Part-Of", "transformer layer:Method"], ["feed - forward adapter module:Method", "Part-Of", "transformer layer:Method"]]}
{"doc_id": "204901567", "sentence": "Note that the original transformer body is still frozen , and only parameters of the adapter modules are trainable ( in addition to the embedding matrix in L 2 ) .", "ner": [["transformer", "Method"], ["adapter modules", "Method"]], "rel": [["adapter modules", "Part-Of", "transformer"]], "rel_plus": [["adapter modules:Method", "Part-Of", "transformer:Method"]]}
{"doc_id": "204901567", "sentence": "We describe the models that we compare ( \u00a7 3. 1 ) , the experimental setting ( \u00a7 3. 2 ) , and the results on three classification datasets : XNLI ( \u00a7 3. 3 ) , MLDoc ( \u00a7 3. 4 ) and PAWS - X ( \u00a7 3. 5 ) .", "ner": [["classification", "Task"], ["XNLI", "Dataset"], ["MLDoc", "Dataset"], ["PAWS - X", "Dataset"]], "rel": [["XNLI", "Benchmark-For", "classification"], ["MLDoc", "Benchmark-For", "classification"], ["PAWS - X", "Benchmark-For", "classification"]], "rel_plus": [["XNLI:Dataset", "Benchmark-For", "classification:Task"], ["MLDoc:Dataset", "Benchmark-For", "classification:Task"], ["PAWS - X:Dataset", "Benchmark-For", "classification:Task"]]}
{"doc_id": "204901567", "sentence": "We compare four main models in our experiments : Joint multilingual models ( JOINTMULTI ) .", "ner": [["Joint multilingual models", "Method"], ["JOINTMULTI", "Method"]], "rel": [["JOINTMULTI", "Synonym-Of", "Joint multilingual models"]], "rel_plus": [["JOINTMULTI:Method", "Synonym-Of", "Joint multilingual models:Method"]]}
{"doc_id": "204901567", "sentence": "This model is analogous to mBERT and closely related to other variants like XLM .", "ner": [["mBERT", "Method"], ["XLM", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "204901567", "sentence": "Cross - lingual word embedding mappings ( CLWE ) .", "ner": [["Cross - lingual word embedding", "Method"], ["CLWE", "Method"]], "rel": [["CLWE", "Synonym-Of", "Cross - lingual word embedding"]], "rel_plus": [["CLWE:Method", "Synonym-Of", "Cross - lingual word embedding:Method"]]}
{"doc_id": "204901567", "sentence": "The method we described in \u00a7 2 operates at the lexical level , and can be seen as a form of learning cross - lingual word embeddings that are aligned to a monolingual transformer body .", "ner": [["cross - lingual word embeddings", "Method"], ["monolingual transformer", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "204901567", "sentence": "We also include a method based on this alternative approach where we train skip - gram embeddings for each language , and map them to a shared space using VecMap ( Artetxe et al. , 2 0 1 8) . 4 We then train an English BERT model using MLM and NSP on top of the frozen mapped embeddings .", "ner": [["BERT", "Method"], ["MLM", "Task"], ["NSP", "Task"]], "rel": [["BERT", "Trained-With", "MLM"], ["BERT", "Trained-With", "NSP"]], "rel_plus": [["BERT:Method", "Trained-With", "MLM:Task"], ["BERT:Method", "Trained-With", "NSP:Task"]]}
{"doc_id": "204901567", "sentence": "Cross - lingual transfer of monolingual models ( MONOTRANS ) .", "ner": [["Cross - lingual transfer of monolingual models", "Method"], ["MONOTRANS", "Method"]], "rel": [["MONOTRANS", "Synonym-Of", "Cross - lingual transfer of monolingual models"]], "rel_plus": [["MONOTRANS:Method", "Synonym-Of", "Cross - lingual transfer of monolingual models:Method"]]}
{"doc_id": "204901567", "sentence": "We use Wikipedia as our training corpus , similar to mBERT and XLM ( Lample and Conneau , 2 0 1 9 ) , which we extract using the WikiExtractor tool . 5 We do not perform any lowercasing or normalization .", "ner": [["Wikipedia", "Dataset"], ["mBERT", "Method"], ["XLM", "Method"]], "rel": [["mBERT", "Trained-With", "Wikipedia"], ["XLM", "Trained-With", "Wikipedia"]], "rel_plus": [["mBERT:Method", "Trained-With", "Wikipedia:Dataset"], ["XLM:Method", "Trained-With", "Wikipedia:Dataset"]]}
{"doc_id": "204901567", "sentence": "We use the model architecture of BERT BASE , similar to mBERT .", "ner": [["BERT BASE", "Method"], ["mBERT", "Method"]], "rel": [["BERT BASE", "Compare-With", "mBERT"]], "rel_plus": [["BERT BASE:Method", "Compare-With", "mBERT:Method"]]}
{"doc_id": "204901567", "sentence": "In natural language inference ( NLI ) , given two sentences ( a premise and a hypothesis ) , the goal is to decide whether there is an entailment , with the previous results from mBERT and XLM . 6 We summarize our main findings below : \u2022 Our JOINTMULTI results are comparable with similar models reported in the literature .", "ner": [["natural language inference", "Task"], ["NLI", "Task"], ["entailment", "Task"], ["mBERT", "Method"], ["XLM", "Method"], ["JOINTMULTI", "Method"]], "rel": [["NLI", "Synonym-Of", "natural language inference"], ["entailment", "SubTask-Of", "natural language inference"], ["mBERT", "Used-For", "natural language inference"], ["XLM", "Used-For", "natural language inference"], ["mBERT", "Used-For", "entailment"], ["XLM", "Used-For", "entailment"]], "rel_plus": [["NLI:Task", "Synonym-Of", "natural language inference:Task"], ["entailment:Task", "SubTask-Of", "natural language inference:Task"], ["mBERT:Method", "Used-For", "natural language inference:Task"], ["XLM:Method", "Used-For", "natural language inference:Task"], ["mBERT:Method", "Used-For", "entailment:Task"], ["XLM:Method", "Used-For", "entailment:Task"]]}
{"doc_id": "204901567", "sentence": "Our best JOINTMULTI model is substantially better than mBERT , and only one point worse ( on average ) than the unsupervised XLM model , which is larger in size . \u2022 Among the tested JOINTMULTI variants , we observe that using a larger vocabulary size has a notable positive impact . \u2022 JOINTPAIR models with a joint vocabulary perform comparably with JOINTMULTI .", "ner": [["JOINTMULTI", "Method"], ["mBERT", "Method"], ["XLM", "Method"], ["JOINTMULTI", "Method"], ["JOINTPAIR", "Method"], ["JOINTMULTI", "Method"]], "rel": [["JOINTMULTI", "Compare-With", "mBERT"], ["JOINTMULTI", "Compare-With", "XLM"], ["JOINTPAIR", "Compare-With", "JOINTMULTI"]], "rel_plus": [["JOINTMULTI:Method", "Compare-With", "mBERT:Method"], ["JOINTMULTI:Method", "Compare-With", "XLM:Method"], ["JOINTPAIR:Method", "Compare-With", "JOINTMULTI:Method"]]}
{"doc_id": "204901567", "sentence": "This shows that modeling more languages does not affect the quality of the learned representations ( evaluated on XNLI ) . \u2022 The equivalent JOINTPAIR models with a disjoint vocabulary for each language perform better , which demonstrates that a shared subword vocabulary is not necessary for joint multilingual pre - training to work . \u2022 CLWE performs poorly .", "ner": [["XNLI", "Dataset"], ["JOINTPAIR", "Method"], ["CLWE", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "204901567", "sentence": "Larger dimensionalities and weak supervision improve CLWE , but its performance is still below other models . \u2022 The basic version of MONOTRANS is only 2. 5 6 mBERT covers 1 0 2 languages and has a shared vocabulary of 1 1 0 k subwords .", "ner": [["CLWE", "Method"], ["MONOTRANS", "Method"], ["mBERT", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "204901567", "sentence": "In subsequent experiments , we include results for all variants of MONOTRANS and JOINTPAIR , the best CLWE variant ( 7 6 8 d ident ) , and JOINTMULTI with 3 2 k and 2 0 0 k voc .", "ner": [["MONOTRANS", "Method"], ["JOINTPAIR", "Method"], ["CLWE variant ( 7 6 8 d ident )", "Method"], ["JOINTMULTI", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "204901567", "sentence": "Our classification experiments demonstrate that MONOTRANS is competitive with JOINTMULTI and JOINTPAIR , despite being multilingual at the embedding layer only ( i.e. the transformer body is trained exclusively on English ) .", "ner": [["classification", "Task"], ["MONOTRANS", "Method"], ["JOINTMULTI", "Method"], ["JOINTPAIR", "Method"], ["transformer", "Method"]], "rel": [["MONOTRANS", "Used-For", "classification"], ["JOINTMULTI", "Used-For", "classification"], ["JOINTPAIR", "Used-For", "classification"], ["MONOTRANS", "Compare-With", "JOINTMULTI"], ["MONOTRANS", "Compare-With", "JOINTPAIR"]], "rel_plus": [["MONOTRANS:Method", "Used-For", "classification:Task"], ["JOINTMULTI:Method", "Used-For", "classification:Task"], ["JOINTPAIR:Method", "Used-For", "classification:Task"], ["MONOTRANS:Method", "Compare-With", "JOINTMULTI:Method"], ["MONOTRANS:Method", "Compare-With", "JOINTPAIR:Method"]]}
{"doc_id": "204901567", "sentence": "For example , previous work has shown that models trained on MultiNLI - from which XNLI was derived - learn to exploit superficial cues in the data ( Gururangan et al. , 2 0 1 8) .", "ner": [["MultiNLI", "Dataset"], ["XNLI", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "204901567", "sentence": "To better understand the cross - lingual generalization ability of these models , we create a new Cross - lingual Question Answering Dataset ( XQuAD ) .", "ner": [["Cross - lingual Question Answering Dataset", "Dataset"], ["XQuAD", "Dataset"]], "rel": [["XQuAD", "Synonym-Of", "Cross - lingual Question Answering Dataset"]], "rel_plus": [["XQuAD:Dataset", "Synonym-Of", "Cross - lingual Question Answering Dataset:Dataset"]]}
{"doc_id": "204901567", "sentence": "Question answering is a classic probe for natural language understanding ( Hermann et al. , 2 0 1 5 ) and has been shown to be less susceptible to annotation artifacts than other popular tasks ( Kaushik and Lipton , 2 0 1 8) .", "ner": [["Question answering", "Task"], ["natural language understanding", "Task"]], "rel": [["Question answering", "SubTask-Of", "natural language understanding"]], "rel_plus": [["Question answering:Task", "SubTask-Of", "natural language understanding:Task"]]}
{"doc_id": "204901567", "sentence": "In contrast to existing classification benchmarks , question answering requires identifying relevant answer spans in longer context paragraphs , thus requiring some degree of structural transfer across languages .", "ner": [["classification", "Task"], ["question answering", "Task"]], "rel": [["question answering", "Compare-With", "classification"]], "rel_plus": [["question answering:Task", "Compare-With", "classification:Task"]]}
{"doc_id": "204901567", "sentence": "XQuAD consists of a subset of 2 4 0 paragraphs and 1 1 9 0 question - answer pairs from the development set of SQuAD v 1 . 1 7 together with their translations into ten languages : Spanish , German , Greek , Russian , Turkish , Arabic , Vietnamese , Thai , Chinese , and Hindi .", "ner": [["XQuAD", "Dataset"], ["SQuAD v 1 . 1 7", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "204901567", "sentence": "Similar to our findings in the XNLI experiment , the vocabulary size has a large impact in JOINTMULTI , and JOINTPAIR models with disjoint vocabularies perform the best .", "ner": [["XNLI", "Dataset"], ["JOINTMULTI", "Method"], ["JOINTPAIR", "Method"]], "rel": [["JOINTMULTI", "Evaluated-With", "XNLI"], ["JOINTPAIR", "Evaluated-With", "XNLI"], ["JOINTMULTI", "Compare-With", "JOINTPAIR"]], "rel_plus": [["JOINTMULTI:Method", "Evaluated-With", "XNLI:Dataset"], ["JOINTPAIR:Method", "Evaluated-With", "XNLI:Dataset"], ["JOINTMULTI:Method", "Compare-With", "JOINTPAIR:Method"]]}
{"doc_id": "204901567", "sentence": "The gap between MONOTRANS and joint and models is larger , but MONOTRANS still performs surprisingly well given the nature Table 3 : XQuAD results ( F 1 ) . of the task .", "ner": [["MONOTRANS", "Method"], ["MONOTRANS", "Method"], ["XQuAD", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "204901567", "sentence": "We probe representations from the resulting English models with the Word in Context ( WiC ; Pilehvar and Camacho - Collados , 2 0 1 9 ) , Stanford Contextual Word Similarity ( SCWS ; Huang et al. , 2 0 1 2 ) , and the syntactic evaluation ( Marvin and Linzen , 2 0 1 8) datasets .", "ner": [["Word in Context", "Dataset"], ["WiC", "Dataset"], ["Stanford Contextual Word Similarity", "Dataset"], ["SCWS", "Dataset"], ["syntactic evaluation", "Dataset"]], "rel": [["WiC", "Synonym-Of", "Word in Context"], ["SCWS", "Synonym-Of", "Stanford Contextual Word Similarity"]], "rel_plus": [["WiC:Dataset", "Synonym-Of", "Word in Context:Dataset"], ["SCWS:Dataset", "Synonym-Of", "Stanford Contextual Word Similarity:Dataset"]]}
{"doc_id": "204901567", "sentence": "CLWE models - although similar in spirit to MONOTRANS - are only competitive on the easiest and smallest task ( MLDoc ) , and perform poorly on the more challenging ones ( XNLI and XQuAD ) .", "ner": [["CLWE", "Method"], ["MONOTRANS", "Method"], ["MLDoc", "Dataset"], ["XNLI", "Dataset"], ["XQuAD", "Dataset"]], "rel": [["CLWE", "Compare-With", "MONOTRANS"], ["CLWE", "Evaluated-With", "MLDoc"], ["CLWE", "Evaluated-With", "XNLI"], ["CLWE", "Evaluated-With", "XQuAD"]], "rel_plus": [["CLWE:Method", "Compare-With", "MONOTRANS:Method"], ["CLWE:Method", "Evaluated-With", "MLDoc:Dataset"], ["CLWE:Method", "Evaluated-With", "XNLI:Dataset"], ["CLWE:Method", "Evaluated-With", "XQuAD:Dataset"]]}
{"doc_id": "204901567", "sentence": "To provide a more comprehensive benchmark to evaluate cross - lingual models , we also released the Cross - lingual Question Answering Dataset ( XQuAD ) .   In contrast to You et al. ( 2 0 1 9 ) , we train with a sequence length of 5 1 2 from the beginning , instead of dividing training into two stages .", "ner": [["Cross - lingual Question Answering Dataset", "Dataset"], ["XQuAD", "Dataset"]], "rel": [["XQuAD", "Synonym-Of", "Cross - lingual Question Answering Dataset"]], "rel_plus": [["XQuAD:Dataset", "Synonym-Of", "Cross - lingual Question Answering Dataset:Dataset"]]}
{"doc_id": "204901567", "sentence": "XQuAD consists of a subset of 2 4 0 context paragraphs and 1 1 9 0 question - answer pairs from the development set of SQuAD v 1 . 1 ( Rajpurkar et al. , 2 0 1 6 ) together with their translations into 1 0 other languages : Spanish , German , Greek , Russian , Turkish , Arabic , Vietnamese , Thai , Chinese , and Hindi .", "ner": [["XQuAD", "Dataset"], ["SQuAD v 1 . 1", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "204901567", "sentence": "We show the complete results for cross - lingual word embedding mappings and joint multilingual training on MLDoc and PAWS - X in Table 7 .", "ner": [["cross - lingual word embedding", "Method"], ["MLDoc", "Dataset"], ["PAWS - X", "Dataset"]], "rel": [["cross - lingual word embedding", "Trained-With", "MLDoc"], ["cross - lingual word embedding", "Trained-With", "PAWS - X"]], "rel_plus": [["cross - lingual word embedding:Method", "Trained-With", "MLDoc:Dataset"], ["cross - lingual word embedding:Method", "Trained-With", "PAWS - X:Dataset"]]}
{"doc_id": "204901567", "sentence": "Table   8 reports exact match results on XQuAD , while Table 9 reports results for all cross - lingual word embedding mappings and joint multilingual training variants .", "ner": [["XQuAD", "Dataset"], ["cross - lingual word embedding", "Method"]], "rel": [["cross - lingual word embedding", "Evaluated-With", "XQuAD"]], "rel_plus": [["cross - lingual word embedding:Method", "Evaluated-With", "XQuAD:Dataset"]]}
{"doc_id": "204901567", "sentence": "To control for the amount of data , we use 3M sentences both for pretraining and alignment in every language . 1 0 Semantic probing We evaluate the representations on two semantic probing tasks , the Word in Context ( WiC ; Pilehvar and Camacho - Collados , 2 0 1 9 ) and Stanford Contextual Word Similarity ( SCWS ; Huang et al. , 2 0 1 2 ) datasets .", "ner": [["Semantic probing", "Task"], ["semantic probing", "Task"], ["Word in Context", "Dataset"], ["WiC", "Dataset"], ["Stanford Contextual Word Similarity", "Dataset"], ["SCWS", "Dataset"]], "rel": [["Word in Context", "Benchmark-For", "semantic probing"], ["Stanford Contextual Word Similarity", "Benchmark-For", "semantic probing"], ["WiC", "Synonym-Of", "Word in Context"], ["SCWS", "Synonym-Of", "Stanford Contextual Word Similarity"]], "rel_plus": [["Word in Context:Dataset", "Benchmark-For", "semantic probing:Task"], ["Stanford Contextual Word Similarity:Dataset", "Benchmark-For", "semantic probing:Task"], ["WiC:Dataset", "Synonym-Of", "Word in Context:Dataset"], ["SCWS:Dataset", "Synonym-Of", "Stanford Contextual Word Similarity:Dataset"]]}
{"doc_id": "204901567", "sentence": "WiC is a binary classification task , which requires the model to determine if the occurrences of a word in two contexts refer to the same or different meanings .", "ner": [["WiC", "Dataset"], ["binary classification", "Task"]], "rel": [["WiC", "Benchmark-For", "binary classification"]], "rel_plus": [["WiC:Dataset", "Benchmark-For", "binary classification:Task"]]}
{"doc_id": "204901567", "sentence": "SCWS requires estimating the semantic similarity of word pairs that occur in context .", "ner": [["SCWS", "Dataset"], ["semantic similarity", "Task"]], "rel": [["SCWS", "Benchmark-For", "semantic similarity"]], "rel_plus": [["SCWS:Dataset", "Benchmark-For", "semantic similarity:Task"]]}
{"doc_id": "4539700", "sentence": "The purpose of this study is to determine whether current video datasets have sufficient data for training very deep convolutional neural networks ( CNNs ) with spatio - temporal three - dimensional ( 3D ) kernels .", "ner": [["convolutional neural networks", "Method"], ["CNNs", "Method"]], "rel": [["CNNs", "Synonym-Of", "convolutional neural networks"]], "rel_plus": [["CNNs:Method", "Synonym-Of", "convolutional neural networks:Method"]]}
{"doc_id": "4539700", "sentence": "Recently , the performance levels of 3D CNNs in the field of action recognition have improved significantly .", "ner": [["3D CNNs", "Method"], ["action recognition", "Task"]], "rel": [["3D CNNs", "Used-For", "action recognition"]], "rel_plus": [["3D CNNs:Method", "Used-For", "action recognition:Task"]]}
{"doc_id": "4539700", "sentence": "Based on the results of those experiments , the following conclusions could be obtained : ( i ) ResNet - 1 8 training resulted in significant overfitting for UCF - 1 0 1 , HMDB - 5 1 , and ActivityNet but not for Kinetics . ( ii ) The Kinetics dataset has sufficient data for training of deep 3D CNNs , and enables training of up to 1 5 2 ResNets layers , interestingly similar to 2D ResNets on ImageNet .", "ner": [["ResNet - 1 8", "Method"], ["UCF - 1 0 1", "Dataset"], ["HMDB - 5 1", "Dataset"], ["ActivityNet", "Dataset"], ["Kinetics", "Dataset"], ["Kinetics", "Dataset"], ["3D CNNs", "Method"], ["ResNets", "Method"], ["2D ResNets", "Method"], ["ImageNet", "Dataset"]], "rel": [["ResNet - 1 8", "Trained-With", "UCF - 1 0 1"], ["ResNet - 1 8", "Trained-With", "HMDB - 5 1"], ["ResNet - 1 8", "Trained-With", "ActivityNet"], ["ResNet - 1 8", "Trained-With", "Kinetics"], ["3D CNNs", "Trained-With", "Kinetics"], ["ResNets", "Part-Of", "3D CNNs"], ["2D ResNets", "Trained-With", "ImageNet"]], "rel_plus": [["ResNet - 1 8:Method", "Trained-With", "UCF - 1 0 1:Dataset"], ["ResNet - 1 8:Method", "Trained-With", "HMDB - 5 1:Dataset"], ["ResNet - 1 8:Method", "Trained-With", "ActivityNet:Dataset"], ["ResNet - 1 8:Method", "Trained-With", "Kinetics:Dataset"], ["3D CNNs:Method", "Trained-With", "Kinetics:Dataset"], ["ResNets:Method", "Part-Of", "3D CNNs:Method"], ["2D ResNets:Method", "Trained-With", "ImageNet:Dataset"]]}
{"doc_id": "4539700", "sentence": "ResNeXt - 1 0 1 achieved 7 8 . 4 % average accuracy on the Kinetics test set . ( iii ) Kinetics pretrained simple 3D architectures outperforms complex 2D architectures , and the pretrained ResNeXt - 1 0 1 achieved 9 4 . 5 % and 7 0 . 2 % on UCF - 1 0 1 and HMDB - 5 1 , respectively .", "ner": [["ResNeXt - 1 0 1", "Method"], ["Kinetics test set", "Dataset"], ["Kinetics", "Dataset"], ["ResNeXt - 1 0 1", "Method"], ["UCF - 1 0 1", "Dataset"], ["HMDB - 5 1", "Dataset"]], "rel": [["ResNeXt - 1 0 1", "Evaluated-With", "Kinetics test set"], ["ResNeXt - 1 0 1", "Trained-With", "Kinetics"], ["ResNeXt - 1 0 1", "Evaluated-With", "UCF - 1 0 1"], ["ResNeXt - 1 0 1", "Evaluated-With", "HMDB - 5 1"]], "rel_plus": [["ResNeXt - 1 0 1:Method", "Evaluated-With", "Kinetics test set:Dataset"], ["ResNeXt - 1 0 1:Method", "Trained-With", "Kinetics:Dataset"], ["ResNeXt - 1 0 1:Method", "Evaluated-With", "UCF - 1 0 1:Dataset"], ["ResNeXt - 1 0 1:Method", "Evaluated-With", "HMDB - 5 1:Dataset"]]}
{"doc_id": "4539700", "sentence": "The use of 2D CNNs trained on ImageNet has produced significant progress in various tasks in image .", "ner": [["2D CNNs", "Method"], ["ImageNet", "Dataset"]], "rel": [["2D CNNs", "Trained-With", "ImageNet"]], "rel_plus": [["2D CNNs:Method", "Trained-With", "ImageNet:Dataset"]]}
{"doc_id": "4539700", "sentence": "We believe that using deep 3D CNNs together with Kinetics will retrace the successful history of 2D CNNs and ImageNet , and stimulate advances in computer vision for videos .", "ner": [["3D CNNs", "Method"], ["Kinetics", "Dataset"], ["2D CNNs", "Method"], ["ImageNet", "Dataset"], ["computer vision", "Task"]], "rel": [["3D CNNs", "Used-For", "computer vision"], ["2D CNNs", "Used-For", "computer vision"], ["Kinetics", "Benchmark-For", "computer vision"], ["ImageNet", "Benchmark-For", "computer vision"]], "rel_plus": [["3D CNNs:Method", "Used-For", "computer vision:Task"], ["2D CNNs:Method", "Used-For", "computer vision:Task"], ["Kinetics:Dataset", "Benchmark-For", "computer vision:Task"], ["ImageNet:Dataset", "Benchmark-For", "computer vision:Task"]]}
{"doc_id": "4539700", "sentence": "The codes and pretrained models used in this study are publicly available . https://github.com/kenshohara/ 3 D - ResNets - PyTorch The use of large - scale datasets is extremely important when using deep convolutional neural networks ( CNNs ) , which have massive parameter numbers , and the use of CNNs in the field of computer vision has expanded significantly in recent years .", "ner": [["3 D - ResNets", "Method"], ["convolutional neural networks", "Method"], ["CNNs", "Method"], ["CNNs", "Method"], ["computer vision", "Task"]], "rel": [["CNNs", "Synonym-Of", "convolutional neural networks"], ["CNNs", "Used-For", "computer vision"]], "rel_plus": [["CNNs:Method", "Synonym-Of", "convolutional neural networks:Method"], ["CNNs:Method", "Used-For", "computer vision:Task"]]}
{"doc_id": "4539700", "sentence": "ImageNet [ 4 ] , which includes more 1https://github.com/kenshohara/ 3 D - ResNets - PyTorch than a million images , has contributed substantially to the creation of successful vision - based algorithms .", "ner": [["ImageNet", "Dataset"], ["3 D - ResNets", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "4539700", "sentence": "In addition to such large - scale datasets , a large number of algorithms , such as residual learning [ 1 0 ] , have been used to improve image classification performance by adding increased depth to CNNs , and the use of very deep CNNs trained on Im - ageNet have facilitated the acquisition of generic feature representation .", "ner": [["residual learning", "Method"], ["image classification", "Task"], ["CNNs", "Method"], ["CNNs", "Method"], ["Im - ageNet", "Dataset"]], "rel": [["residual learning", "Used-For", "image classification"], ["residual learning", "Part-Of", "CNNs"], ["CNNs", "Trained-With", "Im - ageNet"]], "rel_plus": [["residual learning:Method", "Used-For", "image classification:Task"], ["residual learning:Method", "Part-Of", "CNNs:Method"], ["CNNs:Method", "Trained-With", "Im - ageNet:Dataset"]]}
{"doc_id": "4539700", "sentence": "Using such feature representation , in turn , has significantly improved the performance of several other tasks including object detection , semantic segmentation , and image captioning ( see top row in Figure 1 ) .", "ner": [["object detection", "Task"], ["semantic segmentation", "Task"], ["image captioning", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "4539700", "sentence": "To date , the video datasets available for action recognition have been relatively small when compared with image recognition datasets .", "ner": [["action recognition", "Task"], ["image recognition", "Task"]], "rel": [["action recognition", "Compare-With", "image recognition"]], "rel_plus": [["action recognition:Task", "Compare-With", "image recognition:Task"]]}
{"doc_id": "4539700", "sentence": "Representative video datasets , such as UCF - 1 0 1 [ 2 1 ] and HMDB - 5 1 [ 1 7 ] , can be used to provide realistic videos with sizes around 1 0 K , but even though they are still used as standard benchmarks , such datasets are obviously too small to be used for optimizing CNN representations from scratch .", "ner": [["UCF - 1 0 1", "Dataset"], ["HMDB - 5 1", "Dataset"], ["CNN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "4539700", "sentence": "In the last couple of years , ActivityNet [ 5 ] , which is a somewhat larger video dataset , has become available , and its use has make it possible to accomplish additional tasks such as untrimmed action classification and detection , but the number of action instances it contains is still limited .", "ner": [["ActivityNet", "Dataset"], ["action classification", "Task"], ["detection", "Task"]], "rel": [["ActivityNet", "Benchmark-For", "action classification"], ["ActivityNet", "Benchmark-For", "detection"]], "rel_plus": [["ActivityNet:Dataset", "Benchmark-For", "action classification:Task"], ["ActivityNet:Dataset", "Benchmark-For", "detection:Task"]]}
{"doc_id": "4539700", "sentence": "More recently , the Kinetics dataset [ 1 6 ] was created with the aim of being positioned as a de facto video dataset standard that is roughly equivalent to the position held by ImageNet in relation to image datasets .", "ner": [["Kinetics", "Dataset"], ["ImageNet", "Dataset"]], "rel": [["Kinetics", "Compare-With", "ImageNet"]], "rel_plus": [["Kinetics:Dataset", "Compare-With", "ImageNet:Dataset"]]}
{"doc_id": "4539700", "sentence": "For action recognition , CNNs with spatio - temporal threedimensional ( 3D ) convolutional kernels ( 3D CNNs ) are recently more effective than CNNs with two - dimensional ( 2D ) kernels [ 2 ] .", "ner": [["action recognition", "Task"], ["CNNs with spatio - temporal threedimensional ( 3D ) convolutional kernels", "Method"], ["3D CNNs", "Method"], ["CNNs with two - dimensional ( 2D ) kernels", "Method"]], "rel": [["CNNs with spatio - temporal threedimensional ( 3D ) convolutional kernels", "Used-For", "action recognition"], ["CNNs with two - dimensional ( 2D ) kernels", "Used-For", "action recognition"], ["3D CNNs", "Synonym-Of", "CNNs with spatio - temporal threedimensional ( 3D ) convolutional kernels"], ["CNNs with spatio - temporal threedimensional ( 3D ) convolutional kernels", "Compare-With", "CNNs with two - dimensional ( 2D ) kernels"]], "rel_plus": [["CNNs with spatio - temporal threedimensional ( 3D ) convolutional kernels:Method", "Used-For", "action recognition:Task"], ["CNNs with two - dimensional ( 2D ) kernels:Method", "Used-For", "action recognition:Task"], ["3D CNNs:Method", "Synonym-Of", "CNNs with spatio - temporal threedimensional ( 3D ) convolutional kernels:Method"], ["CNNs with spatio - temporal threedimensional ( 3D ) convolutional kernels:Method", "Compare-With", "CNNs with two - dimensional ( 2D ) kernels:Method"]]}
{"doc_id": "4539700", "sentence": "From several years ago [ 1 4 ] , 3D CNNs are explored to provide an effective tool for accurate action recognition .", "ner": [["3D CNNs", "Method"], ["action recognition", "Task"]], "rel": [["3D CNNs", "Used-For", "action recognition"]], "rel_plus": [["3D CNNs:Method", "Used-For", "action recognition:Task"]]}
{"doc_id": "4539700", "sentence": "The primary reason for this failure has been the relatively small data - scale of video datasets that are available for optimizing the immense number of parameters in 3D CNNs , which are much larger than those of 2D CNNs .", "ner": [["3D CNNs", "Method"], ["2D CNNs", "Method"]], "rel": [["3D CNNs", "Compare-With", "2D CNNs"]], "rel_plus": [["3D CNNs:Method", "Compare-With", "2D CNNs:Method"]]}
{"doc_id": "4539700", "sentence": "In addition , basically , 3D CNNs can only be trained on video datasets whereas 2D CNNs can be pretrained on ImageNet .", "ner": [["3D CNNs", "Method"], ["2D CNNs", "Method"], ["ImageNet", "Dataset"]], "rel": [["2D CNNs", "Trained-With", "ImageNet"]], "rel_plus": [["2D CNNs:Method", "Trained-With", "ImageNet:Dataset"]]}
{"doc_id": "4539700", "sentence": "Recently , however , Carreira and Zisserman achieved a significant breakthrough using the Kinetics dataset as well as the inflation of 2D kernels pretrained on ImageNet into 3D ones [ 2 ] .", "ner": [["Kinetics", "Dataset"], ["ImageNet", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "4539700", "sentence": "Thus , we now have the benefit of a sophisticated 3D convolution that can be engaged by the Kinetics dataset .", "ner": [["3D convolution", "Method"], ["Kinetics", "Dataset"]], "rel": [["Kinetics", "Used-For", "3D convolution"]], "rel_plus": [["Kinetics:Dataset", "Used-For", "3D convolution:Method"]]}
{"doc_id": "4539700", "sentence": "However , can 3D CNNs retrace the successful history of 2D CNNs and ImageNet ?", "ner": [["3D CNNs", "Method"], ["2D CNNs", "Method"], ["ImageNet", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "4539700", "sentence": "More specifically , can the use of 3D CNNs trained on Kinetics produces significant progress in action recognition and other various tasks ? ( See bottom row in Figure 1 . ) To achieve such progress , we consider that Kinetics for 3D CNNs should be as large - scale as Ima - geNet for 2D CNNs , though no previous work has examined enough about the scale of Kinetics .", "ner": [["3D CNNs", "Method"], ["Kinetics", "Dataset"], ["action recognition", "Task"], ["Kinetics", "Dataset"], ["3D CNNs", "Method"], ["Ima - geNet", "Dataset"], ["2D CNNs", "Method"], ["Kinetics", "Dataset"]], "rel": [["3D CNNs", "Trained-With", "Kinetics"], ["3D CNNs", "Used-For", "action recognition"], ["Kinetics", "Benchmark-For", "action recognition"], ["Kinetics", "Used-For", "3D CNNs"], ["Ima - geNet", "Used-For", "2D CNNs"]], "rel_plus": [["3D CNNs:Method", "Trained-With", "Kinetics:Dataset"], ["3D CNNs:Method", "Used-For", "action recognition:Task"], ["Kinetics:Dataset", "Benchmark-For", "action recognition:Task"], ["Kinetics:Dataset", "Used-For", "3D CNNs:Method"], ["Ima - geNet:Dataset", "Used-For", "2D CNNs:Method"]]}
{"doc_id": "4539700", "sentence": "Conventional 3D CNN architectures trained on Kinetics are still relatively shallow ( 1 0 [ 1 6 ] , 2 2 [ 2 ] , and 3 4 [ 9 , 2 4 ] layers ) .", "ner": [["3D CNN", "Method"], ["Kinetics", "Dataset"]], "rel": [["3D CNN", "Trained-With", "Kinetics"]], "rel_plus": [["3D CNN:Method", "Trained-With", "Kinetics:Dataset"]]}
{"doc_id": "4539700", "sentence": "If using the Kinetics dataset enables very deep 3D CNNs at a level similar to ImageNet , which can train 1 5 2 - layer 2D CNNs [ 1 0 ] , that question could be answered in the affirmative .", "ner": [["Kinetics", "Dataset"], ["3D CNNs", "Method"], ["ImageNet", "Dataset"], ["2D CNNs", "Method"]], "rel": [["3D CNNs", "Trained-With", "Kinetics"], ["2D CNNs", "Trained-With", "ImageNet"]], "rel_plus": [["3D CNNs:Method", "Trained-With", "Kinetics:Dataset"], ["2D CNNs:Method", "Trained-With", "ImageNet:Dataset"]]}
{"doc_id": "4539700", "sentence": "In this study , we examine various 3D CNN architectures from relatively shallow to very deep ones using the Kinetics and other popular video datasets ( UCF - 1 0 1 , HMDB - 5 1 , and ActivityNet ) in order to provide us insights for answering the above question .", "ner": [["3D CNN", "Method"], ["Kinetics", "Dataset"], ["UCF - 1 0 1", "Dataset"], ["HMDB - 5 1", "Dataset"], ["ActivityNet", "Dataset"]], "rel": [["3D CNN", "Evaluated-With", "Kinetics"], ["3D CNN", "Evaluated-With", "UCF - 1 0 1"], ["3D CNN", "Evaluated-With", "HMDB - 5 1"], ["3D CNN", "Evaluated-With", "ActivityNet"]], "rel_plus": [["3D CNN:Method", "Evaluated-With", "Kinetics:Dataset"], ["3D CNN:Method", "Evaluated-With", "UCF - 1 0 1:Dataset"], ["3D CNN:Method", "Evaluated-With", "HMDB - 5 1:Dataset"], ["3D CNN:Method", "Evaluated-With", "ActivityNet:Dataset"]]}
{"doc_id": "4539700", "sentence": "The 3D CNN architectures tested in this study are based on residual networks ( ResNets ) [ 1 0 ] and their extended versions [ 1 1 , 1 2 , 3 0 , 3 1 ] as their fine - tuning .", "ner": [["3D CNN", "Method"], ["residual networks", "Method"], ["ResNets", "Method"]], "rel": [["residual networks", "Part-Of", "3D CNN"], ["ResNets", "Synonym-Of", "residual networks"]], "rel_plus": [["residual networks:Method", "Part-Of", "3D CNN:Method"], ["ResNets:Method", "Synonym-Of", "residual networks:Method"]]}
{"doc_id": "4539700", "sentence": "The results of those experiments ( see Section 4 for details ) show the Kinetics dataset can train 3D ResNet - 1 5 2 from scratch to a level that is similar to the training accomplished by 2D ResNets on ImageNet , as shown in Figure 2 .", "ner": [["Kinetics", "Dataset"], ["3D ResNet - 1 5 2", "Method"], ["2D ResNets", "Method"], ["ImageNet", "Dataset"]], "rel": [["3D ResNet - 1 5 2", "Trained-With", "Kinetics"], ["2D ResNets", "Trained-With", "ImageNet"]], "rel_plus": [["3D ResNet - 1 5 2:Method", "Trained-With", "Kinetics:Dataset"], ["2D ResNets:Method", "Trained-With", "ImageNet:Dataset"]]}
{"doc_id": "4539700", "sentence": "To our best knowledge , this is the first work to focus on the training of very deep 3D CNNs from scratch for action recognition .", "ner": [["3D CNNs", "Method"], ["action recognition", "Task"]], "rel": [["3D CNNs", "Used-For", "action recognition"]], "rel_plus": [["3D CNNs:Method", "Used-For", "action recognition:Task"]]}
{"doc_id": "4539700", "sentence": "Previous studies showed deeper 2D CNNs trained on ImageNet achieved better performance [ 1 0 ] .", "ner": [["2D CNNs", "Method"], ["ImageNet", "Dataset"]], "rel": [["2D CNNs", "Trained-With", "ImageNet"]], "rel_plus": [["2D CNNs:Method", "Trained-With", "ImageNet:Dataset"]]}
{"doc_id": "4539700", "sentence": "The results of this study , which indicate deeper 3D CNNs are more effective , can be expected to facilitate further progress in computer vision for videos .   The HMDB - 5 1 [ 1 7 ] and UCF - 1 0 1 [ 2 1 ] datasets are currently the most successful in the field of action recognition .", "ner": [["3D CNNs", "Method"], ["computer vision", "Task"], ["HMDB - 5 1", "Dataset"], ["UCF - 1 0 1", "Dataset"], ["action recognition", "Task"]], "rel": [["3D CNNs", "Used-For", "computer vision"], ["HMDB - 5 1", "Benchmark-For", "action recognition"], ["UCF - 1 0 1", "Benchmark-For", "action recognition"]], "rel_plus": [["3D CNNs:Method", "Used-For", "computer vision:Task"], ["HMDB - 5 1:Dataset", "Benchmark-For", "action recognition:Task"], ["UCF - 1 0 1:Dataset", "Benchmark-For", "action recognition:Task"]]}
{"doc_id": "4539700", "sentence": "ActivityNet also provides some additional tasks , such as untrimmed classification and detection , but the number of action instances is still on the order of tens of thousands .", "ner": [["ActivityNet", "Dataset"], ["classification", "Task"], ["detection", "Task"]], "rel": [["ActivityNet", "Benchmark-For", "classification"], ["ActivityNet", "Benchmark-For", "detection"]], "rel_plus": [["ActivityNet:Dataset", "Benchmark-For", "classification:Task"], ["ActivityNet:Dataset", "Benchmark-For", "detection:Task"]]}
{"doc_id": "4539700", "sentence": "In order to determine whether it can train deeper 3D CNNs , we performed a number of experiments using these recent datasets , as well as the UCF - 1 0 1 and HMDB - 5 1 datasets .", "ner": [["3D CNNs", "Method"], ["UCF - 1 0 1", "Dataset"], ["HMDB - 5 1", "Dataset"]], "rel": [["3D CNNs", "Trained-With", "UCF - 1 0 1"], ["3D CNNs", "Trained-With", "HMDB - 5 1"]], "rel_plus": [["3D CNNs:Method", "Trained-With", "UCF - 1 0 1:Dataset"], ["3D CNNs:Method", "Trained-With", "HMDB - 5 1:Dataset"]]}
{"doc_id": "4539700", "sentence": "Other huge datasets such as Sports - 1 M [ 1 5 ] and YouTube - 8 M [ 1 ] have been proposed .", "ner": [["Sports - 1 M", "Dataset"], ["YouTube - 8 M", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "4539700", "sentence": "One of the popular approaches to CNN - based action recognition is the use of two - stream CNNs with 2D convolutional kernels .", "ner": [["CNN", "Method"], ["action recognition", "Task"], ["CNNs", "Method"], ["2D convolutional kernels", "Method"]], "rel": [["CNN", "Used-For", "action recognition"], ["2D convolutional kernels", "Part-Of", "CNNs"]], "rel_plus": [["CNN:Method", "Used-For", "action recognition:Task"], ["2D convolutional kernels:Method", "Part-Of", "CNNs:Method"]]}
{"doc_id": "4539700", "sentence": "Since that study , numerous methods based on the two - stream CNNs have been proposed to improve action recognition performance [ 6 , 7 , 8 , 2 7 , 2 8 , 2 9 ] .", "ner": [["CNNs", "Method"], ["action recognition", "Task"]], "rel": [["CNNs", "Used-For", "action recognition"]], "rel_plus": [["CNNs:Method", "Used-For", "action recognition:Task"]]}
{"doc_id": "4539700", "sentence": "Unlike the abovementioned approaches , we focused on CNNs with 3D convolutional kernels , which have recently begun to outperform 2D CNNs through the use of large - scale video datasets .", "ner": [["CNNs with 3D convolutional kernels", "Method"], ["2D CNNs", "Method"]], "rel": [["CNNs with 3D convolutional kernels", "Compare-With", "2D CNNs"]], "rel_plus": [["CNNs with 3D convolutional kernels:Method", "Compare-With", "2D CNNs:Method"]]}
{"doc_id": "4539700", "sentence": "These 3D CNNs are intuitively effective because such 3D convolution can be used to directly extract spatio - temporal features from raw videos .", "ner": [["3D CNNs", "Method"], ["3D convolution", "Method"]], "rel": [["3D convolution", "Compare-With", "3D CNNs"]], "rel_plus": [["3D convolution:Method", "Compare-With", "3D CNNs:Method"]]}
{"doc_id": "4539700", "sentence": "For example , Ji et al. proposed applying 3D convolution to extract spatiotemporal features from videos , while Tran et al. trained 3D CNNs , which they referred to as C 3 D , using the Sports - 1 M dataset [ 1 5 ] .", "ner": [["3D convolution", "Method"], ["extract spatiotemporal features from videos", "Task"], ["3D CNNs", "Method"], ["C 3 D", "Method"], ["Sports - 1 M", "Dataset"]], "rel": [["3D convolution", "Used-For", "extract spatiotemporal features from videos"], ["C 3 D", "Synonym-Of", "3D CNNs"], ["3D CNNs", "Trained-With", "Sports - 1 M"]], "rel_plus": [["3D convolution:Method", "Used-For", "extract spatiotemporal features from videos:Task"], ["C 3 D:Method", "Synonym-Of", "3D CNNs:Method"], ["3D CNNs:Method", "Trained-With", "Sports - 1 M:Dataset"]]}
{"doc_id": "4539700", "sentence": "Since that study , C 3 D has been seen as a de facto standard for 3D CNNs .", "ner": [["C 3 D", "Method"], ["3D CNNs", "Method"]], "rel": [["C 3 D", "SubClass-Of", "3D CNNs"]], "rel_plus": [["C 3 D:Method", "SubClass-Of", "3D CNNs:Method"]]}
{"doc_id": "4539700", "sentence": "Meanwhile , Kay et al. showed that the results of 3D CNNs trained from scratch on their Kinetics dataset were comparable with the results of 2D CNNs pretrained on ImageNet , even though the results of 3D CNNs trained on the UCF 1 0 1 and HMDB 5 1 datasets were inferior to the 2D CNNs results .", "ner": [["3D CNNs", "Method"], ["Kinetics", "Dataset"], ["2D CNNs", "Method"], ["ImageNet", "Dataset"], ["3D CNNs", "Method"], ["UCF 1 0 1", "Dataset"], ["HMDB 5 1", "Dataset"], ["2D CNNs", "Method"]], "rel": [["3D CNNs", "Evaluated-With", "Kinetics"], ["2D CNNs", "Trained-With", "ImageNet"], ["3D CNNs", "Trained-With", "UCF 1 0 1"], ["3D CNNs", "Trained-With", "HMDB 5 1"], ["3D CNNs", "Compare-With", "2D CNNs"]], "rel_plus": [["3D CNNs:Method", "Evaluated-With", "Kinetics:Dataset"], ["2D CNNs:Method", "Trained-With", "ImageNet:Dataset"], ["3D CNNs:Method", "Trained-With", "UCF 1 0 1:Dataset"], ["3D CNNs:Method", "Trained-With", "HMDB 5 1:Dataset"], ["3D CNNs:Method", "Compare-With", "2D CNNs:Method"]]}
{"doc_id": "4539700", "sentence": "In still another study , Carreira et al. proposed inception [ 2 2 ] based 3D CNNs , which they referred to as I 3 D , and achieved state - of - the - art performance [ 2 ] .", "ner": [["inception [ 2 2 ] based 3D CNNs", "Method"], ["I 3 D", "Method"]], "rel": [["I 3 D", "Synonym-Of", "inception [ 2 2 ] based 3D CNNs"]], "rel_plus": [["I 3 D:Method", "Synonym-Of", "inception [ 2 2 ] based 3D CNNs:Method"]]}
{"doc_id": "4539700", "sentence": "More recently , some works introduced ResNet architectures into 3D CNNs [ 9 , 2 4 ] , though they examined only relatively shallow ones .   In this study , in order to determine whether current video datasets have sufficient data for training of deep 3D CNNs , we conducted the three experiments described below using UCF - 1 0 1 [ 2 1 ] , HMDB - 5 1 [ 1 7 ] , ActivityNet [ 5 ] , and Kinetics [ 1 6 ] .", "ner": [["ResNet", "Method"], ["3D CNNs", "Method"], ["3D CNNs", "Method"], ["UCF - 1 0 1", "Dataset"], ["HMDB - 5 1", "Dataset"], ["ActivityNet", "Dataset"], ["Kinetics", "Dataset"]], "rel": [["ResNet", "Part-Of", "3D CNNs"], ["3D CNNs", "Evaluated-With", "UCF - 1 0 1"], ["3D CNNs", "Evaluated-With", "HMDB - 5 1"], ["3D CNNs", "Evaluated-With", "ActivityNet"]], "rel_plus": [["ResNet:Method", "Part-Of", "3D CNNs:Method"], ["3D CNNs:Method", "Evaluated-With", "UCF - 1 0 1:Dataset"], ["3D CNNs:Method", "Evaluated-With", "HMDB - 5 1:Dataset"], ["3D CNNs:Method", "Evaluated-With", "ActivityNet:Dataset"]]}
{"doc_id": "4539700", "sentence": "According to previous works [ 9 , 1 6 ] , 3D CNNs trained on UCF - 1 0 1 , HMDB - 5 1 , and ActivityNet do not achieve high accuracy whereas ones trained on Kinetics work well .", "ner": [["3D CNNs", "Method"], ["UCF - 1 0 1", "Dataset"], ["HMDB - 5 1", "Dataset"], ["ActivityNet", "Dataset"], ["Kinetics", "Dataset"]], "rel": [["3D CNNs", "Trained-With", "UCF - 1 0 1"], ["3D CNNs", "Trained-With", "HMDB - 5 1"], ["3D CNNs", "Trained-With", "ActivityNet"], ["3D CNNs", "Trained-With", "Kinetics"], ["UCF - 1 0 1", "Compare-With", "Kinetics"], ["HMDB - 5 1", "Compare-With", "Kinetics"], ["ActivityNet", "Compare-With", "Kinetics"]], "rel_plus": [["3D CNNs:Method", "Trained-With", "UCF - 1 0 1:Dataset"], ["3D CNNs:Method", "Trained-With", "HMDB - 5 1:Dataset"], ["3D CNNs:Method", "Trained-With", "ActivityNet:Dataset"], ["3D CNNs:Method", "Trained-With", "Kinetics:Dataset"], ["UCF - 1 0 1:Dataset", "Compare-With", "Kinetics:Dataset"], ["HMDB - 5 1:Dataset", "Compare-With", "Kinetics:Dataset"], ["ActivityNet:Dataset", "Compare-With", "Kinetics:Dataset"]]}
{"doc_id": "4539700", "sentence": "Specifically , we used ResNet - 1 8 , which is the shallowest ResNet architecture , based on the assumption that if the ResNet - 1 8 overfits when being trained on a dataset , that dataset is too small to be used for training deep 3D CNNs from scratch .", "ner": [["ResNet - 1 8", "Method"], ["ResNet", "Method"], ["ResNet - 1 8", "Method"], ["3D CNNs", "Method"]], "rel": [["ResNet - 1 8", "SubClass-Of", "ResNet"], ["ResNet - 1 8", "Used-For", "3D CNNs"]], "rel_plus": [["ResNet - 1 8:Method", "SubClass-Of", "ResNet:Method"], ["ResNet - 1 8:Method", "Used-For", "3D CNNs:Method"]]}
{"doc_id": "4539700", "sentence": "We then conducted a separate experiment to determine whether the Kinetics dataset could train deeper 3D CNNs .", "ner": [["Kinetics", "Dataset"], ["3D CNNs", "Method"]], "rel": [["3D CNNs", "Trained-With", "Kinetics"]], "rel_plus": [["3D CNNs:Method", "Trained-With", "Kinetics:Dataset"]]}
{"doc_id": "4539700", "sentence": "Therefore , we trained 3D ResNets on Kinetics while varying the model depth from 1 8 to 2 0 0 .", "ner": [["3D ResNets", "Method"], ["Kinetics", "Dataset"]], "rel": [["3D ResNets", "Trained-With", "Kinetics"]], "rel_plus": [["3D ResNets:Method", "Trained-With", "Kinetics:Dataset"]]}
{"doc_id": "4539700", "sentence": "If Kinetics can train very deep CNNs , such as ResNet - 1 5 2 , which achieved the best performance in ResNets on ImageNet [ 1 0 ] , we can be confident that they have sufficient data to train 3D CNNs .", "ner": [["Kinetics", "Dataset"], ["CNNs", "Method"], ["ResNet - 1 5 2", "Method"], ["ResNets", "Method"], ["ImageNet", "Dataset"], ["3D CNNs", "Method"]], "rel": [["CNNs", "Trained-With", "Kinetics"], ["ResNet - 1 5 2", "SubClass-Of", "CNNs"], ["ResNets", "Evaluated-With", "ImageNet"], ["ResNet - 1 5 2", "Evaluated-With", "ImageNet"]], "rel_plus": [["CNNs:Method", "Trained-With", "Kinetics:Dataset"], ["ResNet - 1 5 2:Method", "SubClass-Of", "CNNs:Method"], ["ResNets:Method", "Evaluated-With", "ImageNet:Dataset"], ["ResNet - 1 5 2:Method", "Evaluated-With", "ImageNet:Dataset"]]}
{"doc_id": "4539700", "sentence": "In the final experiment , we examined the fine - tuning of Kinetics pretrained 3D CNNs on UCF - 1 0 1 and HMDB - 5 1 .", "ner": [["Kinetics", "Dataset"], ["3D CNNs", "Method"], ["UCF - 1 0 1", "Dataset"], ["HMDB - 5 1", "Dataset"]], "rel": [["3D CNNs", "Trained-With", "Kinetics"], ["3D CNNs", "Evaluated-With", "UCF - 1 0 1"], ["3D CNNs", "Evaluated-With", "HMDB - 5 1"]], "rel_plus": [["3D CNNs:Method", "Trained-With", "Kinetics:Dataset"], ["3D CNNs:Method", "Evaluated-With", "UCF - 1 0 1:Dataset"], ["3D CNNs:Method", "Evaluated-With", "HMDB - 5 1:Dataset"]]}
{"doc_id": "4539700", "sentence": "Since pretraining on large - scale datasets is an effective way to achieve good performance levels on small datasets , we expect that the deep 3D ResNets pretrained on Kinetics would perform well on relatively small UCF - 1 0 1 and HMDB - 5 1 .", "ner": [["3D ResNets", "Method"], ["Kinetics", "Dataset"], ["UCF - 1 0 1", "Dataset"], ["HMDB - 5 1", "Dataset"]], "rel": [["3D ResNets", "Trained-With", "Kinetics"], ["3D ResNets", "Evaluated-With", "UCF - 1 0 1"], ["3D ResNets", "Evaluated-With", "HMDB - 5 1"]], "rel_plus": [["3D ResNets:Method", "Trained-With", "Kinetics:Dataset"], ["3D ResNets:Method", "Evaluated-With", "UCF - 1 0 1:Dataset"], ["3D ResNets:Method", "Evaluated-With", "HMDB - 5 1:Dataset"]]}
{"doc_id": "4539700", "sentence": "BN refers to batch normalization [ 1 3 ] .", "ner": [["BN", "Method"], ["batch normalization", "Method"]], "rel": [["BN", "Synonym-Of", "batch normalization"]], "rel_plus": [["BN:Method", "Synonym-Of", "batch normalization:Method"]]}
{"doc_id": "4539700", "sentence": "Each convolutional layer is followed by batch normalization [ 1 3 ] and a ReLU [ 1 8 ] .", "ner": [["convolutional layer", "Method"], ["batch normalization", "Method"], ["ReLU", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "4539700", "sentence": "DenseNet down - samples inputs using the transition layer , that consists of a 3 \u00d7 3 \u00d7 3 convolutional layer and a 2 \u00d7 2 \u00d7 2 average pooling layer with a stride of two , after conv 2 _x , conv 3 _x , and conv 4 _x .", "ner": [["DenseNet", "Method"], ["3 \u00d7 3 \u00d7 3 convolutional layer", "Method"], ["2 \u00d7 2 \u00d7 2 average pooling layer", "Method"]], "rel": [["3 \u00d7 3 \u00d7 3 convolutional layer", "Part-Of", "DenseNet"], ["2 \u00d7 2 \u00d7 2 average pooling layer", "Part-Of", "DenseNet"]], "rel_plus": [["3 \u00d7 3 \u00d7 3 convolutional layer:Method", "Part-Of", "DenseNet:Method"], ["2 \u00d7 2 \u00d7 2 average pooling layer:Method", "Part-Of", "DenseNet:Method"]]}
{"doc_id": "4539700", "sentence": "Block conv 1 conv 2 _x conv 3 _x conv 4 _x conv 5 _x   Next , we explain the various ResNet - based architectures with 3D convolutions used in this study .", "ner": [["ResNet", "Method"], ["3D convolutions", "Method"]], "rel": [["3D convolutions", "Part-Of", "ResNet"]], "rel_plus": [["3D convolutions:Method", "Part-Of", "ResNet:Method"]]}
{"doc_id": "4539700", "sentence": "ResNet , which is one of the most successful architectures in image classification , provides shortcut connections that allow a signal to bypass one layer and move to the next layer in the sequence .", "ner": [["ResNet", "Method"], ["image classification", "Task"]], "rel": [["ResNet", "Used-For", "image classification"]], "rel_plus": [["ResNet:Method", "Used-For", "image classification:Task"]]}
{"doc_id": "4539700", "sentence": "Unlike previous studies that examined only limited 3D ResNet architectures [ 9 , 2 4 ] , we examine not only deeper architectures but also some extended versions of ResNet .", "ner": [["3D ResNet", "Method"], ["ResNet", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "4539700", "sentence": "In particular , we explore the following architectures : ResNet ( basic and bottleneck blocks ) [ 1 0 ] , pre - activation ResNet [ 1 1 ] , wide ResNet ( WRN ) [ 3 1 ] , ResNeXt [ 3 0 ] , and DenseNet [ 1 2 ] .", "ner": [["ResNet", "Method"], ["pre - activation ResNet", "Method"], ["wide ResNet", "Method"], ["WRN", "Method"], ["ResNeXt", "Method"], ["DenseNet", "Method"]], "rel": [["WRN", "Synonym-Of", "wide ResNet"]], "rel_plus": [["WRN:Method", "Synonym-Of", "wide ResNet:Method"]]}
{"doc_id": "4539700", "sentence": "A basic ResNets block consists of two convolutional layers , and each convolutional layer is followed by batch normalization and a ReLU .", "ner": [["ResNets", "Method"], ["convolutional layers", "Method"], ["convolutional layer", "Method"], ["batch normalization", "Method"], ["ReLU", "Method"]], "rel": [["convolutional layers", "Part-Of", "ResNets"], ["batch normalization", "Part-Of", "ResNets"], ["ReLU", "Part-Of", "ResNets"]], "rel_plus": [["convolutional layers:Method", "Part-Of", "ResNets:Method"], ["batch normalization:Method", "Part-Of", "ResNets:Method"], ["ReLU:Method", "Part-Of", "ResNets:Method"]]}
{"doc_id": "4539700", "sentence": "The pre - activation ResNet is similar to bottleneck ResNet architectures , but there are differences in the convolution , batch normalization , and ReLU order .", "ner": [["pre - activation ResNet", "Method"], ["bottleneck ResNet", "Method"], ["convolution", "Method"], ["batch normalization", "Method"], ["ReLU", "Method"]], "rel": [["ReLU", "Part-Of", "pre - activation ResNet"], ["batch normalization", "Part-Of", "pre - activation ResNet"], ["convolution", "Part-Of", "pre - activation ResNet"], ["pre - activation ResNet", "Compare-With", "bottleneck ResNet"], ["convolution", "Part-Of", "bottleneck ResNet"], ["batch normalization", "Part-Of", "bottleneck ResNet"], ["ReLU", "Part-Of", "bottleneck ResNet"]], "rel_plus": [["ReLU:Method", "Part-Of", "pre - activation ResNet:Method"], ["batch normalization:Method", "Part-Of", "pre - activation ResNet:Method"], ["convolution:Method", "Part-Of", "pre - activation ResNet:Method"], ["pre - activation ResNet:Method", "Compare-With", "bottleneck ResNet:Method"], ["convolution:Method", "Part-Of", "bottleneck ResNet:Method"], ["batch normalization:Method", "Part-Of", "bottleneck ResNet:Method"], ["ReLU:Method", "Part-Of", "bottleneck ResNet:Method"]]}
{"doc_id": "4539700", "sentence": "In ResNet , each convolutional layer is followed by batch normalization and a ReLU , whereas each batch normalization of the preactivation ResNet is followed by the ReLU and a convolutional layer .", "ner": [["ResNet", "Method"], ["convolutional layer", "Method"], ["batch normalization", "Method"], ["ReLU", "Method"], ["batch normalization", "Method"], ["preactivation ResNet", "Method"], ["ReLU", "Method"], ["convolutional layer", "Method"]], "rel": [["convolutional layer", "Part-Of", "ResNet"], ["batch normalization", "Part-Of", "ResNet"], ["ReLU", "Part-Of", "ResNet"], ["batch normalization", "Part-Of", "preactivation ResNet"], ["ReLU", "Part-Of", "preactivation ResNet"], ["convolutional layer", "Part-Of", "preactivation ResNet"], ["ResNet", "Compare-With", "preactivation ResNet"]], "rel_plus": [["convolutional layer:Method", "Part-Of", "ResNet:Method"], ["batch normalization:Method", "Part-Of", "ResNet:Method"], ["ReLU:Method", "Part-Of", "ResNet:Method"], ["batch normalization:Method", "Part-Of", "preactivation ResNet:Method"], ["ReLU:Method", "Part-Of", "preactivation ResNet:Method"], ["convolutional layer:Method", "Part-Of", "preactivation ResNet:Method"], ["ResNet:Method", "Compare-With", "preactivation ResNet:Method"]]}
{"doc_id": "4539700", "sentence": "The WRN architecture is the same as the ResNet ( bottleneck ) , but there are differences in the number of feature maps for each convolutional layer .", "ner": [["WRN", "Method"], ["ResNet ( bottleneck )", "Method"], ["convolutional layer", "Method"]], "rel": [["convolutional layer", "Part-Of", "WRN"], ["WRN", "Compare-With", "ResNet ( bottleneck )"], ["convolutional layer", "Part-Of", "ResNet ( bottleneck )"]], "rel_plus": [["convolutional layer:Method", "Part-Of", "WRN:Method"], ["WRN:Method", "Compare-With", "ResNet ( bottleneck ):Method"], ["convolutional layer:Method", "Part-Of", "ResNet ( bottleneck ):Method"]]}
{"doc_id": "4539700", "sentence": "Unlike the original bottleneck block , the ResNeXt block introduces group convolutions , which divide the feature maps into small groups .", "ner": [["bottleneck block", "Method"], ["ResNeXt block", "Method"], ["group convolutions", "Method"]], "rel": [["ResNeXt block", "Compare-With", "bottleneck block"], ["group convolutions", "Part-Of", "ResNeXt block"]], "rel_plus": [["ResNeXt block:Method", "Compare-With", "bottleneck block:Method"], ["group convolutions:Method", "Part-Of", "ResNeXt block:Method"]]}
{"doc_id": "4539700", "sentence": "DenseNet makes connections from early layers to later layers by the use of a concatenation that is different from the ResNets summation .", "ner": [["DenseNet", "Method"], ["ResNets", "Method"]], "rel": [["DenseNet", "Compare-With", "ResNets"]], "rel_plus": [["DenseNet:Method", "Compare-With", "ResNets:Method"]]}
{"doc_id": "4539700", "sentence": "We use stochastic gradient descent with momentum to train the networks and randomly generate training samples from videos in training data in order to perform data augmentation .", "ner": [["stochastic gradient descent", "Method"], ["momentum", "Method"], ["data augmentation", "Method"]], "rel": [["momentum", "Part-Of", "stochastic gradient descent"]], "rel_plus": [["momentum:Method", "Part-Of", "stochastic gradient descent:Method"]]}
{"doc_id": "4539700", "sentence": "As stated above , this study focuses on four datasets : UCF - 1 0 1 [ 2 1 ] , HMDB - 5 1 [ 1 7 ] , ActivityNet [ 5 ] , and Kinetics [ 1 6 ] .", "ner": [["UCF - 1 0 1", "Dataset"], ["HMDB - 5 1", "Dataset"], ["ActivityNet", "Dataset"], ["Kinetics", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "4539700", "sentence": "Similar to UCF - 1 0 1 , the videos were temporally Unlike the other datasets , ActivityNet consists of untrimmed videos , which include non - action frames .", "ner": [["UCF - 1 0 1", "Dataset"], ["ActivityNet", "Dataset"]], "rel": [["UCF - 1 0 1", "Compare-With", "ActivityNet"]], "rel_plus": [["UCF - 1 0 1:Dataset", "Compare-With", "ActivityNet:Dataset"]]}
{"doc_id": "4539700", "sentence": "According to previous works [ 9 , 1 6 ] , 3D CNNs trained on UCF - 1 0 1 , HMDB - 5 1 , and ActivityNet do not achieve high accuracy whereas ones trained on Kinetics work well .", "ner": [["3D CNNs", "Method"], ["UCF - 1 0 1", "Dataset"], ["HMDB - 5 1", "Dataset"], ["ActivityNet", "Dataset"], ["Kinetics", "Dataset"]], "rel": [["3D CNNs", "Trained-With", "UCF - 1 0 1"], ["3D CNNs", "Trained-With", "HMDB - 5 1"], ["3D CNNs", "Trained-With", "ActivityNet"], ["3D CNNs", "Trained-With", "Kinetics"], ["UCF - 1 0 1", "Compare-With", "Kinetics"], ["HMDB - 5 1", "Compare-With", "Kinetics"], ["ActivityNet", "Compare-With", "Kinetics"]], "rel_plus": [["3D CNNs:Method", "Trained-With", "UCF - 1 0 1:Dataset"], ["3D CNNs:Method", "Trained-With", "HMDB - 5 1:Dataset"], ["3D CNNs:Method", "Trained-With", "ActivityNet:Dataset"], ["3D CNNs:Method", "Trained-With", "Kinetics:Dataset"], ["UCF - 1 0 1:Dataset", "Compare-With", "Kinetics:Dataset"], ["HMDB - 5 1:Dataset", "Compare-With", "Kinetics:Dataset"], ["ActivityNet:Dataset", "Compare-With", "Kinetics:Dataset"]]}
{"doc_id": "4539700", "sentence": "In this process , we used split 1 of UCF - 1 0 1 and HMDB - 5 1 , and the training and validation sets of ActivityNet and Kinetics .", "ner": [["UCF - 1 0 1", "Dataset"], ["HMDB - 5 1", "Dataset"], ["ActivityNet", "Dataset"], ["Kinetics", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "4539700", "sentence": "As can be seen in the figure , the validation losses on UCF - 1 0 1 , HMDB - 5 1 , and ActivityNet quickly converged to high values and were clearly higher than their corresponding training losses .", "ner": [["UCF - 1 0 1", "Dataset"], ["HMDB - 5 1", "Dataset"], ["ActivityNet", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "4539700", "sentence": "The validation accuracies of UCF - 1 0 1 , HMDB - 5 1 , and ActivityNet are 4 0 . 1 , 1 6 . 2 , and 2 6 . 8 % , respectively .", "ner": [["UCF - 1 0 1", "Dataset"], ["HMDB - 5 1", "Dataset"], ["ActivityNet", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "4539700", "sentence": "However , since these accuracies are very low even compared with earlier methods [ 5 , 2 6 ] , our results indicate that it is difficult to train deep 3D CNNs from scratch on UCF - 1 0 1 , HMDB - 5 1 , and ActivityNet .", "ner": [["3D CNNs", "Method"], ["UCF - 1 0 1", "Dataset"], ["HMDB - 5 1", "Dataset"], ["ActivityNet", "Dataset"]], "rel": [["3D CNNs", "Trained-With", "UCF - 1 0 1"], ["3D CNNs", "Trained-With", "HMDB - 5 1"], ["3D CNNs", "Trained-With", "ActivityNet"]], "rel_plus": [["3D CNNs:Method", "Trained-With", "UCF - 1 0 1:Dataset"], ["3D CNNs:Method", "Trained-With", "HMDB - 5 1:Dataset"], ["3D CNNs:Method", "Trained-With", "ActivityNet:Dataset"]]}
{"doc_id": "4539700", "sentence": "Since the validation losses were slightly higher than the training losses , we could conclude that training ResNet - 1 8 on Kinetics did not result in overfitting , and that it is possible for Kinetics to train deep 3D CNNs .", "ner": [["ResNet - 1 8", "Method"], ["Kinetics", "Dataset"], ["Kinetics", "Dataset"], ["3D CNNs", "Method"]], "rel": [["ResNet - 1 8", "Trained-With", "Kinetics"], ["3D CNNs", "Trained-With", "Kinetics"]], "rel_plus": [["ResNet - 1 8:Method", "Trained-With", "Kinetics:Dataset"], ["3D CNNs:Method", "Trained-With", "Kinetics:Dataset"]]}
{"doc_id": "4539700", "sentence": "In the next section , we will further investigate deeper 3D CNNs on Kinetics .", "ner": [["3D CNNs", "Method"], ["Kinetics", "Dataset"]], "rel": [["3D CNNs", "Trained-With", "Kinetics"]], "rel_plus": [["3D CNNs:Method", "Trained-With", "Kinetics:Dataset"]]}
{"doc_id": "4539700", "sentence": "Since the abovementioned experiment showed Kinetics could be used to train ResNet - 1 8 without overfitting , we next examined deeper ResNets using the Kinetics training and validation sets .", "ner": [["Kinetics", "Dataset"], ["ResNet - 1 8", "Method"], ["ResNets", "Method"], ["Kinetics", "Dataset"]], "rel": [["ResNet - 1 8", "Trained-With", "Kinetics"], ["ResNets", "Trained-With", "Kinetics"]], "rel_plus": [["ResNet - 1 8:Method", "Trained-With", "Kinetics:Dataset"], ["ResNets:Method", "Trained-With", "Kinetics:Dataset"]]}
{"doc_id": "4539700", "sentence": "We can also see that deeper ResNet - 1 5 2 achieved significant improvement of accuracies compared with ResNet - 1 8 , which was the previously examined architecture [ 9 , 2 4 ] .", "ner": [["ResNet - 1 5 2", "Method"], ["ResNet - 1 8", "Method"]], "rel": [["ResNet - 1 5 2", "Compare-With", "ResNet - 1 8"]], "rel_plus": [["ResNet - 1 5 2:Method", "Compare-With", "ResNet - 1 8:Method"]]}
{"doc_id": "4539700", "sentence": "In con - trast , the accuracy of ResNet - 2 0 0 was almost the same as that of ResNet - 1 5 2 .", "ner": [["ResNet - 2 0 0", "Method"], ["ResNet - 1 5 2", "Method"]], "rel": [["ResNet - 2 0 0", "Compare-With", "ResNet - 1 5 2"]], "rel_plus": [["ResNet - 2 0 0:Method", "Compare-With", "ResNet - 1 5 2:Method"]]}
{"doc_id": "4539700", "sentence": "Interestingly , the results are similar to those for 2D ResNets on ImageNet [ 1 1 ] .", "ner": [["2D ResNets", "Method"], ["ImageNet", "Dataset"]], "rel": [["2D ResNets", "Trained-With", "ImageNet"]], "rel_plus": [["2D ResNets:Method", "Trained-With", "ImageNet:Dataset"]]}
{"doc_id": "4539700", "sentence": "These results indicate that the Kinetics dataset has sufficient data to train 3D CNNs in a manner similar to ImageNet .", "ner": [["Kinetics", "Dataset"], ["3D CNNs", "Method"], ["ImageNet", "Dataset"]], "rel": [["3D CNNs", "Trained-With", "Kinetics"], ["Kinetics", "Compare-With", "ImageNet"]], "rel_plus": [["3D CNNs:Method", "Trained-With", "Kinetics:Dataset"], ["Kinetics:Dataset", "Compare-With", "ImageNet:Dataset"]]}
{"doc_id": "4539700", "sentence": "Here , it can be seen that the accuracies of preactivation ResNet - 2 0 0 are slightly low when compared with the standard ResNet - 2 0 0 though He et al. reported that the pre - activation reduces overfitting and improves 2D ResNet - 2 0 0 on ImageNet [ 1 1 ] .", "ner": [["ResNet - 2 0 0", "Method"], ["ResNet - 2 0 0", "Method"], ["2D ResNet - 2 0 0", "Method"], ["ImageNet", "Dataset"]], "rel": [["2D ResNet - 2 0 0", "Evaluated-With", "ImageNet"]], "rel_plus": [["2D ResNet - 2 0 0:Method", "Evaluated-With", "ImageNet:Dataset"]]}
{"doc_id": "4539700", "sentence": "We can also see that the WRN - 5 0 achieved higher accuracies when compared with the ResNet - 1 5 2 , which is similar to the results on ImageNet [ 3 1 ] .", "ner": [["WRN - 5 0", "Method"], ["ResNet - 1 5 2", "Method"], ["ImageNet", "Dataset"]], "rel": [["WRN - 5 0", "Compare-With", "ResNet - 1 5 2"], ["WRN - 5 0", "Evaluated-With", "ImageNet"], ["ResNet - 1 5 2", "Evaluated-With", "ImageNet"]], "rel_plus": [["WRN - 5 0:Method", "Compare-With", "ResNet - 1 5 2:Method"], ["WRN - 5 0:Method", "Evaluated-With", "ImageNet:Dataset"], ["ResNet - 1 5 2:Method", "Evaluated-With", "ImageNet:Dataset"]]}
{"doc_id": "4539700", "sentence": "This result also supports that Kinetics is sufficient large for the training of 3D CNNs because the number of parameters of WRN - 5 0 is larger than the ResNet - 1 5 2 .", "ner": [["Kinetics", "Dataset"], ["3D CNNs", "Method"], ["WRN - 5 0", "Method"], ["ResNet - 1 5 2", "Method"]], "rel": [["3D CNNs", "Trained-With", "Kinetics"], ["WRN - 5 0", "Compare-With", "ResNet - 1 5 2"]], "rel_plus": [["3D CNNs:Method", "Trained-With", "Kinetics:Dataset"], ["WRN - 5 0:Method", "Compare-With", "ResNet - 1 5 2:Method"]]}
{"doc_id": "4539700", "sentence": "This result is also similar to that seen for ImageNet [ 3 0 ] , and means that introducing the cardinality works well for the 3D ResNets on Kinetics .", "ner": [["ImageNet", "Dataset"], ["3D ResNets", "Method"], ["Kinetics", "Dataset"]], "rel": [["3D ResNets", "Trained-With", "Kinetics"]], "rel_plus": [["3D ResNets:Method", "Trained-With", "Kinetics:Dataset"]]}
{"doc_id": "4539700", "sentence": "However , Kinetics did not need such techniques to train deep 3D CNNs .", "ner": [["Kinetics", "Dataset"], ["3D CNNs", "Method"]], "rel": [["3D CNNs", "Trained-With", "Kinetics"]], "rel_plus": [["3D CNNs:Method", "Trained-With", "Kinetics:Dataset"]]}
{"doc_id": "4539700", "sentence": "Table 3 shows the results of the Kinetics test set used to compare ResNeXt - 1 0 1 , which achieved the highest accuracies , with the state - of - the - art methods .", "ner": [["Kinetics test set", "Dataset"], ["ResNeXt - 1 0 1", "Method"]], "rel": [["ResNeXt - 1 0 1", "Evaluated-With", "Kinetics test set"]], "rel_plus": [["ResNeXt - 1 0 1:Method", "Evaluated-With", "Kinetics test set:Dataset"]]}
{"doc_id": "4539700", "sentence": "Here , it can be seen that the accuracies of ResNeXt - 1 0 1 are clearly high compared with C 3 D with batch normalization [ 1 6 ] , which is 1 0 - layer network , as well as CNN+LSTM and two - stream CNN [ 1 6 ] .", "ner": [["ResNeXt - 1 0 1", "Method"], ["C 3 D with batch normalization", "Method"], ["CNN+LSTM", "Method"], ["two - stream CNN", "Method"]], "rel": [["ResNeXt - 1 0 1", "Compare-With", "C 3 D with batch normalization"], ["ResNeXt - 1 0 1", "Compare-With", "CNN+LSTM"], ["ResNeXt - 1 0 1", "Compare-With", "two - stream CNN"]], "rel_plus": [["ResNeXt - 1 0 1:Method", "Compare-With", "C 3 D with batch normalization:Method"], ["ResNeXt - 1 0 1:Method", "Compare-With", "CNN+LSTM:Method"], ["ResNeXt - 1 0 1:Method", "Compare-With", "two - stream CNN:Method"]]}
{"doc_id": "4539700", "sentence": "In contrast , RGB - I 3 D trained on Kinetics from scratch [ 3 ] were found to outperform ResNeXt - 1 0 1 even though ResNeXt - 1 0 1 is a deeper architecture than I 3 D .", "ner": [["RGB - I 3 D", "Method"], ["Kinetics", "Dataset"], ["ResNeXt - 1 0 1", "Method"], ["ResNeXt - 1 0 1", "Method"], ["I 3 D", "Method"]], "rel": [["RGB - I 3 D", "Trained-With", "Kinetics"], ["RGB - I 3 D", "Compare-With", "ResNeXt - 1 0 1"], ["ResNeXt - 1 0 1", "Compare-With", "I 3 D"]], "rel_plus": [["RGB - I 3 D:Method", "Trained-With", "Kinetics:Dataset"], ["RGB - I 3 D:Method", "Compare-With", "ResNeXt - 1 0 1:Method"], ["ResNeXt - 1 0 1:Method", "Compare-With", "I 3 D:Method"]]}
{"doc_id": "4539700", "sentence": "Specifically , the size of I 3 D is 3 \u00d7 6 4 \u00d7 2 2 4 \u00d7 2 2 4 , whereas that of ResNeXt - 1 0 1 is 3 \u00d7 1 6 \u00d7 1 1 2 \u00d7 1 1 2 .", "ner": [["I 3 D", "Method"], ["ResNeXt - 1 0 1", "Method"]], "rel": [["I 3 D", "Compare-With", "ResNeXt - 1 0 1"]], "rel_plus": [["I 3 D:Method", "Compare-With", "ResNeXt - 1 0 1:Method"]]}
{"doc_id": "4539700", "sentence": "Thus , I 3 D is 6 4 times larger than ResNeXt - 1 0 1 .", "ner": [["I 3 D", "Method"], ["ResNeXt - 1 0 1", "Method"]], "rel": [["I 3 D", "Compare-With", "ResNeXt - 1 0 1"]], "rel_plus": [["I 3 D:Method", "Compare-With", "ResNeXt - 1 0 1:Method"]]}
{"doc_id": "4539700", "sentence": "We can see that the network , referred as ResNeXt - 1 0 1 ( 6 4 f ) in Table 3 , outperformed RGB - I 3 D even though the input size is still four times smaller than that of I 3 D .", "ner": [["ResNeXt - 1 0 1 ( 6 4 f )", "Method"], ["RGB - I 3 D", "Method"], ["I 3 D", "Method"]], "rel": [["ResNeXt - 1 0 1 ( 6 4 f )", "Compare-With", "RGB - I 3 D"], ["ResNeXt - 1 0 1 ( 6 4 f )", "Compare-With", "I 3 D"]], "rel_plus": [["ResNeXt - 1 0 1 ( 6 4 f ):Method", "Compare-With", "RGB - I 3 D:Method"], ["ResNeXt - 1 0 1 ( 6 4 f ):Method", "Compare-With", "I 3 D:Method"]]}
{"doc_id": "4539700", "sentence": "In the experiments above , we showed that Kinetics can train deep 3D CNNs from scratch , but that it is difficult to train such networks on other datasets .", "ner": [["Kinetics", "Dataset"], ["3D CNNs", "Method"]], "rel": [["3D CNNs", "Trained-With", "Kinetics"]], "rel_plus": [["3D CNNs:Method", "Trained-With", "Kinetics:Dataset"]]}
{"doc_id": "4539700", "sentence": "In this section , we fine - tuned the Kinetics pretrained 3D CNNs on UCF - 1 0 1 and HMDB - 5 1 .", "ner": [["Kinetics", "Dataset"], ["3D CNNs", "Method"], ["UCF - 1 0 1", "Dataset"], ["HMDB - 5 1", "Dataset"]], "rel": [["3D CNNs", "Trained-With", "Kinetics"], ["3D CNNs", "Trained-With", "UCF - 1 0 1"], ["3D CNNs", "Trained-With", "HMDB - 5 1"]], "rel_plus": [["3D CNNs:Method", "Trained-With", "Kinetics:Dataset"], ["3D CNNs:Method", "Trained-With", "UCF - 1 0 1:Dataset"], ["3D CNNs:Method", "Trained-With", "HMDB - 5 1:Dataset"]]}
{"doc_id": "4539700", "sentence": "Table 4 shows the accuracies of Kinetics pretrained 3D CNNs , as well as ResNet - 1 8 trained from scratch , in UCF - We can also see that the accuracies basically improved as the depth increased , similar to the results obtained on Kinetics .", "ner": [["Kinetics", "Dataset"], ["3D CNNs", "Method"], ["ResNet - 1 8", "Method"], ["UCF", "Dataset"], ["Kinetics", "Dataset"]], "rel": [["3D CNNs", "Trained-With", "Kinetics"], ["ResNet - 1 8", "Trained-With", "Kinetics"], ["3D CNNs", "Evaluated-With", "UCF"], ["ResNet - 1 8", "Evaluated-With", "UCF"], ["3D CNNs", "Evaluated-With", "Kinetics"], ["ResNet - 1 8", "Evaluated-With", "Kinetics"]], "rel_plus": [["3D CNNs:Method", "Trained-With", "Kinetics:Dataset"], ["ResNet - 1 8:Method", "Trained-With", "Kinetics:Dataset"], ["3D CNNs:Method", "Evaluated-With", "UCF:Dataset"], ["ResNet - 1 8:Method", "Evaluated-With", "UCF:Dataset"], ["3D CNNs:Method", "Evaluated-With", "Kinetics:Dataset"], ["ResNet - 1 8:Method", "Evaluated-With", "Kinetics:Dataset"]]}
{"doc_id": "4539700", "sentence": "However , unlike the results on Kinetics , ResNet - 2 0 0 also improved the accuracies in HMDB - 5 1 .", "ner": [["Kinetics", "Dataset"], ["ResNet - 2 0 0", "Method"], ["HMDB - 5 1", "Dataset"]], "rel": [["ResNet - 2 0 0", "Evaluated-With", "Kinetics"], ["ResNet - 2 0 0", "Evaluated-With", "HMDB - 5 1"], ["Kinetics", "Compare-With", "HMDB - 5 1"]], "rel_plus": [["ResNet - 2 0 0:Method", "Evaluated-With", "Kinetics:Dataset"], ["ResNet - 2 0 0:Method", "Evaluated-With", "HMDB - 5 1:Dataset"], ["Kinetics:Dataset", "Compare-With", "HMDB - 5 1:Dataset"]]}
{"doc_id": "4539700", "sentence": "Because , as described above , the fine - tuning in this experiment was only performed to train conv 5 _x and the fully connected layer , the numbers of trained parameters were the same from ResNet - 5 0 to ResNet - 2 0 0 .", "ner": [["fully connected layer", "Method"], ["ResNet - 5 0", "Method"], ["ResNet - 2 0 0", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "4539700", "sentence": "These results indicate that feature representations of ResNet - 2 0 0 would be suitable for HMDB - 5 1 even though the 2 0 0 - layer network might start to overfit on Kinetics .", "ner": [["ResNet - 2 0 0", "Method"], ["HMDB - 5 1", "Dataset"], ["Kinetics", "Dataset"]], "rel": [["ResNet - 2 0 0", "Evaluated-With", "HMDB - 5 1"], ["ResNet - 2 0 0", "Trained-With", "Kinetics"]], "rel_plus": [["ResNet - 2 0 0:Method", "Evaluated-With", "HMDB - 5 1:Dataset"], ["ResNet - 2 0 0:Method", "Trained-With", "Kinetics:Dataset"]]}
{"doc_id": "4539700", "sentence": "Table 4 also shows that ResNeXt - 1 0 1 , which achieved the best performance on Kinetics , achieved the highest levels of performance on both datasets when compared with the other networks .", "ner": [["ResNeXt - 1 0 1", "Method"], ["Kinetics", "Dataset"]], "rel": [["ResNeXt - 1 0 1", "Evaluated-With", "Kinetics"]], "rel_plus": [["ResNeXt - 1 0 1:Method", "Evaluated-With", "Kinetics:Dataset"]]}
{"doc_id": "4539700", "sentence": "However , the DenseNet - 1 2 1 results were lower than those of ResNet - 5 0 , thereby indicating that its greater efficiency did not contribute on fine - tuning of 3D CNNs .", "ner": [["DenseNet - 1 2 1", "Method"], ["ResNet - 5 0", "Method"], ["3D CNNs", "Method"]], "rel": [["DenseNet - 1 2 1", "Compare-With", "ResNet - 5 0"]], "rel_plus": [["DenseNet - 1 2 1:Method", "Compare-With", "ResNet - 5 0:Method"]]}
{"doc_id": "4539700", "sentence": "Here , we can see that ResNeXt - 1 0 1 achieved higher accuracies compared with C 3 D [ 2 3 ] , P 3 D [ 1 9 ] , two - stream CNN [ 2 0 ] , and TDD [ 2 7 ] .", "ner": [["ResNeXt - 1 0 1", "Method"], ["C 3 D", "Method"], ["P 3 D", "Method"], ["two - stream CNN", "Method"], ["TDD", "Method"]], "rel": [["ResNeXt - 1 0 1", "Compare-With", "C 3 D"], ["ResNeXt - 1 0 1", "Compare-With", "P 3 D"], ["ResNeXt - 1 0 1", "Compare-With", "two - stream CNN"], ["ResNeXt - 1 0 1", "Compare-With", "TDD"]], "rel_plus": [["ResNeXt - 1 0 1:Method", "Compare-With", "C 3 D:Method"], ["ResNeXt - 1 0 1:Method", "Compare-With", "P 3 D:Method"], ["ResNeXt - 1 0 1:Method", "Compare-With", "two - stream CNN:Method"], ["ResNeXt - 1 0 1:Method", "Compare-With", "TDD:Method"]]}
{"doc_id": "4539700", "sentence": "Furthermore , we can also see that ResNeXt - 1 0 1 ( 6 4 f ) , which utilize larger inputs described in previous section , slightly outperformed ST Multiplier Net [ 7 ] and TSN [ 2 9 ] , which utilize more complex two - stream architectures .", "ner": [["ResNeXt - 1 0 1", "Method"], ["ST Multiplier Net", "Method"], ["TSN", "Method"]], "rel": [["ResNeXt - 1 0 1", "Compare-With", "ST Multiplier Net"], ["ResNeXt - 1 0 1", "Compare-With", "TSN"]], "rel_plus": [["ResNeXt - 1 0 1:Method", "Compare-With", "ST Multiplier Net:Method"], ["ResNeXt - 1 0 1:Method", "Compare-With", "TSN:Method"]]}
{"doc_id": "4539700", "sentence": "We can also see Table 5 : Top - 1 accuracies on UCF - 1 0 1 and HMDB - 5 1 comared with the state - of - the - art methods .", "ner": [["UCF - 1 0 1", "Dataset"], ["HMDB - 5 1", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "4539700", "sentence": "Dim UCF - 1 0 1 HMDB - 5 1 that two - stream I 3 D [ 3 ] , which utilizes simple two - stream 3D architectures pretrained on Kinetics , achieved the best accuracies .", "ner": [["UCF - 1 0 1", "Dataset"], ["HMDB - 5 1", "Dataset"], ["two - stream I 3 D", "Method"], ["Kinetics", "Dataset"]], "rel": [["two - stream I 3 D", "Trained-With", "Kinetics"]], "rel_plus": [["two - stream I 3 D:Method", "Trained-With", "Kinetics:Dataset"]]}
{"doc_id": "4539700", "sentence": "We believe that development of 3D CNNs rapidly grows and contributes to significant advances in video recognition and its related tasks .", "ner": [["3D CNNs", "Method"], ["video recognition", "Task"]], "rel": [["3D CNNs", "Used-For", "video recognition"]], "rel_plus": [["3D CNNs:Method", "Used-For", "video recognition:Task"]]}
{"doc_id": "4539700", "sentence": "Based on the results of those experiments , the following conclusions could be obtained : ( i ) ResNet - 1 8 training resulted in significant overfitting for UCF - 1 0 1 , HMDB - 5 1 , and ActivityNet but not for Kinetics . ( ii ) The Kinetics dataset has sufficient data for training of deep 3D CNNs , and enables training of up to 1 5 2 ResNets layers , interestingly similar to 2D ResNets on ImageNet . ( iii ) Kinetics pretrained simple 3D architectures outperforms complex 2D architectures on UCF - 1 0 1 and HMDB - 5 1 , and the pretrained ResNeXt - 1 0 1 achieved 9 4 . 5 % and 7 0 . 2 % on UCF - 1 0 1 and HMDB - 5 1 , respectively .", "ner": [["ResNet - 1 8", "Method"], ["UCF - 1 0 1", "Dataset"], ["HMDB - 5 1", "Dataset"], ["ActivityNet", "Dataset"], ["Kinetics", "Dataset"], ["Kinetics", "Dataset"], ["3D CNNs", "Method"], ["ResNets layers", "Method"], ["2D ResNets", "Method"], ["ImageNet", "Dataset"], ["Kinetics", "Dataset"], ["UCF - 1 0 1", "Dataset"], ["HMDB - 5 1", "Dataset"], ["ResNeXt - 1 0 1", "Method"], ["UCF - 1 0 1", "Dataset"], ["HMDB - 5 1", "Dataset"]], "rel": [["ResNet - 1 8", "Trained-With", "UCF - 1 0 1"], ["ResNet - 1 8", "Trained-With", "HMDB - 5 1"], ["ResNet - 1 8", "Trained-With", "ActivityNet"], ["ResNet - 1 8", "Trained-With", "Kinetics"], ["3D CNNs", "Trained-With", "Kinetics"], ["ResNets layers", "Part-Of", "3D CNNs"], ["ResNets layers", "Part-Of", "2D ResNets"], ["2D ResNets", "Trained-With", "ImageNet"], ["ResNeXt - 1 0 1", "Trained-With", "Kinetics"], ["ResNeXt - 1 0 1", "Evaluated-With", "UCF - 1 0 1"], ["ResNeXt - 1 0 1", "Evaluated-With", "HMDB - 5 1"]], "rel_plus": [["ResNet - 1 8:Method", "Trained-With", "UCF - 1 0 1:Dataset"], ["ResNet - 1 8:Method", "Trained-With", "HMDB - 5 1:Dataset"], ["ResNet - 1 8:Method", "Trained-With", "ActivityNet:Dataset"], ["ResNet - 1 8:Method", "Trained-With", "Kinetics:Dataset"], ["3D CNNs:Method", "Trained-With", "Kinetics:Dataset"], ["ResNets layers:Method", "Part-Of", "3D CNNs:Method"], ["ResNets layers:Method", "Part-Of", "2D ResNets:Method"], ["2D ResNets:Method", "Trained-With", "ImageNet:Dataset"], ["ResNeXt - 1 0 1:Method", "Trained-With", "Kinetics:Dataset"], ["ResNeXt - 1 0 1:Method", "Evaluated-With", "UCF - 1 0 1:Dataset"], ["ResNeXt - 1 0 1:Method", "Evaluated-With", "HMDB - 5 1:Dataset"]]}
{"doc_id": "4539700", "sentence": "Following the significant advances in image recognition made by 2D CNNs and ImageNet , pretrained 2D CNNs on Ima - geNet experienced significant progress in various tasks such as object detection , semantic segmentation , and image captioning .", "ner": [["image recognition", "Task"], ["2D CNNs", "Method"], ["ImageNet", "Dataset"], ["2D CNNs", "Method"], ["Ima - geNet", "Dataset"], ["object detection", "Task"], ["semantic segmentation", "Task"], ["image captioning", "Task"]], "rel": [["2D CNNs", "Used-For", "image recognition"], ["ImageNet", "Benchmark-For", "image recognition"], ["2D CNNs", "Trained-With", "Ima - geNet"], ["2D CNNs", "Used-For", "object detection"], ["2D CNNs", "Used-For", "semantic segmentation"], ["2D CNNs", "Used-For", "image captioning"]], "rel_plus": [["2D CNNs:Method", "Used-For", "image recognition:Task"], ["ImageNet:Dataset", "Benchmark-For", "image recognition:Task"], ["2D CNNs:Method", "Trained-With", "Ima - geNet:Dataset"], ["2D CNNs:Method", "Used-For", "object detection:Task"], ["2D CNNs:Method", "Used-For", "semantic segmentation:Task"], ["2D CNNs:Method", "Used-For", "image captioning:Task"]]}
{"doc_id": "4539700", "sentence": "It is felt that , similar to these , 3D CNNs and Kinetics have the potential to contribute to significant progress in fields related to various video tasks such as action detection , video summarization , and optical flow estimation .", "ner": [["3D CNNs", "Method"], ["Kinetics", "Dataset"], ["action detection", "Task"], ["video summarization", "Task"], ["optical flow estimation", "Task"]], "rel": [["3D CNNs", "Used-For", "action detection"], ["3D CNNs", "Used-For", "video summarization"], ["3D CNNs", "Used-For", "optical flow estimation"]], "rel_plus": [["3D CNNs:Method", "Used-For", "action detection:Task"], ["3D CNNs:Method", "Used-For", "video summarization:Task"], ["3D CNNs:Method", "Used-For", "optical flow estimation:Task"]]}
{"doc_id": "4539700", "sentence": "In our future work , we will investigate transfer learning not only for action recognition but also for other such tasks .", "ner": [["transfer learning", "Task"], ["action recognition", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "202888986", "sentence": "As a result , our best model establishes new state - of - the - art results on the GLUE , RACE , and \\squad benchmarks while having fewer parameters compared to BERT - large .", "ner": [["GLUE", "Dataset"], ["RACE", "Dataset"], ["\\squad", "Dataset"], ["BERT - large", "Method"]], "rel": [["BERT - large", "Evaluated-With", "GLUE"], ["BERT - large", "Evaluated-With", "RACE"], ["BERT - large", "Evaluated-With", "\\squad"]], "rel_plus": [["BERT - large:Method", "Evaluated-With", "GLUE:Dataset"], ["BERT - large:Method", "Evaluated-With", "RACE:Dataset"], ["BERT - large:Method", "Evaluated-With", "\\squad:Dataset"]]}
{"doc_id": "202888986", "sentence": "One of the most compelling signs of these breakthroughs is the evolution of machine performance on a reading comprehension task designed for middle and high - school English exams in China , the RACE test ( Lai et al. , 2 0 1 7 ) : the paper that originally describes the task and formulates the modeling challenge reports then state - of - the - art machine accuracy at 4 4 . 1 % ; the latest published result reports their model performance at 8 3 . 2 % ; the work we present here pushes it even higher to 8 9 . 4 % , a stunning 4 5 . 3 % improvement that is mainly attributable to our current ability to build high - performance pretrained language representations .", "ner": [["reading comprehension", "Task"], ["RACE", "Dataset"]], "rel": [["RACE", "Benchmark-For", "reading comprehension"]], "rel_plus": [["RACE:Dataset", "Benchmark-For", "reading comprehension:Task"]]}
{"doc_id": "202888986", "sentence": "Table 1 and Fig. 1 show a typical example , where we simply increase the hidden size of BERT - large to be 2x larger and get worse results with this BERT - xlarge model .", "ner": [["BERT - large", "Method"], ["BERT - xlarge", "Method"]], "rel": [["BERT - large", "Compare-With", "BERT - xlarge"]], "rel_plus": [["BERT - large:Method", "Compare-With", "BERT - xlarge:Method"]]}
{"doc_id": "202888986", "sentence": "Model Hidden Size Parameters RACE ( Accuracy ) BERT - large 1 0 2 4 3 3 4 M Existing solutions to the aforementioned problems include model parallelization ( Shoeybi et al. , 2 0 1 9 ) and clever memory management ( Chen et al. , 2 0 1 6 ; .", "ner": [["RACE", "Dataset"], ["BERT - large", "Method"]], "rel": [["BERT - large", "Evaluated-With", "RACE"]], "rel_plus": [["BERT - large:Method", "Evaluated-With", "RACE:Dataset"]]}
{"doc_id": "202888986", "sentence": "In this paper , we address all of the aforementioned problems , by designing A Lite BERT ( ALBERT ) architecture that has significantly fewer parameters than a traditional BERT architecture .", "ner": [["BERT", "Method"], ["ALBERT", "Method"], ["BERT", "Method"]], "rel": [["ALBERT", "SubClass-Of", "BERT"]], "rel_plus": [["ALBERT:Method", "SubClass-Of", "BERT:Method"]]}
{"doc_id": "202888986", "sentence": "An ALBERT configuration similar to BERT - large has 1 8 x fewer parameters and can be trained about 1. 7 x faster .", "ner": [["ALBERT", "Method"], ["BERT - large", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202888986", "sentence": "To further improve the performance of ALBERT , we also introduce a self - supervised loss for sentence - order prediction ( SOP ) .", "ner": [["ALBERT", "Method"], ["self - supervised loss", "Method"], ["sentence - order prediction", "Task"], ["SOP", "Task"]], "rel": [["self - supervised loss", "Part-Of", "ALBERT"], ["sentence - order prediction", "Trained-With", "ALBERT"], ["SOP", "Synonym-Of", "sentence - order prediction"], ["self - supervised loss", "Used-For", "sentence - order prediction"]], "rel_plus": [["self - supervised loss:Method", "Part-Of", "ALBERT:Method"], ["sentence - order prediction:Task", "Trained-With", "ALBERT:Method"], ["SOP:Task", "Synonym-Of", "sentence - order prediction:Task"], ["self - supervised loss:Method", "Used-For", "sentence - order prediction:Task"]]}
{"doc_id": "202888986", "sentence": "SOP primary focuses on inter - sentence coherence and is designed to address the ineffectiveness of the next sentence prediction ( NSP ) loss proposed in the original BERT .", "ner": [["SOP", "Task"], ["next sentence prediction", "Task"], ["NSP", "Task"], ["BERT", "Method"]], "rel": [["NSP", "Synonym-Of", "next sentence prediction"], ["SOP", "Compare-With", "next sentence prediction"], ["BERT", "Trained-With", "next sentence prediction"]], "rel_plus": [["NSP:Task", "Synonym-Of", "next sentence prediction:Task"], ["SOP:Task", "Compare-With", "next sentence prediction:Task"], ["BERT:Method", "Trained-With", "next sentence prediction:Task"]]}
{"doc_id": "202888986", "sentence": "As a result of these design decisions , we are able to scale up to much larger ALBERT configurations that still have fewer parameters than BERT - large but achieve significantly better performance .", "ner": [["ALBERT", "Method"], ["BERT - large", "Method"]], "rel": [["ALBERT", "Compare-With", "BERT - large"]], "rel_plus": [["ALBERT:Method", "Compare-With", "BERT - large:Method"]]}
{"doc_id": "202888986", "sentence": "We establish new state - of - the - art results on the well - known GLUE , SQuAD , and RACE benchmarks for natural language understanding .", "ner": [["GLUE", "Dataset"], ["SQuAD", "Dataset"], ["RACE", "Dataset"], ["natural language understanding", "Task"]], "rel": [["GLUE", "Benchmark-For", "natural language understanding"], ["SQuAD", "Benchmark-For", "natural language understanding"], ["RACE", "Benchmark-For", "natural language understanding"]], "rel_plus": [["GLUE:Dataset", "Benchmark-For", "natural language understanding:Task"], ["SQuAD:Dataset", "Benchmark-For", "natural language understanding:Task"], ["RACE:Dataset", "Benchmark-For", "natural language understanding:Task"]]}
{"doc_id": "202888986", "sentence": "Specifically , we push the RACE accuracy to 8 9 . 4 % , the GLUE benchmark to 8 9 . 4 , and the F 1 score of SQuAD 2. 0 to 9 2 . 2 .   Learning representations of natural language has been shown to be useful for a wide range of NLP tasks and has been widely adopted ( Mikolov et al. , 2 0 1 3 ; Le & Mikolov , 2 0 1 4 ; Peters et al. , 2 0 1 8 ; Radford et al. , 2 0 1 8 ; .", "ner": [["RACE", "Dataset"], ["GLUE", "Dataset"], ["SQuAD", "Dataset"], ["NLP", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "202888986", "sentence": "Different from our observations , Dehghani et al. ( 2 0 1 8) show that networks with cross - layer parameter sharing ( Universal Transformer , UT ) get better performance on language modeling and subject - verb agreement than the standard transformer .", "ner": [["Universal Transformer", "Method"], ["UT", "Method"], ["language modeling", "Task"], ["subject - verb agreement", "Task"], ["transformer", "Method"]], "rel": [["UT", "Synonym-Of", "Universal Transformer"], ["Universal Transformer", "Used-For", "language modeling"], ["transformer", "Used-For", "language modeling"], ["Universal Transformer", "Used-For", "subject - verb agreement"], ["transformer", "Used-For", "subject - verb agreement"], ["Universal Transformer", "Compare-With", "transformer"]], "rel_plus": [["UT:Method", "Synonym-Of", "Universal Transformer:Method"], ["Universal Transformer:Method", "Used-For", "language modeling:Task"], ["transformer:Method", "Used-For", "language modeling:Task"], ["Universal Transformer:Method", "Used-For", "subject - verb agreement:Task"], ["transformer:Method", "Used-For", "subject - verb agreement:Task"], ["Universal Transformer:Method", "Compare-With", "transformer:Method"]]}
{"doc_id": "202888986", "sentence": "Very recently , Bai et al. ( 2 0 1 9 ) propose a Deep Equilibrium Model ( DQE ) for transformer networks and show that DQE can reach an equilibrium point for which the input embedding and the output embedding of a certain layer stay the same .", "ner": [["Deep Equilibrium Model", "Method"], ["DQE", "Method"], ["transformer", "Method"], ["DQE", "Method"]], "rel": [["DQE", "Synonym-Of", "Deep Equilibrium Model"], ["Deep Equilibrium Model", "Part-Of", "transformer"]], "rel_plus": [["DQE:Method", "Synonym-Of", "Deep Equilibrium Model:Method"], ["Deep Equilibrium Model:Method", "Part-Of", "transformer:Method"]]}
{"doc_id": "202888986", "sentence": "Hao et al. ( 2 0 1 9 ) combine a parameter - sharing transformer with the standard one , which further increases the number of parameters of the standard transformer .", "ner": [["transformer", "Method"], ["transformer", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202888986", "sentence": "Our loss is most similar to the sentence ordering objective of Jernite et al. ( 2 0 1 7 ) , where sentence embeddings are learned in order to determine the ordering of two consecutive sentences .", "ner": [["sentence ordering", "Task"], ["sentence embeddings", "Task"]], "rel": [["sentence embeddings", "SubTask-Of", "sentence ordering"]], "rel_plus": [["sentence embeddings:Task", "SubTask-Of", "sentence ordering:Task"]]}
{"doc_id": "202888986", "sentence": "In this section , we present the design decisions for ALBERT and provide quantified comparisons against corresponding configurations of the original BERT architecture .", "ner": [["ALBERT", "Method"], ["BERT", "Method"]], "rel": [["ALBERT", "Compare-With", "BERT"]], "rel_plus": [["ALBERT:Method", "Compare-With", "BERT:Method"]]}
{"doc_id": "202888986", "sentence": "The backbone of the ALBERT architecture is similar to BERT in that it uses a transformer encoder ( Vaswani et al. , 2 0 1 7 ) with GELU nonlinearities ( Hendrycks & Gimpel , 2 0 1 6 ) .", "ner": [["ALBERT", "Method"], ["BERT", "Method"], ["transformer encoder", "Method"], ["GELU nonlinearities", "Method"]], "rel": [["transformer encoder", "Part-Of", "ALBERT"], ["ALBERT", "Compare-With", "BERT"], ["transformer encoder", "Part-Of", "BERT"], ["GELU nonlinearities", "Part-Of", "transformer encoder"]], "rel_plus": [["transformer encoder:Method", "Part-Of", "ALBERT:Method"], ["ALBERT:Method", "Compare-With", "BERT:Method"], ["transformer encoder:Method", "Part-Of", "BERT:Method"], ["GELU nonlinearities:Method", "Part-Of", "transformer encoder:Method"]]}
{"doc_id": "202888986", "sentence": "There are three main contributions that ALBERT makes over the design choices of BERT .", "ner": [["ALBERT", "Method"], ["BERT", "Method"]], "rel": [["ALBERT", "SubClass-Of", "BERT"]], "rel_plus": [["ALBERT:Method", "SubClass-Of", "BERT:Method"]]}
{"doc_id": "202888986", "sentence": "In BERT , as well as subsequent modeling improvements such as XLNet and RoBERTa , the WordPiece embedding size E is tied with the hidden layer size H , i.e. , E \u2261 H. This decision appears suboptimal for both modeling and practical reasons , as follows .", "ner": [["BERT", "Method"], ["XLNet", "Method"], ["RoBERTa", "Method"]], "rel": [["XLNet", "SubClass-Of", "BERT"], ["RoBERTa", "SubClass-Of", "BERT"]], "rel_plus": [["XLNet:Method", "SubClass-Of", "BERT:Method"], ["RoBERTa:Method", "SubClass-Of", "BERT:Method"]]}
{"doc_id": "202888986", "sentence": "As such , untying the WordPiece embedding size E from the hidden layer size H allows us to make a more efficient usage of the total model parameters as informed by modeling needs , which dictate that H E. From a practical perspective , natural language processing usually require the vocabulary size V to be large . 1 If E \u2261 H , then increasing H increases the size of the embedding matrix , which has size V \u00d7 E. This can easily result in a model with billions of parameters , most of which are only updated sparsely during training .", "ner": [["natural language processing", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "202888986", "sentence": "There are multiple ways to share parameters , e.g. , only sharing feed - forward network ( FFN ) parameters across layers , or only sharing attention parameters .", "ner": [["feed - forward network", "Method"], ["FFN", "Method"]], "rel": [["FFN", "Synonym-Of", "feed - forward network"]], "rel_plus": [["FFN:Method", "Synonym-Of", "feed - forward network:Method"]]}
{"doc_id": "202888986", "sentence": "Figure 2 shows the L 2 distances and cosine similarity of the input and output embeddings for each layer , using BERT - large and ALBERT - large configurations ( see Table 2 ) .", "ner": [["L 2 distances", "Method"], ["cosine similarity", "Method"], ["BERT - large", "Method"], ["ALBERT - large", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202888986", "sentence": "We observe that the transitions from layer to layer are much smoother for ALBERT than for BERT .", "ner": [["ALBERT", "Method"], ["BERT", "Method"]], "rel": [["ALBERT", "Compare-With", "BERT"]], "rel_plus": [["ALBERT:Method", "Compare-With", "BERT:Method"]]}
{"doc_id": "202888986", "sentence": "This shows that the solution space for ALBERT parameters is very different from the one found by DQE .", "ner": [["ALBERT", "Method"], ["DQE", "Method"]], "rel": [["ALBERT", "Compare-With", "DQE"]], "rel_plus": [["ALBERT:Method", "Compare-With", "DQE:Method"]]}
{"doc_id": "202888986", "sentence": "In addition to the masked language modeling ( MLM ) loss , BERT uses an additional loss called next - sentence prediction ( NSP ) .", "ner": [["masked language modeling", "Task"], ["MLM", "Task"], ["BERT", "Method"], ["next - sentence prediction", "Task"], ["NSP", "Task"]], "rel": [["MLM", "Synonym-Of", "masked language modeling"], ["BERT", "Trained-With", "masked language modeling"], ["NSP", "Synonym-Of", "next - sentence prediction"], ["BERT", "Trained-With", "next - sentence prediction"]], "rel_plus": [["MLM:Task", "Synonym-Of", "masked language modeling:Task"], ["BERT:Method", "Trained-With", "masked language modeling:Task"], ["NSP:Task", "Synonym-Of", "next - sentence prediction:Task"], ["BERT:Method", "Trained-With", "next - sentence prediction:Task"]]}
{"doc_id": "202888986", "sentence": "The NSP objective was designed to improve performance on downstream tasks , such as natural language inference , that require reasoning about the relationship between sentence pairs .", "ner": [["NSP", "Task"], ["natural language inference", "Task"]], "rel": [["NSP", "Used-For", "natural language inference"]], "rel_plus": [["NSP:Task", "Used-For", "natural language inference:Task"]]}
{"doc_id": "202888986", "sentence": "We conjecture that the main reason behind NSP 's ineffectiveness is its lack of difficulty as a task , as compared to MLM .", "ner": [["NSP", "Task"], ["MLM", "Task"]], "rel": [["NSP", "Compare-With", "MLM"]], "rel_plus": [["NSP:Task", "Compare-With", "MLM:Task"]]}
{"doc_id": "202888986", "sentence": "That is , for ALBERT , we use a sentence - order prediction ( SOP ) loss , which avoids topic prediction and instead focuses on modeling inter - sentence coherence .", "ner": [["ALBERT", "Method"], ["sentence - order prediction", "Task"], ["SOP", "Task"]], "rel": [["SOP", "Synonym-Of", "sentence - order prediction"], ["ALBERT", "Trained-With", "sentence - order prediction"]], "rel_plus": [["SOP:Task", "Synonym-Of", "sentence - order prediction:Task"], ["ALBERT:Method", "Trained-With", "sentence - order prediction:Task"]]}
{"doc_id": "202888986", "sentence": "The SOP loss uses as positive examples the same technique as BERT ( two consecutive segments from the same document ) , and as negative examples the same two consecutive segments but with their order swapped .", "ner": [["SOP", "Task"], ["BERT", "Method"]], "rel": [["BERT", "Trained-With", "SOP"]], "rel_plus": [["BERT:Method", "Trained-With", "SOP:Task"]]}
{"doc_id": "202888986", "sentence": "As we show in Sec. 4. 6 , it turns out that NSP can not solve the SOP task at all ( i.e. , it ends up learning the easier topic - prediction signal , and performs at randombaseline level on the SOP task ) , while SOP can solve the NSP task to a reasonable degree , presumably based on analyzing misaligned coherence cues .", "ner": [["NSP", "Task"], ["SOP", "Task"], ["SOP", "Task"], ["SOP", "Task"], ["NSP", "Task"]], "rel": [["SOP", "Used-For", "NSP"]], "rel_plus": [["SOP:Task", "Used-For", "NSP:Task"]]}
{"doc_id": "202888986", "sentence": "We present the differences between BERT and ALBERT models with comparable hyperparameter settings in Table 2 .", "ner": [["BERT", "Method"], ["ALBERT", "Method"]], "rel": [["BERT", "Compare-With", "ALBERT"]], "rel_plus": [["BERT:Method", "Compare-With", "ALBERT:Method"]]}
{"doc_id": "202888986", "sentence": "Due to the design choices discussed above , ALBERT models have much smaller parameter size compared to corresponding BERT models .", "ner": [["ALBERT", "Method"], ["BERT", "Method"]], "rel": [["ALBERT", "Compare-With", "BERT"]], "rel_plus": [["ALBERT:Method", "Compare-With", "BERT:Method"]]}
{"doc_id": "202888986", "sentence": "For example , ALBERT - large has about 1 8 x fewer parameters compared to BERT - large , 1 8 M versus 3 3 4 M .", "ner": [["ALBERT - large", "Method"], ["BERT - large", "Method"]], "rel": [["ALBERT - large", "Compare-With", "BERT - large"]], "rel_plus": [["ALBERT - large:Method", "Compare-With", "BERT - large:Method"]]}
{"doc_id": "202888986", "sentence": "In contrast , an ALBERT - xlarge configuration with H = 2 0 4 8 has only 5 9 M parameters , while an ALBERT - xxlarge configuration with H = 4 0 9 6 has 2 3 3 M parameters , i.e. , around 7 0 % of BERT - large 's parameters .", "ner": [["ALBERT - xlarge", "Method"], ["ALBERT - xxlarge", "Method"], ["BERT - large", "Method"]], "rel": [["ALBERT - xlarge", "Compare-With", "ALBERT - xxlarge"]], "rel_plus": [["ALBERT - xlarge:Method", "Compare-With", "ALBERT - xxlarge:Method"]]}
{"doc_id": "202888986", "sentence": "Parameter - sharing    BERT   base   1 0 8 M   1 2   7 6 8   7 6 8   False   large   3 3 4 M   2 4   1 0 2 4   1 0 2 4   False   xlarge   1 2 7 0 M   2 4   2 0 4 8   2 0 4 8   False    ALBERT    base   1 2 M   1 2   7 6 8   1 2 8   True   large   1 8 M   2 4   1 0 2 4   1 2 8   True   xlarge   5 9 M   2 4   2 0 4 8   1 2 8   True   xxlarge   2 3 3 M   1 2   4 0 9 6   1 2 8   True    Table 2 : The configurations of the main BERT and ALBERT models analyzed in this paper .   This improvement in parameter efficiency is the most important advantage of ALBERT 's design choices .", "ner": [["BERT   base", "Method"], ["ALBERT    base", "Method"], ["BERT", "Method"], ["ALBERT", "Method"], ["ALBERT", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202888986", "sentence": "Before we can quantify this advantage , we need to introduce our experimental setup in more detail .   To keep the comparison as meaningful as possible , we follow the BERT setup in using the BOOKCORPUS and English Wikipedia for pretraining baseline models .", "ner": [["BERT", "Method"], ["BOOKCORPUS", "Dataset"], ["English Wikipedia", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "202888986", "sentence": "Like BERT , we use a vocabulary size of 3 0 , 0 0 0 , tokenized using SentencePiece ( Kudo & Richardson , 2 0 1 8) as in XLNet .", "ner": [["BERT", "Method"], ["SentencePiece", "Method"], ["XLNet", "Method"]], "rel": [["SentencePiece", "Part-Of", "BERT"], ["SentencePiece", "Part-Of", "XLNet"]], "rel_plus": [["SentencePiece:Method", "Part-Of", "BERT:Method"], ["SentencePiece:Method", "Part-Of", "XLNet:Method"]]}
{"doc_id": "202888986", "sentence": "The experimental setup described in this section is used for all of our own versions of BERT as well as ALBERT models , unless otherwise specified .   To monitor the training progress , we create a development set based on the development sets from SQuAD and RACE using the same procedure as in Sec. 4. 1 .", "ner": [["BERT", "Method"], ["ALBERT", "Method"], ["SQuAD", "Dataset"], ["RACE", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "202888986", "sentence": "We report accuracies for both MLM and sentence classification tasks .", "ner": [["MLM", "Task"], ["sentence classification", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "202888986", "sentence": "Following Yang et al. ( 2 0 1 9 ) and , we evaluate our models on three popular benchmarks : The General Language Understanding Evaluation ( GLUE ) benchmark ( Wang et al. , 2 0 1 8) , two versions of the Stanford Question Answering Dataset ( SQuAD ; , and the ReAding Comprehension from Examinations ( RACE ) dataset ( Lai et al. , 2 0 1 7 ) .", "ner": [["General Language Understanding Evaluation", "Dataset"], ["GLUE", "Dataset"], ["Stanford Question Answering Dataset", "Dataset"], ["SQuAD ;", "Dataset"], ["ReAding Comprehension from Examinations", "Dataset"], ["RACE", "Dataset"]], "rel": [["GLUE", "Synonym-Of", "General Language Understanding Evaluation"], ["SQuAD ;", "Synonym-Of", "Stanford Question Answering Dataset"], ["RACE", "Synonym-Of", "ReAding Comprehension from Examinations"]], "rel_plus": [["GLUE:Dataset", "Synonym-Of", "General Language Understanding Evaluation:Dataset"], ["SQuAD ;:Dataset", "Synonym-Of", "Stanford Question Answering Dataset:Dataset"], ["RACE:Dataset", "Synonym-Of", "ReAding Comprehension from Examinations:Dataset"]]}
{"doc_id": "202888986", "sentence": "The improvement in parameter efficiency showcases the most important advantage of ALBERT 's design choices , as shown in Table 3 : with only around 7 0 % of BERT - large 's parameters , ALBERT - xxlarge achieves significant improvements over BERT - large , as measured by the difference on development set scores for several representative downstream tasks : SQuAD v 1 . 1 ( + 1. 7 % ) , SQuAD v 2 . 0 ( + 4. 2 % ) , MNLI ( + 2. 2 % ) , SST - 2 ( + 3. 0 % ) , and RACE ( + 8. 5 % ) .", "ner": [["ALBERT", "Method"], ["BERT - large", "Method"], ["ALBERT - xxlarge", "Method"], ["BERT - large", "Method"], ["SQuAD v 1 . 1", "Dataset"], ["SQuAD v 2 . 0", "Dataset"], ["MNLI", "Dataset"], ["SST - 2", "Dataset"], ["RACE", "Dataset"]], "rel": [["ALBERT - xxlarge", "Compare-With", "BERT - large"], ["ALBERT - xxlarge", "Compare-With", "BERT - large"], ["ALBERT - xxlarge", "Evaluated-With", "SQuAD v 1 . 1"], ["BERT - large", "Evaluated-With", "SQuAD v 1 . 1"], ["ALBERT - xxlarge", "Evaluated-With", "SQuAD v 2 . 0"], ["BERT - large", "Evaluated-With", "SQuAD v 2 . 0"], ["ALBERT - xxlarge", "Evaluated-With", "MNLI"], ["BERT - large", "Evaluated-With", "MNLI"], ["ALBERT - xxlarge", "Evaluated-With", "SST - 2"], ["BERT - large", "Evaluated-With", "SST - 2"], ["ALBERT - xxlarge", "Evaluated-With", "RACE"], ["BERT - large", "Evaluated-With", "RACE"]], "rel_plus": [["ALBERT - xxlarge:Method", "Compare-With", "BERT - large:Method"], ["ALBERT - xxlarge:Method", "Compare-With", "BERT - large:Method"], ["ALBERT - xxlarge:Method", "Evaluated-With", "SQuAD v 1 . 1:Dataset"], ["BERT - large:Method", "Evaluated-With", "SQuAD v 1 . 1:Dataset"], ["ALBERT - xxlarge:Method", "Evaluated-With", "SQuAD v 2 . 0:Dataset"], ["BERT - large:Method", "Evaluated-With", "SQuAD v 2 . 0:Dataset"], ["ALBERT - xxlarge:Method", "Evaluated-With", "MNLI:Dataset"], ["BERT - large:Method", "Evaluated-With", "MNLI:Dataset"], ["ALBERT - xxlarge:Method", "Evaluated-With", "SST - 2:Dataset"], ["BERT - large:Method", "Evaluated-With", "SST - 2:Dataset"], ["ALBERT - xxlarge:Method", "Evaluated-With", "RACE:Dataset"], ["BERT - large:Method", "Evaluated-With", "RACE:Dataset"]]}
{"doc_id": "202888986", "sentence": "We also observe that BERT - xlarge gets significantly worse results than BERT - base on all metrics .", "ner": [["BERT - xlarge", "Method"], ["BERT - base", "Method"]], "rel": [["BERT - xlarge", "Compare-With", "BERT - base"]], "rel_plus": [["BERT - xlarge:Method", "Compare-With", "BERT - base:Method"]]}
{"doc_id": "202888986", "sentence": "Because of less communication and fewer computations , ALBERT models have higher data throughput compared to their corresponding BERT models .", "ner": [["ALBERT", "Method"], ["BERT", "Method"]], "rel": [["ALBERT", "Compare-With", "BERT"]], "rel_plus": [["ALBERT:Method", "Compare-With", "BERT:Method"]]}
{"doc_id": "202888986", "sentence": "As the models get larger , the differences between BERT and ALBERT models become bigger , e.g. , ALBERT - xlarge can be trained 2. 4 x faster than BERT - xlarge .", "ner": [["BERT", "Method"], ["ALBERT", "Method"], ["ALBERT - xlarge", "Method"], ["BERT - xlarge", "Method"]], "rel": [["BERT", "Compare-With", "ALBERT"], ["ALBERT - xlarge", "Compare-With", "BERT - xlarge"]], "rel_plus": [["BERT:Method", "Compare-With", "ALBERT:Method"], ["ALBERT - xlarge:Method", "Compare-With", "BERT - xlarge:Method"]]}
{"doc_id": "202888986", "sentence": "We compare the all - shared strategy ( ALBERT - style ) , the not - shared strategy ( BERT - style ) , and intermediate strategies in which only the attention parameters are shared ( but not the FNN ones ) or only the FFN parameters are shared ( but not the attention ones ) .   The all - shared strategy hurts performance under both conditions , but it is less severe for E = 1 2 8 ( - 1 . 5 on Avg ) compared to E = 7 6 8 ( - 2 . 5 on Avg ) .", "ner": [["ALBERT", "Method"], ["BERT", "Method"], ["FFN", "Method"]], "rel": [["ALBERT", "Compare-With", "BERT"]], "rel_plus": [["ALBERT:Method", "Compare-With", "BERT:Method"]]}
{"doc_id": "202888986", "sentence": "We compare head - to - head three experimental conditions for the additional inter - sentence loss : none ( XLNet - and RoBERTa - style ) , NSP ( BERT - style ) , and SOP ( ALBERT - style ) , using an ALBERTbase configuration .", "ner": [["XLNet", "Method"], ["RoBERTa", "Method"], ["NSP", "Task"], ["BERT", "Method"], ["SOP", "Task"], ["ALBERT", "Method"], ["ALBERTbase", "Method"]], "rel": [["BERT", "Trained-With", "NSP"], ["ALBERT", "Trained-With", "SOP"]], "rel_plus": [["BERT:Method", "Trained-With", "NSP:Task"], ["ALBERT:Method", "Trained-With", "SOP:Task"]]}
{"doc_id": "202888986", "sentence": "Results are shown in Table 6 , both over intrinsic ( accuracy for the MLM , NSP , and SOP tasks ) and downstream tasks .", "ner": [["MLM", "Task"], ["NSP", "Task"], ["SOP", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "202888986", "sentence": "The results on the intrinsic tasks reveal that the NSP loss brings no discriminative power to the SOP task ( 5 2 . 0 % accuracy , similar to the random - guess performance for the \" None \" condition ) .", "ner": [["NSP", "Task"], ["SOP", "Task"]], "rel": [["NSP", "Compare-With", "SOP"]], "rel_plus": [["NSP:Task", "Compare-With", "SOP:Task"]]}
{"doc_id": "202888986", "sentence": "In contrast , the SOP loss does solve the NSP task relatively well ( 7 8 . 9 % accuracy ) , and the SOP task even better ( 8 6 . 5 % accuracy ) .", "ner": [["SOP", "Task"], ["NSP", "Task"], ["SOP", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "202888986", "sentence": "Even more importantly , the SOP loss appears to consistently improve downstream task performance for multi - sentence encoding tasks ( around + 1% for SQuAD 1 . 1 , + 2% for SQuAD 2 . 0 , + 1. 7 % for RACE ) , for an Avg score improvement of around + 1% .", "ner": [["SOP", "Task"], ["SQuAD 1 . 1", "Dataset"], ["SQuAD 2 . 0", "Dataset"], ["RACE", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "202888986", "sentence": "Networks with 3 or more layers are trained by fine - tuning using the parameters from the depth before ( e.g. , the 1 2 - layer network parameters are fine - tuned from the checkpoint of the 6 - layer network parameters ) . 4 If we compare a 3 - layer ALBERT model with a 1 - layer ALBERT model , although they have the same number of parameters , the performance increases significantly .", "ner": [["ALBERT", "Method"], ["ALBERT", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202888986", "sentence": "The speed - up results in Table 3 indicate that data - throughput for BERT - large is about 3. 1 7 x higher compared to ALBERT - xxlarge .", "ner": [["BERT - large", "Method"], ["ALBERT - xxlarge", "Method"]], "rel": [["BERT - large", "Compare-With", "ALBERT - xxlarge"]], "rel_plus": [["BERT - large:Method", "Compare-With", "ALBERT - xxlarge:Method"]]}
{"doc_id": "202888986", "sentence": "The experiments done up to this point use only the Wikipedia and BOOKCORPUS datasets , as in .", "ner": [["Wikipedia", "Dataset"], ["BOOKCORPUS", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "202888986", "sentence": "In this section , we report measurements on the impact of the additional data used by both XLNet and RoBERTa .", "ner": [["XLNet", "Method"], ["RoBERTa", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202888986", "sentence": "The plot in Fig. 3b shows that removing dropout significantly improves MLM accuracy .", "ner": [["dropout", "Method"], ["MLM", "Task"]], "rel": [["dropout", "Used-For", "MLM"]], "rel_plus": [["dropout:Method", "Used-For", "MLM:Task"]]}
{"doc_id": "202888986", "sentence": "Intermediate evaluation on ALBERT - xxlarge at around 1M training steps ( Table 1 2 ) also confirms that removing dropout helps the downstream tasks .", "ner": [["ALBERT - xxlarge", "Method"], ["dropout", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202888986", "sentence": "The single - model ALBERT configuration incorporates the best - performing settings discussed : an ALBERT - xxlarge configuration ( Table 2 ) using combined MLM and SOP losses , and no dropout .", "ner": [["ALBERT", "Method"], ["ALBERT - xxlarge", "Method"], ["MLM", "Task"], ["SOP", "Task"], ["dropout", "Method"]], "rel": [["ALBERT - xxlarge", "Trained-With", "MLM"], ["ALBERT - xxlarge", "Trained-With", "SOP"]], "rel_plus": [["ALBERT - xxlarge:Method", "Trained-With", "MLM:Task"], ["ALBERT - xxlarge:Method", "Trained-With", "SOP:Task"]]}
{"doc_id": "202888986", "sentence": "For the GLUE ( Table 1 3 ) and RACE ( Table 1 4 ) benchmarks , we average the model predictions for the ensemble models , where the candidates are fine - tuned from different training steps using the 1 2 - layer and 2 4 - layer architectures .", "ner": [["GLUE", "Dataset"], ["RACE", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "202888986", "sentence": "Both single - model and ensemble results indicate that ALBERT improves the state - of - the - art significantly for all three benchmarks , achieving a GLUE score of 8 9 . 4 , a SQuAD 2. 0 test F 1 score of 9 2 . 2 , and a RACE test accuracy of 8 9 . 4 .", "ner": [["ALBERT", "Method"], ["GLUE", "Dataset"], ["SQuAD", "Dataset"], ["RACE", "Dataset"]], "rel": [["ALBERT", "Evaluated-With", "GLUE"], ["ALBERT", "Evaluated-With", "SQuAD"], ["ALBERT", "Evaluated-With", "RACE"]], "rel_plus": [["ALBERT:Method", "Evaluated-With", "GLUE:Dataset"], ["ALBERT:Method", "Evaluated-With", "SQuAD:Dataset"], ["ALBERT:Method", "Evaluated-With", "RACE:Dataset"]]}
{"doc_id": "202888986", "sentence": "The latter appears to be a particularly strong improvement , a jump of + 1 7 . 4 % absolute points over BERT , + 7. 6 % over XLNet , + 6. 2 % over RoBERTa , and 5. 3 % over DCMI+ , an ensemble of multiple models specifically designed for reading comprehension tasks .", "ner": [["BERT", "Method"], ["XLNet", "Method"], ["RoBERTa", "Method"], ["DCMI+", "Method"], ["reading comprehension", "Task"]], "rel": [["DCMI+", "Used-For", "reading comprehension"]], "rel_plus": [["DCMI+:Method", "Used-For", "reading comprehension:Task"]]}
{"doc_id": "202888986", "sentence": "Our single model achieves an accuracy of 8 6 . 5 % , which is still 2. 4 % better than the state - of - the - art ensemble model .   While ALBERT - xxlarge has less parameters than BERT - large and gets significantly better results , it is computationally more expensive due to its larger structure .", "ner": [["ALBERT - xxlarge", "Method"], ["BERT - large", "Method"]], "rel": [["ALBERT - xxlarge", "Compare-With", "BERT - large"]], "rel_plus": [["ALBERT - xxlarge:Method", "Compare-With", "BERT - large:Method"]]}
{"doc_id": "52910494", "sentence": "Recently , Convolution Neural Networks ( CNNs ) obtained huge success in numerous vision tasks .", "ner": [["Convolution Neural Networks", "Method"], ["CNNs", "Method"]], "rel": [["CNNs", "Synonym-Of", "Convolution Neural Networks"]], "rel_plus": [["CNNs:Method", "Synonym-Of", "Convolution Neural Networks:Method"]]}
{"doc_id": "52910494", "sentence": "In particular , DenseNets have demonstrated that feature reuse via dense skip connections can effectively alleviate the difficulty of training very deep networks and that reusing features generated by the initial layers in all subsequent layers has strong impact on performance .", "ner": [["DenseNets", "Method"], ["dense skip connections", "Method"]], "rel": [["dense skip connections", "Part-Of", "DenseNets"]], "rel_plus": [["dense skip connections:Method", "Part-Of", "DenseNets:Method"]]}
{"doc_id": "52910494", "sentence": "Composed of layers for multi - scale convolutions , trainable cross - scale aggregation , maxout , and concatenation , this module is highly non - linear and can boost the accuracy of DenseNet while using much fewer parameters .", "ner": [["multi - scale convolutions", "Method"], ["cross - scale aggregation", "Method"], ["maxout", "Method"], ["concatenation", "Method"], ["DenseNet", "Method"]], "rel": [["concatenation", "Part-Of", "DenseNet"], ["maxout", "Part-Of", "DenseNet"], ["cross - scale aggregation", "Part-Of", "DenseNet"], ["multi - scale convolutions", "Part-Of", "DenseNet"]], "rel_plus": [["concatenation:Method", "Part-Of", "DenseNet:Method"], ["maxout:Method", "Part-Of", "DenseNet:Method"], ["cross - scale aggregation:Method", "Part-Of", "DenseNet:Method"], ["multi - scale convolutions:Method", "Part-Of", "DenseNet:Method"]]}
{"doc_id": "52910494", "sentence": "Experimental results on CIFAR - 1 0 , CIFAR - 1 0 0 and SVHN benchmarks demonstrated the effectiveness of the proposed methods .", "ner": [["CIFAR - 1 0", "Dataset"], ["CIFAR - 1 0 0", "Dataset"], ["SVHN", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "52910494", "sentence": "Recently , deep learning became a dominant field of machine learning for various vision tasks , such as recognition and classification .", "ner": [["deep learning", "Method"], ["machine learning", "Method"], ["recognition", "Task"], ["classification", "Task"]], "rel": [["deep learning", "SubClass-Of", "machine learning"], ["machine learning", "Used-For", "recognition"], ["deep learning", "Used-For", "recognition"], ["machine learning", "Used-For", "classification"], ["deep learning", "Used-For", "classification"]], "rel_plus": [["deep learning:Method", "SubClass-Of", "machine learning:Method"], ["machine learning:Method", "Used-For", "recognition:Task"], ["deep learning:Method", "Used-For", "recognition:Task"], ["machine learning:Method", "Used-For", "classification:Task"], ["deep learning:Method", "Used-For", "classification:Task"]]}
{"doc_id": "52910494", "sentence": "In particular , Convolutional Neural Networks ( CNNs ) have achieved an unprecedented success through AlexNet [ 1 4 ] , which has incurred a new line of research concentrating on constructing better performing CNNs [ 2 8 ] .", "ner": [["Convolutional Neural Networks", "Method"], ["CNNs", "Method"], ["AlexNet", "Method"], ["CNNs", "Method"]], "rel": [["CNNs", "Synonym-Of", "Convolutional Neural Networks"], ["AlexNet", "SubClass-Of", "Convolutional Neural Networks"]], "rel_plus": [["CNNs:Method", "Synonym-Of", "Convolutional Neural Networks:Method"], ["AlexNet:Method", "SubClass-Of", "Convolutional Neural Networks:Method"]]}
{"doc_id": "52910494", "sentence": "AlexNets have 5 convolutional layers [ 1 4 ] , VGG Nets [ 2 3 ] have 1 6 or 1 9 , GoogLeNets [ 2 7 ] have 2 2 , and ResNets [ 6 ] feature over 1 0 0 0 layers employing residual connections .", "ner": [["AlexNets", "Method"], ["convolutional layers", "Method"], ["VGG Nets", "Method"], ["GoogLeNets", "Method"], ["ResNets", "Method"], ["residual connections", "Method"]], "rel": [["convolutional layers", "Part-Of", "AlexNets"], ["convolutional layers", "Part-Of", "VGG Nets"], ["convolutional layers", "Part-Of", "ResNets"], ["residual connections", "Part-Of", "ResNets"]], "rel_plus": [["convolutional layers:Method", "Part-Of", "AlexNets:Method"], ["convolutional layers:Method", "Part-Of", "VGG Nets:Method"], ["convolutional layers:Method", "Part-Of", "ResNets:Method"], ["residual connections:Method", "Part-Of", "ResNets:Method"]]}
{"doc_id": "52910494", "sentence": "To deal with these problems , several creative architectures , such as Highway networks [ 2 4 ] , Deeply - Supervised Nets [ 1 6 ] and ResNets [ 6 ] , have been designed .", "ner": [["Highway networks", "Method"], ["Deeply - Supervised Nets", "Method"], ["ResNets", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "52910494", "sentence": "Stochastic depth [ 1 0 ] trains an ensemble of ResNets with different depth values by randomly dropping a set of layers during the training phase .", "ner": [["Stochastic depth", "Method"], ["ResNets", "Method"]], "rel": [["Stochastic depth", "Used-For", "ResNets"]], "rel_plus": [["Stochastic depth:Method", "Used-For", "ResNets:Method"]]}
{"doc_id": "52910494", "sentence": "This new connection pattern allows DenseNets to obtain significant improvements over the state - of - the - art on several object recognition benchmark tasks .", "ner": [["DenseNets", "Method"], ["object recognition", "Task"]], "rel": [["DenseNets", "Used-For", "object recognition"]], "rel_plus": [["DenseNets:Method", "Used-For", "object recognition:Task"]]}
{"doc_id": "52910494", "sentence": "This module is composed by convolutions with different kernel size ( 1 \u00d7 1 , 3 \u00d7 3 , 5 \u00d7 5 ) and a 3 \u00d7 3 max pooling , and then concatenates results from the convolutions and pooling .", "ner": [["convolutions", "Method"], ["3 \u00d7 3 max pooling", "Method"], ["convolutions", "Method"], ["pooling", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "52910494", "sentence": "Recently , feature pyramid networks ( FPN ) [ 1 9 ] and deep layer aggregation [ 3 4 ] have been proposed , which aim at exploiting the inherent multi - scale , pyramidal hierarchy of CNNs .", "ner": [["feature pyramid networks", "Method"], ["FPN", "Method"], ["deep layer aggregation", "Method"], ["CNNs", "Method"]], "rel": [["FPN", "Synonym-Of", "feature pyramid networks"], ["feature pyramid networks", "Part-Of", "CNNs"], ["deep layer aggregation", "Part-Of", "CNNs"]], "rel_plus": [["FPN:Method", "Synonym-Of", "feature pyramid networks:Method"], ["feature pyramid networks:Method", "Part-Of", "CNNs:Method"], ["deep layer aggregation:Method", "Part-Of", "CNNs:Method"]]}
{"doc_id": "52910494", "sentence": "Inspired by the benefits of multi - scale convolutions [ 1 9 , 3 4 ] and features fusion for training deep networks , we design a novel module , referred as Multi - scale Convolution Aggregation ( MCA ) to work with DenseNets .", "ner": [["multi - scale convolutions", "Method"], ["features fusion", "Method"], ["deep networks", "Method"], ["Multi - scale Convolution Aggregation", "Method"], ["MCA", "Method"], ["DenseNets", "Method"]], "rel": [["multi - scale convolutions", "Part-Of", "deep networks"], ["features fusion", "Part-Of", "deep networks"], ["MCA", "Synonym-Of", "Multi - scale Convolution Aggregation"], ["Multi - scale Convolution Aggregation", "Part-Of", "DenseNets"]], "rel_plus": [["multi - scale convolutions:Method", "Part-Of", "deep networks:Method"], ["features fusion:Method", "Part-Of", "deep networks:Method"], ["MCA:Method", "Synonym-Of", "Multi - scale Convolution Aggregation:Method"], ["Multi - scale Convolution Aggregation:Method", "Part-Of", "DenseNets:Method"]]}
{"doc_id": "52910494", "sentence": "As shown in Fig. 1 , the MCA module consists of layers for multiscale convolutions , cross - scale aggregation , maxout , and concatenation .", "ner": [["MCA", "Method"], ["multiscale convolutions", "Method"], ["cross - scale aggregation", "Method"], ["maxout", "Method"], ["concatenation", "Method"]], "rel": [["multiscale convolutions", "Part-Of", "MCA"], ["cross - scale aggregation", "Part-Of", "MCA"], ["maxout", "Part-Of", "MCA"], ["concatenation", "Part-Of", "MCA"]], "rel_plus": [["multiscale convolutions:Method", "Part-Of", "MCA:Method"], ["cross - scale aggregation:Method", "Part-Of", "MCA:Method"], ["maxout:Method", "Part-Of", "MCA:Method"], ["concatenation:Method", "Part-Of", "MCA:Method"]]}
{"doc_id": "52910494", "sentence": "We observe that DenseNets utilizing MCA Figure 1 .", "ner": [["DenseNets", "Method"], ["MCA", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "52910494", "sentence": "DenseNets with Multi - scale Convolution Aggregation ( MCA ) module .", "ner": [["DenseNets", "Method"], ["Multi - scale Convolution Aggregation", "Method"], ["MCA", "Method"]], "rel": [["Multi - scale Convolution Aggregation", "Part-Of", "DenseNets"], ["MCA", "Synonym-Of", "Multi - scale Convolution Aggregation"]], "rel_plus": [["Multi - scale Convolution Aggregation:Method", "Part-Of", "DenseNets:Method"], ["MCA:Method", "Synonym-Of", "Multi - scale Convolution Aggregation:Method"]]}
{"doc_id": "52910494", "sentence": "The two channels are concatenated into a layer of feature maps , which is fed into the DenseNets represented by with 3 composite layers on the right . module can substantially reduce parameters number and classification error than using other multi - scale designs .", "ner": [["DenseNets", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "52910494", "sentence": "The increase in accuracy is attributed to the following factors : 1 ) strengthening scale - invariance because of the multi - scale convolutions with four kernels with different receptive field sizes ; 2 ) given a specific task , the network automatically chooses the most suitable scales via four trainable gating units to adaptively make use of multiscale information ; 3 ) the use of two maxout activations stimulates the competition among neural units of different receptive fields and enhances the learning ability of the network ; 4 ) higher non - linearity ; and 5 ) compared with traditional concatenation in GoogleNets , our module dramatically reduces the number of parameters while preserving sufficient multi - scale information by aggregation and maxout functions .", "ner": [["multi - scale convolutions", "Method"], ["maxout", "Method"], ["concatenation", "Method"], ["GoogleNets", "Method"], ["maxout", "Method"]], "rel": [["concatenation", "Part-Of", "GoogleNets"], ["maxout", "Part-Of", "GoogleNets"]], "rel_plus": [["concatenation:Method", "Part-Of", "GoogleNets:Method"], ["maxout:Method", "Part-Of", "GoogleNets:Method"]]}
{"doc_id": "52910494", "sentence": "These include dropout [ 8 ] , maxout activation [ 4 ] , batch normalization [ 1 1 , 1 2 ] , group normalization [ 3 1 ] , Xavier initialization [ 2 ] , He initialization [ 5 ] , etc . , which have been applied in a wide range of networks as essential components .", "ner": [["dropout", "Method"], ["maxout activation", "Method"], ["batch normalization", "Method"], ["group normalization", "Method"], ["Xavier initialization", "Method"], ["He initialization", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "52910494", "sentence": "To reduce the possibility of overfitting in DenseNets and to further boost the generalization of networks , we also develop a regularization method named Stochastic Feature Reuse ( SFR ) .", "ner": [["DenseNets", "Method"], ["Stochastic Feature Reuse", "Method"], ["SFR", "Method"]], "rel": [["Stochastic Feature Reuse", "Part-Of", "DenseNets"], ["SFR", "Synonym-Of", "Stochastic Feature Reuse"]], "rel_plus": [["Stochastic Feature Reuse:Method", "Part-Of", "DenseNets:Method"], ["SFR:Method", "Synonym-Of", "Stochastic Feature Reuse:Method"]]}
{"doc_id": "52910494", "sentence": "Similar to stochastic depth [ 1 0 ] , SFR contains gates for dropping selected feature maps delivered from preceding layers ; see Fig. 4 .", "ner": [["stochastic depth", "Method"], ["SFR", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "52910494", "sentence": "This approach effectively addresses overfitting problem of DenseNet by substantially reducing the number of parameters while improving the performance of DenseNets .", "ner": [["DenseNet", "Method"], ["DenseNets", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "52910494", "sentence": "We evaluate the impacts of both MCA module and SFR on three widely used benchmark datasets : CIFAR - 1 0 [ 1 3 ] , CIFAR - 1 0 0 [ 1 3 ] and Street View House Number ( SVHN ) [ 2 1 ] .", "ner": [["MCA", "Method"], ["SFR", "Method"], ["CIFAR - 1 0", "Dataset"], ["CIFAR - 1 0 0", "Dataset"], ["Street View House Number", "Dataset"], ["SVHN", "Dataset"]], "rel": [["MCA", "Evaluated-With", "CIFAR - 1 0"], ["SFR", "Evaluated-With", "CIFAR - 1 0"], ["SFR", "Evaluated-With", "CIFAR - 1 0 0"], ["MCA", "Evaluated-With", "CIFAR - 1 0 0"], ["SVHN", "Synonym-Of", "Street View House Number"], ["MCA", "Evaluated-With", "Street View House Number"], ["SFR", "Evaluated-With", "Street View House Number"]], "rel_plus": [["MCA:Method", "Evaluated-With", "CIFAR - 1 0:Dataset"], ["SFR:Method", "Evaluated-With", "CIFAR - 1 0:Dataset"], ["SFR:Method", "Evaluated-With", "CIFAR - 1 0 0:Dataset"], ["MCA:Method", "Evaluated-With", "CIFAR - 1 0 0:Dataset"], ["SVHN:Dataset", "Synonym-Of", "Street View House Number:Dataset"], ["MCA:Method", "Evaluated-With", "Street View House Number:Dataset"], ["SFR:Method", "Evaluated-With", "Street View House Number:Dataset"]]}
{"doc_id": "52910494", "sentence": "This leads to the recent resurgence of exploration in sophisticated CNNs architectures [ 9 ] with hugely increased classification accuracy on ImageNet [ 1 ] , e.g. from AlexNet [ 1 4 ] to GoogLeNets [ 2 7 ] , and ResNets [ 6 ] to DenseNets [ 9 ] .", "ner": [["CNNs", "Method"], ["classification", "Task"], ["ImageNet", "Dataset"], ["AlexNet", "Method"], ["GoogLeNets", "Method"], ["ResNets", "Method"], ["DenseNets", "Method"]], "rel": [["ResNets", "SubClass-Of", "CNNs"], ["GoogLeNets", "SubClass-Of", "CNNs"], ["AlexNet", "SubClass-Of", "CNNs"], ["DenseNets", "SubClass-Of", "CNNs"], ["CNNs", "Used-For", "classification"], ["ImageNet", "Benchmark-For", "classification"], ["DenseNets", "Used-For", "classification"], ["ResNets", "Used-For", "classification"], ["GoogLeNets", "Used-For", "classification"], ["AlexNet", "Used-For", "classification"], ["CNNs", "Evaluated-With", "ImageNet"], ["AlexNet", "Evaluated-With", "ImageNet"], ["GoogLeNets", "Evaluated-With", "ImageNet"], ["ResNets", "Evaluated-With", "ImageNet"], ["DenseNets", "Evaluated-With", "ImageNet"]], "rel_plus": [["ResNets:Method", "SubClass-Of", "CNNs:Method"], ["GoogLeNets:Method", "SubClass-Of", "CNNs:Method"], ["AlexNet:Method", "SubClass-Of", "CNNs:Method"], ["DenseNets:Method", "SubClass-Of", "CNNs:Method"], ["CNNs:Method", "Used-For", "classification:Task"], ["ImageNet:Dataset", "Benchmark-For", "classification:Task"], ["DenseNets:Method", "Used-For", "classification:Task"], ["ResNets:Method", "Used-For", "classification:Task"], ["GoogLeNets:Method", "Used-For", "classification:Task"], ["AlexNet:Method", "Used-For", "classification:Task"], ["CNNs:Method", "Evaluated-With", "ImageNet:Dataset"], ["AlexNet:Method", "Evaluated-With", "ImageNet:Dataset"], ["GoogLeNets:Method", "Evaluated-With", "ImageNet:Dataset"], ["ResNets:Method", "Evaluated-With", "ImageNet:Dataset"], ["DenseNets:Method", "Evaluated-With", "ImageNet:Dataset"]]}
{"doc_id": "52910494", "sentence": "GoogLeNets [ 1 2 , 2 3 , 2 6 , 2 8 ] use the inception module to build deep networks and this component concatenates feature maps produced by a set of filters with different receptive field size .", "ner": [["GoogLeNets", "Method"], ["inception module", "Method"]], "rel": [["inception module", "Part-Of", "GoogLeNets"]], "rel_plus": [["inception module:Method", "Part-Of", "GoogLeNets:Method"]]}
{"doc_id": "52910494", "sentence": "Other well - known structures , such as Resnet in Resnet [ 2 9 ] and Wide residual networks [ 3 5 ] , also demonstrate that simply increasing the number of filters in each layer can dramatically improve test accuracy .", "ner": [["Resnet", "Method"], ["Resnet", "Method"], ["Wide residual networks", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "52910494", "sentence": "Deep Layer Aggregation [ 3 4 ] provides a novel approach to fuse features vertically across layers , which substantially improves recognition accuracy with less computational cost .", "ner": [["Deep Layer Aggregation", "Method"], ["recognition", "Task"]], "rel": [["Deep Layer Aggregation", "Used-For", "recognition"]], "rel_plus": [["Deep Layer Aggregation:Method", "Used-For", "recognition:Task"]]}
{"doc_id": "52910494", "sentence": "Inspired by these findings , we design a novel MCA module , which first broadens the width of the initial convolution layer of DenseNets through multi - scale convolutions , then fuses the filters using cross - scale aggregation parameterised by trainable weights .", "ner": [["MCA", "Method"], ["convolution layer", "Method"], ["DenseNets", "Method"], ["multi - scale convolutions", "Method"], ["cross - scale aggregation", "Method"]], "rel": [["MCA", "Part-Of", "convolution layer"], ["convolution layer", "Part-Of", "DenseNets"], ["multi - scale convolutions", "Part-Of", "DenseNets"], ["cross - scale aggregation", "Part-Of", "DenseNets"]], "rel_plus": [["MCA:Method", "Part-Of", "convolution layer:Method"], ["convolution layer:Method", "Part-Of", "DenseNets:Method"], ["multi - scale convolutions:Method", "Part-Of", "DenseNets:Method"], ["cross - scale aggregation:Method", "Part-Of", "DenseNets:Method"]]}
{"doc_id": "52910494", "sentence": "ResNets [ 6 ] further enhance this new connection pattern through substituting bypassing paths with residual connections , and achieve record - breaking performance on Im - ageNet [ 1 ] .", "ner": [["ResNets", "Method"], ["residual connections", "Method"], ["Im - ageNet", "Dataset"]], "rel": [["residual connections", "Part-Of", "ResNets"], ["ResNets", "Evaluated-With", "Im - ageNet"]], "rel_plus": [["residual connections:Method", "Part-Of", "ResNets:Method"], ["ResNets:Method", "Evaluated-With", "Im - ageNet:Dataset"]]}
{"doc_id": "52910494", "sentence": "Moreover , stochastic depth [ 1 0 ] was proposed as a successful approach to train an over 1 0 0 0 - layer ResNet through randomly dropping a few layers during training .", "ner": [["stochastic depth", "Method"], ["ResNet", "Method"]], "rel": [["stochastic depth", "Part-Of", "ResNet"]], "rel_plus": [["stochastic depth:Method", "Part-Of", "ResNet:Method"]]}
{"doc_id": "52910494", "sentence": "Our SFR regularizer was motivated by the observations on Dropout , Stochastic Depth and DenseNets .", "ner": [["SFR regularizer", "Method"], ["Dropout", "Method"], ["Stochastic Depth", "Method"], ["DenseNets", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "52910494", "sentence": "Both the MCA module and the SFR regularizer proposed in this paper are based on DenseNets [ 9 ] .", "ner": [["MCA", "Method"], ["SFR regularizer", "Method"], ["DenseNets", "Method"]], "rel": [["SFR regularizer", "Part-Of", "DenseNets"], ["MCA", "Part-Of", "DenseNets"]], "rel_plus": [["SFR regularizer:Method", "Part-Of", "DenseNets:Method"], ["MCA:Method", "Part-Of", "DenseNets:Method"]]}
{"doc_id": "52910494", "sentence": "Each layer l comprises a composite function H l ( \u00b7 ) that includes one Batch Normalization layer [ 1 2 ] , one ReLU layer [ 3 ] , and one 3 \u00d7 3 convolution layer .", "ner": [["Batch Normalization", "Method"], ["ReLU", "Method"], ["3 \u00d7 3 convolution layer", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "52910494", "sentence": "The total number of channels in a L - layer DenseNet , N ( L ) , can be approximatively computed as : where N ( 0 ) represents the number of input channels into first dense block and k is the growth rate of the DenseNet .", "ner": [["DenseNet", "Method"], ["dense block", "Method"], ["DenseNet", "Method"]], "rel": [["dense block", "Part-Of", "DenseNet"]], "rel_plus": [["dense block:Method", "Part-Of", "DenseNet:Method"]]}
{"doc_id": "52910494", "sentence": "Inspired by their findings , we design a novel MCA module to enhance the representative and learning capacity of DenseNets .", "ner": [["MCA", "Method"], ["DenseNets", "Method"]], "rel": [["MCA", "Part-Of", "DenseNets"]], "rel_plus": [["MCA:Method", "Part-Of", "DenseNets:Method"]]}
{"doc_id": "52910494", "sentence": "The new module consists of layers for multi - scale convolutions , cross - scale aggregation , maxout , and concatenation .", "ner": [["multi - scale convolutions", "Method"], ["cross - scale aggregation", "Method"], ["maxout", "Method"], ["concatenation", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "52910494", "sentence": "Given the input image x , the multi - scale convolutions layer computes the following : where G n \u00d7 n ( n = 1 , 3 , 5 , 7 ) are the results of convolutions with 1 \u00d7 1 , 3 \u00d7 3 , 5 \u00d7 5 , and 7 \u00d7 7 kernels respectively .", "ner": [["multi - scale convolutions", "Method"], ["convolutions", "Method"]], "rel": [["convolutions", "Part-Of", "multi - scale convolutions"]], "rel_plus": [["convolutions:Method", "Part-Of", "multi - scale convolutions:Method"]]}
{"doc_id": "52910494", "sentence": "Feeding the concatenation of four groups of convolutions , M 1 ( x , W ) , into DenseNets directly helps to improve the performance of the network since the network bandwidth is increased .", "ner": [["concatenation", "Method"], ["convolutions", "Method"], ["DenseNets", "Method"]], "rel": [["concatenation", "Used-For", "convolutions"], ["convolutions", "Part-Of", "DenseNets"], ["concatenation", "Part-Of", "DenseNets"]], "rel_plus": [["concatenation:Method", "Used-For", "convolutions:Method"], ["convolutions:Method", "Part-Of", "DenseNets:Method"], ["concatenation:Method", "Part-Of", "DenseNets:Method"]]}
{"doc_id": "52910494", "sentence": "When evaluated on CIFAR - 1 0 dataset , a standard DenseNet with depth L = 4 0 and growth rate k = 2 4 achieves 9 3 . 4 5 % , whereas the DenseNet with M 1 ( x , W ) as input achieves a test accuracy of 9 4 . 3 1 % .", "ner": [["CIFAR - 1 0", "Dataset"], ["DenseNet", "Method"], ["DenseNet", "Method"]], "rel": [["DenseNet", "Evaluated-With", "CIFAR - 1 0"]], "rel_plus": [["DenseNet:Method", "Evaluated-With", "CIFAR - 1 0:Dataset"]]}
{"doc_id": "52910494", "sentence": "Compared to the Inception module that simply concatenates different groups of convolutions , the aggregation layer we used can significantly reduces the number of parameters .", "ner": [["Inception module", "Method"], ["convolutions", "Method"], ["aggregation layer", "Method"]], "rel": [["convolutions", "Part-Of", "Inception module"], ["aggregation layer", "Part-Of", "Inception module"]], "rel_plus": [["convolutions:Method", "Part-Of", "Inception module:Method"], ["aggregation layer:Method", "Part-Of", "Inception module:Method"]]}
{"doc_id": "52910494", "sentence": "On CIFAR - 1 0 dataset , the number of parameters is reduced from 5. 7 millions to 4. 2 millions in DenseNet with depth = 4 0 and growth rate k = 2 4 .", "ner": [["CIFAR - 1 0", "Dataset"], ["DenseNet", "Method"]], "rel": [["DenseNet", "Trained-With", "CIFAR - 1 0"]], "rel_plus": [["DenseNet:Method", "Trained-With", "CIFAR - 1 0:Dataset"]]}
{"doc_id": "52910494", "sentence": "Previous work have shown that : 1 ) maxout exploits the model averaging behavior as the approximation is more accurate ; 2 ) back - forward flow of maxout can avoid pitfalls such as failing to use a large set of filters [ 4 ] ; and 3 ) grouping is important in deep networks [ 3 1 ] .", "ner": [["maxout", "Method"], ["maxout", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "52910494", "sentence": "Hence , to better regularize our fusion results , here two maxout operations are independently performed after cross - scale aggregation layer , one for the two fines scale channels and the other for the two coarser scale channels .", "ner": [["maxout", "Method"], ["cross - scale aggregation", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "52910494", "sentence": "That is , we have the final output of MCA module MC(x , W ): With maxout layer introduced , the whole MCA module can be viewed as a highly non - linear transformation between original input and the first dense block of DenseNets .", "ner": [["MCA", "Method"], ["MC(x , W ):", "Method"], ["maxout", "Method"], ["MCA", "Method"], ["dense block", "Method"], ["DenseNets", "Method"]], "rel": [["MC(x , W ):", "Synonym-Of", "MCA"], ["maxout", "Part-Of", "MCA"], ["dense block", "Part-Of", "DenseNets"], ["MCA", "Part-Of", "DenseNets"]], "rel_plus": [["MC(x , W )::Method", "Synonym-Of", "MCA:Method"], ["maxout:Method", "Part-Of", "MCA:Method"], ["dense block:Method", "Part-Of", "DenseNets:Method"], ["MCA:Method", "Part-Of", "DenseNets:Method"]]}
{"doc_id": "52910494", "sentence": "Here , we present the derivation formula in terms of weights of multi - scale convolutions ; see Equ . ( 6 ) .   where L is the loss function of the whole network and W 1 , b 1 are the weight and bias of the first layer in the first dense block . \u03b4 l is the sensitivity of l th layer .", "ner": [["multi - scale convolutions", "Method"], ["dense block", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "52910494", "sentence": "Dropout [ 8 ] , Drop - connect [ 3 0 ] and Maxout [ 4 ] provide excellent regularization methods through modifying interactions among neural units or connections between different layers in order to break co - adaptation .", "ner": [["Dropout", "Method"], ["Drop - connect", "Method"], ["Maxout", "Method"], ["regularization methods", "Method"]], "rel": [["Maxout", "SubClass-Of", "regularization methods"], ["Drop - connect", "SubClass-Of", "regularization methods"], ["Dropout", "SubClass-Of", "regularization methods"]], "rel_plus": [["Maxout:Method", "SubClass-Of", "regularization methods:Method"], ["Drop - connect:Method", "SubClass-Of", "regularization methods:Method"], ["Dropout:Method", "SubClass-Of", "regularization methods:Method"]]}
{"doc_id": "52910494", "sentence": "These techniques have been supported by subsequent research and applied in a wide range of network architectures , such as ResNets [ 6 ] and FractalNets [ 1 5 ] .", "ner": [["ResNets", "Method"], ["FractalNets", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "52910494", "sentence": "Recent stochastic depth [ 1 0 ] and drop - path [ 1 5 ] successfully extend dropout and make impressive progress in vision tasks .", "ner": [["stochastic depth", "Method"], ["drop - path", "Method"], ["dropout", "Method"]], "rel": [["drop - path", "SubClass-Of", "dropout"], ["stochastic depth", "SubClass-Of", "dropout"]], "rel_plus": [["drop - path:Method", "SubClass-Of", "dropout:Method"], ["stochastic depth:Method", "SubClass-Of", "dropout:Method"]]}
{"doc_id": "52910494", "sentence": "Motivated by these structures , we propose \" Stochastic Feature Reuse \" ( SFR ) as an effective regularizer in DenseNets to promote the generalization of networks and Figure 4 .", "ner": [["Stochastic Feature Reuse", "Method"], ["SFR", "Method"], ["regularizer", "Method"], ["DenseNets", "Method"]], "rel": [["SFR", "Synonym-Of", "Stochastic Feature Reuse"], ["Stochastic Feature Reuse", "SubClass-Of", "regularizer"], ["Stochastic Feature Reuse", "Part-Of", "DenseNets"], ["regularizer", "Part-Of", "DenseNets"]], "rel_plus": [["SFR:Method", "Synonym-Of", "Stochastic Feature Reuse:Method"], ["Stochastic Feature Reuse:Method", "SubClass-Of", "regularizer:Method"], ["Stochastic Feature Reuse:Method", "Part-Of", "DenseNets:Method"], ["regularizer:Method", "Part-Of", "DenseNets:Method"]]}
{"doc_id": "52910494", "sentence": "At a given layer of a dense block , the original DenseNets concatenate of all feature maps produced by preceding layers as input .", "ner": [["dense block", "Method"], ["DenseNets", "Method"]], "rel": [["dense block", "Part-Of", "DenseNets"]], "rel_plus": [["dense block:Method", "Part-Of", "DenseNets:Method"]]}
{"doc_id": "52910494", "sentence": "As a regularizer , SFR can enhance the performance of DenseNets and deal with the overfitting issue [ 9 ] through discouraging co - adaptation .", "ner": [["SFR", "Method"], ["DenseNets", "Method"]], "rel": [["SFR", "Part-Of", "DenseNets"]], "rel_plus": [["SFR:Method", "Part-Of", "DenseNets:Method"]]}
{"doc_id": "52910494", "sentence": "In addition , SFR also implicitly trains an ensemble of DenseNets , which helps to improve the performance .", "ner": [["SFR", "Method"], ["DenseNets", "Method"]], "rel": [["SFR", "Part-Of", "DenseNets"]], "rel_plus": [["SFR:Method", "Part-Of", "DenseNets:Method"]]}
{"doc_id": "52910494", "sentence": "The presented MCA module and SFR regularizer are evaluated using three widely adopted benchmarks : CIFAR - 1 0 [ 1 3 ] , CIFAR - 1 0 0 [ 1 3 ] and SVHN [ 2 1 ] .", "ner": [["MCA", "Method"], ["SFR", "Method"], ["CIFAR - 1 0", "Dataset"], ["CIFAR - 1 0 0", "Dataset"], ["SVHN", "Dataset"]], "rel": [["MCA", "Evaluated-With", "CIFAR - 1 0"], ["SFR", "Evaluated-With", "CIFAR - 1 0"], ["MCA", "Evaluated-With", "CIFAR - 1 0 0"], ["SFR", "Evaluated-With", "CIFAR - 1 0 0"], ["MCA", "Evaluated-With", "SVHN"], ["SFR", "Evaluated-With", "SVHN"]], "rel_plus": [["MCA:Method", "Evaluated-With", "CIFAR - 1 0:Dataset"], ["SFR:Method", "Evaluated-With", "CIFAR - 1 0:Dataset"], ["MCA:Method", "Evaluated-With", "CIFAR - 1 0 0:Dataset"], ["SFR:Method", "Evaluated-With", "CIFAR - 1 0 0:Dataset"], ["MCA:Method", "Evaluated-With", "SVHN:Dataset"], ["SFR:Method", "Evaluated-With", "SVHN:Dataset"]]}
{"doc_id": "52910494", "sentence": "The results show that the performance of DenseNets with MCA modules is superior to the original DenseNets and that the SFR regularizer can effectively prevent overfitting .", "ner": [["DenseNets", "Method"], ["MCA", "Method"], ["DenseNets", "Method"], ["SFR", "Method"]], "rel": [["MCA", "Part-Of", "DenseNets"], ["SFR", "Part-Of", "DenseNets"]], "rel_plus": [["MCA:Method", "Part-Of", "DenseNets:Method"], ["SFR:Method", "Part-Of", "DenseNets:Method"]]}
{"doc_id": "52910494", "sentence": "When evaluating the MCA module , the DenseNet part has three dense blocks , all have equal numbers of layers and the same growth rate .", "ner": [["MCA", "Method"], ["DenseNet", "Method"], ["dense blocks", "Method"]], "rel": [["MCA", "Part-Of", "DenseNet"], ["dense blocks", "Part-Of", "DenseNet"]], "rel_plus": [["MCA:Method", "Part-Of", "DenseNet:Method"], ["dense blocks:Method", "Part-Of", "DenseNet:Method"]]}
{"doc_id": "52910494", "sentence": "When evaluating SFR regularizer , an additional dense block with SFR is added so that the performance of the original DenseNet is not affected .", "ner": [["SFR", "Method"], ["dense block", "Method"], ["SFR", "Method"], ["DenseNet", "Method"]], "rel": [["SFR", "Part-Of", "dense block"], ["dense block", "Part-Of", "DenseNet"]], "rel_plus": [["SFR:Method", "Part-Of", "dense block:Method"], ["dense block:Method", "Part-Of", "DenseNet:Method"]]}
{"doc_id": "52910494", "sentence": "Each composite function of dense block uses a 3 \u00d7 3 convolution layer with zero - padding to keep the feature maps fixed .", "ner": [["dense block", "Method"], ["3 \u00d7 3 convolution layer", "Method"]], "rel": [["3 \u00d7 3 convolution layer", "Part-Of", "dense block"]], "rel_plus": [["3 \u00d7 3 convolution layer:Method", "Part-Of", "dense block:Method"]]}
{"doc_id": "52910494", "sentence": "In this paper , we set compression factor as 1. 0 in standard DenseNet while set as 0. 5 in the structure of DenseNet with bottleneck and compression ( DenseNet - BC ) .", "ner": [["DenseNet", "Method"], ["DenseNet with bottleneck and compression", "Method"], ["DenseNet - BC", "Method"]], "rel": [["DenseNet - BC", "Synonym-Of", "DenseNet with bottleneck and compression"]], "rel_plus": [["DenseNet - BC:Method", "Synonym-Of", "DenseNet with bottleneck and compression:Method"]]}
{"doc_id": "52910494", "sentence": "At the end of the last dense block , a global average pooling layer , followed by a softmax layer , is attached .", "ner": [["dense block", "Method"], ["global average pooling", "Method"], ["softmax", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "52910494", "sentence": "Similar to the standard DenseNet [ 9 ] , DenseNets in our experiments are optimized through the first - order SGD optimizer .", "ner": [["DenseNet", "Method"], ["DenseNets", "Method"], ["SGD", "Method"]], "rel": [["SGD", "Part-Of", "DenseNets"]], "rel_plus": [["SGD:Method", "Part-Of", "DenseNets:Method"]]}
{"doc_id": "52910494", "sentence": "We train 3 5 0 epochs for CIFAR and 4 0 epochs for SVHN .", "ner": [["CIFAR", "Dataset"], ["SVHN", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "52910494", "sentence": "Initial learning rate is 0. 1 and divided by 1 0 at epochs 1 5 0 , 2 2 5 and 3 0 0 for CIFAR and epochs 2 0 and 3 0 for SVHN .", "ner": [["CIFAR", "Dataset"], ["SVHN", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "52910494", "sentence": "We also add weight decay ( 0.0 0 0 1 ) term into our loss function and use Nesterov momentum [ 2 5 ] of 0. 9 for optimization .", "ner": [["weight decay", "Method"], ["Nesterov momentum", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "52910494", "sentence": "Hinton 's Dropout [ 8 ] layer with drop probability p = 0. 2 , Batch Normalization [ 1 2 ] layer and He Initialization of weights [ 5 ] are applied as well .", "ner": [["Dropout", "Method"], ["Batch Normalization", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "52910494", "sentence": "The CIFAR - 1 0 0 dataset extends the number of classes in CIFAR - 1 0 to 1 0 0 , but each class only consists of 6 0 0 images .", "ner": [["CIFAR - 1 0 0", "Dataset"], ["CIFAR - 1 0", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "52910494", "sentence": "Due to more classes and fewer samples for each class , the classification for CIFAR - 1 0 0 is considered as more challenging .", "ner": [["classification", "Task"], ["CIFAR - 1 0 0", "Dataset"]], "rel": [["CIFAR - 1 0 0", "Benchmark-For", "classification"]], "rel_plus": [["CIFAR - 1 0 0:Dataset", "Benchmark-For", "classification:Task"]]}
{"doc_id": "52910494", "sentence": "Street View House Number ( SVHN ) dataset is also a well - known benchmark in computer vision , which consists of color images of dig - 6 5 % and 1. 6 1 % ) .", "ner": [["Street View House Number", "Dataset"], ["SVHN", "Dataset"], ["computer vision", "Task"]], "rel": [["SVHN", "Synonym-Of", "Street View House Number"], ["Street View House Number", "Benchmark-For", "computer vision"]], "rel_plus": [["SVHN:Dataset", "Synonym-Of", "Street View House Number:Dataset"], ["Street View House Number:Dataset", "Benchmark-For", "computer vision:Task"]]}
{"doc_id": "52910494", "sentence": "In the structure of DenseNet - BC , our MCA also has positive impacts on the performance . its 0 to 9 of 3 2 \u00d7 3 2 resolution .", "ner": [["DenseNet - BC", "Method"], ["MCA", "Method"]], "rel": [["MCA", "Part-Of", "DenseNet - BC"]], "rel_plus": [["MCA:Method", "Part-Of", "DenseNet - BC:Method"]]}
{"doc_id": "52910494", "sentence": "For CIFAR dataset , we subtract mean values and divide standard deviations , whereas for SVHN images , the pixel values were divided by 2 5 5 .", "ner": [["CIFAR", "Dataset"], ["SVHN", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "52910494", "sentence": "We train our networks with different depths ( 4 0 , 5 3 , 1 0 0 ) and growth rates k ( k = 1 2 , 2 4 , 4 0 ) and compare our approach with other well - known models on CIFAR - 1 0 , CIFAR - 1 0 0 , SVHN ; see Table 1 .", "ner": [["CIFAR - 1 0", "Dataset"], ["CIFAR - 1 0 0", "Dataset"], ["SVHN ;", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "52910494", "sentence": "To better evaluate our novel module , we train different patterns of aggregation on CIFAR - 1 0 and test the best model on CIFAR - 1 0 , CIFAR - 1 0 0 and SVHN .", "ner": [["CIFAR - 1 0", "Dataset"], ["CIFAR - 1 0", "Dataset"], ["CIFAR - 1 0 0", "Dataset"], ["SVHN", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "52910494", "sentence": "With relatively fewer parameters ( 4. 2 M ) , it obtains the lowest classification error rate on CIFAR - 1 0 ( 5. 3 8 % ) and CIFAR - 1 0 0 ( 2 3 . 7 8 % ) , and second best results on SVHN ( 1. 6 6 % ) .", "ner": [["classification", "Task"], ["CIFAR - 1 0", "Dataset"], ["CIFAR - 1 0 0", "Dataset"], ["SVHN", "Dataset"]], "rel": [["CIFAR - 1 0", "Benchmark-For", "classification"], ["CIFAR - 1 0 0", "Benchmark-For", "classification"], ["SVHN", "Benchmark-For", "classification"]], "rel_plus": [["CIFAR - 1 0:Dataset", "Benchmark-For", "classification:Task"], ["CIFAR - 1 0 0:Dataset", "Benchmark-For", "classification:Task"], ["SVHN:Dataset", "Benchmark-For", "classification:Task"]]}
{"doc_id": "52910494", "sentence": "In the case of k = 4 0 , depth = 4 0 , our model gets impressive results ( 2 2 . 6 5 % ) on CIFAR - 1 0 0 and ( 1. 6 1 % ) on SVHN .", "ner": [["CIFAR - 1 0 0", "Dataset"], ["SVHN", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "52910494", "sentence": "This demonstrates that our MCA module has much higher representative capacity and is able to preserves abundant information of multi - scale convolutions .", "ner": [["MCA", "Method"], ["multi - scale convolutions", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "52910494", "sentence": "Left : comparison between DenseNets with and without SFR regularizer .", "ner": [["DenseNets", "Method"], ["SFR", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "52910494", "sentence": "For instance , in CIFAR tasks , the module assigns high weights ( w 1 and w 3 ) to fine - scale features , whereas less coarse - scale information is delivered to subsequent DenseNet .", "ner": [["CIFAR", "Dataset"], ["DenseNet", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "52910494", "sentence": "To further demonstrate this point , we also run our module on another simple dataset MNIST and obtain the similar observation ( w 7 = 0. 5 4 5 6 for MNIST vs. w 7 = 0. 1 7 0 6 for CIFAR - 1 0 ) .", "ner": [["MNIST", "Dataset"], ["MNIST", "Dataset"], ["CIFAR - 1 0", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "52910494", "sentence": "We evaluate SFR on the same three datasets and compare it with the original DensNet with depth = 5 3 and growth rate k = 2 4 .", "ner": [["SFR", "Method"], ["DensNet", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "52910494", "sentence": "The additional dense block with SFR is placed at the front or at the end of the original DenseNet ; see Table 2 for details .", "ner": [["dense block", "Method"], ["SFR", "Method"], ["DenseNet", "Method"]], "rel": [["SFR", "Part-Of", "dense block"], ["dense block", "Part-Of", "DenseNet"]], "rel_plus": [["SFR:Method", "Part-Of", "dense block:Method"], ["dense block:Method", "Part-Of", "DenseNet:Method"]]}
{"doc_id": "52910494", "sentence": "The comparison shows that placing the additional dense block with SFR at the end of the DenseNet generates lower error rates on all three datasets .", "ner": [["dense block", "Method"], ["SFR", "Method"], ["DenseNet", "Method"]], "rel": [["SFR", "Part-Of", "dense block"], ["dense block", "Part-Of", "DenseNet"]], "rel_plus": [["SFR:Method", "Part-Of", "dense block:Method"], ["dense block:Method", "Part-Of", "DenseNet:Method"]]}
{"doc_id": "52910494", "sentence": "On the other hand , adding the additional dense block with SFR to the front of DenseNet actually hurt the performance since this will lead that shallow layers are too narrow to pass sufficient information flow .", "ner": [["dense block", "Method"], ["SFR", "Method"], ["DenseNet", "Method"]], "rel": [["SFR", "Part-Of", "dense block"], ["dense block", "Part-Of", "DenseNet"]], "rel_plus": [["SFR:Method", "Part-Of", "dense block:Method"], ["dense block:Method", "Part-Of", "DenseNet:Method"]]}
{"doc_id": "52910494", "sentence": "In addition , we observe that SFR should work with Hinton 's Dropout , without which the accuracy also degenerates .", "ner": [["SFR", "Method"], ["Dropout", "Method"]], "rel": [["Dropout", "Part-Of", "SFR"]], "rel_plus": [["Dropout:Method", "Part-Of", "SFR:Method"]]}
{"doc_id": "52910494", "sentence": "Hence , to illustrate the impact of different growth rate on the performance of our SFR , we firstly evaluate on CIFAR - 1 0 based on three growth rates 1 2 , 2 4 and 4 0 .", "ner": [["SFR", "Method"], ["CIFAR - 1 0", "Dataset"]], "rel": [["SFR", "Evaluated-With", "CIFAR - 1 0"]], "rel_plus": [["SFR:Method", "Evaluated-With", "CIFAR - 1 0:Dataset"]]}
{"doc_id": "52910494", "sentence": "Under the Stochastic Gradient Descend ( SGD ) , the model adaptively controls the flow of multi - scale information so that the scales with high discrimination power are preserved whereas the redundant ones are suppressed .", "ner": [["Stochastic Gradient Descend", "Method"], ["SGD", "Method"]], "rel": [["SGD", "Synonym-Of", "Stochastic Gradient Descend"]], "rel_plus": [["SGD:Method", "Synonym-Of", "Stochastic Gradient Descend:Method"]]}
{"doc_id": "52910494", "sentence": "SFR w. SFR Improve SFR(k = 1 2 ) 1 7 1 9 6 6. 9 3 6. 8 0 0. 1 3 SFR(k = 2 4 ) 3 4 3 9 2 6. 4 5 6. 0 8 0. 3 7 SFR(WIL ) 3 4 5 3 6 6. 0 9 5. 7 6 0. 3 3 SFR(k = 4 0 ) 5 7 3 2 0 6. 5 3 6. 3 2 0. 2 1 Table 3 .", "ner": [["SFR", "Method"], ["SFR", "Method"], ["SFR(k = 1 2 )", "Method"], ["SFR(k", "Method"], ["SFR(WIL )", "Method"], ["SFR(k = 4 0 )", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "52910494", "sentence": "Test error with or without SFR under different growth rates k and wider initial layer on CIFAR - 1 0 .", "ner": [["SFR", "Method"], ["CIFAR - 1 0", "Dataset"]], "rel": [["SFR", "Evaluated-With", "CIFAR - 1 0"]], "rel_plus": [["SFR:Method", "Evaluated-With", "CIFAR - 1 0:Dataset"]]}
{"doc_id": "52910494", "sentence": "When growth rate is set as 2 4 , our SFR is more beneficial for improvements of performance on CIFAR - 1 0 . four - scale convolutions and the training results are shown in Fig. 5(left ) .", "ner": [["SFR", "Method"], ["CIFAR - 1 0", "Dataset"], ["convolutions", "Method"]], "rel": [["SFR", "Evaluated-With", "CIFAR - 1 0"]], "rel_plus": [["SFR:Method", "Evaluated-With", "CIFAR - 1 0:Dataset"]]}
{"doc_id": "52910494", "sentence": "It consists of 4 groups of multi - scale convolutions , cross - scale aggregation parametrized by 4 trainable weights and 2 maxout that produces 2 branches of feature maps representing smaller and larger receptive fields respectively .", "ner": [["multi - scale convolutions", "Method"], ["cross - scale aggregation", "Method"], ["maxout", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "52910494", "sentence": "In our experiments , Densenets with our new model obtain excellent performance while requiring substantially fewer parameters than utilizing traditional inception module .", "ner": [["Densenets", "Method"], ["inception module", "Method"]], "rel": [["inception module", "Part-Of", "Densenets"]], "rel_plus": [["inception module:Method", "Part-Of", "Densenets:Method"]]}
{"doc_id": "52910494", "sentence": "Trainable aggregation guarantees the maximum use of multi - scale convolutions and is the key for reducing parameters , whereas maxout strengthens the competitions among units in fine - scale and coarse - scale branches .", "ner": [["multi - scale convolutions", "Method"], ["maxout", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "52910494", "sentence": "In addition , a Stochastic Feature Reuse strategy is also presented for training deep DenseNets effectively and efficiently .", "ner": [["Stochastic Feature Reuse", "Method"], ["DenseNets", "Method"]], "rel": [["Stochastic Feature Reuse", "Part-Of", "DenseNets"]], "rel_plus": [["Stochastic Feature Reuse:Method", "Part-Of", "DenseNets:Method"]]}
{"doc_id": "52910494", "sentence": "Being a simple and easy - to - apply approach , SFR is more useful for wider DenseNets with a larger growth rate and can effectively alleviate the difficulties of training wide networks .", "ner": [["SFR", "Method"], ["DenseNets", "Method"]], "rel": [["SFR", "Part-Of", "DenseNets"]], "rel_plus": [["SFR:Method", "Part-Of", "DenseNets:Method"]]}
{"doc_id": "52910494", "sentence": "For future work , we would like to explore the applications of the MCA module in other prominent deep architectures , as we felt MCA can be beneficial through introducing scale - invariance information without adding feature redundancy .", "ner": [["MCA", "Method"], ["MCA", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202540590", "sentence": "In this paper , we introduce Cosmos QA , a large - scale dataset of 3 5 , 6 0 0 problems that require commonsense - based reading comprehension , formulated as multiple - choice questions .", "ner": [["Cosmos QA", "Dataset"], ["commonsense - based reading comprehension", "Task"]], "rel": [["commonsense - based reading comprehension", "Benchmark-For", "Cosmos QA"]], "rel_plus": [["commonsense - based reading comprehension:Task", "Benchmark-For", "Cosmos QA:Dataset"]]}
{"doc_id": "202540590", "sentence": "To establish baseline performances on Cosmos QA , we experiment with several state - of - the - art neural architectures for reading comprehension , and also propose a new architecture that improves over the competitive baselines .", "ner": [["Cosmos QA", "Dataset"], ["reading comprehension", "Task"]], "rel": [["Cosmos QA", "Benchmark-For", "reading comprehension"]], "rel_plus": [["Cosmos QA:Dataset", "Benchmark-For", "reading comprehension:Task"]]}
{"doc_id": "202540590", "sentence": "For example , after reading the first paragraph in Figure 1 , we can understand that the writer is not a child , yet needs someone to dress him or her every ( indicates the correct answer . ) Importantly , ( 1 ) the correct answer is not explicitly mentioned anywhere in the context paragraph , thus requiring reading between the lines through commonsense inference and ( 2 ) answering the question correctly requires reading the context paragraph , thus requiring reading comprehension and contextual commonsense reasoning . morning , and appears frustrated with the current situation .", "ner": [["commonsense inference", "Task"], ["reading comprehension", "Task"], ["contextual commonsense reasoning", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "202540590", "sentence": "In this paper , we focus on reading comprehension that requires contextual commonsense reasoning , as illustrated in the examples in Figure 1 .", "ner": [["reading comprehension", "Task"], ["contextual commonsense reasoning", "Task"]], "rel": [["contextual commonsense reasoning", "Used-For", "reading comprehension"]], "rel_plus": [["contextual commonsense reasoning:Task", "Used-For", "reading comprehension:Task"]]}
{"doc_id": "202540590", "sentence": "To support research toward commonsense reading comprehension , we introduce COSMOS QA ( Commonsense Machine Comprehension ) , a new dataset with 3 5 , 5 8 8 reading comprehension problems that require reasoning about the causes and effects of events , the likely facts about people and objects in the scene , and hypotheticals and counterfactuals .", "ner": [["commonsense reading comprehension", "Task"], ["COSMOS QA", "Dataset"], ["Commonsense Machine Comprehension", "Dataset"], ["reading comprehension", "Task"]], "rel": [["COSMOS QA", "Benchmark-For", "commonsense reading comprehension"], ["COSMOS QA", "Synonym-Of", "Commonsense Machine Comprehension"], ["COSMOS QA", "Benchmark-For", "reading comprehension"]], "rel_plus": [["COSMOS QA:Dataset", "Benchmark-For", "commonsense reading comprehension:Task"], ["COSMOS QA:Dataset", "Synonym-Of", "Commonsense Machine Comprehension:Dataset"], ["COSMOS QA:Dataset", "Benchmark-For", "reading comprehension:Task"]]}
{"doc_id": "202540590", "sentence": "The vast majority ( 9 3 . 8 % ) of our dataset requires contextual commonsense reasoning , in contrast with existing machine comprehension ( MRC ) datasets such as SQuAD ( Rajpurkar et al. , 2 0 1 6 ) , RACE ( Lai et al. , 2 0 1 7 ) , Narrative QA ( Ko\u010disk\u1ef3 et al. , 2 0 1 8) , and MCScript ( Ostermann et al. , 2 0 1 8) , where only a relatively smaller portion of the questions ( e.g. , 2 7 . 4 % in MCScript ) require commonsense inference .", "ner": [["contextual commonsense reasoning", "Task"], ["machine comprehension", "Task"], ["MRC", "Task"], ["SQuAD", "Dataset"], ["RACE", "Dataset"], ["Narrative QA", "Dataset"], ["MCScript", "Dataset"], ["MCScript", "Dataset"], ["commonsense inference", "Task"]], "rel": [["SQuAD", "Benchmark-For", "machine comprehension"], ["RACE", "Benchmark-For", "machine comprehension"], ["Narrative QA", "Benchmark-For", "machine comprehension"], ["MCScript", "Benchmark-For", "machine comprehension"], ["MCScript", "Benchmark-For", "commonsense inference"]], "rel_plus": [["SQuAD:Dataset", "Benchmark-For", "machine comprehension:Task"], ["RACE:Dataset", "Benchmark-For", "machine comprehension:Task"], ["Narrative QA:Dataset", "Benchmark-For", "machine comprehension:Task"], ["MCScript:Dataset", "Benchmark-For", "machine comprehension:Task"], ["MCScript:Dataset", "Benchmark-For", "commonsense inference:Task"]]}
{"doc_id": "202540590", "sentence": "To establish baseline performances on COS - MOS QA , we explore several state - of - the - art neural models developed for reading comprehension .", "ner": [["COS - MOS QA", "Dataset"], ["reading comprehension", "Task"]], "rel": [["COS - MOS QA", "Benchmark-For", "reading comprehension"]], "rel_plus": [["COS - MOS QA:Dataset", "Benchmark-For", "reading comprehension:Task"]]}
{"doc_id": "202540590", "sentence": "Specifically , we fine - tune three BERT 2 next sentence prediction models on COSMOS : BERT(A|P , Q ) , BERT(A|P ) , BERT(A|Q ) , where P , Q , A denotes the paragraph , question , and answer .", "ner": [["BERT", "Method"], ["next sentence prediction", "Task"], ["COSMOS", "Dataset"], ["BERT(A|P , Q )", "Method"], ["BERT(A|P", "Method"], ["BERT(A|Q", "Method"]], "rel": [["BERT(A|P , Q )", "SubClass-Of", "BERT"], ["BERT(A|P", "SubClass-Of", "BERT"], ["BERT(A|Q", "SubClass-Of", "BERT"], ["BERT", "Trained-With", "next sentence prediction"], ["BERT", "Trained-With", "COSMOS"]], "rel_plus": [["BERT(A|P , Q ):Method", "SubClass-Of", "BERT:Method"], ["BERT(A|P:Method", "SubClass-Of", "BERT:Method"], ["BERT(A|Q:Method", "SubClass-Of", "BERT:Method"], ["BERT:Method", "Trained-With", "next sentence prediction:Task"], ["BERT:Method", "Trained-With", "COSMOS:Dataset"]]}
{"doc_id": "202540590", "sentence": "Figure 2 compares frequent trigram prefixes in COSMOS and SQuAD 2. 0 ( Rajpurkar et al. , 2 0 1 8) .", "ner": [["COSMOS", "Dataset"], ["SQuAD 2. 0", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "202540590", "sentence": "Most of the frequent trigram prefixes in COS - MOS , e.g. , why , what may happen , what will happen are almost absent from SQuAD 2. 0 , which demonstrates the unique challenge our dataset contributes .", "ner": [["COS - MOS", "Dataset"], ["SQuAD 2. 0", "Dataset"]], "rel": [["COS - MOS", "Compare-With", "SQuAD 2. 0"]], "rel_plus": [["COS - MOS:Dataset", "Compare-With", "SQuAD 2. 0:Dataset"]]}
{"doc_id": "202540590", "sentence": "To further enhance the context understanding ability of BERT fine - tuning , we perform multiway bidirectional attention over the BERT encoding output .", "ner": [["BERT", "Method"], ["BERT", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202540590", "sentence": "Stanford Attentive Reader ( Chen et al. , 2 0 1 6 ) performs a bilinear attention between the question and paragraph for answer prediction .", "ner": [["Stanford Attentive Reader", "Method"], ["bilinear attention", "Method"]], "rel": [["bilinear attention", "Part-Of", "Stanford Attentive Reader"]], "rel_plus": [["bilinear attention:Method", "Part-Of", "Stanford Attentive Reader:Method"]]}
{"doc_id": "202540590", "sentence": "GPT - FT ( Radford et al. , 2 0 1 8 ) is based on a generative pre - trained transformer language model , following a fine - tuning step on COSMOS QA .", "ner": [["GPT - FT", "Method"], ["generative pre - trained transformer language model", "Method"], ["COSMOS QA", "Dataset"]], "rel": [["GPT - FT", "SubTask-Of", "generative pre - trained transformer language model"], ["GPT - FT", "Trained-With", "COSMOS QA"]], "rel_plus": [["GPT - FT:Method", "SubTask-Of", "generative pre - trained transformer language model:Method"], ["GPT - FT:Method", "Trained-With", "COSMOS QA:Dataset"]]}
{"doc_id": "202540590", "sentence": "BERT - FT ( Devlin et al. , 2 0 1 8 ) is a pre - trained bidirectional transformer language model following a fine - tuning step on COSMOS QA .", "ner": [["BERT - FT", "Method"], ["pre - trained bidirectional transformer language model", "Method"], ["COSMOS QA", "Dataset"]], "rel": [["BERT - FT", "SubClass-Of", "pre - trained bidirectional transformer language model"], ["BERT - FT", "Trained-With", "COSMOS QA"]], "rel_plus": [["BERT - FT:Method", "SubClass-Of", "pre - trained bidirectional transformer language model:Method"], ["BERT - FT:Method", "Trained-With", "COSMOS QA:Dataset"]]}
{"doc_id": "202540590", "sentence": "DMCN ( Zhang et al. , 2 0 1 9 a ) performs dual attention between paragraph and question/answer over BERT encoding output . over BERT encoding output .", "ner": [["DMCN", "Method"], ["dual attention", "Method"], ["BERT", "Method"], ["BERT", "Method"]], "rel": [["dual attention", "Part-Of", "DMCN"]], "rel_plus": [["dual attention:Method", "Part-Of", "DMCN:Method"]]}
{"doc_id": "202540590", "sentence": "Stanford Attentive Reader ( Chen et al. , 2 0 1 6 ) performs a bilinear attention between the question and paragraph for answer prediction .", "ner": [["Stanford Attentive Reader", "Method"], ["bilinear attention", "Method"]], "rel": [["bilinear attention", "Part-Of", "Stanford Attentive Reader"]], "rel_plus": [["bilinear attention:Method", "Part-Of", "Stanford Attentive Reader:Method"]]}
{"doc_id": "202540590", "sentence": "GPT - FT ( Radford et al. , 2 0 1 8 ) is based on a generative pre - trained transformer language model , following a fine - tuning step on COSMOS .   where W t and b t are learnable parameters .", "ner": [["GPT - FT", "Method"], ["generative pre - trained transformer language model", "Method"], ["COSMOS", "Dataset"]], "rel": [["GPT - FT", "SubClass-Of", "generative pre - trained transformer language model"], ["GPT - FT", "Trained-With", "COSMOS"]], "rel_plus": [["GPT - FT:Method", "SubClass-Of", "generative pre - trained transformer language model:Method"], ["GPT - FT:Method", "Trained-With", "COSMOS:Dataset"]]}
{"doc_id": "202540590", "sentence": "Stanford Attentive Reader ( Chen et al. , 2 0 1 6 ) performs a bilinear attention between the question and paragraph for answer prediction .", "ner": [["Stanford Attentive Reader", "Method"], ["bilinear attention", "Method"]], "rel": [["bilinear attention", "Part-Of", "Stanford Attentive Reader"]], "rel_plus": [["bilinear attention:Method", "Part-Of", "Stanford Attentive Reader:Method"]]}
{"doc_id": "202540590", "sentence": "GPT - FT ( Radford et al. , 2 0 1 8 ) is based on a generative pre - trained transformer language model , following a fine - tuning step on COSMOS .", "ner": [["GPT - FT", "Method"], ["generative pre - trained transformer language model", "Method"], ["COSMOS", "Dataset"]], "rel": [["GPT - FT", "SubClass-Of", "generative pre - trained transformer language model"], ["GPT - FT", "Trained-With", "COSMOS"]], "rel_plus": [["GPT - FT:Method", "SubClass-Of", "generative pre - trained transformer language model:Method"], ["GPT - FT:Method", "Trained-With", "COSMOS:Dataset"]]}
{"doc_id": "202540590", "sentence": "For example , in Figure 5 , the Commonsense - RC baseline mistakenly selected the choice which has the most overlapped words with the paragraph without any commonsense reasoning .", "ner": [["Commonsense - RC", "Task"], ["commonsense reasoning", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "202540590", "sentence": "Considering the unique challenge of COSMOS , we explore two related multiple - choice datasets for knowledge transfer : RACE ( Lai et al. , 2 0 1 7 ) , a large - scale reading comprehension dataset , and SWAG ( Zellers et al. , 2 0 1 8) , a large - scale commonsense inference dataset .", "ner": [["COSMOS", "Dataset"], ["RACE", "Dataset"], ["reading comprehension", "Task"], ["SWAG", "Dataset"], ["commonsense inference", "Task"]], "rel": [["RACE", "Benchmark-For", "reading comprehension"], ["SWAG", "Benchmark-For", "commonsense inference"]], "rel_plus": [["RACE:Dataset", "Benchmark-For", "reading comprehension:Task"], ["SWAG:Dataset", "Benchmark-For", "commonsense inference:Task"]]}
{"doc_id": "202540590", "sentence": "Specifically , we first fine - tune BERT on RACE or SWAG or both , and directly test on COS - MOS to show the impact of knowledge transfer .", "ner": [["BERT", "Method"], ["RACE", "Dataset"], ["SWAG", "Dataset"], ["COS - MOS", "Dataset"]], "rel": [["BERT", "Trained-With", "RACE"], ["BERT", "Trained-With", "SWAG"], ["BERT", "Evaluated-With", "COS - MOS"]], "rel_plus": [["BERT:Method", "Trained-With", "RACE:Dataset"], ["BERT:Method", "Trained-With", "SWAG:Dataset"], ["BERT:Method", "Evaluated-With", "COS - MOS:Dataset"]]}
{"doc_id": "202540590", "sentence": "Furthermore , we sequentially fine - tune BERT on both RACE or SWAG and COSMOS .", "ner": [["BERT", "Method"], ["RACE", "Dataset"], ["SWAG", "Dataset"], ["COSMOS", "Dataset"]], "rel": [["BERT", "Trained-With", "RACE"], ["BERT", "Trained-With", "SWAG"], ["BERT", "Trained-With", "COSMOS"]], "rel_plus": [["BERT:Method", "Trained-With", "RACE:Dataset"], ["BERT:Method", "Trained-With", "SWAG:Dataset"], ["BERT:Method", "Trained-With", "COSMOS:Dataset"]]}
{"doc_id": "202540590", "sentence": "As Table 5 shows , with direct knowledge transfer , RACE provides significant benefit than SWAG since COS - MOS requires more understanding of the interaction between paragraph , question and each candidate answer .", "ner": [["RACE", "Dataset"], ["SWAG", "Dataset"], ["COS - MOS", "Dataset"]], "rel": [["RACE", "Compare-With", "SWAG"]], "rel_plus": [["RACE:Dataset", "Compare-With", "SWAG:Dataset"]]}
{"doc_id": "202540590", "sentence": "She walked through the unit unimpeded and took the elevator to the top floor . with fine - tuning on SWAG , BERT can obtain better commonsense inference ability , which is also beneficial to COSMOS .", "ner": [["SWAG", "Dataset"], ["BERT", "Method"], ["commonsense inference", "Task"], ["COSMOS", "Dataset"]], "rel": [["BERT", "Trained-With", "SWAG"], ["BERT", "Used-For", "commonsense inference"]], "rel_plus": [["BERT:Method", "Trained-With", "SWAG:Dataset"], ["BERT:Method", "Used-For", "commonsense inference:Task"]]}
{"doc_id": "202540590", "sentence": "Specifically , we fine - tune a pre - trained GPT 2 language model on all the Paragraph , Question , Correct Answer of COSMOS training set , then given each Paragraph , Question from test set , we use GPT 2 - FT to generate a plausible answer .", "ner": [["GPT 2", "Method"], ["COSMOS", "Dataset"], ["GPT 2 - FT", "Method"]], "rel": [["GPT 2", "Trained-With", "COSMOS"]], "rel_plus": [["GPT 2:Method", "Trained-With", "COSMOS:Dataset"]]}
{"doc_id": "202540590", "sentence": "Figure 7 shows examples of automatically generated answers by pre - trained GPT 2 and GPT 2 - FT as well as human authored correct answers .", "ner": [["GPT 2", "Method"], ["GPT 2 - FT", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202540590", "sentence": "We observe that by fine - tuning on COSMOS , GPT 2 - FT generates more accurate answers .", "ner": [["COSMOS", "Dataset"], ["GPT 2 - FT", "Method"]], "rel": [["GPT 2 - FT", "Trained-With", "COSMOS"]], "rel_plus": [["GPT 2 - FT:Method", "Trained-With", "COSMOS:Dataset"]]}
{"doc_id": "202540590", "sentence": "Although intuitively there may be multiple correct answers to the questions in COSMOS QA , our analysis shows that more than 8 4 % of generated correct answers identified by human are semantically consistent with the gold answers in COSMOS , which demonstrates that COSMOS can also be used as a benchmark for generative commonsense reasoning .", "ner": [["COSMOS QA", "Dataset"], ["COSMOS", "Dataset"], ["COSMOS", "Dataset"], ["commonsense reasoning", "Task"]], "rel": [["COSMOS", "Benchmark-For", "commonsense reasoning"]], "rel_plus": [["COSMOS:Dataset", "Benchmark-For", "commonsense reasoning:Task"]]}
{"doc_id": "202540590", "sentence": "Figure 7 : Examples of human authored correct answers , and automatically generated answers by pretrained GPT 2 and GPT 2 - FT . ( indicates the answer is correct while shows that the answer is incorrect . ) Metrics GPT 2 GPT 2 - FT BLEU ( Papineni et al. , 2 0 0 2 ) 1 0 . 7 2 1 . 0 METEOR ( Banerjee and Lavie , 2 0 0 5 ) 7. 2 8. 6 ROUGE - L ( Lin , 2 0 0 4 ) 1 3 . 9 2 2 . 1 CIDEr ( Vedantam et al. , 2 0 1 5 ) 0.0 5 0. 1 7 BERTScore F 1 ( Zhang et al. , 2 0 1 9 b ) 4 1 . 9 4 4 . 5 Human 1 1 . 0 % 2 9 . 0 % Table 6 : Generative performance of pre - trained GPT 2 and GPT 2 - FT on COSMOS QA .", "ner": [["GPT 2", "Method"], ["GPT 2 - FT", "Method"], ["GPT 2", "Method"], ["GPT 2 - FT", "Method"], ["METEOR", "Method"], ["ROUGE - L", "Method"], ["CIDEr", "Method"], ["BERTScore", "Method"], ["GPT 2", "Method"], ["GPT 2 - FT", "Method"], ["COSMOS QA", "Dataset"]], "rel": [["GPT 2", "Evaluated-With", "COSMOS QA"], ["GPT 2 - FT", "Evaluated-With", "COSMOS QA"]], "rel_plus": [["GPT 2:Method", "Evaluated-With", "COSMOS QA:Dataset"], ["GPT 2 - FT:Method", "Evaluated-With", "COSMOS QA:Dataset"]]}
{"doc_id": "202540590", "sentence": "All automatic metric scores are averaged from 1 0 sets of sample output .   There have been many exciting new datasets developed for reading comprehension , such as SQuAD ( Rajpurkar et al. , 2 0 1 6 ) , NEWSQA ( Trischler et al. , 2 0 1 7 ) , SearchQA ( Dunn et al. , 2 0 1 7 ) , NarrativeQA ( Ko\u010disk\u1ef3 et al. , 2 0 1 8) , ProPara ( Mishra et al. , 2 0 1 8) , CoQA ( Reddy et al. , 2 0 1 8) , ReCoRD ( Zhang et al. , 2 0 1 8) , MCTest ( Richardson et al. , 2 0 1 3 ) , RACE ( Lai et al. , 2 0 1 7 ) , CNN/Daily Mail ( Hermann et al. , 2 0 1 5 ) , Children 's Book Test ( Hill et al. , 2 0 1 5 ) , and MCScript ( Ostermann et al. , 2 0 1 8) .", "ner": [["reading comprehension", "Task"], ["SQuAD", "Dataset"], ["NEWSQA", "Dataset"], ["SearchQA", "Dataset"], ["NarrativeQA", "Dataset"], ["ProPara", "Dataset"], ["CoQA", "Dataset"], ["ReCoRD", "Dataset"], ["MCTest", "Dataset"], ["RACE", "Dataset"], ["CNN/Daily Mail", "Dataset"], ["Children 's Book Test", "Dataset"], ["MCScript", "Dataset"]], "rel": [["SQuAD", "Benchmark-For", "reading comprehension"], ["NEWSQA", "Benchmark-For", "reading comprehension"], ["SearchQA", "Benchmark-For", "reading comprehension"], ["NarrativeQA", "Benchmark-For", "reading comprehension"], ["ProPara", "Benchmark-For", "reading comprehension"], ["CoQA", "Benchmark-For", "reading comprehension"], ["ReCoRD", "Benchmark-For", "reading comprehension"], ["MCTest", "Benchmark-For", "reading comprehension"], ["RACE", "Benchmark-For", "reading comprehension"], ["CNN/Daily Mail", "Benchmark-For", "reading comprehension"], ["Children 's Book Test", "Benchmark-For", "reading comprehension"], ["MCScript", "Benchmark-For", "reading comprehension"]], "rel_plus": [["SQuAD:Dataset", "Benchmark-For", "reading comprehension:Task"], ["NEWSQA:Dataset", "Benchmark-For", "reading comprehension:Task"], ["SearchQA:Dataset", "Benchmark-For", "reading comprehension:Task"], ["NarrativeQA:Dataset", "Benchmark-For", "reading comprehension:Task"], ["ProPara:Dataset", "Benchmark-For", "reading comprehension:Task"], ["CoQA:Dataset", "Benchmark-For", "reading comprehension:Task"], ["ReCoRD:Dataset", "Benchmark-For", "reading comprehension:Task"], ["MCTest:Dataset", "Benchmark-For", "reading comprehension:Task"], ["RACE:Dataset", "Benchmark-For", "reading comprehension:Task"], ["CNN/Daily Mail:Dataset", "Benchmark-For", "reading comprehension:Task"], ["Children 's Book Test:Dataset", "Benchmark-For", "reading comprehension:Task"], ["MCScript:Dataset", "Benchmark-For", "reading comprehension:Task"]]}
{"doc_id": "202540590", "sentence": "A notable exception is ReCoRD ( Zhang et al. , 2 0 1 8) that is designed specifically for challenging reading comprehension with commonsense reasoning .", "ner": [["ReCoRD", "Dataset"], ["reading comprehension", "Task"], ["commonsense reasoning", "Task"]], "rel": [["ReCoRD", "Benchmark-For", "reading comprehension"], ["commonsense reasoning", "Used-For", "reading comprehension"]], "rel_plus": [["ReCoRD:Dataset", "Benchmark-For", "reading comprehension:Task"], ["commonsense reasoning:Task", "Used-For", "reading comprehension:Task"]]}
{"doc_id": "202540590", "sentence": "COSMOS complements ReCoRD with three unique challenges : ( 1 ) our context is from webblogs rather than news , thus requiring commonsense reasoning for everyday events rather than news - worthy events . ( 2 ) All the answers of ReCoRD are contained in the paragraphs and are assumed to be entities .", "ner": [["COSMOS", "Dataset"], ["ReCoRD", "Dataset"], ["commonsense reasoning", "Task"], ["ReCoRD", "Dataset"]], "rel": [["COSMOS", "Compare-With", "ReCoRD"]], "rel_plus": [["COSMOS:Dataset", "Compare-With", "ReCoRD:Dataset"]]}
{"doc_id": "202540590", "sentence": "In contrast , in COSMOS , more than 8 3 % of answers are not stated in the paragraphs , creating unique modeling challenges . ( 3 ) COSMOS can be used for generative evaluation in addition to multiple - choice evaluation .", "ner": [["COSMOS", "Dataset"], ["COSMOS", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "202540590", "sentence": "There also have been other datasets focusing specifically on question answering with commonsense , such as CommonsenseQA ( Talmor et al. , 2 0 1 8) and Social IQa ( Sap et al. , 2 0 1 9 ) , and various other types of commonsense inferences ( Levesque et al. , 2 0 1 2 ; Rahman and Ng , 2 0 1 2 ; Gordon , 2 0 1 6 ; Roemmele et al. , 2 0 1 1 ; Mostafazadeh et al. , 2 0 1 7 ; Zellers et al. , 2 0 1 8) .", "ner": [["question answering", "Task"], ["commonsense", "Task"], ["CommonsenseQA", "Dataset"], ["Social IQa", "Dataset"]], "rel": [["commonsense", "Used-For", "question answering"], ["CommonsenseQA", "Benchmark-For", "question answering"], ["Social IQa", "Benchmark-For", "question answering"]], "rel_plus": [["commonsense:Task", "Used-For", "question answering:Task"], ["CommonsenseQA:Dataset", "Benchmark-For", "question answering:Task"], ["Social IQa:Dataset", "Benchmark-For", "question answering:Task"]]}
{"doc_id": "202540590", "sentence": "The unique contribution of COSMOS is combining reading comprehension with commonsense reasoning , requiring contextual commonsense reasoning over considerably more complex , diverse , and longer context .", "ner": [["COSMOS", "Dataset"], ["reading comprehension", "Task"], ["commonsense reasoning", "Task"], ["contextual commonsense reasoning", "Task"]], "rel": [["commonsense reasoning", "Used-For", "reading comprehension"], ["COSMOS", "Benchmark-For", "reading comprehension"], ["COSMOS", "Benchmark-For", "commonsense reasoning"], ["COSMOS", "Benchmark-For", "contextual commonsense reasoning"]], "rel_plus": [["commonsense reasoning:Task", "Used-For", "reading comprehension:Task"], ["COSMOS:Dataset", "Benchmark-For", "reading comprehension:Task"], ["COSMOS:Dataset", "Benchmark-For", "commonsense reasoning:Task"], ["COSMOS:Dataset", "Benchmark-For", "contextual commonsense reasoning:Task"]]}
{"doc_id": "202540590", "sentence": "Our work investigates various state - of - the - art approaches to reading comprehension , and provide empirical insights into the design choices that are the most effective for contextual commonsense reasoning required for COSMOS .", "ner": [["reading comprehension", "Task"], ["contextual commonsense reasoning", "Task"], ["COSMOS", "Dataset"]], "rel": [["COSMOS", "Benchmark-For", "contextual commonsense reasoning"]], "rel_plus": [["COSMOS:Dataset", "Benchmark-For", "contextual commonsense reasoning:Task"]]}
{"doc_id": "202540590", "sentence": "We introduced COSMOS QA , a large - scale dataset for machine comprehension with contextual commonsense reasoning .", "ner": [["COSMOS QA", "Dataset"], ["contextual commonsense reasoning", "Task"]], "rel": [["COSMOS QA", "Benchmark-For", "contextual commonsense reasoning"]], "rel_plus": [["COSMOS QA:Dataset", "Benchmark-For", "contextual commonsense reasoning:Task"]]}
{"doc_id": "202540590", "sentence": "For BERT - FT based approaches , we optimize the parameters with grid search : training epochs 1 0 , learning rate l \u2208 { 2e - 5 , 3e - 5 , 5e - 5 } , gradient accumulation steps g \u2208 { 1 , 4 , 8 } , training batch size b \u2208 { 2g , 3g , 4g , 5g}. We will make all the resources and implementations publicly available . 4 https://spacy.io/ 5 Through the whole paper , BERT refers to the pre - trained BERT large uncased model from https://github . com/huggingface/pytorch - pretrained - BERT To explore the impact of the amount of training data , we divide the whole training dataset into 1 0 - fold and successively add another 1 0 % into the training data .", "ner": [["BERT - FT", "Method"], ["BERT", "Method"], ["pre - trained BERT large uncased model", "Method"], ["BERT", "Method"]], "rel": [["BERT", "Synonym-Of", "pre - trained BERT large uncased model"]], "rel_plus": [["BERT:Method", "Synonym-Of", "pre - trained BERT large uncased model:Method"]]}
{"doc_id": "202540590", "sentence": "However , we do not observe significant improvement when we further increase the training data after 1 5 K questions .   For generative evaluation , we base on the OpenAI pre - trained GPT 2 transformer language model , 6 which has 1 1 7 M parameters , and fine - tune it with all Paragraph , Question , Correct Answer in COSMOS QA training set with top - k sampling , where k \u2208 { 3 , 1 0 , 5 0 , 1 0 0 , 1 0 0 0 }. After finetuning , we use GPT 2 - FT to generate 1 0 candidate answers conditioned on each Paragraph , Question from development and test sets .", "ner": [["GPT 2 transformer language model", "Method"], ["COSMOS QA", "Dataset"], ["GPT 2 - FT", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202540590", "sentence": "For AMT based human evaluation , we randomly sample 2 0 0 paragraphs and questions , and for each question we randomly sample 4 automatically generated answers from the outputs of GPT 2 without fine - tuning and GPT 2 - FT .", "ner": [["AMT based human evaluation", "Dataset"], ["GPT 2", "Method"], ["GPT 2 - FT", "Method"]], "rel": [["GPT 2", "Evaluated-With", "AMT based human evaluation"], ["GPT 2 - FT", "Evaluated-With", "AMT based human evaluation"]], "rel_plus": [["GPT 2:Method", "Evaluated-With", "AMT based human evaluation:Dataset"], ["GPT 2 - FT:Method", "Evaluated-With", "AMT based human evaluation:Dataset"]]}
{"doc_id": "202888751", "sentence": "In this paper , we present a simple pipeline that uses GANs in an unsupervised image translation environment to improve learning with respect to the data distribution in a plant disease dataset , reducing the partiality introduced by acute class imbalance and hence shifting the classification decision boundary towards better performance .", "ner": [["GANs", "Method"], ["unsupervised image translation", "Task"]], "rel": [["GANs", "Used-For", "unsupervised image translation"]], "rel_plus": [["GANs:Method", "Used-For", "unsupervised image translation:Task"]]}
{"doc_id": "202888751", "sentence": "First , we extend the state of the art for the GAN - based image - to - image translation method by enhancing the perceptual quality of the generated images and preserving the semantics .", "ner": [["GAN", "Method"], ["image - to - image translation", "Task"]], "rel": [["GAN", "Used-For", "image - to - image translation"]], "rel_plus": [["GAN:Method", "Used-For", "image - to - image translation:Task"]]}
{"doc_id": "202888751", "sentence": "We introduce AR - GAN , where in addition to the adversarial loss , our synthetic image generator optimizes on Activation Reconstruction loss ( ARL ) function that optimizes feature activations against the natural image .", "ner": [["AR - GAN", "Method"], ["adversarial loss", "Method"], ["synthetic image generator", "Method"], ["Activation Reconstruction loss", "Method"], ["ARL", "Method"]], "rel": [["adversarial loss", "Part-Of", "AR - GAN"], ["Activation Reconstruction loss", "Part-Of", "synthetic image generator"], ["ARL", "Synonym-Of", "Activation Reconstruction loss"]], "rel_plus": [["adversarial loss:Method", "Part-Of", "AR - GAN:Method"], ["Activation Reconstruction loss:Method", "Part-Of", "synthetic image generator:Method"], ["ARL:Method", "Synonym-Of", "Activation Reconstruction loss:Method"]]}
{"doc_id": "202888751", "sentence": "Second , we evaluate the performance of a baseline convolutional neural network classifier for improved recognition using the resulting synthetic samples to augment our training set and compare it with the classical data augmentation scheme .", "ner": [["convolutional neural network", "Method"], ["recognition", "Task"], ["classical data augmentation", "Method"]], "rel": [["convolutional neural network", "Used-For", "recognition"]], "rel_plus": [["convolutional neural network:Method", "Used-For", "recognition:Task"]]}
{"doc_id": "202888751", "sentence": "Therefore , these variations are hard to be distinguished , which furthermore makes their recognition a challenge for an early detection and treatment that could help avoid several losses in the whole crop .", "ner": [["recognition", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "202888751", "sentence": "Compared to shallow learning methods which include support vector machines ( SVM ) , decision trees and na\u00efve bayes , deep learning models exhibit more representative power by passing input data through several non - linearity functions to produce robust descriptive features and perform recognition based on those features .", "ner": [["support vector machines", "Method"], ["SVM", "Method"], ["decision trees", "Method"], ["na\u00efve bayes", "Method"], ["deep learning models", "Method"], ["recognition", "Task"]], "rel": [["SVM", "Synonym-Of", "support vector machines"], ["deep learning models", "Compare-With", "support vector machines"], ["deep learning models", "Compare-With", "decision trees"], ["deep learning models", "Compare-With", "na\u00efve bayes"], ["deep learning models", "Used-For", "recognition"], ["support vector machines", "Used-For", "recognition"], ["decision trees", "Used-For", "recognition"], ["na\u00efve bayes", "Used-For", "recognition"], ["deep learning models", "Used-For", "recognition"]], "rel_plus": [["SVM:Method", "Synonym-Of", "support vector machines:Method"], ["deep learning models:Method", "Compare-With", "support vector machines:Method"], ["deep learning models:Method", "Compare-With", "decision trees:Method"], ["deep learning models:Method", "Compare-With", "na\u00efve bayes:Method"], ["deep learning models:Method", "Used-For", "recognition:Task"], ["support vector machines:Method", "Used-For", "recognition:Task"], ["decision trees:Method", "Used-For", "recognition:Task"], ["na\u00efve bayes:Method", "Used-For", "recognition:Task"], ["deep learning models:Method", "Used-For", "recognition:Task"]]}
{"doc_id": "202888751", "sentence": "Generative Adversarial Networks ( GANs ) introduced by Goodfellow et al. , ( 2 0 1 4 ) , have been found successful at various tasks of generating synthetic images .", "ner": [["Generative Adversarial Networks", "Method"], ["GANs", "Method"], ["generating synthetic images", "Task"]], "rel": [["GANs", "Synonym-Of", "Generative Adversarial Networks"], ["Generative Adversarial Networks", "Used-For", "generating synthetic images"]], "rel_plus": [["GANs:Method", "Synonym-Of", "Generative Adversarial Networks:Method"], ["Generative Adversarial Networks:Method", "Used-For", "generating synthetic images:Task"]]}
{"doc_id": "202888751", "sentence": "Motivated by the success , GANs used in image - to - image translation i.e. translation of one possible representation of a scene to another , further proves its representational power Zhu et al., 2 0 1 7 ; Yi et al. , 2 0 1 7 ) .", "ner": [["GANs", "Method"], ["image - to - image translation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "202888751", "sentence": "The main reason to use GANs in an image translation setting is the effective synthesis of viable output , given a limited amount of input data .", "ner": [["GANs", "Method"], ["image translation", "Task"]], "rel": [["GANs", "Used-For", "image translation"]], "rel_plus": [["GANs:Method", "Used-For", "image translation:Task"]]}
{"doc_id": "202888751", "sentence": "Our approach builds on CycleGAN and DualGAN , where they propose a self - consistency or reconstruction loss that preserves the input image after the translation cycle .", "ner": [["CycleGAN", "Method"], ["DualGAN", "Method"], ["self - consistency", "Method"], ["reconstruction loss", "Method"], ["translation", "Task"]], "rel": [["reconstruction loss", "Part-Of", "CycleGAN"], ["reconstruction loss", "Part-Of", "DualGAN"]], "rel_plus": [["reconstruction loss:Method", "Part-Of", "CycleGAN:Method"], ["reconstruction loss:Method", "Part-Of", "DualGAN:Method"]]}
{"doc_id": "202888751", "sentence": "In this work we introduce AR - GAN that differs from the previous approaches by optimizing on an activation reconstruction loss ( Johnson et al. , 2 0 1 6 ; Cha et al. 2 0 1 7 ) in addition to regularizing the original GAN objective function and cycle - consistency optimizations to present visually more compelling synthetic images on an unaligned dataset .", "ner": [["AR - GAN", "Method"], ["activation reconstruction loss", "Method"], ["original GAN objective function", "Method"], ["cycle - consistency", "Method"]], "rel": [["activation reconstruction loss", "Part-Of", "AR - GAN"], ["cycle - consistency", "Part-Of", "AR - GAN"], ["original GAN objective function", "Part-Of", "AR - GAN"]], "rel_plus": [["activation reconstruction loss:Method", "Part-Of", "AR - GAN:Method"], ["cycle - consistency:Method", "Part-Of", "AR - GAN:Method"], ["original GAN objective function:Method", "Part-Of", "AR - GAN:Method"]]}
{"doc_id": "202888751", "sentence": "To the best of our knowledge , this is the first work that uses GANs to synthetically augment the dataset to improve the plant disease recognition performance .", "ner": [["GANs", "Method"], ["plant disease recognition", "Task"]], "rel": [["GANs", "Used-For", "plant disease recognition"]], "rel_plus": [["GANs:Method", "Used-For", "plant disease recognition:Task"]]}
{"doc_id": "202888751", "sentence": "Our proposed pipeline containing two components : a Synthetic Data Augmentation module with AR - GAN for unsupervised learning and a Recognition System with a Convolutional Neural Network for supervised learning .", "ner": [["Synthetic Data Augmentation", "Method"], ["AR - GAN", "Method"], ["unsupervised learning", "Task"], ["Recognition System", "Method"], ["Convolutional Neural Network", "Method"], ["supervised learning", "Task"]], "rel": [["AR - GAN", "Part-Of", "Synthetic Data Augmentation"], ["Synthetic Data Augmentation", "Used-For", "unsupervised learning"], ["Convolutional Neural Network", "Part-Of", "Recognition System"], ["Recognition System", "Used-For", "supervised learning"]], "rel_plus": [["AR - GAN:Method", "Part-Of", "Synthetic Data Augmentation:Method"], ["Synthetic Data Augmentation:Method", "Used-For", "unsupervised learning:Task"], ["Convolutional Neural Network:Method", "Part-Of", "Recognition System:Method"], ["Recognition System:Method", "Used-For", "supervised learning:Task"]]}
{"doc_id": "202888751", "sentence": "We aim to learn more discriminative embeddings with the \" training data \" containing both \" real data \" and \" generated data \" . \u2022 We propose a simple pipeline for synthetic augmentation of plant disease datasets using AR - GAN to improve the plant disease recognition performance in a data deficient environment . \u2022 We introduce our limited dataset of tomato plant disease images to validate the effectiveness of our pipeline to prove or disprove the hypothesis : ( i ) Does synthetic augmentation improve the performance of a deep convolutional neural network for plant disease recognition ? ( ii ) How does synthetic augmentation compare to classic augmentation in terms of performance in a plant disease recognition system ?   The biggest limitation with machine learning algorithms is that they require huge amounts of training data before they become effective .", "ner": [["synthetic augmentation", "Method"], ["AR - GAN", "Method"], ["plant disease recognition", "Task"], ["synthetic augmentation", "Method"], ["convolutional neural network", "Method"], ["plant disease recognition", "Task"], ["synthetic augmentation", "Method"], ["classic augmentation", "Method"], ["plant disease recognition", "Task"], ["machine learning", "Method"]], "rel": [["AR - GAN", "Used-For", "synthetic augmentation"], ["AR - GAN", "Used-For", "plant disease recognition"], ["convolutional neural network", "Used-For", "plant disease recognition"], ["synthetic augmentation", "Compare-With", "classic augmentation"], ["synthetic augmentation", "Used-For", "plant disease recognition"], ["classic augmentation", "Used-For", "plant disease recognition"]], "rel_plus": [["AR - GAN:Method", "Used-For", "synthetic augmentation:Method"], ["AR - GAN:Method", "Used-For", "plant disease recognition:Task"], ["convolutional neural network:Method", "Used-For", "plant disease recognition:Task"], ["synthetic augmentation:Method", "Compare-With", "classic augmentation:Method"], ["synthetic augmentation:Method", "Used-For", "plant disease recognition:Task"], ["classic augmentation:Method", "Used-For", "plant disease recognition:Task"]]}
{"doc_id": "202888751", "sentence": "The main reason for using GAN based image translation in our study is that in practice , other generative models tend to produce blurry images relative to GANs .", "ner": [["GAN", "Method"], ["image translation", "Task"], ["generative models", "Method"], ["GANs", "Method"]], "rel": [["GAN", "Used-For", "image translation"]], "rel_plus": [["GAN:Method", "Used-For", "image translation:Task"]]}
{"doc_id": "202888751", "sentence": "More recent approaches ( Radford et al. , 2 0 1 5 ; Dumoulin et al. , 2 0 1 6 ) successfully demonstrated the use of fully convolutional neural networks to achieve better performance , and since then convolution and transposed convolution layers ( Dumoulin et al. , 2 0 1 6 ) have become the fundamental components in many models .", "ner": [["fully convolutional neural networks", "Method"], ["convolution", "Method"], ["transposed convolution layers", "Method"]], "rel": [["convolution", "Part-Of", "fully convolutional neural networks"], ["transposed convolution layers", "Part-Of", "fully convolutional neural networks"]], "rel_plus": [["convolution:Method", "Part-Of", "fully convolutional neural networks:Method"], ["transposed convolution layers:Method", "Part-Of", "fully convolutional neural networks:Method"]]}
{"doc_id": "202888751", "sentence": "WGAN proposes to use the Wasserstein distance to measure the similarity between true and the learned data distribution , instead of using Jensen - Shannon divergence as in the original GAN model ( Goodfellow et al. , 2 0 1 4 ) .", "ner": [["WGAN", "Method"], ["Wasserstein distance", "Method"], ["measure the similarity", "Task"], ["Jensen - Shannon divergence", "Method"], ["GAN", "Method"]], "rel": [["Wasserstein distance", "Part-Of", "WGAN"], ["Wasserstein distance", "Used-For", "measure the similarity"], ["Jensen - Shannon divergence", "Part-Of", "GAN"]], "rel_plus": [["Wasserstein distance:Method", "Part-Of", "WGAN:Method"], ["Wasserstein distance:Method", "Used-For", "measure the similarity:Task"], ["Jensen - Shannon divergence:Method", "Part-Of", "GAN:Method"]]}
{"doc_id": "202888751", "sentence": "To avoid this problem , WGAN - GP ( Gulrajani et al. , 2 0 1 7 ) suggests using gradient penalty , instead of weight clipping used in WGAN .", "ner": [["WGAN - GP", "Method"], ["WGAN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202888751", "sentence": "GANs have demonstrated potential in generating images for specific fields like image in - painting , image super resolution , text - to - image synthesis , image - to - image translation ( Creswell et al. , 2 0 1 8) .", "ner": [["GANs", "Method"], ["image in - painting", "Task"], ["image super resolution", "Task"], ["text - to - image synthesis", "Task"], ["image - to - image translation", "Task"]], "rel": [["GANs", "Used-For", "image in - painting"], ["GANs", "Used-For", "image super resolution"], ["GANs", "Used-For", "text - to - image synthesis"], ["GANs", "Used-For", "image - to - image translation"]], "rel_plus": [["GANs:Method", "Used-For", "image in - painting:Task"], ["GANs:Method", "Used-For", "image super resolution:Task"], ["GANs:Method", "Used-For", "text - to - image synthesis:Task"], ["GANs:Method", "Used-For", "image - to - image translation:Task"]]}
{"doc_id": "202888751", "sentence": "Instead , we use GANs for image translation to generate samples from the training data .", "ner": [["GANs", "Method"], ["image translation", "Task"]], "rel": [["GANs", "Used-For", "image translation"]], "rel_plus": [["GANs:Method", "Used-For", "image translation:Task"]]}
{"doc_id": "202888751", "sentence": "Image - to - Image translation is defined as the task of translation of one possible representation of a scene to another , such as mapping grayscale images to RGB or the other way around .", "ner": [["Image - to - Image translation", "Task"], ["translation", "Task"]], "rel": [["Image - to - Image translation", "SubTask-Of", "translation"]], "rel_plus": [["Image - to - Image translation:Task", "SubTask-Of", "translation:Task"]]}
{"doc_id": "202888751", "sentence": "GANs have been used for image - to - image translation in both supervised as well as unsupervised settings .", "ner": [["GANs", "Method"], ["image - to - image translation", "Task"]], "rel": [["GANs", "Used-For", "image - to - image translation"]], "rel_plus": [["GANs:Method", "Used-For", "image - to - image translation:Task"]]}
{"doc_id": "202888751", "sentence": "Our approach builds on unsupervised imageto - image translation GAN architectures where they propose a self - consistency or reconstruction loss that preserves the input image after the translation cycle Yi et al. , 2 0 1 7 ; Kim et al. , 2 0 1 7 ) .", "ner": [["unsupervised imageto - image translation GAN", "Method"], ["self - consistency", "Method"], ["reconstruction loss", "Method"]], "rel": [["self - consistency", "Part-Of", "unsupervised imageto - image translation GAN"], ["reconstruction loss", "Part-Of", "unsupervised imageto - image translation GAN"]], "rel_plus": [["self - consistency:Method", "Part-Of", "unsupervised imageto - image translation GAN:Method"], ["reconstruction loss:Method", "Part-Of", "unsupervised imageto - image translation GAN:Method"]]}
{"doc_id": "202888751", "sentence": "In disease recognition , shallow learning approaches have been applied for classifying tomato powdery mildew against healthy leaves by means of thermal and stereo images ( Prince et al. , 2 0 1 5 ) , detecting yellow leaf curl virus in tomatoes by using a set of classic feature extraction steps , classified by SVM ( Mokhtar et al. , 2 0 1 5 ) , recognition of tomato diseases in a greenhouse ( Chai and Wang , 2 0 1 3 ) etc .", "ner": [["disease recognition", "Task"], ["shallow learning approaches", "Method"], ["classifying tomato powdery mildew", "Task"], ["detecting yellow leaf curl virus in tomatoes", "Task"], ["feature extraction", "Task"], ["SVM", "Method"], ["recognition of tomato diseases in a greenhouse", "Task"]], "rel": [["classifying tomato powdery mildew", "SubTask-Of", "disease recognition"], ["shallow learning approaches", "Used-For", "classifying tomato powdery mildew"], ["feature extraction", "Used-For", "detecting yellow leaf curl virus in tomatoes"], ["SVM", "Used-For", "detecting yellow leaf curl virus in tomatoes"], ["SVM", "Used-For", "recognition of tomato diseases in a greenhouse"]], "rel_plus": [["classifying tomato powdery mildew:Task", "SubTask-Of", "disease recognition:Task"], ["shallow learning approaches:Method", "Used-For", "classifying tomato powdery mildew:Task"], ["feature extraction:Task", "Used-For", "detecting yellow leaf curl virus in tomatoes:Task"], ["SVM:Method", "Used-For", "detecting yellow leaf curl virus in tomatoes:Task"], ["SVM:Method", "Used-For", "recognition of tomato diseases in a greenhouse:Task"]]}
{"doc_id": "202888751", "sentence": "Ghazi et al. , ( 2 0 1 7 ) used three popular CNN architectures , AlexNet ( Krizhevsky et al. , 2 0 1 2 ) , VggNet ( Simonyan and Zisserman , 2 0 1 4 ) , and GoogLeNet ( Szegedy et al. , 2 0 1 5 ) , and evaluated the numerous factors affecting the performance of these networks on task of plant species identification .", "ner": [["CNN", "Method"], ["AlexNet", "Method"], ["VggNet", "Method"], ["GoogLeNet", "Method"]], "rel": [["AlexNet", "SubClass-Of", "CNN"], ["VggNet", "SubClass-Of", "CNN"], ["GoogLeNet", "SubClass-Of", "CNN"]], "rel_plus": [["AlexNet:Method", "SubClass-Of", "CNN:Method"], ["VggNet:Method", "SubClass-Of", "CNN:Method"], ["GoogLeNet:Method", "SubClass-Of", "CNN:Method"]]}
{"doc_id": "202888751", "sentence": "They use Transfer learning to fine - tune the pre - trained models , using LifeCLEF plant dataset ( Go\u00ebau et al. , 2 0 1 4 ) and applied classic data augmentation techniques based on image transforms such as rotation , translation , reflection , and scaling to decrease the chance of overfitting .", "ner": [["Transfer learning", "Task"], ["LifeCLEF plant dataset", "Dataset"], ["data augmentation", "Method"], ["image transforms", "Method"], ["rotation", "Method"], ["translation", "Method"], ["reflection", "Method"], ["scaling", "Method"]], "rel": [["image transforms", "Used-For", "data augmentation"], ["rotation", "SubClass-Of", "image transforms"], ["translation", "SubClass-Of", "image transforms"], ["reflection", "SubClass-Of", "image transforms"], ["scaling", "SubClass-Of", "image transforms"]], "rel_plus": [["image transforms:Method", "Used-For", "data augmentation:Method"], ["rotation:Method", "SubClass-Of", "image transforms:Method"], ["translation:Method", "SubClass-Of", "image transforms:Method"], ["reflection:Method", "SubClass-Of", "image transforms:Method"], ["scaling:Method", "SubClass-Of", "image transforms:Method"]]}
{"doc_id": "202888751", "sentence": "Other applications of the deep CNNs in plant disease recognition include the diagnosis of crop leaf disease in ( Mohanty et al. , 2 0 1 6 ) , where they used a deep convolutional neural network to identify 2 6 diseases and 1 4 crop species using a publicly available PlantVillage dataset ( Hughes et al. , 2 0 1 5 ) with over 5 0 , 0 0 0 images .", "ner": [["CNNs", "Method"], ["plant disease recognition", "Task"], ["diagnosis of crop leaf disease", "Task"], ["convolutional neural network", "Method"], ["PlantVillage", "Dataset"]], "rel": [["CNNs", "Used-For", "plant disease recognition"], ["diagnosis of crop leaf disease", "SubTask-Of", "plant disease recognition"], ["CNNs", "Used-For", "diagnosis of crop leaf disease"], ["PlantVillage", "Used-For", "convolutional neural network"]], "rel_plus": [["CNNs:Method", "Used-For", "plant disease recognition:Task"], ["diagnosis of crop leaf disease:Task", "SubTask-Of", "plant disease recognition:Task"], ["CNNs:Method", "Used-For", "diagnosis of crop leaf disease:Task"], ["PlantVillage:Dataset", "Used-For", "convolutional neural network:Method"]]}
{"doc_id": "202888751", "sentence": "Among the recently published works in the literature [ ( Brahimi et al. , 2 0 1 7 ; Atabay et al. , 2 0 1 7 ) ] , they use transfer learning of pre - trained models and introduce deep residual learning to identify tomato plant diseases of the PlantVillage dataset .", "ner": [["transfer learning", "Task"], ["deep residual learning", "Method"], ["tomato plant diseases", "Task"], ["PlantVillage", "Dataset"]], "rel": [["deep residual learning", "Used-For", "tomato plant diseases"], ["PlantVillage", "Benchmark-For", "tomato plant diseases"], ["transfer learning", "Used-For", "tomato plant diseases"]], "rel_plus": [["deep residual learning:Method", "Used-For", "tomato plant diseases:Task"], ["PlantVillage:Dataset", "Benchmark-For", "tomato plant diseases:Task"], ["transfer learning:Task", "Used-For", "tomato plant diseases:Task"]]}
{"doc_id": "202888751", "sentence": "We propose to synthetically generate additional training data using GAN and further train the recognition network using that data in addition to the original data .", "ner": [["GAN", "Method"], ["recognition network", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202888751", "sentence": "Further , we also investigate the effectiveness of the synthetic data augmentation by GAN over classical data augmentation for improved recognition .", "ner": [["synthetic data augmentation", "Method"], ["GAN", "Method"], ["classical data augmentation", "Method"], ["recognition", "Task"]], "rel": [["GAN", "Used-For", "synthetic data augmentation"], ["synthetic data augmentation", "Compare-With", "classical data augmentation"], ["synthetic data augmentation", "Used-For", "recognition"], ["classical data augmentation", "Used-For", "recognition"]], "rel_plus": [["GAN:Method", "Used-For", "synthetic data augmentation:Method"], ["synthetic data augmentation:Method", "Compare-With", "classical data augmentation:Method"], ["synthetic data augmentation:Method", "Used-For", "recognition:Task"], ["classical data augmentation:Method", "Used-For", "recognition:Task"]]}
{"doc_id": "202888751", "sentence": "As shown in Fig. 1 , our proposed system consists of two blocks : a ) Synthetic Data Augmentation block using AR - GAN for data synthesis and class balancing b ) Recognition System that uses a deep convolutional neural network for image recognition .", "ner": [["Synthetic Data Augmentation", "Method"], ["AR - GAN", "Method"], ["Recognition System", "Method"], ["convolutional neural network", "Method"], ["image recognition", "Task"]], "rel": [["AR - GAN", "Used-For", "Synthetic Data Augmentation"], ["convolutional neural network", "Used-For", "Recognition System"], ["Recognition System", "Used-For", "image recognition"]], "rel_plus": [["AR - GAN:Method", "Used-For", "Synthetic Data Augmentation:Method"], ["convolutional neural network:Method", "Used-For", "Recognition System:Method"], ["Recognition System:Method", "Used-For", "image recognition:Task"]]}
{"doc_id": "202888751", "sentence": "The generated data from AR - GAN is added to the real data to yield training data for the CNN .", "ner": [["AR - GAN", "Method"], ["CNN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202888751", "sentence": "In the following section , we introduce the architecture for our AR - GAN , followed by the baseline CNN architecture used to analyze the recognition performance .", "ner": [["AR - GAN", "Method"], ["CNN", "Method"], ["recognition", "Task"]], "rel": [["CNN", "Used-For", "recognition"]], "rel_plus": [["CNN:Method", "Used-For", "recognition:Task"]]}
{"doc_id": "202888751", "sentence": "The AR - GAN improves on CycleGAN , unsupervised image - to - image translator , by introducing an activation reconstruction module consisting of a feature extraction network to calculate Activation Reconstruction Loss ( ARL ) between an image and its translation , in addition to the cycle - consistency loss and adversarial loss of CycleGAN .", "ner": [["AR - GAN", "Method"], ["CycleGAN", "Method"], ["unsupervised image - to - image translator", "Method"], ["activation reconstruction module", "Method"], ["feature extraction network", "Method"], ["Activation Reconstruction Loss", "Method"], ["ARL", "Method"], ["cycle - consistency loss", "Method"], ["adversarial loss", "Method"], ["CycleGAN", "Method"]], "rel": [["activation reconstruction module", "Part-Of", "AR - GAN"], ["cycle - consistency loss", "Part-Of", "AR - GAN"], ["adversarial loss", "Part-Of", "AR - GAN"], ["AR - GAN", "Compare-With", "CycleGAN"], ["AR - GAN", "Compare-With", "unsupervised image - to - image translator"], ["feature extraction network", "Part-Of", "activation reconstruction module"], ["ARL", "Synonym-Of", "Activation Reconstruction Loss"], ["feature extraction network", "Used-For", "Activation Reconstruction Loss"], ["adversarial loss", "Part-Of", "CycleGAN"]], "rel_plus": [["activation reconstruction module:Method", "Part-Of", "AR - GAN:Method"], ["cycle - consistency loss:Method", "Part-Of", "AR - GAN:Method"], ["adversarial loss:Method", "Part-Of", "AR - GAN:Method"], ["AR - GAN:Method", "Compare-With", "CycleGAN:Method"], ["AR - GAN:Method", "Compare-With", "unsupervised image - to - image translator:Method"], ["feature extraction network:Method", "Part-Of", "activation reconstruction module:Method"], ["ARL:Method", "Synonym-Of", "Activation Reconstruction Loss:Method"], ["feature extraction network:Method", "Used-For", "Activation Reconstruction Loss:Method"], ["adversarial loss:Method", "Part-Of", "CycleGAN:Method"]]}
{"doc_id": "202888751", "sentence": "Let and be the activation outputs of the nth layer within any convolutional recognition network ( feature extraction network ) where a and b are used as inputs respectively .", "ner": [["convolutional recognition network", "Method"], ["feature extraction network", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202888751", "sentence": "The feature extraction network , to calculate \u2112 , consists of a down convolution layer followed by 4 - residual blocks .", "ner": [["feature extraction network", "Method"], ["convolution layer", "Method"], ["4 - residual blocks", "Method"]], "rel": [["4 - residual blocks", "Part-Of", "feature extraction network"], ["convolution layer", "Part-Of", "feature extraction network"]], "rel_plus": [["4 - residual blocks:Method", "Part-Of", "feature extraction network:Method"], ["convolution layer:Method", "Part-Of", "feature extraction network:Method"]]}
{"doc_id": "202888751", "sentence": "Let : - \u2022 C 7 - k - n : 7 \u00d7 7 Convolution Instance Norm ReLU layer with n filters and stride k. \u2022 D 3 - 2 - n : 3 \u00d7 3 Convolution Instance Norm ReLU layer with n filters and stride 2 . \u2022 R 3 - n : Residual block that contains two 3 \u00d7 3 Convolutional layers with the same n number of filters on both layers . \u2022 U 3 - n : 3 \u00d7 3 fractional strided Convolution Instance Norm ReLU layer with n filters and stride 1/ 2 . \u2022 P 4 - 2 - n : 4 \u00d7 4 Convolution Instance Norm Leaky ReLU layer with n filters and stride 2 .", "ner": [["Convolution Instance Norm ReLU", "Method"], ["Convolution Instance Norm ReLU", "Method"], ["Residual block", "Method"], ["3 \u00d7 3 Convolutional layers", "Method"], ["3 \u00d7 3 fractional strided Convolution Instance Norm ReLU", "Method"], ["4 \u00d7 4 Convolution Instance Norm Leaky ReLU layer", "Method"]], "rel": [["3 \u00d7 3 Convolutional layers", "Part-Of", "Residual block"]], "rel_plus": [["3 \u00d7 3 Convolutional layers:Method", "Part-Of", "Residual block:Method"]]}
{"doc_id": "202888751", "sentence": "The feature extraction network to calculate \u2112 is given by : C 7 - 2 - 6 4 , R 3 - 6 4 , R 3 - 1 2 8 , R 3 - 2 5 6 , R 3 - 5 1 2 The feature extraction network is warm - started with pre - trained ImageNet weights .", "ner": [["feature extraction network", "Method"], ["feature extraction network", "Method"], ["ImageNet", "Dataset"]], "rel": [["feature extraction network", "Trained-With", "ImageNet"]], "rel_plus": [["feature extraction network:Method", "Trained-With", "ImageNet:Dataset"]]}
{"doc_id": "202888751", "sentence": "CNNs are extensively used for solving image recognition tasks in computer vision .", "ner": [["CNNs", "Method"], ["image recognition", "Task"], ["computer vision", "Task"]], "rel": [["CNNs", "Used-For", "image recognition"], ["image recognition", "SubTask-Of", "computer vision"]], "rel_plus": [["CNNs:Method", "Used-For", "image recognition:Task"], ["image recognition:Task", "SubTask-Of", "computer vision:Task"]]}
{"doc_id": "202888751", "sentence": "CNN architectures for tomato plant disease recognition used in ( Sladojevic et al. , 2 0 1 6 ; Lu et al. , 2 0 1 7 ; Brahimi et al. , 2 0 1 7 ) usually either contain few convolution layers because of the small datasets or many convolution layers with large datasets .", "ner": [["CNN", "Method"], ["tomato plant disease recognition", "Task"], ["convolution layers", "Method"], ["convolution layers", "Method"]], "rel": [["convolution layers", "Part-Of", "CNN"], ["CNN", "Used-For", "tomato plant disease recognition"]], "rel_plus": [["convolution layers:Method", "Part-Of", "CNN:Method"], ["CNN:Method", "Used-For", "tomato plant disease recognition:Task"]]}
{"doc_id": "202888751", "sentence": "We fine - tune our recognition networks from ImageNet pre - trained weights ( Chai and Wang , 2 0 1 3 ) .", "ner": [["recognition networks", "Method"], ["ImageNet", "Dataset"]], "rel": [["recognition networks", "Trained-With", "ImageNet"]], "rel_plus": [["recognition networks:Method", "Trained-With", "ImageNet:Dataset"]]}
{"doc_id": "202888751", "sentence": "We use stochastic gradient descent optimization with Nesterov momentum updates ( Nesterov , 1 9 8 3 ) .", "ner": [["stochastic gradient descent", "Method"], ["Nesterov momentum", "Method"]], "rel": [["Nesterov momentum", "Part-Of", "stochastic gradient descent"]], "rel_plus": [["Nesterov momentum:Method", "Part-Of", "stochastic gradient descent:Method"]]}
{"doc_id": "202888751", "sentence": "For the implementation of the recognition CNN architecture we used the Keras framework .", "ner": [["recognition", "Task"], ["CNN", "Method"]], "rel": [["CNN", "Used-For", "recognition"]], "rel_plus": [["CNN:Method", "Used-For", "recognition:Task"]]}
{"doc_id": "202888751", "sentence": "We first introduce our limited tomato plant disease recognition dataset along with other datasets used to evaluate the performance of our GAN network .", "ner": [["tomato plant disease recognition", "Task"], ["GAN", "Method"]], "rel": [["GAN", "Used-For", "tomato plant disease recognition"]], "rel_plus": [["GAN:Method", "Used-For", "tomato plant disease recognition:Task"]]}
{"doc_id": "202888751", "sentence": "We compare our AR - GAN against recent methods for unpaired image - to - image translation on both paired as well as unpaired datasets .", "ner": [["AR - GAN", "Method"], ["unpaired image - to - image translation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "202888751", "sentence": "The tomato plant disease dataset is also used to investigate the effectiveness of synthetic data augmentation in the recognition system of our proposed method .", "ner": [["tomato plant disease dataset", "Dataset"], ["synthetic data augmentation", "Method"], ["recognition system", "Task"]], "rel": [["synthetic data augmentation", "Evaluated-With", "tomato plant disease dataset"], ["tomato plant disease dataset", "Benchmark-For", "recognition system"], ["synthetic data augmentation", "Used-For", "recognition system"]], "rel_plus": [["synthetic data augmentation:Method", "Evaluated-With", "tomato plant disease dataset:Dataset"], ["tomato plant disease dataset:Dataset", "Benchmark-For", "recognition system:Task"], ["synthetic data augmentation:Method", "Used-For", "recognition system:Task"]]}
{"doc_id": "202888751", "sentence": "The FCN metric evaluates the comprehensibility of the generated photos in accordance to a semantic segmentation algorithm ( the fully - convolutional network , FCN [ ( Long et al. , 2 0 1 5 ) ] ) .", "ner": [["FCN", "Method"], ["semantic segmentation", "Task"], ["fully - convolutional network", "Method"], ["FCN", "Method"]], "rel": [["FCN", "Used-For", "semantic segmentation"], ["fully - convolutional network", "Used-For", "semantic segmentation"]], "rel_plus": [["FCN:Method", "Used-For", "semantic segmentation:Task"], ["fully - convolutional network:Method", "Used-For", "semantic segmentation:Task"]]}
{"doc_id": "202888751", "sentence": "Apart from the metrics mentioned above , we also evaluate our model using the state - of - the art evaluation model namely neural image assessment ( NIMA ) ( Talebi and Milanfar , 2 0 1 8) .", "ner": [["neural image assessment", "Method"], ["NIMA", "Method"]], "rel": [["NIMA", "Synonym-Of", "neural image assessment"]], "rel_plus": [["NIMA:Method", "Synonym-Of", "neural image assessment:Method"]]}
{"doc_id": "202888751", "sentence": "We compare the results of AR - GAN with various approaches including supervised pix 2 pix .", "ner": [["AR - GAN", "Method"], ["supervised pix 2 pix", "Method"]], "rel": [["AR - GAN", "Compare-With", "supervised pix 2 pix"]], "rel_plus": [["AR - GAN:Method", "Compare-With", "supervised pix 2 pix:Method"]]}
{"doc_id": "202888751", "sentence": "We present sample results from AR - GAN across given datasets to demonstrate the performance in effective translation of features between two domains .", "ner": [["AR - GAN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202888751", "sentence": "Further , we analyze our recognition CNN to evaluate the performance of classical and synthetic augmentation using various instances of our tomato plant disease data .", "ner": [["recognition CNN", "Method"], ["synthetic augmentation", "Method"], ["tomato plant disease data", "Dataset"]], "rel": [["synthetic augmentation", "Used-For", "recognition CNN"], ["synthetic augmentation", "Evaluated-With", "tomato plant disease data"]], "rel_plus": [["synthetic augmentation:Method", "Used-For", "recognition CNN:Method"], ["synthetic augmentation:Method", "Evaluated-With", "tomato plant disease data:Dataset"]]}
{"doc_id": "202888751", "sentence": "We first compare our AR - GAN against several recently proposed models for unpaired image - to - image translation on paired cityscapes dataset with 2 9 7 5 training images .", "ner": [["AR - GAN", "Method"], ["unpaired image - to - image translation", "Task"], ["cityscapes", "Dataset"]], "rel": [["AR - GAN", "Used-For", "unpaired image - to - image translation"], ["cityscapes", "Benchmark-For", "unpaired image - to - image translation"], ["AR - GAN", "Evaluated-With", "cityscapes"]], "rel_plus": [["AR - GAN:Method", "Used-For", "unpaired image - to - image translation:Task"], ["cityscapes:Dataset", "Benchmark-For", "unpaired image - to - image translation:Task"], ["AR - GAN:Method", "Evaluated-With", "cityscapes:Dataset"]]}
{"doc_id": "202888751", "sentence": "Here , our AR - GAN uses 9 residual blocks and Activation Reconstruction Loss in the forward direction : Per - pixel accuracy .", "ner": [["AR - GAN", "Method"], ["residual blocks", "Method"], ["Activation Reconstruction Loss", "Method"]], "rel": [["residual blocks", "Part-Of", "AR - GAN"], ["Activation Reconstruction Loss", "Part-Of", "AR - GAN"]], "rel_plus": [["residual blocks:Method", "Part-Of", "AR - GAN:Method"], ["Activation Reconstruction Loss:Method", "Part-Of", "AR - GAN:Method"]]}
{"doc_id": "202888751", "sentence": "We observe that the CycleGAN model based on cycle reconstruction and adversarial loss can be improved with our proposed addition of ARL between the real and fake image pairs .", "ner": [["CycleGAN", "Method"], ["cycle reconstruction", "Method"], ["adversarial loss", "Method"], ["ARL", "Method"]], "rel": [["adversarial loss", "Part-Of", "CycleGAN"], ["cycle reconstruction", "Part-Of", "CycleGAN"], ["ARL", "Part-Of", "CycleGAN"]], "rel_plus": [["adversarial loss:Method", "Part-Of", "CycleGAN:Method"], ["cycle reconstruction:Method", "Part-Of", "CycleGAN:Method"], ["ARL:Method", "Part-Of", "CycleGAN:Method"]]}
{"doc_id": "202888751", "sentence": "The translations produced using our approach are often of similar quality to the fully supervised pix 2 pix .    In Tables 2 and 3 , we evaluate different generator architectural variants and compare their performance in terms of FID and NIMA using powdery mildew class from the tomato plant disease dataset and cityscapes dataset , respectively .", "ner": [["fully supervised pix 2 pix", "Method"], ["FID", "Method"], ["NIMA", "Method"], ["tomato plant disease dataset", "Dataset"], ["cityscapes", "Dataset"]], "rel": [["FID", "Evaluated-With", "tomato plant disease dataset"], ["NIMA", "Evaluated-With", "tomato plant disease dataset"], ["FID", "Evaluated-With", "cityscapes"], ["NIMA", "Evaluated-With", "cityscapes"]], "rel_plus": [["FID:Method", "Evaluated-With", "tomato plant disease dataset:Dataset"], ["NIMA:Method", "Evaluated-With", "tomato plant disease dataset:Dataset"], ["FID:Method", "Evaluated-With", "cityscapes:Dataset"], ["NIMA:Method", "Evaluated-With", "cityscapes:Dataset"]]}
{"doc_id": "202888751", "sentence": "Last model in Table 2 uses AR - GAN with 9 residual blocks in the generator and ARL in both directions : \u2112 + \u2112 .", "ner": [["AR - GAN", "Method"], ["residual blocks", "Method"], ["generator", "Method"], ["ARL", "Method"]], "rel": [["generator", "Part-Of", "AR - GAN"], ["ARL", "Part-Of", "AR - GAN"], ["residual blocks", "Part-Of", "generator"]], "rel_plus": [["generator:Method", "Part-Of", "AR - GAN:Method"], ["ARL:Method", "Part-Of", "AR - GAN:Method"], ["residual blocks:Method", "Part-Of", "generator:Method"]]}
{"doc_id": "202888751", "sentence": "CycleGAN often confuses between the foreground and the background , while AR - GAN explicitly deals with it by getting sharper edges of the target , without treating the whole image as one object .", "ner": [["CycleGAN", "Method"], ["AR - GAN", "Method"]], "rel": [["CycleGAN", "Compare-With", "AR - GAN"]], "rel_plus": [["CycleGAN:Method", "Compare-With", "AR - GAN:Method"]]}
{"doc_id": "202888751", "sentence": "As can also be seen from Table 1 , CycleGAN finds it hard to capture perceptual information from the input image in the cityscapes dataset , resulting in a poor FCN score , when compared to AR - GAN .", "ner": [["CycleGAN", "Method"], ["cityscapes", "Dataset"], ["FCN", "Method"], ["AR - GAN", "Method"]], "rel": [["CycleGAN", "Compare-With", "AR - GAN"]], "rel_plus": [["CycleGAN:Method", "Compare-With", "AR - GAN:Method"]]}
{"doc_id": "202888751", "sentence": "FID and NIMA on cityscapes labels\u2192photos at 2 5 6 \u00d7 2 5 6 resolution .", "ner": [["FID", "Method"], ["NIMA", "Method"], ["cityscapes", "Dataset"]], "rel": [["NIMA", "Evaluated-With", "cityscapes"], ["FID", "Evaluated-With", "cityscapes"]], "rel_plus": [["NIMA:Method", "Evaluated-With", "cityscapes:Dataset"], ["FID:Method", "Evaluated-With", "cityscapes:Dataset"]]}
{"doc_id": "202888751", "sentence": "In image translation methods , the fidelity and resemblance between images generated by GANs and real images is a trade - off .", "ner": [["image translation", "Task"], ["GANs", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202888751", "sentence": "This effect is demonstrated in Fig. 6 , on horse\u2192zebra translation task on unpaired Horse 2 Zebra dataset 6 .", "ner": [["horse\u2192zebra translation", "Task"], ["Horse 2 Zebra", "Dataset"]], "rel": [["Horse 2 Zebra", "Benchmark-For", "horse\u2192zebra translation"]], "rel_plus": [["Horse 2 Zebra:Dataset", "Benchmark-For", "horse\u2192zebra translation:Task"]]}
{"doc_id": "202888751", "sentence": "From left to right : input , AR - GAN ( = 0. 5 ) , AR - GAN ( = 0. 7 5 ) and AR - GAN ( = 1 ) .", "ner": [["AR - GAN ( = 0. 5 )", "Method"], ["AR - GAN ( = 0. 7 5 )", "Method"], ["AR - GAN ( = 1 )", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202888751", "sentence": "While = 0. 7 5 gives the best results for the Horse 2 Zebra dataset , in our experiments with Tomato Plant Disease Dataset we find it suitable to use = , which depicts as a hyperparameter depending on the nature of data in the two domains while training the model . for each specific task and dataset gives results that are both faithful to the original image and similar to the target domain , and thus penalizes the perceptual quality between the images from the two domains .", "ner": [["Horse 2 Zebra", "Dataset"], ["Tomato Plant Disease Dataset", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "202888751", "sentence": "To address this problem of data insufficiency and heavy class imbalance , we augment dataset using two kinds of data augmentation methods : classical data augmentation and synthetic data augmentation using AR - GAN .", "ner": [["data augmentation", "Method"], ["classical data augmentation", "Method"], ["synthetic data augmentation", "Method"], ["AR - GAN", "Method"]], "rel": [["synthetic data augmentation", "SubClass-Of", "data augmentation"], ["classical data augmentation", "SubClass-Of", "data augmentation"], ["AR - GAN", "Used-For", "classical data augmentation"], ["AR - GAN", "Used-For", "synthetic data augmentation"]], "rel_plus": [["synthetic data augmentation:Method", "SubClass-Of", "data augmentation:Method"], ["classical data augmentation:Method", "SubClass-Of", "data augmentation:Method"], ["AR - GAN:Method", "Used-For", "classical data augmentation:Method"], ["AR - GAN:Method", "Used-For", "synthetic data augmentation:Method"]]}
{"doc_id": "202888751", "sentence": "To analyze the results of our baseline recognition CNN -ResNet 5 0 and show the performance of classical and synthetic data augmentation , we use three instances of our tomato plant disease dataset ( , + , + ) as training data .", "ner": [["recognition", "Task"], ["CNN -ResNet 5 0", "Method"], ["synthetic data augmentation", "Method"]], "rel": [["CNN -ResNet 5 0", "Used-For", "recognition"]], "rel_plus": [["CNN -ResNet 5 0:Method", "Used-For", "recognition:Task"]]}
{"doc_id": "202888751", "sentence": "To evaluate the recognition system , we use 2 0 0 test samples from each class , which remain unseen to this recognition system while training .", "ner": [["recognition system", "Method"], ["recognition system", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202888751", "sentence": "These results also indicate that using generated tomato plant   This work focused on using GANs in an image translation setting to synthetically augment plant disease dataset and further improve the performance on the plant disease recognition task using deep CNN .", "ner": [["GANs", "Method"], ["image translation", "Task"], ["plant disease recognition", "Task"], ["CNN", "Method"]], "rel": [["GANs", "Used-For", "image translation"], ["CNN", "Used-For", "plant disease recognition"]], "rel_plus": [["GANs:Method", "Used-For", "image translation:Task"], ["CNN:Method", "Used-For", "plant disease recognition:Task"]]}
{"doc_id": "202888751", "sentence": "Our relatively small dataset reflects the size of datasets available to most researchers in the field of plant disease detection and recognition .", "ner": [["plant disease detection", "Task"], ["recognition", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "202888751", "sentence": "In addition to a cycle consistent and adversarial term , our GAN based image - to - image translation framework ( AR - GAN ) optimizes on Activation Reconstruction loss function that measures feature activation against the real image .", "ner": [["GAN based image - to - image translation framework", "Method"], ["AR - GAN", "Method"], ["Activation Reconstruction loss", "Method"]], "rel": [["AR - GAN", "Synonym-Of", "GAN based image - to - image translation framework"], ["Activation Reconstruction loss", "Part-Of", "GAN based image - to - image translation framework"]], "rel_plus": [["AR - GAN:Method", "Synonym-Of", "GAN based image - to - image translation framework:Method"], ["Activation Reconstruction loss:Method", "Part-Of", "GAN based image - to - image translation framework:Method"]]}
{"doc_id": "202888751", "sentence": "We analyze a baseline convolutional neural network classifier for improved recognition using the synthetic samples for augmentation of our training set and compare it with the classical data augmentation scheme .", "ner": [["convolutional neural network", "Method"], ["recognition", "Task"], ["classical data augmentation", "Method"]], "rel": [["convolutional neural network", "Used-For", "recognition"]], "rel_plus": [["convolutional neural network:Method", "Used-For", "recognition:Task"]]}
{"doc_id": "202888751", "sentence": "We observe a significant improvement in classification accuracy ( + 5. 2 % ) using the synthetic samples generated by AR - GAN framework as compared to ( + 0. 8 % ) increase using classic augmentation strategy .", "ner": [["classification", "Task"], ["AR - GAN", "Method"], ["classic augmentation strategy", "Method"]], "rel": [["AR - GAN", "Used-For", "classification"], ["classic augmentation strategy", "Used-For", "classification"], ["AR - GAN", "Compare-With", "classic augmentation strategy"]], "rel_plus": [["AR - GAN:Method", "Used-For", "classification:Task"], ["classic augmentation strategy:Method", "Used-For", "classification:Task"], ["AR - GAN:Method", "Compare-With", "classic augmentation strategy:Method"]]}
{"doc_id": "202888751", "sentence": "We believe that problems like detection and recognition of plant diseases can benefit from using synthetic augmentation , and that the presented approach can lead to a stronger and more robust plant disease detection systems .", "ner": [["detection", "Task"], ["recognition of plant diseases", "Task"], ["synthetic augmentation", "Method"], ["plant disease detection systems", "Method"]], "rel": [["synthetic augmentation", "Used-For", "detection"], ["synthetic augmentation", "Used-For", "recognition of plant diseases"]], "rel_plus": [["synthetic augmentation:Method", "Used-For", "detection:Task"], ["synthetic augmentation:Method", "Used-For", "recognition of plant diseases:Task"]]}
{"doc_id": "153312532", "sentence": "As a state - of - the - art language model pre - training model , BERT ( Bidirectional Encoder Representations from Transformers ) has achieved amazing results in many language understanding tasks .", "ner": [["BERT", "Method"], ["Bidirectional Encoder Representations from Transformers", "Method"]], "rel": [["BERT", "Synonym-Of", "Bidirectional Encoder Representations from Transformers"]], "rel_plus": [["BERT:Method", "Synonym-Of", "Bidirectional Encoder Representations from Transformers:Method"]]}
{"doc_id": "153312532", "sentence": "In this paper , we conduct exhaustive experiments to investigate different fine - tuning methods of BERT on text classification task and provide a general solution for BERT fine - tuning .", "ner": [["BERT", "Method"], ["text classification", "Task"], ["BERT", "Method"]], "rel": [["BERT", "Used-For", "text classification"]], "rel_plus": [["BERT:Method", "Used-For", "text classification:Task"]]}
{"doc_id": "153312532", "sentence": "Text classification is a classic problem in Natural Language Processing ( NLP ) .", "ner": [["Text classification", "Task"], ["Natural Language Processing", "Task"], ["NLP", "Task"]], "rel": [["NLP", "Synonym-Of", "Natural Language Processing"], ["Text classification", "SubTask-Of", "Natural Language Processing"]], "rel_plus": [["NLP:Task", "Synonym-Of", "Natural Language Processing:Task"], ["Text classification:Task", "SubTask-Of", "Natural Language Processing:Task"]]}
{"doc_id": "153312532", "sentence": "Alternatively , substantial work has shown that pre - trained models on large corpus are beneficial for text classification and other NLP tasks , which can avoid training a new model from scratch .", "ner": [["text classification", "Task"], ["NLP", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "153312532", "sentence": "One kind of pre - trained models is the word embeddings , such as word 2 vec ( Mikolov et al. , 2 0 1 3 ) and GloVe ( Pennington et al. , 2 0 1 4 ) , or the contextualized word embeddings , such as CoVe ( Mc - Cann et al. , 2 0 1 7 ) and ELMo ( Peters et al. , 2 0 1 8) .", "ner": [["word embeddings", "Method"], ["word 2 vec", "Method"], ["GloVe", "Method"], ["contextualized word embeddings", "Method"], ["CoVe", "Method"], ["ELMo", "Method"]], "rel": [["word 2 vec", "SubClass-Of", "word embeddings"], ["GloVe", "SubClass-Of", "word embeddings"], ["CoVe", "SubClass-Of", "word embeddings"], ["ELMo", "SubClass-Of", "word embeddings"], ["contextualized word embeddings", "SubClass-Of", "word embeddings"], ["CoVe", "SubClass-Of", "contextualized word embeddings"], ["ELMo", "SubClass-Of", "contextualized word embeddings"]], "rel_plus": [["word 2 vec:Method", "SubClass-Of", "word embeddings:Method"], ["GloVe:Method", "SubClass-Of", "word embeddings:Method"], ["CoVe:Method", "SubClass-Of", "word embeddings:Method"], ["ELMo:Method", "SubClass-Of", "word embeddings:Method"], ["contextualized word embeddings:Method", "SubClass-Of", "word embeddings:Method"], ["CoVe:Method", "SubClass-Of", "contextualized word embeddings:Method"], ["ELMo:Method", "SubClass-Of", "contextualized word embeddings:Method"]]}
{"doc_id": "153312532", "sentence": "Howard and Ruder ( 2 0 1 8) propose ULM - FiT , a fine - tuning method for pre - trained language model that achieves state - of - the - art results on six widely studied text classification datasets .", "ner": [["ULM - FiT", "Method"], ["text classification", "Task"]], "rel": [["ULM - FiT", "Used-For", "text classification"]], "rel_plus": [["ULM - FiT:Method", "Used-For", "text classification:Task"]]}
{"doc_id": "153312532", "sentence": "BERT is based on a multi - layer bidirectional Transformer ( Vaswani et al. , 2 0 1 7 ) and is trained on plain text for masked word prediction and next sentence prediction tasks .", "ner": [["BERT", "Method"], ["multi - layer bidirectional Transformer", "Method"], ["masked word prediction", "Task"], ["next sentence prediction", "Task"]], "rel": [["BERT", "SubClass-Of", "multi - layer bidirectional Transformer"], ["BERT", "Trained-With", "masked word prediction"], ["BERT", "Trained-With", "next sentence prediction"]], "rel_plus": [["BERT:Method", "SubClass-Of", "multi - layer bidirectional Transformer:Method"], ["BERT:Method", "Trained-With", "masked word prediction:Task"], ["BERT:Method", "Trained-With", "next sentence prediction:Task"]]}
{"doc_id": "153312532", "sentence": "Although BERT has achieved amazing results in many natural language understanding ( NLU ) tasks , its potential has yet to be fully explored .", "ner": [["BERT", "Method"], ["natural language understanding", "Task"], ["NLU", "Task"]], "rel": [["NLU", "Synonym-Of", "natural language understanding"], ["BERT", "Used-For", "natural language understanding"]], "rel_plus": [["NLU:Task", "Synonym-Of", "natural language understanding:Task"], ["BERT:Method", "Used-For", "natural language understanding:Task"]]}
{"doc_id": "153312532", "sentence": "In this paper , we investigate how to maximize the utilization of BERT for the text classification task .", "ner": [["BERT", "Method"], ["text classification", "Task"]], "rel": [["BERT", "Used-For", "text classification"]], "rel_plus": [["BERT:Method", "Used-For", "text classification:Task"]]}
{"doc_id": "153312532", "sentence": "We explore several ways of fine - tuning BERT to enhance its performance on text classification task .", "ner": [["BERT", "Method"], ["text classification", "Task"]], "rel": [["BERT", "Used-For", "text classification"]], "rel_plus": [["BERT:Method", "Used-For", "text classification:Task"]]}
{"doc_id": "153312532", "sentence": "The contributions of our paper are as follows : \u2022 We propose a general solution to fine - tune the pre - trained BERT model , which includes three steps : ( 1 ) further pre - train BERT on within - task training data or in - domain data ; ( 2 ) optional fine - tuning BERT with multitask learning if several related tasks are available ; ( 3 ) fine - tune BERT for the target task . \u2022 We also investigate the fine - tuning methods for BERT on target task , including preprocess of long text , layer selection , layerwise learning rate , catastrophic forgetting , and low - shot learning problems . \u2022 We achieve the new state - of - the - art results on seven widely - studied English text classifica - tion datasets and one Chinese news classification dataset .", "ner": [["BERT", "Method"], ["BERT", "Method"], ["BERT", "Method"], ["multitask learning", "Method"], ["BERT", "Method"], ["BERT", "Method"], ["preprocess of long text", "Task"], ["layer selection", "Task"], ["layerwise learning rate", "Task"], ["catastrophic forgetting", "Task"], ["low - shot learning problems", "Task"], ["Chinese news classification", "Task"]], "rel": [["multitask learning", "Used-For", "BERT"], ["BERT", "Used-For", "preprocess of long text"], ["BERT", "Used-For", "layer selection"], ["BERT", "Used-For", "layerwise learning rate"], ["BERT", "Used-For", "catastrophic forgetting"], ["BERT", "Used-For", "low - shot learning problems"]], "rel_plus": [["multitask learning:Method", "Used-For", "BERT:Method"], ["BERT:Method", "Used-For", "preprocess of long text:Task"], ["BERT:Method", "Used-For", "layer selection:Task"], ["BERT:Method", "Used-For", "layerwise learning rate:Task"], ["BERT:Method", "Used-For", "catastrophic forgetting:Task"], ["BERT:Method", "Used-For", "low - shot learning problems:Task"]]}
{"doc_id": "153312532", "sentence": "Pre - trained word embeddings ( Mikolov et al. , 2 0 1 3 ; Pennington et al. , 2 0 1 4 ) , as an important component of modern NLP systems can offer significant improvements over embeddings learned from scratch .", "ner": [["word embeddings", "Method"], ["NLP systems", "Method"]], "rel": [["word embeddings", "Part-Of", "NLP systems"]], "rel_plus": [["word embeddings:Method", "Part-Of", "NLP systems:Method"]]}
{"doc_id": "153312532", "sentence": "The generalization of word embeddings , such as sentence embeddings ( Kiros et al. , 2 0 1 5 ; Logeswaran and Lee , 2 0 1 8) or paragraph embeddings ( Le and Mikolov , 2 0 1 4 ) , are also used as features in downstream models .", "ner": [["word embeddings", "Method"], ["sentence embeddings", "Method"], ["paragraph embeddings", "Method"]], "rel": [["sentence embeddings", "SubClass-Of", "word embeddings"], ["paragraph embeddings", "SubClass-Of", "word embeddings"]], "rel_plus": [["sentence embeddings:Method", "SubClass-Of", "word embeddings:Method"], ["paragraph embeddings:Method", "SubClass-Of", "word embeddings:Method"]]}
{"doc_id": "153312532", "sentence": "In addition to pre - training with unsupervised data , transfer learning with a large amount of supervised data can also achieve good performance , such as natural language inference ( Conneau et al. , 2 0 1 7 ) and machine translation ( McCann et al. , 2 0 1 7 ) .", "ner": [["transfer learning", "Task"], ["natural language inference", "Task"], ["machine translation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "153312532", "sentence": "More recently , the method of pre - training language models on a large network with a large amount of unlabeled data and fine - tuning in downstream tasks has made a breakthrough in several natural language understanding tasks , such as OpenAI GPT ( Radford et al. , 2 0 1 8) and BERT ( Devlin et al. , 2 0 1 8) .", "ner": [["natural language understanding", "Task"], ["GPT", "Method"], ["BERT", "Method"]], "rel": [["GPT", "Used-For", "natural language understanding"], ["BERT", "Used-For", "natural language understanding"]], "rel_plus": [["GPT:Method", "Used-For", "natural language understanding:Task"], ["BERT:Method", "Used-For", "natural language understanding:Task"]]}
{"doc_id": "153312532", "sentence": "Dai and Le ( 2 0 1 5 ) use language model fine - tuning but overfit with 1 0 k labeled examples while Howard and Ruder ( 2 0 1 8) propose ULMFiT and achieve state - of - the - art results in the text classification task .", "ner": [["ULMFiT", "Method"], ["text classification", "Task"]], "rel": [["ULMFiT", "Used-For", "text classification"]], "rel_plus": [["ULMFiT:Method", "Used-For", "text classification:Task"]]}
{"doc_id": "153312532", "sentence": "BERT is pre - trained on Masked Language Model Task and Next Sentence Prediction Task via a large crossdomain corpus .", "ner": [["BERT", "Method"], ["Masked Language Model", "Task"], ["Next Sentence Prediction", "Task"]], "rel": [["Masked Language Model", "Used-For", "BERT"], ["Next Sentence Prediction", "Used-For", "BERT"]], "rel_plus": [["Masked Language Model:Task", "Used-For", "BERT:Method"], ["Next Sentence Prediction:Task", "Used-For", "BERT:Method"]]}
{"doc_id": "153312532", "sentence": "Unlike previous bidirectional language models ( biLM ) limited to a combination of two unidirectional language models ( i.e. , left - toright and right - to - left ) , BERT uses a Masked Language Model to predict words which are randomly masked or replaced .", "ner": [["bidirectional language models", "Method"], ["biLM", "Method"], ["BERT", "Method"]], "rel": [["biLM", "Synonym-Of", "bidirectional language models"]], "rel_plus": [["biLM:Method", "Synonym-Of", "bidirectional language models:Method"]]}
{"doc_id": "153312532", "sentence": "BERT is the first fine - tuning based representation model that achieves state - ofthe - art results for a range of NLP tasks , demonstrating the enormous potential of the fine - tuning method .", "ner": [["BERT", "Method"], ["NLP", "Task"]], "rel": [["BERT", "Used-For", "NLP"]], "rel_plus": [["BERT:Method", "Used-For", "NLP:Task"]]}
{"doc_id": "153312532", "sentence": "In this paper , we have further explored the BERT fine - tuning method for text classification .", "ner": [["BERT", "Method"], ["text classification", "Task"]], "rel": [["BERT", "Used-For", "text classification"]], "rel_plus": [["BERT:Method", "Used-For", "text classification:Task"]]}
{"doc_id": "153312532", "sentence": "Liu et al. ( 2 0 1 9 ) extend the MT - DNN model originally proposed in Liu et al. ( 2 0 1 5 ) by incorporating BERT as its shared text encoding layers .", "ner": [["MT - DNN", "Method"], ["BERT", "Method"]], "rel": [["BERT", "Part-Of", "MT - DNN"]], "rel_plus": [["BERT:Method", "Part-Of", "MT - DNN:Method"]]}
{"doc_id": "153312532", "sentence": "BERT - base model contains an encoder with 1 2 Transformer blocks , 1 2 self - attention heads , and the hidden size of 7 6 8 .", "ner": [["BERT", "Method"], ["Transformer blocks", "Method"], ["self - attention heads", "Method"]], "rel": [["self - attention heads", "Part-Of", "BERT"], ["Transformer blocks", "Part-Of", "BERT"]], "rel_plus": [["self - attention heads:Method", "Part-Of", "BERT:Method"], ["Transformer blocks:Method", "Part-Of", "BERT:Method"]]}
{"doc_id": "153312532", "sentence": "For text classification tasks , BERT takes the final hidden state h of the first token [ CLS ] as the representation of the whole sequence .", "ner": [["text classification", "Task"], ["BERT", "Method"]], "rel": [["BERT", "Used-For", "text classification"]], "rel_plus": [["BERT:Method", "Used-For", "text classification:Task"]]}
{"doc_id": "153312532", "sentence": "A simple softmax classifier is added to the top of BERT to predict the probability of label c : where W is the task - specific parameter matrix .", "ner": [["softmax classifier", "Method"], ["BERT", "Method"]], "rel": [["softmax classifier", "Part-Of", "BERT"]], "rel_plus": [["softmax classifier:Method", "Part-Of", "BERT:Method"]]}
{"doc_id": "153312532", "sentence": "When we adapt BERT to NLP tasks in a target domain , a proper fine - tuning strategy is desired .", "ner": [["BERT", "Method"], ["NLP", "Task"]], "rel": [["BERT", "Used-For", "NLP"]], "rel_plus": [["BERT:Method", "Used-For", "NLP:Task"]]}
{"doc_id": "153312532", "sentence": "In this paper , we look for the proper fine - tuning methods in the following three ways . 1 ) Fine - Tuning Strategies : When we fine - tune BERT for a target task , there are many ways to utilize BERT .", "ner": [["BERT", "Method"], ["BERT", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "153312532", "sentence": "A natural idea is to further pre - train BERT with target domain data . 3 ) Multi - Task Fine - Tuning : Without pretrained LM models , multi - task learning has shown its effectiveness of exploiting the shared knowledge among the multiple tasks .", "ner": [["BERT", "Method"], ["Multi - Task Fine - Tuning", "Method"], ["pretrained LM models", "Method"], ["multi - task learning", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "153312532", "sentence": "To adapt BERT to a target task , we need to consider several factors : 1 ) The first factor is the preprocessing of long text since the maximum sequence length of BERT is 5 1 2 . 2 ) The second factor is layer selection .", "ner": [["BERT", "Method"], ["BERT", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "153312532", "sentence": "When \u03be = 1 , all layers have the same learning rate , which is equivalent to the regular stochastic gradient descent ( SGD ) .", "ner": [["stochastic gradient descent", "Method"], ["SGD", "Method"]], "rel": [["SGD", "Synonym-Of", "stochastic gradient descent"]], "rel_plus": [["SGD:Method", "Synonym-Of", "stochastic gradient descent:Method"]]}
{"doc_id": "153312532", "sentence": "For a text classification task in a specific domain , such as movie reviews , its data distribution may be different from BERT .", "ner": [["text classification", "Task"], ["BERT", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "153312532", "sentence": "Therefore , we can further pre - train BERT with masked language model and next sentence prediction tasks on the domain - specific data .", "ner": [["BERT", "Method"], ["masked language model", "Task"], ["next sentence prediction", "Task"]], "rel": [["BERT", "Trained-With", "masked language model"], ["BERT", "Trained-With", "next sentence prediction"]], "rel_plus": [["BERT:Method", "Trained-With", "masked language model:Task"], ["BERT:Method", "Trained-With", "next sentence prediction:Task"]]}
{"doc_id": "153312532", "sentence": "Similar to Liu et al. ( 2 0 1 9 ) , we also use fine - tune BERT in multi - task learning framework for text classification .", "ner": [["BERT", "Method"], ["multi - task learning", "Method"], ["text classification", "Task"]], "rel": [["multi - task learning", "Used-For", "BERT"], ["BERT", "Used-For", "text classification"]], "rel_plus": [["multi - task learning:Method", "Used-For", "BERT:Method"], ["BERT:Method", "Used-For", "text classification:Task"]]}
{"doc_id": "153312532", "sentence": "We use the base BERT models : the uncased BERT - base model 1 and the Chinese BERTbase model 2 respectively .", "ner": [["base BERT", "Method"], ["uncased BERT - base", "Method"], ["Chinese BERTbase", "Method"]], "rel": [["uncased BERT - base", "SubClass-Of", "base BERT"], ["Chinese BERTbase", "SubClass-Of", "base BERT"]], "rel_plus": [["uncased BERT - base:Method", "SubClass-Of", "base BERT:Method"], ["Chinese BERTbase:Method", "SubClass-Of", "base BERT:Method"]]}
{"doc_id": "153312532", "sentence": "These datasets have varying numbers of documents and varying document lengths , covering three common text classification tasks : sentiment analysis , question classification , and topic classification .", "ner": [["text classification", "Task"], ["sentiment analysis", "Task"], ["question classification", "Task"], ["topic classification", "Task"]], "rel": [["sentiment analysis", "SubTask-Of", "text classification"], ["question classification", "SubTask-Of", "text classification"], ["topic classification", "SubTask-Of", "text classification"]], "rel_plus": [["sentiment analysis:Task", "SubTask-Of", "text classification:Task"], ["question classification:Task", "SubTask-Of", "text classification:Task"], ["topic classification:Task", "SubTask-Of", "text classification:Task"]]}
{"doc_id": "153312532", "sentence": "For sentiment analysis , we use the binary film review IMDb dataset ( Maas et al. , 2 0 1 1 ) and the binary and five - class version of the Yelp review dataset built by Zhang et al. ( 2 0 1 5 ) .", "ner": [["sentiment analysis", "Task"], ["IMDb", "Dataset"], ["Yelp review", "Dataset"]], "rel": [["IMDb", "Benchmark-For", "sentiment analysis"], ["Yelp review", "Benchmark-For", "sentiment analysis"]], "rel_plus": [["IMDb:Dataset", "Benchmark-For", "sentiment analysis:Task"], ["Yelp review:Dataset", "Benchmark-For", "sentiment analysis:Task"]]}
{"doc_id": "153312532", "sentence": "Question classification For question classification , we evaluate our method on the six - class version of the TREC dataset ( Voorhees and Tice , 1 9 9 9 ) and Yahoo !", "ner": [["Question classification", "Task"], ["question classification", "Task"], ["TREC", "Dataset"], ["Yahoo !", "Dataset"]], "rel": [["TREC", "Benchmark-For", "question classification"], ["Yahoo !", "Benchmark-For", "question classification"]], "rel_plus": [["TREC:Dataset", "Benchmark-For", "question classification:Task"], ["Yahoo !:Dataset", "Benchmark-For", "question classification:Task"]]}
{"doc_id": "153312532", "sentence": "TREC dataset is dataset for question classification consisting of open - domain , fact - based questions divided into broad semantic categories .", "ner": [["TREC", "Dataset"], ["question classification", "Task"]], "rel": [["TREC", "Benchmark-For", "question classification"]], "rel_plus": [["TREC:Dataset", "Benchmark-For", "question classification:Task"]]}
{"doc_id": "153312532", "sentence": "Topic classification For topic classification , we use large - scale AG 's News and DBPedia created by Zhang et al. ( 2 0 1 5 ) .", "ner": [["Topic classification", "Task"], ["topic classification", "Task"], ["AG 's News", "Dataset"], ["DBPedia", "Dataset"]], "rel": [["AG 's News", "Benchmark-For", "topic classification"], ["DBPedia", "Benchmark-For", "topic classification"]], "rel_plus": [["AG 's News:Dataset", "Benchmark-For", "topic classification:Task"], ["DBPedia:Dataset", "Benchmark-For", "topic classification:Task"]]}
{"doc_id": "153312532", "sentence": "To test the effectiveness of BERT for Chinese text , we create the Chinese training and test datasets for Sogou news corpus .", "ner": [["BERT", "Method"], ["Sogou news corpus", "Dataset"]], "rel": [["Sogou news corpus", "Evaluated-With", "BERT"]], "rel_plus": [["Sogou news corpus:Dataset", "Evaluated-With", "BERT:Method"]]}
{"doc_id": "153312532", "sentence": "The dataset is a combination of the So - gouCA and SogouCS news corpora ( Wang et al. , 2 0 0 8) .", "ner": [["So - gouCA", "Dataset"], ["SogouCS news corpora", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "153312532", "sentence": "For further pretraining with BERT , we use spaCy 3 to perform sentence segmentation in English datasets and we use \" \u3002 \" , \" \uff1f \" and \" \uff01 \" as separators when dealing with the Chinese Sogou News dataset .", "ner": [["BERT", "Method"], ["sentence segmentation", "Task"], ["Chinese Sogou News", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "153312532", "sentence": "The first problem of applying BERT to text classification is how processing the text with a length larger than 5 1 2 .", "ner": [["BERT", "Method"], ["text classification", "Task"]], "rel": [["BERT", "Used-For", "text classification"]], "rel_plus": [["BERT:Method", "Used-For", "text classification:Task"]]}
{"doc_id": "153312532", "sentence": "The truncation method of head+tail achieves the best performance on IMDb and Sogou datasets .", "ner": [["IMDb", "Dataset"], ["Sogou", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "153312532", "sentence": "IMDb Sogou   Each layer of BERT captures the different features of the input text .", "ner": [["IMDb", "Dataset"], ["Sogou", "Dataset"], ["BERT", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "153312532", "sentence": "We fine - tune BERT with different learning rates , and the learning curves of error rates on IMDb are shown in Figure 2 .", "ner": [["BERT", "Method"], ["IMDb", "Dataset"]], "rel": [["BERT", "Trained-With", "IMDb"]], "rel_plus": [["BERT:Method", "Trained-With", "IMDb:Dataset"]]}
{"doc_id": "153312532", "sentence": "We find that assign a lower learning rate to the lower layer is effective to fine - tuning BERT , and an appropriate setting is \u03be= 0 . 9 5 and lr= 2 . 0 e - 5 .    Besides , fine - tune BERT with supervised learning , we can further pre - train BERT on the training data by unsupervised masked language model and next sentence prediction tasks .", "ner": [["BERT", "Method"], ["BERT", "Method"], ["BERT", "Method"], ["next sentence prediction", "Task"]], "rel": [["BERT", "Trained-With", "next sentence prediction"]], "rel_plus": [["BERT:Method", "Trained-With", "next sentence prediction:Task"]]}
{"doc_id": "153312532", "sentence": "In this subsection , we investigate whether further pre - training BERT with in - domain and cross - domain data can continue to improve the performance of BERT .", "ner": [["BERT", "Method"], ["BERT", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "153312532", "sentence": "We also find that IMDb and Yelp do not help   We compare our model with the following a variety of different methods : CNN - based methods such as Char - level CNN ( Zhang et al. , 2 0 1 5 ) , VD - CNN ( Conneau et al. , 2 0 1 6 ) and DPCNN ( Johnson and Zhang , 2 0 1 7 ) ; RNN - based models such as D - LSTM ( Yogatama et al. , 2 0 1 7 ) , Skim - LSTM ( Seo et al. , 2 0 1 7 ) and hierarchical attention networks ( Yang et al. , 2 0 1 6 ) ; feature - based transfer learning methods such as rigion embedding ( Qiao et al. , 2 0 1 8) and CoVe ( McCann et al. , 2 0 1 7 ) ; and the language model fine - tuning method ( ULMFiT ) ( Howard and Ruder , 2 0 1 8) , which is the current state - of - the - art for text classification .", "ner": [["IMDb", "Dataset"], ["Yelp", "Dataset"], ["CNN", "Method"], ["Char - level CNN", "Method"], ["VD - CNN", "Method"], ["DPCNN", "Method"], ["RNN", "Method"], ["D - LSTM", "Method"], ["Skim - LSTM", "Method"], ["hierarchical attention networks", "Method"], ["feature - based transfer learning", "Method"], ["rigion embedding", "Method"], ["CoVe", "Method"], ["language model fine - tuning", "Method"], ["ULMFiT", "Method"], ["text classification", "Task"]], "rel": [["Char - level CNN", "SubClass-Of", "CNN"], ["VD - CNN", "SubClass-Of", "CNN"], ["DPCNN", "SubClass-Of", "CNN"], ["D - LSTM", "SubClass-Of", "RNN"], ["Skim - LSTM", "SubClass-Of", "RNN"], ["hierarchical attention networks", "SubClass-Of", "RNN"], ["rigion embedding", "SubClass-Of", "feature - based transfer learning"], ["CoVe", "SubClass-Of", "feature - based transfer learning"], ["ULMFiT", "Synonym-Of", "language model fine - tuning"], ["language model fine - tuning", "Used-For", "text classification"]], "rel_plus": [["Char - level CNN:Method", "SubClass-Of", "CNN:Method"], ["VD - CNN:Method", "SubClass-Of", "CNN:Method"], ["DPCNN:Method", "SubClass-Of", "CNN:Method"], ["D - LSTM:Method", "SubClass-Of", "RNN:Method"], ["Skim - LSTM:Method", "SubClass-Of", "RNN:Method"], ["hierarchical attention networks:Method", "SubClass-Of", "RNN:Method"], ["rigion embedding:Method", "SubClass-Of", "feature - based transfer learning:Method"], ["CoVe:Method", "SubClass-Of", "feature - based transfer learning:Method"], ["ULMFiT:Method", "Synonym-Of", "language model fine - tuning:Method"], ["language model fine - tuning:Method", "Used-For", "text classification:Task"]]}
{"doc_id": "153312532", "sentence": "We implement BERT - Feat through using the feature from BERT model as the input embedding of the biLSTM with self - attention ( Lin et al. , 2 0 1 7 ) .", "ner": [["BERT", "Method"], ["BERT", "Method"], ["biLSTM", "Method"], ["self - attention", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "153312532", "sentence": "The result of BERT - IDPT - FiT corresponds to the row of ' all sentiment ' , ' all question ' , and ' all topic ' in Table 5 , and the result of BERT - CDPT - FiT corresponds to the row of ' all ' in it .", "ner": [["BERT - IDPT - FiT", "Method"], ["BERT - CDPT - FiT", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "153312532", "sentence": "As is shown in Table 6 , BERT - Feat performs better than all other baselines except for ULMFiT. In addition to being slightly worse than BERT - Feat on DBpedia dataset , BERT - FiT outperforms BERT - Feat on the other seven datasets .", "ner": [["BERT - Feat", "Method"], ["ULMFiT.", "Method"], ["BERT - Feat", "Method"], ["DBpedia", "Dataset"], ["BERT - FiT", "Method"], ["BERT - Feat", "Method"]], "rel": [["BERT - Feat", "Compare-With", "ULMFiT."], ["DBpedia", "Evaluated-With", "BERT - Feat"], ["BERT - FiT", "Compare-With", "BERT - Feat"], ["BERT - FiT", "Compare-With", "BERT - Feat"]], "rel_plus": [["BERT - Feat:Method", "Compare-With", "ULMFiT.:Method"], ["DBpedia:Dataset", "Evaluated-With", "BERT - Feat:Method"], ["BERT - FiT:Method", "Compare-With", "BERT - Feat:Method"], ["BERT - FiT:Method", "Compare-With", "BERT - Feat:Method"]]}
{"doc_id": "153312532", "sentence": "Using BERT - Feat as a reference , we calculate the average percentage increase of other BERT - FiT models on each dataset .", "ner": [["BERT - Feat", "Method"], ["BERT - FiT", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "153312532", "sentence": "When there are several datasets for the text classification task , to take full advantage of these available data , we further consider a fine - tuning step with multi - task learning .", "ner": [["text classification", "Task"], ["multi - task learning", "Method"]], "rel": [["multi - task learning", "Used-For", "text classification"]], "rel_plus": [["multi - task learning:Method", "Used-For", "text classification:Task"]]}
{"doc_id": "153312532", "sentence": "We use four English text classification datasets ( IMDb , Yelp P. , AG , and DBP ) .", "ner": [["text classification", "Task"], ["IMDb", "Dataset"], ["Yelp P.", "Dataset"], ["AG", "Dataset"], ["DBP", "Dataset"]], "rel": [["IMDb", "Benchmark-For", "text classification"], ["Yelp P.", "Benchmark-For", "text classification"], ["AG", "Benchmark-For", "text classification"], ["DBP", "Benchmark-For", "text classification"]], "rel_plus": [["IMDb:Dataset", "Benchmark-For", "text classification:Task"], ["Yelp P.:Dataset", "Benchmark-For", "text classification:Task"], ["AG:Dataset", "Benchmark-For", "text classification:Task"], ["DBP:Dataset", "Benchmark-For", "text classification:Task"]]}
{"doc_id": "153312532", "sentence": "The dataset Yelp F. is excluded since there is overlap between the test set of Yelp F. and the training set of Yelp P. , and two datasets of question domain are also excluded .", "ner": [["Yelp F.", "Dataset"], ["Yelp F.", "Dataset"], ["Yelp P.", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "153312532", "sentence": "However , multi - task fine - tuning does not seem to be helpful to BERT - CDPT in Yelp P. and AG .", "ner": [["BERT - CDPT", "Method"], ["Yelp P.", "Dataset"], ["AG", "Dataset"]], "rel": [["BERT - CDPT", "Trained-With", "Yelp P."], ["BERT - CDPT", "Trained-With", "AG"]], "rel_plus": [["BERT - CDPT:Method", "Trained-With", "Yelp P.:Dataset"], ["BERT - CDPT:Method", "Trained-With", "AG:Dataset"]]}
{"doc_id": "153312532", "sentence": "Multi - task fine - tuning and cross - domain pre - training may be alternative methods since the BERT - CDPT model already contains rich domain - specific information , Table 7 : Test error rates ( % ) with multi - task finetuning . and multi - task learning may not be necessary to improve generalization on related text classification sub - tasks .", "ner": [["Multi - task fine - tuning", "Method"], ["cross - domain pre - training", "Method"], ["BERT - CDPT", "Method"], ["text classification", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "153312532", "sentence": "We evaluate BERT - FiT and BERT - ITPT - FiT on different numbers of training examples .", "ner": [["BERT - FiT", "Method"], ["BERT - ITPT - FiT", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "153312532", "sentence": "We select a subset of IMDb training data and feed them into BERT - FiT and BERT - ITPT - FiT. We show the result in Figure 4 .", "ner": [["IMDb", "Dataset"], ["BERT - FiT", "Method"], ["BERT - ITPT - FiT.", "Method"]], "rel": [["BERT - FiT", "Trained-With", "IMDb"], ["BERT - ITPT - FiT.", "Trained-With", "IMDb"]], "rel_plus": [["BERT - FiT:Method", "Trained-With", "IMDb:Dataset"], ["BERT - ITPT - FiT.:Method", "Trained-With", "IMDb:Dataset"]]}
{"doc_id": "153312532", "sentence": "Further pre - trained BERT can further boost its performance , which improves the performance from 1 7 . 2 6 % to 9. 2 3 % in error rates with only 0. 4 % training data .   In this paper , we conduct extensive experiments to investigate the different approaches to fine - tuning BERT for the text classification task .", "ner": [["BERT", "Method"], ["BERT", "Method"], ["text classification", "Task"]], "rel": [["BERT", "Used-For", "text classification"]], "rel_plus": [["BERT:Method", "Used-For", "text classification:Task"]]}
{"doc_id": "153312532", "sentence": "There are some experimental findings : 1 ) The top layer of BERT is more useful for text classification ; 2 ) With an appropriate layer - wise decreasing learning rate , BERT can overcome the catastrophic forgetting problem ; 3 ) Within - task and in - domain further pre - training can significantly boost its performance ; 4 ) A preceding multi - task fine - tuning is also helpful to the single - task fine - tuning , but its benefit is smaller than further pre - training ; 5 ) BERT can improve the task with small - size data .", "ner": [["BERT", "Method"], ["text classification", "Task"], ["BERT", "Method"], ["multi - task fine - tuning", "Method"], ["single - task fine - tuning", "Method"], ["pre - training", "Method"], ["BERT", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "153312532", "sentence": "In the future , we will investigate whether the BERT - large model has similar findings to BERT - base and probe more insight of BERT on how it works .", "ner": [["BERT - large", "Method"], ["BERT - base", "Method"], ["BERT", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "51559", "sentence": "Recurrent neural networks are a powerful tool for modeling sequential data , but the dependence of each timestep 's computation on the previous timestep 's output limits parallelism and makes RNNs unwieldy for very long sequences .", "ner": [["Recurrent neural networks", "Method"], ["RNNs", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "51559", "sentence": "We introduce quasi - recurrent neural networks ( QRNNs ) , an approach to neural sequence modeling that alternates convolutional layers , which apply in parallel across timesteps , and a minimalist recurrent pooling function that applies in parallel across channels .", "ner": [["quasi - recurrent neural networks", "Method"], ["QRNNs", "Method"], ["neural sequence modeling", "Task"], ["convolutional layers", "Method"], ["minimalist recurrent pooling function", "Method"]], "rel": [["QRNNs", "Synonym-Of", "quasi - recurrent neural networks"], ["convolutional layers", "Part-Of", "quasi - recurrent neural networks"], ["minimalist recurrent pooling function", "Part-Of", "quasi - recurrent neural networks"], ["quasi - recurrent neural networks", "Used-For", "neural sequence modeling"]], "rel_plus": [["QRNNs:Method", "Synonym-Of", "quasi - recurrent neural networks:Method"], ["convolutional layers:Method", "Part-Of", "quasi - recurrent neural networks:Method"], ["minimalist recurrent pooling function:Method", "Part-Of", "quasi - recurrent neural networks:Method"], ["quasi - recurrent neural networks:Method", "Used-For", "neural sequence modeling:Task"]]}
{"doc_id": "51559", "sentence": "Experiments on language modeling , sentiment classification , and character - level neural machine translation demonstrate these advantages and underline the viability of QRNNs as a basic building block for a variety of sequence tasks .", "ner": [["language modeling", "Task"], ["sentiment classification", "Task"], ["character - level neural machine translation", "Task"], ["QRNNs", "Method"]], "rel": [["QRNNs", "Used-For", "language modeling"], ["QRNNs", "Used-For", "sentiment classification"], ["QRNNs", "Used-For", "character - level neural machine translation"]], "rel_plus": [["QRNNs:Method", "Used-For", "language modeling:Task"], ["QRNNs:Method", "Used-For", "sentiment classification:Task"], ["QRNNs:Method", "Used-For", "character - level neural machine translation:Task"]]}
{"doc_id": "51559", "sentence": "Recurrent neural networks ( RNNs ) , including gated variants such as the long short - term memory ( LSTM ) ( Hochreiter & Schmidhuber , 1 9 9 7 ) have become the standard model architecture for deep learning approaches to sequence modeling tasks .", "ner": [["Recurrent neural networks", "Method"], ["RNNs", "Method"], ["long short - term memory", "Method"], ["LSTM", "Method"], ["deep learning", "Method"], ["sequence modeling", "Task"]], "rel": [["RNNs", "Synonym-Of", "Recurrent neural networks"], ["long short - term memory", "SubClass-Of", "Recurrent neural networks"], ["LSTM", "Synonym-Of", "long short - term memory"], ["Recurrent neural networks", "SubClass-Of", "deep learning"], ["long short - term memory", "SubClass-Of", "deep learning"], ["Recurrent neural networks", "Used-For", "sequence modeling"], ["long short - term memory", "Used-For", "sequence modeling"]], "rel_plus": [["RNNs:Method", "Synonym-Of", "Recurrent neural networks:Method"], ["long short - term memory:Method", "SubClass-Of", "Recurrent neural networks:Method"], ["LSTM:Method", "Synonym-Of", "long short - term memory:Method"], ["Recurrent neural networks:Method", "SubClass-Of", "deep learning:Method"], ["long short - term memory:Method", "SubClass-Of", "deep learning:Method"], ["Recurrent neural networks:Method", "Used-For", "sequence modeling:Task"], ["long short - term memory:Method", "Used-For", "sequence modeling:Task"]]}
{"doc_id": "51559", "sentence": "RNN applications in the natural language domain range from sentence classification ( Wang et al. , 2 0 1 5 ) to word - and character - level language modeling ( Zaremba et al. , 2 0 1 4 ) .", "ner": [["RNN", "Method"], ["sentence classification", "Task"], ["character - level language modeling", "Task"]], "rel": [["RNN", "Used-For", "sentence classification"], ["RNN", "Used-For", "character - level language modeling"]], "rel_plus": [["RNN:Method", "Used-For", "sentence classification:Task"], ["RNN:Method", "Used-For", "character - level language modeling:Task"]]}
{"doc_id": "51559", "sentence": "RNNs are also commonly the basic building block for more complex models for tasks such as machine translation ( Bahdanau et al. , 2 0 1 5 ; Luong et al. , 2 0 1 5 ; or question answering ( Kumar et al. , 2 0 1 6 ; .", "ner": [["RNNs", "Method"], ["machine translation", "Task"], ["question answering", "Task"]], "rel": [["RNNs", "Used-For", "machine translation"]], "rel_plus": [["RNNs:Method", "Used-For", "machine translation:Task"]]}
{"doc_id": "51559", "sentence": "Unfortunately standard RNNs , including LSTMs , are limited in their capability to handle tasks involving very long sequences , such as document classification or character - level machine translation , as the computation of features or states for different parts of the document can not occur in parallel .", "ner": [["RNNs", "Method"], ["LSTMs", "Method"], ["document classification", "Task"], ["character - level machine translation", "Task"]], "rel": [["LSTMs", "SubClass-Of", "RNNs"], ["RNNs", "Used-For", "document classification"], ["LSTMs", "Used-For", "document classification"], ["RNNs", "Used-For", "character - level machine translation"], ["LSTMs", "Used-For", "character - level machine translation"]], "rel_plus": [["LSTMs:Method", "SubClass-Of", "RNNs:Method"], ["RNNs:Method", "Used-For", "document classification:Task"], ["LSTMs:Method", "Used-For", "document classification:Task"], ["RNNs:Method", "Used-For", "character - level machine translation:Task"], ["LSTMs:Method", "Used-For", "character - level machine translation:Task"]]}
{"doc_id": "51559", "sentence": "Convolutional neural networks ( CNNs ) ( Krizhevsky et al. , 2 0 1 2 ) , though more popular on tasks involving image data , have also been applied to sequence encoding tasks ( Zhang et al. , 2 0 1 5 ) .", "ner": [["Convolutional neural networks", "Method"], ["CNNs", "Method"], ["sequence encoding", "Task"]], "rel": [["CNNs", "Synonym-Of", "Convolutional neural networks"], ["Convolutional neural networks", "Used-For", "sequence encoding"]], "rel_plus": [["CNNs:Method", "Synonym-Of", "Convolutional neural networks:Method"], ["Convolutional neural networks:Method", "Used-For", "sequence encoding:Task"]]}
{"doc_id": "51559", "sentence": "Convolutional models for sequence processing have been more successful when combined with RNN layers in a hybrid architecture ( Lee et al. , 2 0 1 6 ) , because traditional max - and average - pooling approaches to combining convolutional features across timesteps assume time invariance and hence can not make full use of large - scale sequence order information .", "ner": [["RNN", "Method"], ["max - and average - pooling", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "51559", "sentence": "QRNNs address both drawbacks of standard models : like CNNs , QRNNs allow for parallel computation across both timestep and minibatch dimensions , enabling high throughput and good scaling to long sequences .", "ner": [["QRNNs", "Method"], ["CNNs", "Method"], ["QRNNs", "Method"]], "rel": [["QRNNs", "Compare-With", "CNNs"]], "rel_plus": [["QRNNs:Method", "Compare-With", "CNNs:Method"]]}
{"doc_id": "51559", "sentence": "Like RNNs , QRNNs allow the output to depend on the overall order of elements in the sequence .", "ner": [["RNNs", "Method"], ["QRNNs", "Method"]], "rel": [["RNNs", "Compare-With", "QRNNs"]], "rel_plus": [["RNNs:Method", "Compare-With", "QRNNs:Method"]]}
{"doc_id": "51559", "sentence": "We describe QRNN variants tailored to several natural language tasks , including document - level sentiment classification , language modeling , and character - level machine translation .", "ner": [["QRNN", "Method"], ["document - level sentiment classification", "Task"], ["language modeling", "Task"], ["character - level machine translation", "Task"]], "rel": [["QRNN", "Used-For", "document - level sentiment classification"], ["QRNN", "Used-For", "language modeling"], ["QRNN", "Used-For", "character - level machine translation"]], "rel_plus": [["QRNN:Method", "Used-For", "document - level sentiment classification:Task"], ["QRNN:Method", "Used-For", "language modeling:Task"], ["QRNN:Method", "Used-For", "character - level machine translation:Task"]]}
{"doc_id": "51559", "sentence": "These models outperform strong LSTM baselines on all three tasks while dramatically reducing computation time .   Each layer of a quasi - recurrent neural network consists of two kinds of subcomponents , analogous to convolution and pooling layers in CNNs .", "ner": [["LSTM", "Method"], ["quasi - recurrent neural network", "Method"], ["convolution and pooling layers", "Method"], ["CNNs", "Method"]], "rel": [["convolution and pooling layers", "Part-Of", "CNNs"]], "rel_plus": [["convolution and pooling layers:Method", "Part-Of", "CNNs:Method"]]}
{"doc_id": "51559", "sentence": "Given an input sequence X \u2208 R T \u00d7n of T n - dimensional vectors x 1 . . . x T , the convolutional subcomponent of a QRNN performs convolutions in the timestep dimension with a bank of m filters , producing a sequence Z \u2208 R T \u00d7m of m - dimensional candidate vectors z t .", "ner": [["convolutional subcomponent", "Method"], ["QRNN", "Method"], ["convolutions", "Method"]], "rel": [["convolutional subcomponent", "Part-Of", "QRNN"]], "rel_plus": [["convolutional subcomponent:Method", "Part-Of", "QRNN:Method"]]}
{"doc_id": "51559", "sentence": "Note that if the filter width is 2 , these equations reduce to the LSTM - like ( 2 ) Convolution filters of larger width effectively compute higher n - gram features at each timestep ; thus larger widths are especially important for character - level tasks .", "ner": [["LSTM", "Method"], ["Convolution filters", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "51559", "sentence": "As with convolutional neural networks , two or more QRNN layers should be stacked to create a model with the capacity to approximate more complex functions .", "ner": [["convolutional neural networks", "Method"], ["QRNN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "51559", "sentence": "Regularization An important extension to the stacked QRNN is a robust regularization scheme inspired by recent work in regularizing LSTMs .", "ner": [["QRNN", "Method"], ["LSTMs", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "51559", "sentence": "The need for an effective regularization method for LSTMs , and dropout 's relative lack of efficacy when applied to recurrent connections , led to the development of recurrent dropout schemes , including variational inference - based dropout ( Gal & Ghahramani , 2 0 1 6 ) and zoneout ( Krueger et al. , 2 0 1 6 ) .", "ner": [["LSTMs", "Method"], ["dropout", "Method"], ["recurrent dropout", "Method"], ["variational inference - based dropout", "Method"], ["zoneout", "Method"]], "rel": [["recurrent dropout", "Part-Of", "LSTMs"], ["variational inference - based dropout", "Part-Of", "LSTMs"], ["zoneout", "Part-Of", "LSTMs"], ["recurrent dropout", "SubClass-Of", "dropout"], ["variational inference - based dropout", "SubClass-Of", "recurrent dropout"], ["zoneout", "SubClass-Of", "recurrent dropout"]], "rel_plus": [["recurrent dropout:Method", "Part-Of", "LSTMs:Method"], ["variational inference - based dropout:Method", "Part-Of", "LSTMs:Method"], ["zoneout:Method", "Part-Of", "LSTMs:Method"], ["recurrent dropout:Method", "SubClass-Of", "dropout:Method"], ["variational inference - based dropout:Method", "SubClass-Of", "recurrent dropout:Method"], ["zoneout:Method", "SubClass-Of", "recurrent dropout:Method"]]}
{"doc_id": "51559", "sentence": "These schemes extend dropout to the recurrent setting by taking advantage of the repeating structure of recurrent networks , providing more powerful and less destructive regularization .", "ner": [["dropout", "Method"], ["recurrent networks", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "51559", "sentence": "Variational inference - based dropout locks the dropout mask used for the recurrent connections across timesteps , so a single RNN pass uses a single stochastic subset of the recurrent weights .", "ner": [["Variational inference - based dropout", "Method"], ["dropout", "Method"], ["RNN", "Method"]], "rel": [["Variational inference - based dropout", "SubClass-Of", "dropout"], ["Variational inference - based dropout", "Part-Of", "RNN"]], "rel_plus": [["Variational inference - based dropout:Method", "SubClass-Of", "dropout:Method"], ["Variational inference - based dropout:Method", "Part-Of", "RNN:Method"]]}
{"doc_id": "51559", "sentence": "As QRNNs lack recurrent weights , the variational inference approach does not apply .", "ner": [["QRNNs", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "51559", "sentence": "Thus we extended zoneout to the QRNN architecture by modifying the pooling function to keep the previous pooling state for a stochastic subset of channels .", "ner": [["zoneout", "Method"], ["QRNN", "Method"], ["pooling function", "Method"]], "rel": [["zoneout", "Part-Of", "QRNN"], ["pooling function", "Part-Of", "QRNN"]], "rel_plus": [["zoneout:Method", "Part-Of", "QRNN:Method"], ["pooling function:Method", "Part-Of", "QRNN:Method"]]}
{"doc_id": "51559", "sentence": "Conveniently , this is equivalent to stochastically setting a subset of the QRNN 's f gate channels to 1 , or applying dropout on 1 \u2212 f : Thus the pooling function itself need not be modified at all .", "ner": [["QRNN", "Method"], ["dropout", "Method"], ["pooling function", "Method"]], "rel": [["dropout", "Part-Of", "QRNN"]], "rel_plus": [["dropout:Method", "Part-Of", "QRNN:Method"]]}
{"doc_id": "51559", "sentence": "In many experiments , we also apply ordinary dropout between layers , including between word embeddings and the first QRNN layer .", "ner": [["dropout", "Method"], ["word embeddings", "Method"], ["QRNN", "Method"]], "rel": [["dropout", "Part-Of", "word embeddings"], ["dropout", "Part-Of", "QRNN"]], "rel_plus": [["dropout:Method", "Part-Of", "word embeddings:Method"], ["dropout:Method", "Part-Of", "QRNN:Method"]]}
{"doc_id": "51559", "sentence": "For sequence classification tasks , we found it helpful to use skip - connections between every QRNN layer , a technique termed \" dense convolution \" by Huang et al. ( 2 0 1 6 ) .", "ner": [["sequence classification", "Task"], ["skip - connections", "Method"], ["QRNN", "Method"], ["dense convolution", "Method"]], "rel": [["skip - connections", "Used-For", "sequence classification"], ["QRNN", "Used-For", "sequence classification"], ["dense convolution", "Used-For", "sequence classification"], ["skip - connections", "Part-Of", "QRNN"]], "rel_plus": [["skip - connections:Method", "Used-For", "sequence classification:Task"], ["QRNN:Method", "Used-For", "sequence classification:Task"], ["dense convolution:Method", "Used-For", "sequence classification:Task"], ["skip - connections:Method", "Part-Of", "QRNN:Method"]]}
{"doc_id": "51559", "sentence": "When applying this technique to the QRNN , we include connections between the input embeddings and every QRNN layer and between every pair of QRNN layers .", "ner": [["QRNN", "Method"], ["QRNN", "Method"], ["QRNN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "51559", "sentence": "Encoder - Decoder Models To demonstrate the generality of QRNNs , we extend the model architecture to sequence - to - sequence tasks , such as machine translation , by using a QRNN as encoder and a modified QRNN , enhanced with attention , as decoder .", "ner": [["QRNNs", "Method"], ["machine translation", "Task"], ["QRNN", "Method"], ["QRNN", "Method"], ["attention", "Method"]], "rel": [["QRNNs", "Used-For", "machine translation"], ["attention", "Part-Of", "QRNN"]], "rel_plus": [["QRNNs:Method", "Used-For", "machine translation:Task"], ["attention:Method", "Part-Of", "QRNN:Method"]]}
{"doc_id": "51559", "sentence": "While the first step of this attention procedure is quadratic in the sequence length , in practice it takes significantly less computation time than the model 's linear and convolutional layers due to the simple and highly parallel dot - product scoring function .   We evaluate the performance of the QRNN on three different natural language tasks : document - level sentiment classification , language modeling , and character - based neural machine translation .", "ner": [["QRNN", "Method"], ["document - level sentiment classification", "Task"], ["language modeling", "Task"], ["character - based neural machine translation", "Task"]], "rel": [["QRNN", "Used-For", "document - level sentiment classification"], ["QRNN", "Used-For", "language modeling"], ["QRNN", "Used-For", "character - based neural machine translation"]], "rel_plus": [["QRNN:Method", "Used-For", "document - level sentiment classification:Task"], ["QRNN:Method", "Used-For", "language modeling:Task"], ["QRNN:Method", "Used-For", "character - based neural machine translation:Task"]]}
{"doc_id": "51559", "sentence": "Our QRNN models outperform LSTM - based models of equal hidden size on all three tasks while dramatically improving computation speed .", "ner": [["QRNN", "Method"], ["LSTM", "Method"]], "rel": [["QRNN", "Compare-With", "LSTM"]], "rel_plus": [["QRNN:Method", "Compare-With", "LSTM:Method"]]}
{"doc_id": "51559", "sentence": "We evaluate the QRNN architecture on a popular document - level sentiment classification benchmark , the IMDb movie review dataset ( Maas et al. , 2 0 1 1 ) .", "ner": [["QRNN", "Method"], ["document - level sentiment classification", "Task"], ["IMDb movie review", "Dataset"]], "rel": [["QRNN", "Used-For", "document - level sentiment classification"], ["IMDb movie review", "Benchmark-For", "document - level sentiment classification"], ["QRNN", "Evaluated-With", "IMDb movie review"]], "rel_plus": [["QRNN:Method", "Used-For", "document - level sentiment classification:Task"], ["IMDb movie review:Dataset", "Benchmark-For", "document - level sentiment classification:Task"], ["QRNN:Method", "Evaluated-With", "IMDb movie review:Dataset"]]}
{"doc_id": "51559", "sentence": "We observed a speedup of 3. 2 x on IMDb train time per epoch compared to the optimized LSTM implementation provided in NVIDIA 's cuDNN library .", "ner": [["IMDb", "Dataset"], ["LSTM", "Method"]], "rel": [["LSTM", "Trained-With", "IMDb"]], "rel_plus": [["LSTM:Method", "Trained-With", "IMDb:Dataset"]]}
{"doc_id": "51559", "sentence": "In Figure 3 , we visualize the hidden state vectors c L t of the final QRNN layer on part of an example from the IMDb dataset .", "ner": [["QRNN", "Method"], ["IMDb", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "51559", "sentence": "The experiment uses a standard preprocessed version of the Penn Treebank ( PTB ) by Mikolov et al. ( 2 0 1 0 ) .", "ner": [["Penn Treebank", "Dataset"], ["PTB", "Dataset"]], "rel": [["PTB", "Synonym-Of", "Penn Treebank"]], "rel_plus": [["PTB:Dataset", "Synonym-Of", "Penn Treebank:Dataset"]]}
{"doc_id": "51559", "sentence": "While the \" medium \" models used in other work ( Zaremba et al. , 2 0 1 4 ; Gal & Ghahramani , 2 0 1 6 ) consist of 6 5 0 units in Figure 3 : Visualization of the final QRNN layer 's hidden state vectors c L t in the IMDb task , with timesteps along the vertical axis .", "ner": [["QRNN", "Method"], ["IMDb", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "51559", "sentence": "It is not obvious in advance which of the many RNN regularization schemes would perform well when applied to the QRNN .", "ner": [["RNN regularization schemes", "Method"], ["QRNN", "Method"]], "rel": [["RNN regularization schemes", "Part-Of", "QRNN"]], "rel_plus": [["RNN regularization schemes:Method", "Part-Of", "QRNN:Method"]]}
{"doc_id": "51559", "sentence": "Our tests showed encouraging results from zoneout applied to the QRNN 's recurrent pooling layer , implemented as described in Section 2. 1 .", "ner": [["zoneout", "Method"], ["QRNN", "Method"]], "rel": [["zoneout", "Part-Of", "QRNN"]], "rel_plus": [["zoneout:Method", "Part-Of", "QRNN:Method"]]}
{"doc_id": "51559", "sentence": "Optimization was performed by stochastic gradient descent ( SGD ) without momentum .", "ner": [["stochastic gradient descent", "Method"], ["SGD", "Method"]], "rel": [["SGD", "Synonym-Of", "stochastic gradient descent"]], "rel_plus": [["SGD:Method", "Synonym-Of", "stochastic gradient descent:Method"]]}
{"doc_id": "51559", "sentence": "Zoneout was applied by performing dropout with ratio 0. 1 on the forget gates of the QRNN , without rescaling the output of the dropout function .", "ner": [["Zoneout", "Method"], ["dropout", "Method"], ["QRNN", "Method"], ["dropout", "Method"]], "rel": [["Zoneout", "SubClass-Of", "dropout"], ["Zoneout", "Part-Of", "QRNN"], ["dropout", "Part-Of", "QRNN"]], "rel_plus": [["Zoneout:Method", "SubClass-Of", "dropout:Method"], ["Zoneout:Method", "Part-Of", "QRNN:Method"], ["dropout:Method", "Part-Of", "QRNN:Method"]]}
{"doc_id": "51559", "sentence": "Comparing our results on the gated QRNN with zoneout to the results of LSTMs with both ordinary and variational dropout in Table 2 , we see that the QRNN is highly competitive .", "ner": [["QRNN", "Method"], ["zoneout", "Method"], ["LSTMs", "Method"], ["variational dropout", "Method"], ["QRNN", "Method"]], "rel": [["zoneout", "Part-Of", "QRNN"], ["QRNN", "Compare-With", "LSTMs"], ["variational dropout", "Part-Of", "LSTMs"]], "rel_plus": [["zoneout:Method", "Part-Of", "QRNN:Method"], ["QRNN:Method", "Compare-With", "LSTMs:Method"], ["variational dropout:Method", "Part-Of", "LSTMs:Method"]]}
{"doc_id": "51559", "sentence": "The QRNN without zoneout strongly outperforms both our medium LSTM and the medium LSTM of Zaremba et al. ( 2 0 1 4 ) which do not use recurrent dropout and is even competitive with variational LSTMs .", "ner": [["QRNN", "Method"], ["zoneout", "Method"], ["LSTM", "Method"], ["LSTM", "Method"], ["recurrent dropout", "Method"], ["variational LSTMs", "Method"]], "rel": [["QRNN", "Compare-With", "LSTM"], ["QRNN", "Compare-With", "variational LSTMs"]], "rel_plus": [["QRNN:Method", "Compare-With", "LSTM:Method"], ["QRNN:Method", "Compare-With", "variational LSTMs:Method"]]}
{"doc_id": "51559", "sentence": "This may be due to the limited computational capacity that the QRNN 's pooling layer has relative to the LSTM 's recurrent weights , providing structural regularization over the recurrence .", "ner": [["QRNN", "Method"], ["LSTM", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "51559", "sentence": "Without zoneout , early stopping based upon validation loss was required as the QRNN would begin overfitting .", "ner": [["zoneout", "Method"], ["early stopping", "Method"], ["QRNN", "Method"]], "rel": [["early stopping", "Used-For", "QRNN"]], "rel_plus": [["early stopping:Method", "Used-For", "QRNN:Method"]]}
{"doc_id": "51559", "sentence": "By applying a small amount of zoneout ( p = 0. 1 ) , no early stopping is required and the QRNN achieves competitive levels of perplexity to the variational LSTM of Gal & Ghahra - ( Gal & Ghahramani , 2 0 1 6 ) 2 0 M 8 1 . 9 7 9 . 7 LSTM with CharCNN embeddings ( Kim et al. , 2 0 1 6 ) 1 9 M \u2212 7 8 . 9 Zoneout + Variational LSTM ( medium ) When training on the PTB dataset with an NVIDIA K 4 0 GPU , we found that the QRNN is substantially faster than a standard LSTM , even when comparing against the optimized cuDNN LSTM .", "ner": [["zoneout", "Method"], ["early stopping", "Method"], ["QRNN", "Method"], ["variational LSTM", "Method"], ["LSTM", "Method"], ["CharCNN embeddings", "Method"], ["Zoneout + Variational LSTM", "Method"], ["PTB", "Dataset"], ["QRNN", "Method"], ["LSTM", "Method"], ["cuDNN LSTM", "Method"]], "rel": [["zoneout", "Part-Of", "QRNN"], ["QRNN", "Compare-With", "variational LSTM"], ["CharCNN embeddings", "Part-Of", "LSTM"], ["QRNN", "Trained-With", "PTB"], ["QRNN", "Compare-With", "LSTM"], ["QRNN", "Compare-With", "cuDNN LSTM"]], "rel_plus": [["zoneout:Method", "Part-Of", "QRNN:Method"], ["QRNN:Method", "Compare-With", "variational LSTM:Method"], ["CharCNN embeddings:Method", "Part-Of", "LSTM:Method"], ["QRNN:Method", "Trained-With", "PTB:Dataset"], ["QRNN:Method", "Compare-With", "LSTM:Method"], ["QRNN:Method", "Compare-With", "cuDNN LSTM:Method"]]}
{"doc_id": "51559", "sentence": "In Figure 4 we provide a breakdown of the time taken for Chainer 's default LSTM , the cuDNN LSTM , and QRNN to perform a full forward and backward pass on a single batch during training of the RNN LM on PTB .", "ner": [["LSTM", "Method"], ["cuDNN LSTM", "Method"], ["QRNN", "Method"], ["RNN LM", "Method"], ["PTB", "Dataset"]], "rel": [["RNN LM", "Trained-With", "PTB"]], "rel_plus": [["RNN LM:Method", "Trained-With", "PTB:Dataset"]]}
{"doc_id": "51559", "sentence": "For both LSTM implementations , running time was dominated by the RNN computations , even with the highly optimized cuDNN implementation .", "ner": [["LSTM", "Method"], ["RNN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "51559", "sentence": "For the QRNN implementation , however , the \" RNN \" layers are no longer the bottleneck .", "ner": [["QRNN", "Method"], ["\" RNN \" layers", "Method"]], "rel": [["\" RNN \" layers", "Part-Of", "QRNN"]], "rel_plus": [["\" RNN \" layers:Method", "Part-Of", "QRNN:Method"]]}
{"doc_id": "51559", "sentence": "Indeed , there are diminishing returns from further optimization of the QRNN itself as the softmax and optimization overhead take equal or greater time .", "ner": [["QRNN", "Method"], ["softmax", "Method"]], "rel": [["softmax", "Part-Of", "QRNN"]], "rel_plus": [["softmax:Method", "Part-Of", "QRNN:Method"]]}
{"doc_id": "51559", "sentence": "Note that the softmax , over a vocabulary size of only 1 0 , 0 0 0 words , is relatively small ; for tasks with larger vocabularies , the softmax would likely dominate computation time .", "ner": [["softmax", "Method"], ["softmax", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "51559", "sentence": "It is also important to note that the cuDNN library 's RNN primitives do not natively support any form of recurrent dropout .", "ner": [["RNN", "Method"], ["recurrent dropout", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "51559", "sentence": "We evaluate the sequence - to - sequence QRNN architecture described in 2. 1 on a challenging neural machine translation task , IWSLT German - English spoken - domain translation , applying fully character - level segmentation .", "ner": [["QRNN", "Method"], ["neural machine translation", "Task"], ["IWSLT German - English spoken - domain translation", "Dataset"], ["character - level segmentation", "Method"]], "rel": [["character - level segmentation", "Used-For", "QRNN"], ["QRNN", "Used-For", "neural machine translation"], ["IWSLT German - English spoken - domain translation", "Benchmark-For", "neural machine translation"], ["QRNN", "Evaluated-With", "IWSLT German - English spoken - domain translation"]], "rel_plus": [["character - level segmentation:Method", "Used-For", "QRNN:Method"], ["QRNN:Method", "Used-For", "neural machine translation:Task"], ["IWSLT German - English spoken - domain translation:Dataset", "Benchmark-For", "neural machine translation:Task"], ["QRNN:Method", "Evaluated-With", "IWSLT German - English spoken - domain translation:Dataset"]]}
{"doc_id": "51559", "sentence": "Our best performance on a development set ( TED.tst 2 0 1 3 ) was achieved using a four - layer encoderdecoder QRNN with 3 2 0 units per layer , no dropout or L 2 regularization , and gradient rescaling to a maximum magnitude of 5 .", "ner": [["TED.tst 2 0 1 3", "Dataset"], ["four - layer encoderdecoder QRNN", "Method"], ["dropout", "Method"], ["L 2 regularization", "Method"]], "rel": [["four - layer encoderdecoder QRNN", "Evaluated-With", "TED.tst 2 0 1 3"]], "rel_plus": [["four - layer encoderdecoder QRNN:Method", "Evaluated-With", "TED.tst 2 0 1 3:Dataset"]]}
{"doc_id": "51559", "sentence": "Results using this architecture were compared to an equal - sized four - layer encoder - decoder LSTM with attention , applying dropout of 0. 2 .", "ner": [["four - layer encoder - decoder LSTM with attention", "Method"], ["dropout", "Method"]], "rel": [["dropout", "Part-Of", "four - layer encoder - decoder LSTM with attention"]], "rel_plus": [["dropout:Method", "Part-Of", "four - layer encoder - decoder LSTM with attention:Method"]]}
{"doc_id": "51559", "sentence": "We again optimized using Adam ; other hyperparameters were equal to their values for the QRNN and the same beam search procedure was applied .", "ner": [["Adam", "Method"], ["QRNN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "51559", "sentence": "Table   3 shows that the QRNN outperformed the character - level LSTM , almost matching the performance of a word - level attentional baseline .", "ner": [["QRNN", "Method"], ["character - level LSTM", "Method"]], "rel": [["QRNN", "Compare-With", "character - level LSTM"]], "rel_plus": [["QRNN:Method", "Compare-With", "character - level LSTM:Method"]]}
{"doc_id": "51559", "sentence": "Table 3 : Translation performance , measured by BLEU , and train speed in hours per epoch , for the IWSLT German - English spoken language translation task .", "ner": [["Translation", "Task"], ["BLEU", "Dataset"], ["IWSLT German - English spoken language translation", "Dataset"]], "rel": [["BLEU", "Benchmark-For", "Translation"], ["IWSLT German - English spoken language translation", "Benchmark-For", "Translation"]], "rel_plus": [["BLEU:Dataset", "Benchmark-For", "Translation:Task"], ["IWSLT German - English spoken language translation:Dataset", "Benchmark-For", "Translation:Task"]]}
{"doc_id": "51559", "sentence": "Quasi - recurrent neural networks are related to several such recently described models , especially the strongly - typed recurrent neural networks ( T - RNN ) introduced by Balduzzi & Ghifary ( 2 0 1 6 ) .", "ner": [["Quasi - recurrent neural networks", "Method"], ["strongly - typed recurrent neural networks", "Method"], ["T - RNN", "Method"]], "rel": [["T - RNN", "Synonym-Of", "strongly - typed recurrent neural networks"]], "rel_plus": [["T - RNN:Method", "Synonym-Of", "strongly - typed recurrent neural networks:Method"]]}
{"doc_id": "51559", "sentence": "As the use of a fully connected layer for recurrent connections violates the constraint of \" strong typing \" , all strongly - typed RNN architectures ( including the T - RNN , T - GRU , and T - LSTM ) are also quasi - recurrent .", "ner": [["fully connected layer", "Method"], ["strongly - typed RNN architectures", "Method"], ["T - RNN", "Method"], ["T - GRU", "Method"], ["T - LSTM", "Method"]], "rel": [["T - RNN", "SubClass-Of", "strongly - typed RNN architectures"], ["T - GRU", "SubClass-Of", "strongly - typed RNN architectures"], ["T - LSTM", "SubClass-Of", "strongly - typed RNN architectures"]], "rel_plus": [["T - RNN:Method", "SubClass-Of", "strongly - typed RNN architectures:Method"], ["T - GRU:Method", "SubClass-Of", "strongly - typed RNN architectures:Method"], ["T - LSTM:Method", "SubClass-Of", "strongly - typed RNN architectures:Method"]]}
{"doc_id": "51559", "sentence": "In particular , a T - RNN differs from a QRNN as described in this paper with filter size 1 and f -pooling only in the absence of an activation function on z. Similarly , T - GRUs and T - LSTMs differ from QRNNs with filter size 2 and fo - or ifo - pooling respectively in that they lack tanh on z and use tanh rather than sigmoid on o. The QRNN is also related to work in hybrid convolutional - recurrent models .", "ner": [["T - RNN", "Method"], ["QRNN", "Method"], ["T - GRUs", "Method"], ["T - LSTMs", "Method"], ["QRNNs", "Method"], ["tanh", "Method"], ["tanh", "Method"], ["sigmoid", "Method"], ["QRNN", "Method"], ["hybrid convolutional - recurrent", "Method"]], "rel": [["T - RNN", "Compare-With", "QRNN"], ["tanh", "Part-Of", "T - GRUs"], ["tanh", "Part-Of", "T - LSTMs"], ["T - GRUs", "Compare-With", "QRNNs"], ["T - LSTMs", "Compare-With", "QRNNs"]], "rel_plus": [["T - RNN:Method", "Compare-With", "QRNN:Method"], ["tanh:Method", "Part-Of", "T - GRUs:Method"], ["tanh:Method", "Part-Of", "T - LSTMs:Method"], ["T - GRUs:Method", "Compare-With", "QRNNs:Method"], ["T - LSTMs:Method", "Compare-With", "QRNNs:Method"]]}
{"doc_id": "51559", "sentence": "Zhou et al. ( 2 0 1 5 ) apply CNNs at the word level to generate n - gram features used by an LSTM for text classification .", "ner": [["CNNs", "Method"], ["LSTM", "Method"], ["text classification", "Task"]], "rel": [["CNNs", "Used-For", "LSTM"], ["LSTM", "Used-For", "text classification"], ["CNNs", "Used-For", "text classification"]], "rel_plus": [["CNNs:Method", "Used-For", "LSTM:Method"], ["LSTM:Method", "Used-For", "text classification:Task"], ["CNNs:Method", "Used-For", "text classification:Task"]]}
{"doc_id": "51559", "sentence": "Xiao & Cho ( 2 0 1 6 ) also tackle text classification by applying convolutions at the character level , with a stride to reduce sequence length , then feeding these features into a bidirectional LSTM .", "ner": [["text classification", "Task"], ["convolutions", "Method"], ["bidirectional LSTM", "Method"]], "rel": [["convolutions", "Used-For", "text classification"], ["bidirectional LSTM", "Used-For", "text classification"]], "rel_plus": [["convolutions:Method", "Used-For", "text classification:Task"], ["bidirectional LSTM:Method", "Used-For", "text classification:Task"]]}
{"doc_id": "51559", "sentence": "Their model 's encoder uses a convolutional layer followed by max - pooling to reduce sequence length , a four - layer highway network , and a bidirectional GRU .", "ner": [["convolutional layer", "Method"], ["max - pooling", "Method"], ["bidirectional GRU", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "51559", "sentence": "The QRNN encoder - decoder model shares the favorable parallelism and path - length properties exhibited by the ByteNet , an architecture for character - level machine translation based on residual convolutions over binary trees .", "ner": [["QRNN encoder - decoder", "Method"], ["ByteNet", "Method"], ["character - level machine translation", "Task"], ["residual convolutions", "Method"]], "rel": [["residual convolutions", "Part-Of", "ByteNet"], ["ByteNet", "Used-For", "character - level machine translation"]], "rel_plus": [["residual convolutions:Method", "Part-Of", "ByteNet:Method"], ["ByteNet:Method", "Used-For", "character - level machine translation:Task"]]}
{"doc_id": "51559", "sentence": "QRNNs have better predictive accuracy than LSTM - based models of equal hidden size , even though they use fewer parameters and run substantially faster .", "ner": [["QRNNs", "Method"], ["LSTM", "Method"]], "rel": [["QRNNs", "Compare-With", "LSTM"]], "rel_plus": [["QRNNs:Method", "Compare-With", "LSTM:Method"]]}
{"doc_id": "51559", "sentence": "Extensions to both CNNs and RNNs are often directly applicable to the QRNN , while the model 's hidden states are more interpretable than those of other recurrent architectures as its channels maintain their independence across timesteps .", "ner": [["CNNs", "Method"], ["RNNs", "Method"], ["QRNN", "Method"]], "rel": [["RNNs", "Part-Of", "QRNN"], ["CNNs", "Part-Of", "QRNN"]], "rel_plus": [["RNNs:Method", "Part-Of", "QRNN:Method"], ["CNNs:Method", "Part-Of", "QRNN:Method"]]}
{"doc_id": "51559", "sentence": "We believe that QRNNs can serve as a building block for long - sequence tasks that were previously impractical with traditional RNNs .", "ner": [["QRNNs", "Method"], ["RNNs", "Method"]], "rel": [["QRNNs", "Compare-With", "RNNs"]], "rel_plus": [["QRNNs:Method", "Compare-With", "RNNs:Method"]]}
{"doc_id": "147703932", "sentence": "Specifically , we study the correlation of 2D/ 3 D pose estimation , body part segmentation and full - body depth estimation .", "ner": [["2D/ 3 D pose estimation", "Task"], ["body part segmentation", "Task"], ["full - body depth estimation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "147703932", "sentence": "Results on the newly released SURREAL dataset show that all four tasks benefit from the multi - task approach , but with different combinations of tasks : while combining all four tasks improves 2D pose estimation the most , 2D pose improves neither 3D pose nor full - body depth estimation .", "ner": [["SURREAL", "Dataset"], ["2D pose estimation", "Task"], ["2D pose", "Task"], ["3D pose", "Task"], ["full - body depth estimation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "147703932", "sentence": "On the other hand 2D parts segmentation can benefit from 2D pose but not from 3D pose .", "ner": [["2D parts segmentation", "Task"], ["2D pose", "Task"], ["3D pose", "Task"]], "rel": [["2D pose", "Used-For", "2D parts segmentation"]], "rel_plus": [["2D pose:Task", "Used-For", "2D parts segmentation:Task"]]}
{"doc_id": "147703932", "sentence": "Related tasks include 2D pose estimation [ 2 ] , [ 3 ] , [ 4 ] , [ 6 ] , [ 7 ] , [ 8 ] , body part segmentation [ 2 ] , [ 4 ] , [ 5 ] , [ 6 ] , [ 7 ] , [ 8 ] , [ 9 ] , human re - identification [ 6 ] , clothes parsing [ 3 ] , [ 4 ] , [ 9 ] , motion/optical flow [ 1 2 ] , [ 1 3 ] , depth estimation [ 1 ] , [ 6 ] , body shape model [ 1 ] , [ 8 ] , body parts shape segmentation [ 1 ] , human 3D pose estimation [ 1 0 ] , [ 1 1 ] , [ 1 4 ] , or sign language recognition [ 2 8 ] , just to name a few .", "ner": [["2D pose estimation", "Task"], ["body part segmentation", "Task"], ["human re - identification", "Task"], ["clothes parsing", "Task"], ["motion/optical flow", "Task"], ["depth estimation", "Task"], ["body shape model", "Task"], ["body parts shape segmentation", "Task"], ["human 3D pose estimation", "Task"], ["sign language recognition", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "147703932", "sentence": "As it is common nowadays in most computer vision problems , deep learning , and particularly Convolutional Neural Networks ( CNN ) , is the predominant methodology used by state of the art approaches .", "ner": [["computer vision", "Task"], ["deep learning", "Method"], ["Convolutional Neural Networks", "Method"], ["CNN", "Method"]], "rel": [["Convolutional Neural Networks", "Used-For", "computer vision"], ["deep learning", "Used-For", "computer vision"], ["Convolutional Neural Networks", "SubClass-Of", "deep learning"], ["CNN", "Synonym-Of", "Convolutional Neural Networks"]], "rel_plus": [["Convolutional Neural Networks:Method", "Used-For", "computer vision:Task"], ["deep learning:Method", "Used-For", "computer vision:Task"], ["Convolutional Neural Networks:Method", "SubClass-Of", "deep learning:Method"], ["CNN:Method", "Synonym-Of", "Convolutional Neural Networks:Method"]]}
{"doc_id": "147703932", "sentence": "However , other related tasks such as 3D pose , pixellevel segmentation , and human body depth estimation from RGB images still require further improvement in order to be accurately applied to real world scenarios .", "ner": [["3D pose", "Task"], ["pixellevel segmentation", "Task"], ["human body depth estimation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "147703932", "sentence": "In this work we focus on multi - task learning of 2D pose , 3D pose , human body depth map , and body part segmentation from still images , which are common input cues for several human analysis tasks .", "ner": [["multi - task learning", "Method"], ["2D pose", "Task"], ["3D pose", "Task"], ["human body depth map", "Task"], ["body part segmentation", "Task"]], "rel": [["multi - task learning", "Used-For", "2D pose"], ["multi - task learning", "Used-For", "3D pose"], ["multi - task learning", "Used-For", "human body depth map"], ["multi - task learning", "Used-For", "body part segmentation"]], "rel_plus": [["multi - task learning:Method", "Used-For", "2D pose:Task"], ["multi - task learning:Method", "Used-For", "3D pose:Task"], ["multi - task learning:Method", "Used-For", "human body depth map:Task"], ["multi - task learning:Method", "Used-For", "body part segmentation:Task"]]}
{"doc_id": "147703932", "sentence": "We show some pairs of tasks do not help each other ( e.g. 3D pose and body part segmentation ) , while others do so significantly ( e.g. 2D pose and depth ) .", "ner": [["3D pose", "Task"], ["body part segmentation", "Task"], ["2D pose", "Task"], ["depth", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "147703932", "sentence": "Pyramid image decomposition is used as input to deal with semantic/boundary/object detection , normal estimation saliency/normal estimation , semantic/human part segmentation , semantic boundary detection , and region proposal generation .", "ner": [["Pyramid image decomposition", "Method"], ["semantic/boundary/object detection", "Task"], ["normal estimation saliency/normal estimation", "Task"], ["semantic/human part segmentation", "Task"], ["semantic boundary detection", "Task"], ["region proposal generation", "Task"]], "rel": [["Pyramid image decomposition", "Used-For", "semantic/boundary/object detection"], ["Pyramid image decomposition", "Used-For", "normal estimation saliency/normal estimation"], ["Pyramid image decomposition", "Used-For", "semantic/human part segmentation"], ["Pyramid image decomposition", "Used-For", "semantic boundary detection"], ["Pyramid image decomposition", "Used-For", "region proposal generation"]], "rel_plus": [["Pyramid image decomposition:Method", "Used-For", "semantic/boundary/object detection:Task"], ["Pyramid image decomposition:Method", "Used-For", "normal estimation saliency/normal estimation:Task"], ["Pyramid image decomposition:Method", "Used-For", "semantic/human part segmentation:Task"], ["Pyramid image decomposition:Method", "Used-For", "semantic boundary detection:Task"], ["Pyramid image decomposition:Method", "Used-For", "region proposal generation:Task"]]}
{"doc_id": "147703932", "sentence": "Other works [ 1 7 ] , [ 2 1 ] , [ 2 2 ] , [ 2 3 ] add additional tasks such as instance segmentation , multi - human parsing , and mask segmentation .", "ner": [["instance segmentation", "Task"], ["multi - human parsing", "Task"], ["mask segmentation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "147703932", "sentence": "As an example , [ 1 7 ] tackles instance segmentation , object detection and mask segmentation in a stacked fashion .", "ner": [["tackles instance segmentation", "Task"], ["object detection", "Task"], ["mask segmentation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "147703932", "sentence": "Authors in [ 2 0 ] build a twostage FCN process that first detects human pose and then performs body parts parsing through a Conditional Random Field .", "ner": [["FCN", "Method"], ["Conditional Random Field", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "147703932", "sentence": "The work of [ 8 ] uses Mask - RCNN [ 1 8 ] in a multi - task cascade fashion , connecting several intermediate layers for pose estimation and body parts parsing , while in [ 1 9 ] Mask R - CNN tackles instance/mask segmentation and object/keypoint detection problems .", "ner": [["Mask - RCNN", "Method"], ["pose estimation", "Task"], ["body parts parsing", "Task"], ["Mask R - CNN", "Method"], ["instance/mask segmentation", "Task"], ["object/keypoint detection", "Task"]], "rel": [["Mask - RCNN", "Used-For", "pose estimation"], ["Mask - RCNN", "Used-For", "body parts parsing"], ["Mask R - CNN", "Used-For", "instance/mask segmentation"], ["Mask R - CNN", "Used-For", "object/keypoint detection"]], "rel_plus": [["Mask - RCNN:Method", "Used-For", "pose estimation:Task"], ["Mask - RCNN:Method", "Used-For", "body parts parsing:Task"], ["Mask R - CNN:Method", "Used-For", "instance/mask segmentation:Task"], ["Mask R - CNN:Method", "Used-For", "object/keypoint detection:Task"]]}
{"doc_id": "147703932", "sentence": "The work of [ 2 1 ] makes use of adversarial networks in a nested way , i.e. , GAN outputs are used as the input to other GANs to deal with pose estimation and body parts parsing .", "ner": [["GAN", "Method"], ["GANs", "Method"], ["pose estimation", "Task"], ["body parts parsing", "Task"]], "rel": [["GANs", "Used-For", "pose estimation"], ["GANs", "Used-For", "body parts parsing"]], "rel_plus": [["GANs:Method", "Used-For", "pose estimation:Task"], ["GANs:Method", "Used-For", "body parts parsing:Task"]]}
{"doc_id": "147703932", "sentence": "Another common combination of tasks is 2D/ 3 D pose and body/clothes parsing [ 1 1 ] on datasets such as Pascal [ 5 ] or COCO [ 7 ] .", "ner": [["2D/ 3 D pose", "Task"], ["body/clothes parsing", "Task"], ["Pascal", "Dataset"], ["COCO", "Dataset"]], "rel": [["Pascal", "Benchmark-For", "2D/ 3 D pose"], ["COCO", "Benchmark-For", "2D/ 3 D pose"], ["Pascal", "Benchmark-For", "body/clothes parsing"], ["COCO", "Benchmark-For", "body/clothes parsing"]], "rel_plus": [["Pascal:Dataset", "Benchmark-For", "2D/ 3 D pose:Task"], ["COCO:Dataset", "Benchmark-For", "2D/ 3 D pose:Task"], ["Pascal:Dataset", "Benchmark-For", "body/clothes parsing:Task"], ["COCO:Dataset", "Benchmark-For", "body/clothes parsing:Task"]]}
{"doc_id": "147703932", "sentence": "The work of [ 1 5 ] uses two encoders ( 2D pose and clothes parsing ) with a module as a middle stream that acts as a parameter adapting to merge the features of both tasks and perform classification separately .", "ner": [["2D pose", "Task"], ["clothes parsing", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "147703932", "sentence": "These are used by two CNNs performing 2D pose estimation and clothes parsing , respectively .", "ner": [["CNNs", "Method"], ["2D pose estimation", "Task"], ["clothes parsing", "Task"]], "rel": [["CNNs", "Used-For", "2D pose estimation"], ["CNNs", "Used-For", "clothes parsing"]], "rel_plus": [["CNNs:Method", "Used-For", "2D pose estimation:Task"], ["CNNs:Method", "Used-For", "clothes parsing:Task"]]}
{"doc_id": "147703932", "sentence": "We select four common tasks in many recent works : 2D/ 3 D pose estimation , body parts segmentation and body depth estimation .", "ner": [["2D/ 3 D pose estimation", "Task"], ["body parts segmentation", "Task"], ["body depth estimation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "147703932", "sentence": "In this paper we use a tensor of size 6 4 \u00d7 6 4 \u00d7 1 6 , where # joints = 1 6 ( see Fig. 1 ( b ) ) . \u2022 Body parts segmentation : The state - of - the - art on human body segmentation advocates training fullyconvolutional networks that generate per pixel body part probabilities [ 1 8 ] , [ 2 0 ] .", "ner": [["Body parts segmentation", "Task"], ["human body segmentation", "Task"], ["fullyconvolutional networks", "Method"]], "rel": [["fullyconvolutional networks", "Used-For", "human body segmentation"]], "rel_plus": [["fullyconvolutional networks:Method", "Used-For", "human body segmentation:Task"]]}
{"doc_id": "147703932", "sentence": "We define the segmentation output as a tensor of size 6 4 \u00d7 6 4 \u00d7 1 5 where # parts = 1 4 + Background ( see Fig. 1 (c ) ) . \u2022 Full - body depth : We tackle depth estimation as described in [ 3 6 ] , i.e. instead of regressing each pixel depth as a continuous value we quantize depth into # bins = 1 9 bins resulting in a tensor of size 6 4 \u00d7 6 4 \u00d7 ( # bins + 1 ) ( see Fig. 1 (d ) ) .", "ner": [["Full - body depth", "Task"], ["depth estimation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "147703932", "sentence": "We define an extra bin for the background . \u2022 3D pose : The standard approach for 3D pose estimation is coordinates regression [ 2 2 ] .", "ner": [["3D pose", "Task"], ["3D pose estimation", "Task"], ["coordinates regression", "Method"]], "rel": [["coordinates regression", "Used-For", "3D pose estimation"], ["3D pose", "Synonym-Of", "3D pose estimation"]], "rel_plus": [["coordinates regression:Method", "Used-For", "3D pose estimation:Task"], ["3D pose:Task", "Synonym-Of", "3D pose estimation:Task"]]}
{"doc_id": "147703932", "sentence": "Each hourglass module consists of an encoderdecoder architecture with residual connections from encoder layers to corresponding decoder ones .", "ner": [["hourglass module", "Method"], ["encoderdecoder", "Method"], ["residual connections", "Method"]], "rel": [["encoderdecoder", "Part-Of", "hourglass module"], ["residual connections", "Part-Of", "encoderdecoder"]], "rel_plus": [["encoderdecoder:Method", "Part-Of", "hourglass module:Method"], ["residual connections:Method", "Part-Of", "encoderdecoder:Method"]]}
{"doc_id": "147703932", "sentence": "Regarding parameter estimation , a root - mean - square - error ( RMSE ) loss is used for 2D ( L 2Dpose ) and 3D ( L 3Dpose ) pose estimation , while cross - entropy ( CE ) across the spatial dimension of the heatmaps is used for depth estimation ( L Depth ) and body part segmentation ( L BodyPart ) .", "ner": [["root - mean - square - error", "Method"], ["RMSE", "Method"], ["L 2Dpose", "Task"], ["L 3Dpose", "Task"], ["pose estimation", "Task"], ["cross - entropy", "Method"], ["CE", "Method"], ["depth estimation", "Task"], ["L Depth", "Task"], ["body part segmentation", "Task"], ["L BodyPart", "Task"]], "rel": [["RMSE", "Synonym-Of", "root - mean - square - error"], ["root - mean - square - error", "Used-For", "L 2Dpose"], ["root - mean - square - error", "Used-For", "L 3Dpose"], ["root - mean - square - error", "Used-For", "pose estimation"], ["CE", "Synonym-Of", "cross - entropy"], ["L Depth", "Synonym-Of", "depth estimation"], ["cross - entropy", "Used-For", "depth estimation"], ["L BodyPart", "Synonym-Of", "body part segmentation"], ["cross - entropy", "Used-For", "body part segmentation"]], "rel_plus": [["RMSE:Method", "Synonym-Of", "root - mean - square - error:Method"], ["root - mean - square - error:Method", "Used-For", "L 2Dpose:Task"], ["root - mean - square - error:Method", "Used-For", "L 3Dpose:Task"], ["root - mean - square - error:Method", "Used-For", "pose estimation:Task"], ["CE:Method", "Synonym-Of", "cross - entropy:Method"], ["L Depth:Task", "Synonym-Of", "depth estimation:Task"], ["cross - entropy:Method", "Used-For", "depth estimation:Task"], ["L BodyPart:Task", "Synonym-Of", "body part segmentation:Task"], ["cross - entropy:Method", "Used-For", "body part segmentation:Task"]]}
{"doc_id": "147703932", "sentence": "We train all models for 3 0 epochs using 2 Stacks of Hourglass , with a batch size of 5 and the RMSprop optimizer with learning rate 1e \u2212 3 .", "ner": [["Hourglass", "Method"], ["RMSprop", "Method"]], "rel": [["RMSprop", "Part-Of", "Hourglass"]], "rel_plus": [["RMSprop:Method", "Part-Of", "Hourglass:Method"]]}
{"doc_id": "147703932", "sentence": "In order to evaluate each modality , we make use of standard metrics : Intersection over Union ( IOU ) for body part segmentation , Percentage of Correct Keypoints thresholded at 5 0 % of the head length ( PCKh ) [ 2 ] for 2D pose estimation , root - mean - square - error ( RMSE ) for full body depth estimation and mean joint distance MJD in millimeters ( mm ) for 3D pose estimation .", "ner": [["body part segmentation", "Task"], ["2D pose estimation", "Task"], ["root - mean - square - error", "Method"], ["RMSE", "Method"], ["body depth estimation", "Task"], ["mean joint distance", "Method"], ["MJD", "Method"], ["3D pose estimation", "Task"]], "rel": [["RMSE", "Synonym-Of", "root - mean - square - error"], ["root - mean - square - error", "Used-For", "body depth estimation"], ["MJD", "Synonym-Of", "mean joint distance"], ["mean joint distance", "Used-For", "3D pose estimation"]], "rel_plus": [["RMSE:Method", "Synonym-Of", "root - mean - square - error:Method"], ["root - mean - square - error:Method", "Used-For", "body depth estimation:Task"], ["MJD:Method", "Synonym-Of", "mean joint distance:Method"], ["mean joint distance:Method", "Used-For", "3D pose estimation:Task"]]}
{"doc_id": "147703932", "sentence": "Here we evaluate the models trained on specific tasks , which will serve as baselines to multi - task comparison . 1 ) Body part segmentation : The first column in Table I shows the single - task segmentation results , with an average IOU 6 7 . 4 8 % .", "ner": [["Body part segmentation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "147703932", "sentence": "This low accuracy in some parts ( feet , hands ) is due to these spanning just a few pixels , and regions of difficult interpretation , such as complex self - occlusions . 2 ) 2D pose estimation : Regarding 2D pose estimation , the single - task 2D pose model already obtained an outstanding accuracy of 9 6 . 5 0 % PCKh , as shown in Table III .", "ner": [["2D pose estimation", "Task"], ["2D pose estimation", "Task"], ["2D pose", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "147703932", "sentence": "In this section we evaluate how multi - task models help improve the accuracy of each individual task . 1 ) Body Part Segmentation : As shown in Table I , the tasks contributing the most to body part segmentation are 2D pose and depth estimation .", "ner": [["Body Part Segmentation", "Task"], ["body part segmentation", "Task"], ["2D pose and depth estimation", "Task"]], "rel": [["2D pose and depth estimation", "Used-For", "body part segmentation"]], "rel_plus": [["2D pose and depth estimation:Task", "Used-For", "body part segmentation:Task"]]}
{"doc_id": "147703932", "sentence": "Possible reasons are : 2D pose estimation may help to disambiguate pixel labels in the segmentation task by providing rough estimates of the body part locations ; and depth estimation can help mitigating effects such as foreshortening , crowding and occlusion .", "ner": [["2D pose estimation", "Task"], ["segmentation", "Task"], ["depth estimation", "Task"]], "rel": [["2D pose estimation", "Used-For", "segmentation"]], "rel_plus": [["2D pose estimation:Task", "Used-For", "segmentation:Task"]]}
{"doc_id": "147703932", "sentence": "Separately , both 2D pose and depth estimation improve the segmentation results relative to both IOU and pixel error .", "ner": [["2D pose and depth estimation", "Task"], ["segmentation", "Task"]], "rel": [["2D pose and depth estimation", "Used-For", "segmentation"]], "rel_plus": [["2D pose and depth estimation:Task", "Used-For", "segmentation:Task"]]}
{"doc_id": "147703932", "sentence": "Table I also shows that 3D body pose estimation is a poor complement for the segmentation task in terms of IOU .", "ner": [["3D body pose estimation", "Task"], ["segmentation", "Task"]], "rel": [["3D body pose estimation", "Used-For", "segmentation"]], "rel_plus": [["3D body pose estimation:Task", "Used-For", "segmentation:Task"]]}
{"doc_id": "147703932", "sentence": "This hypothesis is reinforced by the results of performing 2D+ 3 D pose estimation along with body part segmentation .", "ner": [["2D+ 3 D pose estimation", "Task"], ["body part segmentation", "Task"]], "rel": [["body part segmentation", "Used-For", "2D+ 3 D pose estimation"]], "rel_plus": [["body part segmentation:Task", "Used-For", "2D+ 3 D pose estimation:Task"]]}
{"doc_id": "147703932", "sentence": "While 2D pose estimation does help the segmentation task , further adding 3D pose estimation results in worse accuracy than performing body part segmentation alone .", "ner": [["2D pose estimation", "Task"], ["segmentation", "Task"], ["3D pose estimation", "Task"], ["body part segmentation", "Task"]], "rel": [["2D pose estimation", "Used-For", "segmentation"]], "rel_plus": [["2D pose estimation:Task", "Used-For", "segmentation:Task"]]}
{"doc_id": "147703932", "sentence": "The same effect happens with depth estimation and 3D pose .", "ner": [["depth estimation", "Task"], ["3D pose", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "147703932", "sentence": "While depth estimation improves the overall segmentation accuracy , further performing 3D pose recovery results in worse accuracy .", "ner": [["depth estimation", "Task"], ["segmentation", "Task"], ["3D pose", "Task"]], "rel": [["depth estimation", "Used-For", "segmentation"], ["3D pose", "Used-For", "segmentation"]], "rel_plus": [["depth estimation:Task", "Used-For", "segmentation:Task"], ["3D pose:Task", "Used-For", "segmentation:Task"]]}
{"doc_id": "147703932", "sentence": "Looking at body parts results , one can see that performing 2D pose recovery along with body part segmentation improves IOU for torso , arms and legs .", "ner": [["2D pose recovery", "Task"], ["body part segmentation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "147703932", "sentence": "While adding 3D body pose recovery to the pipeline worsens the overall results of the best model , it does improve the segmentation accuracy of those parts it has been shown to improve on its own such as arms and hands .", "ner": [["3D body pose recovery", "Task"], ["segmentation", "Task"]], "rel": [["3D body pose recovery", "Used-For", "segmentation"]], "rel_plus": [["3D body pose recovery:Task", "Used-For", "segmentation:Task"]]}
{"doc_id": "147703932", "sentence": "Overall , we can say that the cues of 2D pose and depth estimation help to improve the segmentation accuracy .", "ner": [["2D pose and depth estimation", "Task"], ["segmentation", "Task"]], "rel": [["2D pose and depth estimation", "Used-For", "segmentation"]], "rel_plus": [["2D pose and depth estimation:Task", "Used-For", "segmentation:Task"]]}
{"doc_id": "147703932", "sentence": "The best overall model is found by performing 2D pose and depth estimation along with segmentation . 2 ) 2D pose estimation : The results in Table III show the performance of the different multi - task models on 2D human pose estimation .", "ner": [["2D pose and depth estimation", "Task"], ["segmentation", "Task"], ["2D pose estimation", "Task"], ["2D human pose estimation", "Task"]], "rel": [["2D human pose estimation", "Synonym-Of", "2D pose estimation"]], "rel_plus": [["2D human pose estimation:Task", "Synonym-Of", "2D pose estimation:Task"]]}
{"doc_id": "147703932", "sentence": "The single task contributing the most to 2D pose recovery is segmentation , resulting in 0. 3 % increase .", "ner": [["2D pose recovery", "Task"], ["segmentation", "Task"]], "rel": [["segmentation", "Used-For", "2D pose recovery"]], "rel_plus": [["segmentation:Task", "Used-For", "2D pose recovery:Task"]]}
{"doc_id": "147703932", "sentence": "Still , depth estimation slightly improves the results , likely due to it providing an outline of the overall body , along with depth cues of said outline , helping to disambiguate the location of the parts . 3D pose estimation , on the other hand , provides little complementary information about the landmarks location relative to the camera plane , if any at all .", "ner": [["depth estimation", "Task"], ["3D pose estimation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "147703932", "sentence": "If we look at individual joints , combining 2D pose , segmentation and depth improves on ankles and knees .", "ner": [["2D pose", "Task"], ["segmentation", "Task"], ["depth", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "147703932", "sentence": "Combining 2D/ 3 D pose , segmentation and depth improves on the upper body and upper legs at the expense of losing precision on the other joints .", "ner": [["2D/ 3 D pose", "Task"], ["segmentation", "Task"], ["depth", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "147703932", "sentence": "By analyzing the other task combinations , we see that segmentation helps the most , followed by depth estimation .", "ner": [["segmentation", "Task"], ["depth estimation", "Task"]], "rel": [["segmentation", "Compare-With", "depth estimation"]], "rel_plus": [["segmentation:Task", "Compare-With", "depth estimation:Task"]]}
{"doc_id": "147703932", "sentence": "Finally , 3D body pose estimation only helps marginally . 3 ) Full - body depth estimation : Here we evaluate the error on depth estimation for a collection of multi - task networks .", "ner": [["3D body pose estimation", "Task"], ["Full - body depth estimation", "Task"], ["depth estimation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "147703932", "sentence": "Specifically , Table IV shows that complementing depth estimation with 3D pose estimation and body part segmentation results in the best results : while the single - task model obtains a mean 4. 3 9 RMSE calculated directly from the full - body depth prediction , the multi - task model goes down to a RMSE of 4. 0 4 , an 8% error reduction .", "ner": [["depth estimation", "Task"], ["3D pose estimation", "Task"], ["body part segmentation", "Task"], ["RMSE", "Method"], ["RMSE", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "147703932", "sentence": "Looking at tasks individually , segmentation contributes the most , with 3D pose estimation following closely .", "ner": [["segmentation", "Task"], ["3D pose estimation", "Task"]], "rel": [["segmentation", "Compare-With", "3D pose estimation"]], "rel_plus": [["segmentation:Task", "Compare-With", "3D pose estimation:Task"]]}
{"doc_id": "147703932", "sentence": "Segmentation may help depth estimation by providing richer semantic information on the body parts being segmented , allowing for a better model of the possible depth variability .", "ner": [["Segmentation", "Task"], ["depth estimation", "Task"]], "rel": [["Segmentation", "Used-For", "depth estimation"]], "rel_plus": [["Segmentation:Task", "Used-For", "depth estimation:Task"]]}
{"doc_id": "147703932", "sentence": "We see this in higher order combinations : combining the successful tasks ( segmentation and 3D pose estimation in addition to depth estimation ) results in the best results .", "ner": [["segmentation", "Task"], ["3D pose estimation", "Task"], ["depth estimation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "147703932", "sentence": "This is due to the contribution of segmentation to better localize the parts layout and the 3D pose information Fig. 3 .", "ner": [["segmentation", "Task"], ["3D pose", "Task"]], "rel": [["segmentation", "Used-For", "3D pose"]], "rel_plus": [["segmentation:Task", "Used-For", "3D pose:Task"]]}
{"doc_id": "147703932", "sentence": "The best overall results are obtained by considering the segmentation and depth estimation tasks along with 3D pose recovery , reducing the prediction error by 5% ( from 6 0 . 1 3 mm to 5 6 . 9 9 mm ) .", "ner": [["segmentation", "Task"], ["depth estimation", "Task"], ["3D pose recovery", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "147703932", "sentence": "It is interesting to see that , similarly to 2D pose recovery , where 3D pose did not help improve the predictions , now it is 3D pose that does not help .", "ner": [["2D pose recovery", "Task"], ["3D pose", "Task"], ["3D pose", "Task"]], "rel": [["2D pose recovery", "Compare-With", "3D pose"]], "rel_plus": [["2D pose recovery:Task", "Compare-With", "3D pose:Task"]]}
{"doc_id": "147703932", "sentence": "One can consider 2D pose recovery as a subtask of the 3D case , and thus the features used in 3D pose recovery already include those provided by the 2D case .", "ner": [["2D pose recovery", "Task"], ["3D case", "Task"], ["3D pose recovery", "Task"], ["2D case", "Task"]], "rel": [["2D pose recovery", "SubTask-Of", "3D case"]], "rel_plus": [["2D pose recovery:Task", "SubTask-Of", "3D case:Task"]]}
{"doc_id": "147703932", "sentence": "In this case , the single task contributing the most to 3D pose recovery is segmentation , followed by depth estimation .", "ner": [["3D pose recovery", "Task"], ["segmentation", "Task"], ["depth estimation", "Task"]], "rel": [["segmentation", "Used-For", "3D pose recovery"]], "rel_plus": [["segmentation:Task", "Used-For", "3D pose recovery:Task"]]}
{"doc_id": "147703932", "sentence": "Further combining both segmentation and depth estimation , as mentioned , obtains the best results , but not if we further consider 2D pose recovery .", "ner": [["segmentation", "Task"], ["depth estimation", "Task"], ["2D pose recovery", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "147703932", "sentence": "While in the previous section further adding 3D pose recovery to the 2D task did result in marginal benefits , in this case there is no further information provided : 2D landmark localization is a problem already tackled when performing the same task in the 3D space .", "ner": [["3D pose recovery", "Task"], ["2D", "Task"], ["2D landmark localization", "Task"]], "rel": [["3D pose recovery", "Used-For", "2D"]], "rel_plus": [["3D pose recovery:Task", "Used-For", "2D:Task"]]}
{"doc_id": "147703932", "sentence": "If we inspect the results by body joint , we find the best combination of tasks for most joints includes segmentation and depth to the 3D pose .", "ner": [["segmentation", "Task"], ["depth", "Task"], ["3D pose", "Task"]], "rel": [["segmentation", "Used-For", "3D pose"], ["depth", "Used-For", "3D pose"]], "rel_plus": [["segmentation:Task", "Used-For", "3D pose:Task"], ["depth:Task", "Used-For", "3D pose:Task"]]}
{"doc_id": "147703932", "sentence": "We also show the trend for one of the parts that multi - task approach better improves , specifically left wrist for 3D pose , Right ankle for 2D pose , left hand for depth map and left foot for part segmentation .", "ner": [["3D pose", "Task"], ["2D pose", "Task"], ["segmentation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "147703932", "sentence": "As one can see in Fig. 3(c ) , full - body depth estimation benefits the most from multi - task learning , while 2D pose in Fig. 3(b ) is the most accurate modality .", "ner": [["full - body depth estimation", "Task"], ["multi - task learning", "Method"], ["2D pose", "Task"]], "rel": [["multi - task learning", "Used-For", "full - body depth estimation"]], "rel_plus": [["multi - task learning:Method", "Used-For", "full - body depth estimation:Task"]]}
{"doc_id": "147703932", "sentence": "Similar to ours , they use SH modules to compute 2D/ 3 D pose estimation and part segmentation .", "ner": [["SH modules", "Method"], ["2D/ 3 D pose estimation", "Task"], ["part segmentation", "Task"]], "rel": [["SH modules", "Used-For", "2D/ 3 D pose estimation"], ["SH modules", "Used-For", "part segmentation"]], "rel_plus": [["SH modules:Method", "Used-For", "2D/ 3 D pose estimation:Task"], ["SH modules:Method", "Used-For", "part segmentation:Task"]]}
{"doc_id": "147703932", "sentence": "Differently from us , 2D pose and body part segmentation are independent streams feeding information to the 3D pose stream .", "ner": [["2D pose and body part segmentation", "Task"], ["3D pose", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "147703932", "sentence": "Our multi - task network improves independent 3D pose by more than 3 mm while this improvement is 5. 3 mm for [ 1 6 ] .", "ner": [["multi - task network", "Method"], ["3D pose", "Task"]], "rel": [["multi - task network", "Used-For", "3D pose"]], "rel_plus": [["multi - task network:Method", "Used-For", "3D pose:Task"]]}
{"doc_id": "147703932", "sentence": "We have seen that at the 2D level cues from depth estimation are highly useful for both body parts segmentation and human pose recovery , while 3D pose estimation contributes marginally to final performance .", "ner": [["depth estimation", "Task"], ["body parts segmentation", "Task"], ["human pose recovery", "Task"], ["3D pose estimation", "Task"]], "rel": [["depth estimation", "Used-For", "body parts segmentation"], ["3D pose estimation", "Used-For", "body parts segmentation"], ["depth estimation", "Used-For", "human pose recovery"], ["3D pose estimation", "Used-For", "human pose recovery"], ["depth estimation", "Compare-With", "3D pose estimation"]], "rel_plus": [["depth estimation:Task", "Used-For", "body parts segmentation:Task"], ["3D pose estimation:Task", "Used-For", "body parts segmentation:Task"], ["depth estimation:Task", "Used-For", "human pose recovery:Task"], ["3D pose estimation:Task", "Used-For", "human pose recovery:Task"], ["depth estimation:Task", "Compare-With", "3D pose estimation:Task"]]}
{"doc_id": "147703932", "sentence": "At the same time , body part segmentation and 2D pose estimation mutually benefit each other .", "ner": [["body part segmentation", "Task"], ["2D pose estimation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "147703932", "sentence": "Regarding body part segmentation , features from depth estimation improve the results the most , followed by 2D pose .", "ner": [["body part segmentation", "Task"], ["depth estimation", "Task"], ["2D pose", "Task"]], "rel": [["depth estimation", "Used-For", "body part segmentation"], ["2D pose", "Used-For", "body part segmentation"]], "rel_plus": [["depth estimation:Task", "Used-For", "body part segmentation:Task"], ["2D pose:Task", "Used-For", "body part segmentation:Task"]]}
{"doc_id": "147703932", "sentence": "In contrast , at the 3D level , depth estimation and human pose recovery benefit from segmentation , similarly to the two 2D tasks .", "ner": [["depth estimation", "Task"], ["human pose recovery", "Task"], ["segmentation", "Task"]], "rel": [["segmentation", "Used-For", "depth estimation"], ["segmentation", "Used-For", "human pose recovery"]], "rel_plus": [["segmentation:Task", "Used-For", "depth estimation:Task"], ["segmentation:Task", "Used-For", "human pose recovery:Task"]]}
{"doc_id": "147703932", "sentence": "In contrast , 2D pose cues are the least relevant , since we can interpret the task as a subtask of 3D pose recovery .", "ner": [["2D pose", "Task"], ["3D pose recovery", "Task"]], "rel": [["2D pose", "SubTask-Of", "3D pose recovery"]], "rel_plus": [["2D pose:Task", "SubTask-Of", "3D pose recovery:Task"]]}
{"doc_id": "147703932", "sentence": "Both tasks use the same model to get the lowest error , that is , depth + segmentation + 3D pose .", "ner": [["depth", "Task"], ["segmentation", "Task"], ["3D pose", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "147703932", "sentence": "We argue this is due to segmentation enriching the representation with semantic cues , and the extra depth information either providing a more restrictive deformation model ( 3D pose estimation ) or a more dense depth representation ( body depth estimation ) .", "ner": [["segmentation", "Task"], ["3D pose estimation", "Task"], ["body depth estimation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "147703932", "sentence": "In this work we analyze the contribution of multi - tasking on four common body pose analysis problems : 2D/ 3 D body pose recovery , full - body depth estimation and body parts segmentation .", "ner": [["body pose analysis", "Task"], ["2D/ 3 D body pose recovery", "Task"], ["full - body depth estimation", "Task"], ["body parts segmentation", "Task"]], "rel": [["2D/ 3 D body pose recovery", "SubTask-Of", "body pose analysis"], ["full - body depth estimation", "SubTask-Of", "body pose analysis"], ["body parts segmentation", "SubTask-Of", "body pose analysis"]], "rel_plus": [["2D/ 3 D body pose recovery:Task", "SubTask-Of", "body pose analysis:Task"], ["full - body depth estimation:Task", "SubTask-Of", "body pose analysis:Task"], ["body parts segmentation:Task", "SubTask-Of", "body pose analysis:Task"]]}
{"doc_id": "147703932", "sentence": "Depth estimation and body part segmentation help each other , while 2D/ 3 D body pose estimation benefit mainly from body part segmentation , followed by depth estimation .", "ner": [["Depth estimation", "Task"], ["body part segmentation", "Task"], ["2D/ 3 D body pose estimation", "Task"], ["body part segmentation", "Task"], ["depth estimation", "Task"]], "rel": [["body part segmentation", "Used-For", "Depth estimation"], ["Depth estimation", "Used-For", "body part segmentation"], ["body part segmentation", "Used-For", "2D/ 3 D body pose estimation"]], "rel_plus": [["body part segmentation:Task", "Used-For", "Depth estimation:Task"], ["Depth estimation:Task", "Used-For", "body part segmentation:Task"], ["body part segmentation:Task", "Used-For", "2D/ 3 D body pose estimation:Task"]]}
{"doc_id": "147703932", "sentence": "These tasks provide complementary features : depth information helps disambiguate body parts , while body part segmentation provides more robust features for locating joints during body pose estimation .", "ner": [["body part segmentation", "Task"], ["body pose estimation", "Task"]], "rel": [["body part segmentation", "Used-For", "body pose estimation"]], "rel_plus": [["body part segmentation:Task", "Used-For", "body pose estimation:Task"]]}
{"doc_id": "147703932", "sentence": "Also , 3D pose estimation helps depth estimation , likely by reducing ambiguity : 3D pose estimation helps restrict the space of possible body poses .", "ner": [["3D pose estimation", "Task"], ["depth estimation", "Task"], ["3D pose estimation", "Task"]], "rel": [["3D pose estimation", "Used-For", "depth estimation"]], "rel_plus": [["3D pose estimation:Task", "Used-For", "depth estimation:Task"]]}
{"doc_id": "211004033", "sentence": "We introduce the first CNN - based ellipse detector , called Ellipse R - CNN , to represent and infer occluded objects as ellipses .", "ner": [["CNN - based ellipse detector", "Method"], ["Ellipse R - CNN", "Method"]], "rel": [["Ellipse R - CNN", "SubClass-Of", "CNN - based ellipse detector"]], "rel_plus": [["Ellipse R - CNN:Method", "SubClass-Of", "CNN - based ellipse detector:Method"]]}
{"doc_id": "211004033", "sentence": "We first propose a robust and compact ellipse regression based on the Mask R - CNN architecture for elliptical object detection .", "ner": [["ellipse regression", "Task"], ["Mask R - CNN", "Method"], ["elliptical object detection", "Task"]], "rel": [["Mask R - CNN", "Used-For", "ellipse regression"], ["Mask R - CNN", "Used-For", "elliptical object detection"], ["ellipse regression", "Used-For", "elliptical object detection"]], "rel_plus": [["Mask R - CNN:Method", "Used-For", "ellipse regression:Task"], ["Mask R - CNN:Method", "Used-For", "elliptical object detection:Task"], ["ellipse regression:Task", "Used-For", "elliptical object detection:Task"]]}
{"doc_id": "211004033", "sentence": "For better occlusion handling , we exploit refined feature regions for the regression stage , and integrate the U - Net structure for learning different occlusion patterns to compute the final detection score .", "ner": [["occlusion handling", "Task"], ["U - Net", "Method"]], "rel": [["U - Net", "Used-For", "occlusion handling"]], "rel_plus": [["U - Net:Method", "Used-For", "occlusion handling:Task"]]}
{"doc_id": "211004033", "sentence": "We introduce the first CNN - based ellipse detector , called Ellipse R - CNN , to represent and infer occluded objects as ellipses .", "ner": [["CNN - based ellipse detector", "Method"], ["Ellipse R - CNN", "Method"]], "rel": [["Ellipse R - CNN", "SubClass-Of", "CNN - based ellipse detector"]], "rel_plus": [["Ellipse R - CNN:Method", "SubClass-Of", "CNN - based ellipse detector:Method"]]}
{"doc_id": "211004033", "sentence": "We first propose a robust and compact ellipse regression based on the Mask R - CNN architecture for elliptical object detection .", "ner": [["ellipse regression", "Task"], ["Mask R - CNN", "Method"], ["elliptical object detection", "Task"]], "rel": [["Mask R - CNN", "Used-For", "ellipse regression"], ["Mask R - CNN", "Used-For", "elliptical object detection"], ["ellipse regression", "Used-For", "elliptical object detection"]], "rel_plus": [["Mask R - CNN:Method", "Used-For", "ellipse regression:Task"], ["Mask R - CNN:Method", "Used-For", "elliptical object detection:Task"], ["ellipse regression:Task", "Used-For", "elliptical object detection:Task"]]}
{"doc_id": "211004033", "sentence": "For better occlusion handling , we exploit refined feature regions for the regression stage , and integrate the U - Net structure for learning different occlusion patterns to compute the final detection score .", "ner": [["occlusion handling", "Task"], ["U - Net", "Method"]], "rel": [["U - Net", "Used-For", "occlusion handling"]], "rel_plus": [["U - Net:Method", "Used-For", "occlusion handling:Task"]]}
{"doc_id": "211004033", "sentence": "Index Terms - Ellipse regression , occlusion handling , 3D object localization , object detection , convolutional neural networks .", "ner": [["Index Terms - Ellipse regression", "Task"], ["occlusion handling", "Task"], ["3D object localization", "Task"], ["object detection", "Task"], ["convolutional neural networks", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "211004033", "sentence": "D ETECTION of ellipse - like shapes [ 1 ] has been widely used in various image processing tasks , for instance , face detection [ 2 ] and medical imaging diagnosis [ 3 ] .", "ner": [["D ETECTION", "Method"], ["image processing tasks", "Task"], ["face detection", "Task"], ["medical imaging diagnosis", "Task"]], "rel": [["face detection", "SubTask-Of", "image processing tasks"], ["medical imaging diagnosis", "SubTask-Of", "image processing tasks"], ["D ETECTION", "Used-For", "image processing tasks"], ["D ETECTION", "Used-For", "face detection"], ["D ETECTION", "Used-For", "medical imaging diagnosis"]], "rel_plus": [["face detection:Task", "SubTask-Of", "image processing tasks:Task"], ["medical imaging diagnosis:Task", "SubTask-Of", "image processing tasks:Task"], ["D ETECTION:Method", "Used-For", "image processing tasks:Task"], ["D ETECTION:Method", "Used-For", "face detection:Task"], ["D ETECTION:Method", "Used-For", "medical imaging diagnosis:Task"]]}
{"doc_id": "211004033", "sentence": "Adapting convolutional neural networks ( CNNs ) for object detection [ 6 ] and instance segmentation ( e.g. , Mask R - CNN [ 7 ] ) to this canonical task is a promising way to extract object information .", "ner": [["convolutional neural networks", "Method"], ["CNNs", "Method"], ["object detection", "Task"], ["instance segmentation", "Task"], ["Mask R - CNN", "Method"]], "rel": [["CNNs", "Synonym-Of", "convolutional neural networks"], ["Mask R - CNN", "SubClass-Of", "convolutional neural networks"], ["convolutional neural networks", "Used-For", "object detection"], ["convolutional neural networks", "Used-For", "instance segmentation"]], "rel_plus": [["CNNs:Method", "Synonym-Of", "convolutional neural networks:Method"], ["Mask R - CNN:Method", "SubClass-Of", "convolutional neural networks:Method"], ["convolutional neural networks:Method", "Used-For", "object detection:Task"], ["convolutional neural networks:Method", "Used-For", "instance segmentation:Task"]]}
{"doc_id": "211004033", "sentence": "Ellipse detection : traditional ellipse detection in 2D [ 1 ] .", "ner": [["Ellipse detection", "Task"], ["traditional ellipse detection in 2D", "Task"]], "rel": [["Ellipse detection", "Synonym-Of", "traditional ellipse detection in 2D"]], "rel_plus": [["Ellipse detection:Task", "Synonym-Of", "traditional ellipse detection in 2D:Task"]]}
{"doc_id": "211004033", "sentence": "Mask R - CNN+ : Directly fitting ellipses from the entire object masks outputed by Mask R - CNN [ 7 ] .", "ner": [["Mask R - CNN+", "Method"], ["Mask R - CNN", "Method"]], "rel": [["Mask R - CNN", "Part-Of", "Mask R - CNN+"]], "rel_plus": [["Mask R - CNN:Method", "Part-Of", "Mask R - CNN+:Method"]]}
{"doc_id": "211004033", "sentence": "Our proposed Ellipse R - CNN outputs accurate ellipses compared to the ground truth ( green colored ) . the Mask R - CNN model for predicting ellipses : ( 1 ) adding a regression model right after the mask branch ; ( 2 ) performing regression directly on the features from RoiAlign .", "ner": [["Ellipse R - CNN", "Method"], ["Mask R - CNN", "Method"], ["predicting ellipses", "Task"], ["regression", "Task"], ["RoiAlign", "Method"]], "rel": [["Mask R - CNN", "Used-For", "predicting ellipses"], ["RoiAlign", "Used-For", "regression"]], "rel_plus": [["Mask R - CNN:Method", "Used-For", "predicting ellipses:Task"], ["RoiAlign:Method", "Used-For", "regression:Task"]]}
{"doc_id": "211004033", "sentence": "Right : In the clustered and occluded case , multi - view 3D object estimation [ 9 ] from proposed ellipse detections outputs much more accurate sizes in 3D and poses in 6D than using bounding - box constraints [ 8 ] ( 3D ground - truth points are green colored ) . be able to retrieve the whole elliptical objects by focusing on their partially visible boundary information so as to handle different occluded patterns effectively .", "ner": [["multi - view 3D object estimation", "Task"], ["ellipse detections", "Task"]], "rel": [["ellipse detections", "Used-For", "multi - view 3D object estimation"]], "rel_plus": [["ellipse detections:Task", "Used-For", "multi - view 3D object estimation:Task"]]}
{"doc_id": "211004033", "sentence": "The proposed regression method is general and flexible enough to be applicable to any state - of - the - art detection model , in our case , a Mask R - CNN detector [ 7 ] .", "ner": [["detection model", "Method"], ["Mask R - CNN", "Method"]], "rel": [["Mask R - CNN", "SubClass-Of", "detection model"]], "rel_plus": [["Mask R - CNN:Method", "SubClass-Of", "detection model:Method"]]}
{"doc_id": "211004033", "sentence": "We further analyze the improvement of our ellipse regression in an ablation study using the FDDB dataset [ 1 0 ] . \u2022 For better handling occlusion , we integrate the U - Net [ 1 1 ] structure into the detection model to generate decoded feature maps that contain retrieved hidden information .", "ner": [["ellipse regression", "Task"], ["FDDB", "Dataset"], ["handling occlusion", "Task"], ["U - Net", "Method"]], "rel": [["FDDB", "Benchmark-For", "ellipse regression"], ["U - Net", "Used-For", "handling occlusion"]], "rel_plus": [["FDDB:Dataset", "Benchmark-For", "ellipse regression:Task"], ["U - Net:Method", "Used-For", "handling occlusion:Task"]]}
{"doc_id": "211004033", "sentence": "In ablation experiments , we demonstrate that our approach indeed improves the detection performance compared to the Mask R - CNN baseline and its three variants using both synthetic and real datasets of occluded and clustered objects .", "ner": [["detection", "Task"], ["Mask R - CNN", "Method"]], "rel": [["Mask R - CNN", "Used-For", "detection"]], "rel_plus": [["Mask R - CNN:Method", "Used-For", "detection:Task"]]}
{"doc_id": "211004033", "sentence": "Since we develop the Mask R - CNN model as our base object detector to predict ellipse parameters in occluded cases , we review recent work on CNN - based object detectors , 3D object localization , and occlusion handling , respectively .", "ner": [["Mask R - CNN", "Method"], ["object detector", "Method"], ["predict ellipse parameters in occluded cases", "Task"], ["CNN - based object detectors", "Method"], ["3D object localization", "Task"], ["occlusion handling", "Task"]], "rel": [["Mask R - CNN", "SubClass-Of", "object detector"], ["Mask R - CNN", "Used-For", "predict ellipse parameters in occluded cases"]], "rel_plus": [["Mask R - CNN:Method", "SubClass-Of", "object detector:Method"], ["Mask R - CNN:Method", "Used-For", "predict ellipse parameters in occluded cases:Task"]]}
{"doc_id": "211004033", "sentence": "Recent success in the general object detection tasks on Pascal [ 1 2 ] , ImageNet [ 1 3 ] , and MS COCO datasets [ 1 4 ] , have been achieved by both singleshot [ 1 5 ] , [ 1 6 ] and R - CNN [ 6 ] , [ 7 ] , [ 1 7 ] architectures .", "ner": [["object detection", "Task"], ["Pascal", "Dataset"], ["ImageNet", "Dataset"], ["MS COCO", "Dataset"], ["singleshot", "Method"], ["R - CNN", "Method"]], "rel": [["Pascal", "Benchmark-For", "object detection"], ["ImageNet", "Benchmark-For", "object detection"], ["MS COCO", "Benchmark-For", "object detection"], ["singleshot", "Used-For", "object detection"], ["R - CNN", "Used-For", "object detection"], ["singleshot", "Evaluated-With", "Pascal"], ["R - CNN", "Evaluated-With", "Pascal"], ["singleshot", "Evaluated-With", "ImageNet"], ["R - CNN", "Evaluated-With", "ImageNet"], ["singleshot", "Evaluated-With", "MS COCO"], ["R - CNN", "Evaluated-With", "MS COCO"]], "rel_plus": [["Pascal:Dataset", "Benchmark-For", "object detection:Task"], ["ImageNet:Dataset", "Benchmark-For", "object detection:Task"], ["MS COCO:Dataset", "Benchmark-For", "object detection:Task"], ["singleshot:Method", "Used-For", "object detection:Task"], ["R - CNN:Method", "Used-For", "object detection:Task"], ["singleshot:Method", "Evaluated-With", "Pascal:Dataset"], ["R - CNN:Method", "Evaluated-With", "Pascal:Dataset"], ["singleshot:Method", "Evaluated-With", "ImageNet:Dataset"], ["R - CNN:Method", "Evaluated-With", "ImageNet:Dataset"], ["singleshot:Method", "Evaluated-With", "MS COCO:Dataset"], ["R - CNN:Method", "Evaluated-With", "MS COCO:Dataset"]]}
{"doc_id": "211004033", "sentence": "The single - shot methods formulate object detection as a singlestage regression problem to predict objects extremely fast .", "ner": [["single - shot", "Method"], ["object detection", "Task"]], "rel": [["single - shot", "Used-For", "object detection"]], "rel_plus": [["single - shot:Method", "Used-For", "object detection:Task"]]}
{"doc_id": "211004033", "sentence": "The R - CNN approaches by integrating region proposal and classification , have greatly improved the accuracy , and are currently one of the best performing detection paradigms .", "ner": [["R - CNN", "Method"], ["region proposal", "Task"], ["classification", "Task"], ["detection", "Task"]], "rel": [["R - CNN", "Used-For", "region proposal"], ["R - CNN", "Used-For", "classification"], ["R - CNN", "Used-For", "detection"]], "rel_plus": [["R - CNN:Method", "Used-For", "region proposal:Task"], ["R - CNN:Method", "Used-For", "classification:Task"], ["R - CNN:Method", "Used-For", "detection:Task"]]}
{"doc_id": "211004033", "sentence": "Fruit detection , as a challenging example of elliptical object detection , recently has attracted intensive interests in machine vision and agricultural robotics [ 5 ] , [ 1 8 ] , [ 1 9 ] .", "ner": [["Fruit detection", "Task"], ["elliptical object detection", "Task"], ["machine vision", "Task"], ["agricultural robotics", "Task"]], "rel": [["Fruit detection", "SubClass-Of", "elliptical object detection"], ["Fruit detection", "SubTask-Of", "machine vision"], ["Fruit detection", "SubTask-Of", "agricultural robotics"]], "rel_plus": [["Fruit detection:Task", "SubClass-Of", "elliptical object detection:Task"], ["Fruit detection:Task", "SubTask-Of", "machine vision:Task"], ["Fruit detection:Task", "SubTask-Of", "agricultural robotics:Task"]]}
{"doc_id": "211004033", "sentence": "Although recent works [ 2 0 ] , [ 2 1 ] by tunning head layers of R - CNN models , have presented a good performance on well - separated fruits , the detection accuracy drops significantly as fruits cluster and occlude each other .", "ner": [["R - CNN", "Method"], ["detection", "Task"]], "rel": [["R - CNN", "Used-For", "detection"]], "rel_plus": [["R - CNN:Method", "Used-For", "detection:Task"]]}
{"doc_id": "211004033", "sentence": "Object Localization from 2D Detection .", "ner": [["Object Localization", "Task"], ["2D Detection", "Task"]], "rel": [["Object Localization", "SubTask-Of", "2D Detection"]], "rel_plus": [["Object Localization:Task", "SubTask-Of", "2D Detection:Task"]]}
{"doc_id": "211004033", "sentence": "While some efforts have been made for 3D fruit localization [ 2 4 ] - [ 2 7 ] by utilizing standard mapping techniques [ 2 8 ] , [ 2 9 ] , none of them could perform object size estimation because of low - resolution 3D reconstructions .", "ner": [["3D fruit localization", "Task"], ["object size estimation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "211004033", "sentence": "We thus propose ellipse representation by developing the Mask R - CNN architecture as the baseline , and compare the detection accuracy in terms of masks generated from ellipse parameters .", "ner": [["Mask R - CNN", "Method"], ["detection", "Task"]], "rel": [["Mask R - CNN", "Used-For", "detection"]], "rel_plus": [["Mask R - CNN:Method", "Used-For", "detection:Task"]]}
{"doc_id": "211004033", "sentence": "One of the most common applications that apply occlusion handling strategy is pedestrian detection .", "ner": [["occlusion handling", "Task"], ["pedestrian detection", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "211004033", "sentence": "Most single - stage and R - CNN based networks formulate the object detection as a regression problem , which outputs a class - specific bounding box for each prediction [ 6 ] , [ 7 ] , [ 1 5 ] - [ 1 7 ] , [ 3 4 ] .", "ner": [["R - CNN", "Method"], ["object detection", "Task"]], "rel": [["R - CNN", "Used-For", "object detection"]], "rel_plus": [["R - CNN:Method", "Used-For", "object detection:Task"]]}
{"doc_id": "211004033", "sentence": "In our case ( see Fig. 3 ) , the input to the R - CNN models is a set of N training pairs { ( i P , i G ) } i= 1 , ... ,N , where i P = ( i P x , i P y , i P w , i P h ) denotes the pixel coordinates of the center of a region proposal i P together with its weight and height in pixels .", "ner": [["R - CNN", "Method"], ["region proposal", "Task"]], "rel": [["R - CNN", "Used-For", "region proposal"]], "rel_plus": [["R - CNN:Method", "Used-For", "region proposal:Task"]]}
{"doc_id": "211004033", "sentence": "The ground - truth ( GT ) bounding box i G corresponding to i P is defined in the same way For example , in Mask R - CNN , proposals are generated by a region proposal network ( RPN ) [ 6 ] after the input image going through the base net ( i.e. , ResNet - 1 0 1 [ 3 5 ] ) .", "ner": [["Mask R - CNN", "Method"], ["region proposal network", "Method"], ["RPN", "Method"], ["ResNet - 1 0 1", "Method"]], "rel": [["region proposal network", "Part-Of", "Mask R - CNN"], ["ResNet - 1 0 1", "Part-Of", "Mask R - CNN"], ["RPN", "Synonym-Of", "region proposal network"]], "rel_plus": [["region proposal network:Method", "Part-Of", "Mask R - CNN:Method"], ["ResNet - 1 0 1:Method", "Part-Of", "Mask R - CNN:Method"], ["RPN:Method", "Synonym-Of", "region proposal network:Method"]]}
{"doc_id": "211004033", "sentence": "The feature maps for each proposal are cropped from the top convolutional feature maps generated by feature pyramid network ( FPN ) [ 3 6 ] according to different proposal scales .", "ner": [["feature pyramid network", "Method"], ["FPN", "Method"]], "rel": [["FPN", "Synonym-Of", "feature pyramid network"]], "rel_plus": [["FPN:Method", "Synonym-Of", "feature pyramid network:Method"]]}
{"doc_id": "211004033", "sentence": "The following RoiAlign layer [ 7 ] reshapes the cropped features to produce feature maps of the same size per proposal for classification and bounding - box regression .", "ner": [["RoiAlign", "Method"], ["classification", "Task"], ["bounding - box regression", "Task"]], "rel": [["RoiAlign", "Used-For", "classification"], ["RoiAlign", "Used-For", "bounding - box regression"]], "rel_plus": [["RoiAlign:Method", "Used-For", "classification:Task"], ["RoiAlign:Method", "Used-For", "bounding - box regression:Task"]]}
{"doc_id": "211004033", "sentence": "Since Mask R - CNN [ 7 ] obtains the state - of - the - art results in general object detection and instance segmentation , we exploit its base model as our front - end network ( see Fig. 1 ) .", "ner": [["Mask R - CNN", "Method"], ["object detection", "Task"], ["instance segmentation", "Task"]], "rel": [["Mask R - CNN", "Used-For", "object detection"], ["Mask R - CNN", "Used-For", "instance segmentation"]], "rel_plus": [["Mask R - CNN:Method", "Used-For", "object detection:Task"], ["Mask R - CNN:Method", "Used-For", "instance segmentation:Task"]]}
{"doc_id": "211004033", "sentence": "However , the strategy of bounding - box regression can not be directly applied to ellipse regression .", "ner": [["bounding - box regression", "Task"], ["ellipse regression", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "211004033", "sentence": "In such an unoccluded case ( Q l 0 ) , the predicted offset values \u03b4 are all bounded when the proposed region P ( \u2192 Q ) is located close to the groundtruth ellipse E ( see Fig. 5 ) . 2 ) Occluded Ellipse Prediction : For occluded object detection , training RPN [ 6 ] to propose regions of visible parts ( instead of whole object regions ) highly reduces false positives as shown in Sec. V. We infer the whole elliptical object from its visible part through ellipse regression .", "ner": [["occluded object detection", "Task"], ["RPN", "Method"], ["ellipse regression", "Task"]], "rel": [["RPN", "Used-For", "occluded object detection"]], "rel_plus": [["RPN:Method", "Used-For", "occluded object detection:Task"]]}
{"doc_id": "211004033", "sentence": "Traditional R - CNN based methods generate regression and classification outputs directly from proposed feature regions .", "ner": [["R - CNN", "Method"], ["generate regression", "Task"], ["classification", "Task"]], "rel": [["R - CNN", "Used-For", "generate regression"], ["R - CNN", "Used-For", "classification"]], "rel_plus": [["R - CNN:Method", "Used-For", "generate regression:Task"], ["R - CNN:Method", "Used-For", "classification:Task"]]}
{"doc_id": "211004033", "sentence": "Thus , our idea is to perform ellipse regression and classification based on the refined feature region output by a bounding - box regressor .", "ner": [["ellipse regression", "Task"], ["classification", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "211004033", "sentence": "Based on the extended predicted region , the RoiAlign layer re - extracts a small feature map ( e.g. , 1 4 \u00d7 1 4 ) , and accurately aligns the extracted features with the input from FPN .", "ner": [["RoiAlign", "Method"], ["FPN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "211004033", "sentence": "Head architecture of learning occlusion patterns for ellipse regression and classification .", "ner": [["ellipse regression", "Task"], ["classification", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "211004033", "sentence": "Batch normalization [ 4 2 ] is applied for all layers with ReLU [ 4 3 ] except for the last output layers .", "ner": [["Batch normalization", "Method"], ["ReLU", "Method"]], "rel": [["Batch normalization", "Part-Of", "ReLU"]], "rel_plus": [["Batch normalization:Method", "Part-Of", "ReLU:Method"]]}
{"doc_id": "211004033", "sentence": "The ellipse offsets are obtained via a multilayer perceptron ( MLP ) [ 4 1 ] ( see Fig. 8 ) . 2 ) Visible Part Attention : Many recent works [ 4 4 ] , [ 4 5 ] find that convolution filters of different feature channels respond to their specific high - level concepts , which are associated with different semantic parts .", "ner": [["multilayer perceptron", "Method"], ["MLP", "Method"], ["Visible Part Attention", "Method"], ["convolution", "Method"]], "rel": [["MLP", "Synonym-Of", "multilayer perceptron"]], "rel_plus": [["MLP:Method", "Synonym-Of", "multilayer perceptron:Method"]]}
{"doc_id": "211004033", "sentence": "The concatenated feature [ f c , f e ] thus learns various occlusion patterns , and passes through the classification head to output the final confidence scores . 3 ) Training Objective : The R - CNN based models have two types of losses : RPN loss L RPN and head loss [ 6 ] ( composed of classification loss L cls and regression loss L reg ) .", "ner": [["classification head", "Method"], ["R - CNN", "Method"], ["RPN loss", "Method"], ["L RPN", "Method"], ["head loss", "Method"], ["classification loss", "Method"], ["L cls", "Method"], ["regression loss", "Method"], ["L reg", "Method"]], "rel": [["RPN loss", "Part-Of", "R - CNN"], ["head loss", "Part-Of", "R - CNN"], ["L RPN", "Synonym-Of", "RPN loss"], ["classification loss", "Part-Of", "head loss"], ["regression loss", "Part-Of", "head loss"], ["L cls", "Synonym-Of", "classification loss"], ["L reg", "Synonym-Of", "regression loss"]], "rel_plus": [["RPN loss:Method", "Part-Of", "R - CNN:Method"], ["head loss:Method", "Part-Of", "R - CNN:Method"], ["L RPN:Method", "Synonym-Of", "RPN loss:Method"], ["classification loss:Method", "Part-Of", "head loss:Method"], ["regression loss:Method", "Part-Of", "head loss:Method"], ["L cls:Method", "Synonym-Of", "classification loss:Method"], ["L reg:Method", "Synonym-Of", "regression loss:Method"]]}
{"doc_id": "211004033", "sentence": "In the end , we demonstrate how Ellipse R - CNN helps improve the accuracy of 3D object estimation in occluded cases .", "ner": [["Ellipse R - CNN", "Method"], ["3D object estimation", "Task"]], "rel": [["Ellipse R - CNN", "Used-For", "3D object estimation"]], "rel_plus": [["Ellipse R - CNN:Method", "Used-For", "3D object estimation:Task"]]}
{"doc_id": "211004033", "sentence": "We validate the proposed Ellipse R - CNN on four datasets : synthetic occluded ellipses ( SOE ) , synthetic occluded fruits ( SOF ) , real occluded fruits ( ROF ) and FDDB [ 1 0 ] datasets .", "ner": [["Ellipse R - CNN", "Method"], ["synthetic occluded ellipses", "Dataset"], ["SOE", "Dataset"], ["synthetic occluded fruits", "Dataset"], ["SOF", "Dataset"], ["real occluded fruits", "Dataset"], ["ROF", "Dataset"], ["FDDB", "Dataset"]], "rel": [["SOE", "Synonym-Of", "synthetic occluded ellipses"], ["Ellipse R - CNN", "Evaluated-With", "synthetic occluded ellipses"], ["SOF", "Synonym-Of", "synthetic occluded fruits"], ["Ellipse R - CNN", "Evaluated-With", "synthetic occluded fruits"], ["ROF", "Synonym-Of", "real occluded fruits"], ["Ellipse R - CNN", "Evaluated-With", "real occluded fruits"], ["Ellipse R - CNN", "Evaluated-With", "FDDB"]], "rel_plus": [["SOE:Dataset", "Synonym-Of", "synthetic occluded ellipses:Dataset"], ["Ellipse R - CNN:Method", "Evaluated-With", "synthetic occluded ellipses:Dataset"], ["SOF:Dataset", "Synonym-Of", "synthetic occluded fruits:Dataset"], ["Ellipse R - CNN:Method", "Evaluated-With", "synthetic occluded fruits:Dataset"], ["ROF:Dataset", "Synonym-Of", "real occluded fruits:Dataset"], ["Ellipse R - CNN:Method", "Evaluated-With", "real occluded fruits:Dataset"], ["Ellipse R - CNN:Method", "Evaluated-With", "FDDB:Dataset"]]}
{"doc_id": "211004033", "sentence": "The ROF dataset ( 1 1 1 5 images in total ) is human - annotated and is built upon MinneApple [ 4 8 ] and ACFR [ 2 0 ] datasets , from which we crop out the sub - images of heavily occluded fruit clusters .", "ner": [["ROF", "Dataset"], ["MinneApple", "Dataset"], ["ACFR", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "211004033", "sentence": "For the training , we use the pre - trained weights for MS COCO [ 1 4 ] to initialize the Ellipse R - CNN , and use a step strategy with mini - batch stochastic gradient descent ( SGD ) to train the networks on a GeForce GTX 1 0 8 0 GPU .", "ner": [["MS COCO", "Dataset"], ["Ellipse R - CNN", "Method"], ["stochastic gradient descent", "Method"], ["SGD", "Method"]], "rel": [["Ellipse R - CNN", "Trained-With", "MS COCO"], ["stochastic gradient descent", "Part-Of", "Ellipse R - CNN"], ["SGD", "Synonym-Of", "stochastic gradient descent"]], "rel_plus": [["Ellipse R - CNN:Method", "Trained-With", "MS COCO:Dataset"], ["stochastic gradient descent:Method", "Part-Of", "Ellipse R - CNN:Method"], ["SGD:Method", "Synonym-Of", "stochastic gradient descent:Method"]]}
{"doc_id": "211004033", "sentence": "On SOF , ROF , and FDDB datasets , we train with an initial learning rate of 1 0 \u2212 3 for 2 0 , 0 0 0 iterations and train for another 1 0 , 0 0 0 iterations with a decreased learning rate of 1 0 \u2212 4 .", "ner": [["SOF", "Dataset"], ["ROF", "Dataset"], ["FDDB", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "211004033", "sentence": "Ellipse R - CNN Mask R - CNN+ Fig. 1 1 .", "ner": [["Ellipse R - CNN", "Method"], ["Mask R - CNN+", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "211004033", "sentence": "Qualitative results from our Ellipse R - CNN and Mask R - CNN+ on the ROF dataset .", "ner": [["Ellipse R - CNN", "Method"], ["Mask R - CNN+", "Method"], ["ROF", "Dataset"]], "rel": [["Ellipse R - CNN", "Evaluated-With", "ROF"], ["Mask R - CNN+", "Evaluated-With", "ROF"]], "rel_plus": [["Ellipse R - CNN:Method", "Evaluated-With", "ROF:Dataset"], ["Mask R - CNN+:Method", "Evaluated-With", "ROF:Dataset"]]}
{"doc_id": "211004033", "sentence": "The Mask R - CNN+ trained on the whole object regions outputs many false positives in heavily occluded cases .   We compare the proposed Ellipse R - CNN to the baseline model Mask R - CNN , which obtains the state - of - the - art results in general object detection and instance segmentation .", "ner": [["Mask R - CNN+", "Method"], ["Ellipse R - CNN", "Method"], ["Mask R - CNN", "Method"], ["object detection", "Task"], ["instance segmentation", "Task"]], "rel": [["Ellipse R - CNN", "Compare-With", "Mask R - CNN"], ["Ellipse R - CNN", "Used-For", "object detection"], ["Mask R - CNN", "Used-For", "object detection"], ["Ellipse R - CNN", "Used-For", "instance segmentation"], ["Mask R - CNN", "Used-For", "instance segmentation"]], "rel_plus": [["Ellipse R - CNN:Method", "Compare-With", "Mask R - CNN:Method"], ["Ellipse R - CNN:Method", "Used-For", "object detection:Task"], ["Mask R - CNN:Method", "Used-For", "object detection:Task"], ["Ellipse R - CNN:Method", "Used-For", "instance segmentation:Task"], ["Mask R - CNN:Method", "Used-For", "instance segmentation:Task"]]}
{"doc_id": "211004033", "sentence": "Since our model is the first work of ellipse regression , to make a fair comparison , we fit ellipses directly from the mask outputs of Mask R - CNN ( trained on the regions of whole objects ) using the method of minimum volume enclosing ellipsoid [ 5 2 ] in 2D ( i.e. , Mask R - CNN+ ) .", "ner": [["ellipse regression", "Task"], ["Mask R - CNN", "Method"], ["Mask R - CNN+", "Method"]], "rel": [["Mask R - CNN", "Used-For", "ellipse regression"]], "rel_plus": [["Mask R - CNN:Method", "Used-For", "ellipse regression:Task"]]}
{"doc_id": "211004033", "sentence": "For the ablation study of occlusion handling , we adapt two state - of - the - art methods in our model : DeepParts+ and SENet+ .", "ner": [["occlusion handling", "Task"], ["DeepParts+", "Method"], ["SENet+", "Method"]], "rel": [["DeepParts+", "Used-For", "occlusion handling"], ["SENet+", "Used-For", "occlusion handling"]], "rel_plus": [["DeepParts+:Method", "Used-For", "occlusion handling:Task"], ["SENet+:Method", "Used-For", "occlusion handling:Task"]]}
{"doc_id": "211004033", "sentence": "In DeepParts+ , we only keep the U - Net structure to learn a set of 4 5 occlusion patterns , and the final score is obtained via an MLP on the part detection scores [ 3 2 ] .", "ner": [["DeepParts+", "Method"], ["U - Net", "Method"], ["MLP", "Method"]], "rel": [["U - Net", "Part-Of", "DeepParts+"], ["MLP", "Part-Of", "DeepParts+"]], "rel_plus": [["U - Net:Method", "Part-Of", "DeepParts+:Method"], ["MLP:Method", "Part-Of", "DeepParts+:Method"]]}
{"doc_id": "211004033", "sentence": "For SENet+ , we learn the attention vector directly from the refined feature maps ( without U - Net ) , and perform the classification only on the re - weighted features [ 4 6 ] . 1 ) Accuracy of Ellipse Regression : The key component of our Ellipse R - CNN is the ellipse regressor .", "ner": [["SENet+", "Method"], ["U - Net", "Method"], ["classification", "Task"], ["Ellipse Regression", "Task"], ["Ellipse R - CNN", "Method"], ["ellipse regressor", "Method"]], "rel": [["SENet+", "Used-For", "classification"], ["Ellipse R - CNN", "Used-For", "Ellipse Regression"], ["ellipse regressor", "Part-Of", "Ellipse R - CNN"]], "rel_plus": [["SENet+:Method", "Used-For", "classification:Task"], ["Ellipse R - CNN:Method", "Used-For", "Ellipse Regression:Task"], ["ellipse regressor:Method", "Part-Of", "Ellipse R - CNN:Method"]]}
{"doc_id": "211004033", "sentence": "Table I - II show the breakdown performance of the ellipse prediction on the SOE and SOF datasets whose GT is perfectly generated based on the geometry of object models .", "ner": [["ellipse prediction", "Task"], ["SOE", "Dataset"], ["SOF", "Dataset"]], "rel": [["SOF", "Benchmark-For", "ellipse prediction"], ["SOE", "Benchmark-For", "ellipse prediction"]], "rel_plus": [["SOF:Dataset", "Benchmark-For", "ellipse prediction:Task"], ["SOE:Dataset", "Benchmark-For", "ellipse prediction:Task"]]}
{"doc_id": "211004033", "sentence": "Our strategy of ellipse regression ( e.g. , ellipse R - CNN - ) leads significant performance improvement on all metrics compared to the baseline model .", "ner": [["ellipse regression", "Task"], ["ellipse R - CNN -", "Method"]], "rel": [["ellipse R - CNN -", "Used-For", "ellipse regression"]], "rel_plus": [["ellipse R - CNN -:Method", "Used-For", "ellipse regression:Task"]]}
{"doc_id": "211004033", "sentence": "For the ROF dataset , Table III shows a higher sensitivity of our model on AP \u0398 and MR \u0398 compared to those on SOE and SOF datasets : AP is higher than that in Table II but AP \u0398 drops a lot .", "ner": [["ROF", "Dataset"], ["SOE", "Dataset"], ["SOF", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "211004033", "sentence": "Thus , it is hard to quantify the results on AP \u0398 and MR \u0398 but our proposed model still achieves the best performance on AP and MR . 2 ) Validity of Feature Region Refinement : Table I - III region refinement ( i.e. , Ellipse R - CNN - with R ) on the SOE , SOF and ROF datasets .", "ner": [["Ellipse R - CNN - with R", "Method"], ["SOE", "Dataset"], ["SOF", "Dataset"], ["ROF", "Dataset"]], "rel": [["Ellipse R - CNN - with R", "Evaluated-With", "SOE"], ["Ellipse R - CNN - with R", "Evaluated-With", "SOF"], ["Ellipse R - CNN - with R", "Evaluated-With", "ROF"]], "rel_plus": [["Ellipse R - CNN - with R:Method", "Evaluated-With", "SOE:Dataset"], ["Ellipse R - CNN - with R:Method", "Evaluated-With", "SOF:Dataset"], ["Ellipse R - CNN - with R:Method", "Evaluated-With", "ROF:Dataset"]]}
{"doc_id": "211004033", "sentence": "The performance is largely improved when the refined features are used for ellipse regression and classification .", "ner": [["ellipse regression", "Task"], ["classification", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "211004033", "sentence": "Thus , the improvements in Table IV by using the refined features are not as significant as those in Table I - III . 3 ) Performance of Occlusion Handling : One of our evaluation goals is occlusion handling , whose overall performance is measured by MR and MR \u0398 as shown in Table I - III .", "ner": [["Occlusion Handling", "Task"], ["occlusion handling", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "211004033", "sentence": "All three variants with different mechanisms of occlusion handling show some improvements to the baseline ( i.e. , Ellipse R - CNN - with R ) , ranging from 2. 1 to 7. 2 on MR and from 4. 3 to 8. 6 on MR \u0398 .", "ner": [["occlusion handling", "Task"], ["Ellipse R - CNN - with R", "Method"]], "rel": [["Ellipse R - CNN - with R", "Used-For", "occlusion handling"]], "rel_plus": [["Ellipse R - CNN - with R:Method", "Used-For", "occlusion handling:Task"]]}
{"doc_id": "211004033", "sentence": "Overall , the error rates can be sorted in the following order : DeepParts+ \u2248 SENet+ > Ellipse R - CNN*. The reason is that the DeepParts+ is limited by its fixed number of occlusion patterns to learn , while the SENet+ learns a continuous attention vector to adjust feature weights but lacks the whole ellipse information to generalize different occlusions .", "ner": [["DeepParts+", "Method"], ["SENet+", "Method"], ["Ellipse R - CNN*.", "Method"], ["DeepParts+", "Method"], ["SENet+", "Method"]], "rel": [["DeepParts+", "Compare-With", "SENet+"], ["DeepParts+", "Compare-With", "Ellipse R - CNN*."], ["DeepParts+", "Compare-With", "SENet+"]], "rel_plus": [["DeepParts+:Method", "Compare-With", "SENet+:Method"], ["DeepParts+:Method", "Compare-With", "Ellipse R - CNN*.:Method"], ["DeepParts+:Method", "Compare-With", "SENet+:Method"]]}
{"doc_id": "211004033", "sentence": "We further compare our Ellipse R - CNN to the Ellipse R - CNN * ( without concatenating f e ) .", "ner": [["Ellipse R - CNN", "Method"], ["Ellipse R - CNN *", "Method"]], "rel": [["Ellipse R - CNN", "Compare-With", "Ellipse R - CNN *"]], "rel_plus": [["Ellipse R - CNN:Method", "Compare-With", "Ellipse R - CNN *:Method"]]}
{"doc_id": "211004033", "sentence": "Since no GT visible boxes are available and few objects are clustered , we can only evaluate our model without the occlusion handling mechanism ( i.e. , Ellipse R - CNN - with R ) .", "ner": [["occlusion handling", "Task"], ["Ellipse R - CNN - with R", "Method"]], "rel": [["Ellipse R - CNN - with R", "Used-For", "occlusion handling"]], "rel_plus": [["Ellipse R - CNN - with R:Method", "Used-For", "occlusion handling:Task"]]}
{"doc_id": "211004033", "sentence": "Specifically , in all seven examples , several faces 3D object estimation from Ellipse R - CNN 3D object estimation from Mask R - CNN+ Fig. 1 3 .", "ner": [["3D object estimation", "Task"], ["Ellipse R - CNN", "Method"], ["3D object estimation", "Task"], ["Mask R - CNN+", "Method"]], "rel": [["Ellipse R - CNN", "Used-For", "3D object estimation"], ["Mask R - CNN+", "Used-For", "3D object estimation"]], "rel_plus": [["Ellipse R - CNN:Method", "Used-For", "3D object estimation:Task"], ["Mask R - CNN+:Method", "Used-For", "3D object estimation:Task"]]}
{"doc_id": "211004033", "sentence": "Qualitative results of 3D object estimation from our Ellipse R - CNN and Mask R - CNN+ on the SOF dataset .", "ner": [["3D object estimation", "Task"], ["Ellipse R - CNN", "Method"], ["Mask R - CNN+", "Method"], ["SOF", "Dataset"]], "rel": [["Ellipse R - CNN", "Used-For", "3D object estimation"], ["Mask R - CNN+", "Used-For", "3D object estimation"], ["SOF", "Benchmark-For", "3D object estimation"], ["Ellipse R - CNN", "Evaluated-With", "SOF"], ["Mask R - CNN+", "Evaluated-With", "SOF"]], "rel_plus": [["Ellipse R - CNN:Method", "Used-For", "3D object estimation:Task"], ["Mask R - CNN+:Method", "Used-For", "3D object estimation:Task"], ["SOF:Dataset", "Benchmark-For", "3D object estimation:Task"], ["Ellipse R - CNN:Method", "Evaluated-With", "SOF:Dataset"], ["Mask R - CNN+:Method", "Evaluated-With", "SOF:Dataset"]]}
{"doc_id": "211004033", "sentence": "The Mask R - CNN+ produces many distorted face shapes , while our detector accurately infers the whole ellipse regions for all of them . 5 ) Discussion on 3D Object Estimation : In order to understand how Ellipse R - CNN improves the accuracy of 3D object estimation , we implement the multi - view 3D localization using quadrics [ 9 ] from 2D detections on the SOF dataset .", "ner": [["Mask R - CNN+", "Method"], ["3D Object Estimation", "Task"], ["Ellipse R - CNN", "Method"], ["3D object estimation", "Task"], ["multi - view 3D localization", "Task"], ["2D detections", "Task"], ["SOF", "Dataset"]], "rel": [["Ellipse R - CNN", "Used-For", "3D object estimation"], ["Ellipse R - CNN", "Used-For", "multi - view 3D localization"], ["SOF", "Benchmark-For", "2D detections"]], "rel_plus": [["Ellipse R - CNN:Method", "Used-For", "3D object estimation:Task"], ["Ellipse R - CNN:Method", "Used-For", "multi - view 3D localization:Task"], ["SOF:Dataset", "Benchmark-For", "2D detections:Task"]]}
{"doc_id": "211004033", "sentence": "As shown in the comparison , three estimation errors of the Ellipse R - CNN are much lower than the Mask R - CNN+ , especially the rotation error ( i.e. , 1 2 . 6 \u2022 vs. 3 7 . 2 \u2022 ) .", "ner": [["Ellipse R - CNN", "Method"], ["Mask R - CNN+", "Method"]], "rel": [["Ellipse R - CNN", "Compare-With", "Mask R - CNN+"]], "rel_plus": [["Ellipse R - CNN:Method", "Compare-With", "Mask R - CNN+:Method"]]}
{"doc_id": "6423078", "sentence": "Our proposed method , holistically - nested edge detection ( HED ) , performs image - to - image prediction by means of a deep learning model that leverages fully convolutional neural networks and deeply - supervised nets .", "ner": [["holistically - nested edge detection", "Method"], ["HED", "Method"], ["image - to - image prediction", "Task"], ["deep learning", "Method"], ["fully convolutional neural networks", "Method"], ["deeply - supervised nets", "Method"]], "rel": [["HED", "Synonym-Of", "holistically - nested edge detection"], ["fully convolutional neural networks", "Part-Of", "holistically - nested edge detection"], ["deeply - supervised nets", "Part-Of", "holistically - nested edge detection"], ["holistically - nested edge detection", "Used-For", "image - to - image prediction"]], "rel_plus": [["HED:Method", "Synonym-Of", "holistically - nested edge detection:Method"], ["fully convolutional neural networks:Method", "Part-Of", "holistically - nested edge detection:Method"], ["deeply - supervised nets:Method", "Part-Of", "holistically - nested edge detection:Method"], ["holistically - nested edge detection:Method", "Used-For", "image - to - image prediction:Task"]]}
{"doc_id": "6423078", "sentence": "We significantly advance the state - of - the - art on the BSD 5 0 0 dataset ( ODS F - score of . 7 8 2 ) and the NYU Depth dataset ( ODS F - score of . 7 4 6 ) , and do so with an improved speed ( 0. 4 second per image ) that is orders of magnitude faster than some recent CNN - based edge detection algorithms .", "ner": [["BSD 5 0 0", "Dataset"], ["NYU Depth", "Dataset"], ["CNN", "Method"], ["edge detection", "Task"]], "rel": [["CNN", "Used-For", "edge detection"]], "rel_plus": [["CNN:Method", "Used-For", "edge detection:Task"]]}
{"doc_id": "6423078", "sentence": "This problem is both fundamental and of great importance to a variety of computer vision areas ranging from traditional tasks such as visual saliency , segmentation , object detection/recognition , tracking and motion analysis , medical imaging , structurefrom - motion and 3D reconstruction , to modern applications like autonomous driving , mobile computing , and image - totext analysis .", "ner": [["computer vision", "Task"], ["visual saliency", "Task"], ["segmentation", "Task"], ["object detection/recognition", "Task"], ["tracking", "Task"], ["motion analysis", "Task"], ["medical imaging", "Task"], ["structurefrom - motion", "Task"], ["3D reconstruction", "Task"], ["autonomous driving", "Task"], ["mobile computing", "Task"], ["image - totext analysis", "Task"]], "rel": [["object detection/recognition", "SubTask-Of", "computer vision"], ["visual saliency", "SubTask-Of", "computer vision"], ["segmentation", "SubTask-Of", "computer vision"], ["tracking", "SubTask-Of", "computer vision"], ["motion analysis", "SubTask-Of", "computer vision"], ["medical imaging", "SubTask-Of", "computer vision"], ["structurefrom - motion", "SubTask-Of", "computer vision"], ["3D reconstruction", "SubTask-Of", "computer vision"], ["autonomous driving", "SubTask-Of", "computer vision"], ["mobile computing", "SubTask-Of", "computer vision"], ["image - totext analysis", "SubTask-Of", "computer vision"]], "rel_plus": [["object detection/recognition:Task", "SubTask-Of", "computer vision:Task"], ["visual saliency:Task", "SubTask-Of", "computer vision:Task"], ["segmentation:Task", "SubTask-Of", "computer vision:Task"], ["tracking:Task", "SubTask-Of", "computer vision:Task"], ["motion analysis:Task", "SubTask-Of", "computer vision:Task"], ["medical imaging:Task", "SubTask-Of", "computer vision:Task"], ["structurefrom - motion:Task", "SubTask-Of", "computer vision:Task"], ["3D reconstruction:Task", "SubTask-Of", "computer vision:Task"], ["autonomous driving:Task", "SubTask-Of", "computer vision:Task"], ["mobile computing:Task", "SubTask-Of", "computer vision:Task"], ["image - totext analysis:Task", "SubTask-Of", "computer vision:Task"]]}
{"doc_id": "6423078", "sentence": "Broadly speaking , one may categorize works into a few groups such as I : early pioneering methods like the Sobel detector ( Kittler 1 9 8 3 ) , zero - crossing ( Marr and Hildreth 1 9 8 0 ; Torre and Poggio 1 9 8 6 ) , and the widely adopted Canny detector ( Canny 1 9 8 6 ) ; methods driven by II : information theory on top of features arrived at through careful manual design , such as Statistical Edges ( Konishi et al. 2 0 0 3 ) , Pb ( Martin et al. 2 0 0 4 ) , and gPb ( Arbelaez et al. 2 0 1 1 ) ; and III : learning - based methods that remain reliant on features of human design , such as BEL ( Doll\u00e1r et al. 2 0 0 6 ) , Multi - scale ( Ren 2 0 0 8) , Sketch Tokens ( Lim et al. 2 0 1 3 ) , and Structured Edges ( Doll\u00e1r and Zitnick 2 0 1 5 ) .", "ner": [["Sobel detector", "Method"], ["zero - crossing", "Method"], ["Canny detector", "Method"], ["Statistical Edges", "Method"], ["Pb", "Method"], ["gPb", "Method"], ["BEL", "Method"], ["Multi - scale", "Method"], ["Sketch Tokens", "Method"], ["Structured Edges", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "6423078", "sentence": "In addition , there has been a recent wave of development using Convolutional Neural Networks that emphasize the importance of automatic hierarchical feature learning , including N 4 -Fields ( Ganin and Lempitsky 2 0 1 4 ) , DeepContour ( Shen et al. 2 0 1 5 ) , DeepEdge ( Bertasius et al. 2 0 1 5 ) , and CSCNN ( Hwang and Liu 2 0 1 5 ) .", "ner": [["Convolutional Neural Networks", "Method"], ["N 4 -Fields", "Method"], ["DeepContour", "Method"], ["DeepEdge", "Method"], ["CSCNN", "Method"]], "rel": [["N 4 -Fields", "SubClass-Of", "Convolutional Neural Networks"], ["DeepContour", "SubClass-Of", "Convolutional Neural Networks"], ["DeepEdge", "SubClass-Of", "Convolutional Neural Networks"], ["CSCNN", "SubClass-Of", "Convolutional Neural Networks"]], "rel_plus": [["N 4 -Fields:Method", "SubClass-Of", "Convolutional Neural Networks:Method"], ["DeepContour:Method", "SubClass-Of", "Convolutional Neural Networks:Method"], ["DeepEdge:Method", "SubClass-Of", "Convolutional Neural Networks:Method"], ["CSCNN:Method", "SubClass-Of", "Convolutional Neural Networks:Method"]]}
{"doc_id": "6423078", "sentence": "Prior to this explosive development in deep learning , the Structured Edges method ( typically abbreviated SE ) ( Doll\u00e1r and Zitnick 2 0 1 5 ) emerged as one of the most celebrated systems for edge detection , thanks to its state - of - the - art performance on the BSDS 5 0 0 dataset ( Martin et al. 2 0 0 4 ; Arbelaez et al. 2 0 1 1 ) ( with , e.g. , F - score of 0. 7 4 6 ) and its practically significant speed of 2. 5 frames per second .", "ner": [["deep learning", "Method"], ["Structured Edges", "Method"], ["SE", "Method"], ["edge detection", "Task"], ["BSDS 5 0 0", "Dataset"]], "rel": [["SE", "Synonym-Of", "Structured Edges"], ["deep learning", "Used-For", "edge detection"], ["Structured Edges", "Used-For", "edge detection"], ["BSDS 5 0 0", "Benchmark-For", "edge detection"], ["Structured Edges", "Evaluated-With", "BSDS 5 0 0"]], "rel_plus": [["SE:Method", "Synonym-Of", "Structured Edges:Method"], ["deep learning:Method", "Used-For", "edge detection:Task"], ["Structured Edges:Method", "Used-For", "edge detection:Task"], ["BSDS 5 0 0:Dataset", "Benchmark-For", "edge detection:Task"], ["Structured Edges:Method", "Evaluated-With", "BSDS 5 0 0:Dataset"]]}
{"doc_id": "6423078", "sentence": "Convolutional neural networks ( CNN ) ( LeCun et al. 1 9 8 9 ) have achieved a great success in automatically learning thousands ( or even millions or billions ) of features for pattern recognition , under the paradigm of image - to - class classification [ e.g. predicting which category an image belongs to Russakovsky et al. ( 2 0 1 5 ) ] or patch - to - class classification ( e.g predicting which object an image patch contains Girshick et al. 2 0 1 4 ) .", "ner": [["Convolutional neural networks", "Method"], ["CNN", "Method"], ["pattern recognition", "Task"], ["image - to - class classification", "Task"], ["patch - to - class classification", "Task"]], "rel": [["CNN", "Synonym-Of", "Convolutional neural networks"], ["Convolutional neural networks", "Used-For", "pattern recognition"]], "rel_plus": [["CNN:Method", "Synonym-Of", "Convolutional neural networks:Method"], ["Convolutional neural networks:Method", "Used-For", "pattern recognition:Task"]]}
{"doc_id": "6423078", "sentence": "CNN - based edge detection methods before HED ( Ganin and Lempitsky 2 0 1 4 ; Shen et al. 2 0 1 5 ; Bertasius et al. 2 0 1 5 ; Hwang and Liu 2 0 1 5 ) mostly follow a patch - to - class paradigm , which is patch - centric .", "ner": [["CNN", "Method"], ["edge detection", "Task"]], "rel": [["CNN", "Used-For", "edge detection"]], "rel_plus": [["CNN:Method", "Used-For", "edge detection:Task"]]}
{"doc_id": "6423078", "sentence": "Recently proposed fully convolutional neural networks ( FCN ) ( Long et al. 2 0 1 5 ) , targeted for the task of semantic image labeling , instead points to a promising direction of performing training/testing for the entire image altogether , which is under a image - centric paradigm .", "ner": [["fully convolutional neural networks", "Method"], ["FCN", "Method"]], "rel": [["FCN", "Synonym-Of", "fully convolutional neural networks"]], "rel_plus": [["FCN:Method", "Synonym-Of", "fully convolutional neural networks:Method"]]}
{"doc_id": "6423078", "sentence": "Applying FCN to the edge detection problem however produces an unsatisfactory result ( e.g. F - score 0. 7 4 5 on BSDS 5 0 0 ) as edges observe strong multi - scale aspects that is quite different from semantic labeling .", "ner": [["FCN", "Method"], ["edge detection", "Task"], ["BSDS 5 0 0", "Dataset"], ["semantic labeling", "Task"]], "rel": [["FCN", "Used-For", "edge detection"], ["BSDS 5 0 0", "Benchmark-For", "edge detection"], ["FCN", "Evaluated-With", "BSDS 5 0 0"]], "rel_plus": [["FCN:Method", "Used-For", "edge detection:Task"], ["BSDS 5 0 0:Dataset", "Benchmark-For", "edge detection:Task"], ["FCN:Method", "Evaluated-With", "BSDS 5 0 0:Dataset"]]}
{"doc_id": "6423078", "sentence": "In this regard , the deeply - supervised nets method ( DSN ) ( Lee et al. 2 0 1 5 ) provides a principled and clean solution for multi - scale learning and fusion where supervised information is jointly enforced in the individual convolutional layers during training .", "ner": [["deeply - supervised nets", "Method"], ["DSN", "Method"]], "rel": [["DSN", "Synonym-Of", "deeply - supervised nets"]], "rel_plus": [["DSN:Method", "Synonym-Of", "deeply - supervised nets:Method"]]}
{"doc_id": "6423078", "sentence": "Motivated by fully convolutional networks ( Long et al. 2 0 1 5 ) and deeply - supervised nets ( Lee et al. 2 0 1 5 ) , we develop an end - to - end edge detection system , holisticallynested edge detection ( HED ) , that automatically learns the type of rich hierarchical features that are crucial if we are to approach the human ability to resolve ambiguity in natural image edge and object boundary detection .", "ner": [["fully convolutional networks", "Method"], ["deeply - supervised nets", "Method"], ["edge detection system", "Method"], ["holisticallynested edge detection", "Method"], ["HED", "Method"], ["object boundary detection", "Task"]], "rel": [["holisticallynested edge detection", "SubClass-Of", "edge detection system"], ["HED", "Synonym-Of", "holisticallynested edge detection"], ["holisticallynested edge detection", "Used-For", "object boundary detection"]], "rel_plus": [["holisticallynested edge detection:Method", "SubClass-Of", "edge detection system:Method"], ["HED:Method", "Synonym-Of", "holisticallynested edge detection:Method"], ["holisticallynested edge detection:Method", "Used-For", "object boundary detection:Task"]]}
{"doc_id": "6423078", "sentence": "Methods after HED After the acceptance of the conference version of our work , HED has been extended to new applications and applied in different domains : an edge detector is trained using supervised labeling information automatically obtained from videos using motion cues ( Li et al. 2 0 1 6 ) ; a weakly - supervised learning strategy is proposed in ( Khoreva et al. 2 0 1 6 ) to reduce the burden in obtaining a large amount of training labels ; further improvement on the BSDS 5 0 0 dataset is achieved in ( Kokkinos 2 0 1 6 ) by carefully fusing multiple cues ; boundary detection methods towards extracting high - level semantics have been proposed in ( Zhu et al. 2 0 1 5 ; Chen et al. 2 0 1 6 ; Premachandran et al. 2 0 1 5 ) ; extension and refinement to 3D Vascular boundaries in medical imaging is developed in ( Merkow et al. 2 0 1 6 ) ; scale - sensitive deep supervision is introduced in ( Shen et al. 2 0 1 6 ) for object skeleton extraction .", "ner": [["HED", "Method"], ["HED", "Method"], ["BSDS 5 0 0", "Dataset"], ["boundary detection", "Task"], ["object skeleton extraction", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "6423078", "sentence": "The proposed holistically - nested edge detector ( HED ) tackles two critical issues : ( 1 ) holistic image training and prediction , inspired by fully convolutional neural networks ( Long et al. 2 0 1 5 ) , for image - to - image classification ( the system takes an image as input , and directly produces the edge map image as output ) ; and ( 2 ) nested multi - scale feature learning , inspired by deeply - supervised nets ( Lee et al. 2 0 1 5 ) , that performs deep layer supervision to \" guide \" early classification results .", "ner": [["holistically - nested edge detector", "Method"], ["HED", "Method"], ["fully convolutional neural networks", "Method"], ["image - to - image classification", "Task"], ["deeply - supervised nets", "Method"], ["classification", "Task"]], "rel": [["HED", "Synonym-Of", "holistically - nested edge detector"], ["fully convolutional neural networks", "Used-For", "image - to - image classification"], ["deeply - supervised nets", "Used-For", "classification"]], "rel_plus": [["HED:Method", "Synonym-Of", "holistically - nested edge detector:Method"], ["fully convolutional neural networks:Method", "Used-For", "image - to - image classification:Task"], ["deeply - supervised nets:Method", "Used-For", "classification:Task"]]}
{"doc_id": "6423078", "sentence": "HED shows a clear advantage in consistency over Canny The task of edge and object boundary detection is inherently challenging .", "ner": [["Canny", "Method"], ["edge and object boundary detection", "Task"]], "rel": [["Canny", "Used-For", "edge and object boundary detection"]], "rel_plus": [["Canny:Method", "Used-For", "edge and object boundary detection:Task"]]}
{"doc_id": "6423078", "sentence": "Structured Edges ( SE ) ( Doll\u00e1r and Zitnick 2 0 1 5 ) primarily focuses on three of these aspects : using a large number of manually designed features ( property 1 ) , fusing multi - scale responses ( property 2 ) , and incorporating structural information ( property 4 ) .", "ner": [["Structured Edges", "Method"], ["SE", "Method"]], "rel": [["SE", "Synonym-Of", "Structured Edges"]], "rel_plus": [["SE:Method", "Synonym-Of", "Structured Edges:Method"]]}
{"doc_id": "6423078", "sentence": "The entire network is trained with multiple error propagation paths ( dashed lines ) By \" holistically - nested \" , we intend to emphasize that we are producing an end - to - end edge detection system , a strategy inspired by fully convolutional neural networks ( Long et al. 2 0 1 5 ) , but with additional deep supervision on top of trimmed VGG nets ( Simonyan and Zisserman 2 0 1 5 ) ( shown in Fig. 3 ) .", "ner": [["end - to - end edge detection system", "Method"], ["fully convolutional neural networks", "Method"], ["VGG", "Method"]], "rel": [["VGG", "Part-Of", "end - to - end edge detection system"]], "rel_plus": [["VGG:Method", "Part-Of", "end - to - end edge detection system:Method"]]}
{"doc_id": "6423078", "sentence": "In the absence of deep supervision and side outputs , a fully convolutional network ( Long et al. 2 0 1 5 ) ( FCN ) produces a less satisfactory result ( e.g. F - score 0. 7 4 5 on BSDS 5 0 0 ) than HED , since edge detection demands highly accurate edge pixel localization .", "ner": [["fully convolutional network", "Method"], ["FCN", "Method"], ["BSDS 5 0 0", "Dataset"], ["HED", "Method"], ["edge detection", "Task"]], "rel": [["FCN", "Synonym-Of", "fully convolutional network"], ["fully convolutional network", "Evaluated-With", "BSDS 5 0 0"], ["HED", "Evaluated-With", "BSDS 5 0 0"], ["fully convolutional network", "Compare-With", "HED"], ["fully convolutional network", "Used-For", "edge detection"], ["HED", "Used-For", "edge detection"], ["BSDS 5 0 0", "Benchmark-For", "edge detection"]], "rel_plus": [["FCN:Method", "Synonym-Of", "fully convolutional network:Method"], ["fully convolutional network:Method", "Evaluated-With", "BSDS 5 0 0:Dataset"], ["HED:Method", "Evaluated-With", "BSDS 5 0 0:Dataset"], ["fully convolutional network:Method", "Compare-With", "HED:Method"], ["fully convolutional network:Method", "Used-For", "edge detection:Task"], ["HED:Method", "Used-For", "edge detection:Task"], ["BSDS 5 0 0:Dataset", "Benchmark-For", "edge detection:Task"]]}
{"doc_id": "6423078", "sentence": "In addition to the speed gain over patchbased CNN edge detection methods , the performance gain is largely due to three aspects : ( 1 ) FCN - like image - to - image training allows us to simultaneously train on a significantly larger amount of samples ( see Table 5 ) ; ( 2 ) deep supervision in our model guides the learning of more transparent features ( see Table 2 ) ; ( 3 ) interpolating the side outputs in the endto - end learning encourages coherent contributions from each layer ( see Table 4 ) .", "ner": [["CNN edge detection", "Method"], ["FCN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "6423078", "sentence": "Recently , VGGNet ( Simonyan and Zisserman 2 0 1 5 ) has been seen to achieve state - of - the - art performance in the ImageNet challenge , with great depth ( 1 6 convolutional layers ) , great density ( stride - 1 convolutional kernels ) , and multiple stages The bolded convolutional layers are linked to additional side - output layers ( five 2 - stride downsampling layers ) .", "ner": [["VGGNet", "Method"], ["ImageNet", "Dataset"], ["convolutional layers", "Method"], ["stride - 1 convolutional kernels", "Method"], ["convolutional layers", "Method"], ["2 - stride downsampling layers", "Method"]], "rel": [["convolutional layers", "Part-Of", "VGGNet"], ["stride - 1 convolutional kernels", "Part-Of", "VGGNet"], ["VGGNet", "Evaluated-With", "ImageNet"], ["2 - stride downsampling layers", "Part-Of", "convolutional layers"]], "rel_plus": [["convolutional layers:Method", "Part-Of", "VGGNet:Method"], ["stride - 1 convolutional kernels:Method", "Part-Of", "VGGNet:Method"], ["VGGNet:Method", "Evaluated-With", "ImageNet:Dataset"], ["2 - stride downsampling layers:Method", "Part-Of", "convolutional layers:Method"]]}
{"doc_id": "6423078", "sentence": "Recent work ( Bertasius et al. 2 0 1 5 ) also demonstrates that fine - tuning deep neural networks pre - trained on the general image classification task is useful to the low - level edge detection task .", "ner": [["image classification", "Task"], ["edge detection", "Task"]], "rel": [["image classification", "Used-For", "edge detection"]], "rel_plus": [["image classification:Task", "Used-For", "edge detection:Task"]]}
{"doc_id": "6423078", "sentence": "We therefore adopt the VGGNet architecture but make the following modifications : ( a ) we connect our side output layer to the last convolutional layer in each stage , respectively conv 1 _ 2 , conv 2 _ 2 , conv 3 _ 3 , conv 4 _ 3 , conv 5 _ 3 .", "ner": [["VGGNet", "Method"], ["convolutional layer", "Method"]], "rel": [["convolutional layer", "Part-Of", "VGGNet"]], "rel_plus": [["convolutional layer:Method", "Part-Of", "VGGNet:Method"]]}
{"doc_id": "6423078", "sentence": "The receptive field size of each of these convolutional layers is identical to the corresponding side - output layer ; ( b ) we cut the last stage of VGGNet , including the 5th pooling layer and all the fully connected layers .", "ner": [["VGGNet", "Method"], ["pooling layer", "Method"], ["fully connected layers", "Method"]], "rel": [["pooling layer", "Part-Of", "VGGNet"], ["fully connected layers", "Part-Of", "VGGNet"]], "rel_plus": [["pooling layer:Method", "Part-Of", "VGGNet:Method"], ["fully connected layers:Method", "Part-Of", "VGGNet:Method"]]}
{"doc_id": "6423078", "sentence": "Second , the fully connected layers ( even when recast as convolutions ) are computationally intensive , so that trimming layers from pool 5 on can significantly reduce the memory/time cost during both training and testing .", "ner": [["fully connected layers", "Method"], ["convolutions", "Method"]], "rel": [["convolutions", "Part-Of", "fully connected layers"]], "rel_plus": [["convolutions:Method", "Part-Of", "fully connected layers:Method"]]}
{"doc_id": "6423078", "sentence": "As we have discussed , while FCN reinterprets classification nets for per - pixel prediction , it has only one output loss function .", "ner": [["FCN reinterprets classification nets", "Method"], ["per - pixel prediction", "Task"]], "rel": [["FCN reinterprets classification nets", "Used-For", "per - pixel prediction"]], "rel_plus": [["FCN reinterprets classification nets:Method", "Used-For", "per - pixel prediction:Task"]]}
{"doc_id": "6423078", "sentence": "We first try to directly apply the FCN - 8 s model by replacing the loss function with cross - entropy loss for edge detection .", "ner": [["FCN - 8 s", "Method"], ["cross - entropy loss", "Method"], ["edge detection", "Task"]], "rel": [["cross - entropy loss", "Part-Of", "FCN - 8 s"], ["FCN - 8 s", "Used-For", "edge detection"]], "rel_plus": [["cross - entropy loss:Method", "Part-Of", "FCN - 8 s:Method"], ["FCN - 8 s:Method", "Used-For", "edge detection:Task"]]}
{"doc_id": "6423078", "sentence": "With heavy tweaking of FCN , there is a possibility that one might be able to achieve competitive performance on edge detection , but the multi - scale side - outputs in HED are seen to be natural and intuitive for edge detection .", "ner": [["FCN", "Method"], ["edge detection", "Task"], ["HED", "Method"], ["edge detection", "Task"]], "rel": [["FCN", "Used-For", "edge detection"], ["HED", "Used-For", "edge detection"]], "rel_plus": [["FCN:Method", "Used-For", "edge detection:Task"], ["HED:Method", "Used-For", "edge detection:Task"]]}
{"doc_id": "6423078", "sentence": "We experiment HED on four benchmark datasets including BSDS 5 0 0 ( Arbelaez et al. 2 0 1 1 ) , NYUD ( Silberman et al. 2 0 1 2 ) , Multicue - edge/boundary ( M\u00e9ly et al. 2 0 1 5 ) and PASCAL - Context ( Everingham et al. 2 0 1 4 ) .", "ner": [["HED", "Method"], ["BSDS 5 0 0", "Dataset"], ["NYUD", "Dataset"], ["Multicue - edge/boundary", "Dataset"], ["PASCAL - Context", "Dataset"]], "rel": [["HED", "Evaluated-With", "BSDS 5 0 0"], ["HED", "Evaluated-With", "NYUD"], ["HED", "Evaluated-With", "Multicue - edge/boundary"], ["HED", "Evaluated-With", "PASCAL - Context"]], "rel_plus": [["HED:Method", "Evaluated-With", "BSDS 5 0 0:Dataset"], ["HED:Method", "Evaluated-With", "NYUD:Dataset"], ["HED:Method", "Evaluated-With", "Multicue - edge/boundary:Dataset"], ["HED:Method", "Evaluated-With", "PASCAL - Context:Dataset"]]}
{"doc_id": "6423078", "sentence": "We implement our framework using the publicly available Caffe Library and build on top of the publicly available implementations of FCN ( Long et al. 2 0 1 5 ) and DSN ( Lee et al. 2 0 1 5 ) .", "ner": [["FCN", "Method"], ["DSN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "6423078", "sentence": "Model parameters In contrast to fine - tuning CNN for image classification or semantic segmentation , adapting CNN for low - level edge detection requires special care .", "ner": [["CNN", "Method"], ["image classification", "Task"], ["semantic segmentation", "Task"], ["CNN", "Method"], ["low - level edge detection", "Task"]], "rel": [["CNN", "Used-For", "image classification"], ["CNN", "Used-For", "semantic segmentation"], ["CNN", "Used-For", "low - level edge detection"]], "rel_plus": [["CNN:Method", "Used-For", "image classification:Task"], ["CNN:Method", "Used-For", "semantic segmentation:Task"], ["CNN:Method", "Used-For", "low - level edge detection:Task"]]}
{"doc_id": "6423078", "sentence": "As a proof - of - concept and sanity check , we train 5 independent networks induced from the five convolutional blocks of VGG - net .", "ner": [["convolutional blocks", "Method"], ["VGG - net", "Method"]], "rel": [["convolutional blocks", "Part-Of", "VGG - net"]], "rel_plus": [["convolutional blocks:Method", "Part-Of", "VGG - net:Method"]]}
{"doc_id": "6423078", "sentence": "We evaluate HED on the Berkeley Segmentation Dataset and Benchmark ( BSDS 5 0 0 ) ( Arbelaez et al. 2 0 1 1 ) .", "ner": [["HED", "Method"], ["Berkeley Segmentation Dataset and Benchmark", "Dataset"], ["BSDS 5 0 0", "Dataset"]], "rel": [["HED", "Evaluated-With", "Berkeley Segmentation Dataset and Benchmark"], ["BSDS 5 0 0", "Synonym-Of", "Berkeley Segmentation Dataset and Benchmark"], ["HED", "Evaluated-With", "BSDS 5 0 0"]], "rel_plus": [["HED:Method", "Evaluated-With", "Berkeley Segmentation Dataset and Benchmark:Dataset"], ["BSDS 5 0 0:Dataset", "Synonym-Of", "Berkeley Segmentation Dataset and Benchmark:Dataset"], ["HED:Method", "Evaluated-With", "BSDS 5 0 0:Dataset"]]}
{"doc_id": "6423078", "sentence": "HED - fusion ( DSN ) ( 4 0 0 \u00d7 4 0 0 ) refers to the HED algorithm reported in that resizes for all the images in the BSDS 5 0 0 dataset to a fixed size of 4 0 0 \u00d7 4 0 0 a Refers to results on the BSDS 3 0 0 datset in Martin et al. ( 2 0 0 4 ) b Indicates GPU time After our conference paper , we updated the training procedure for HED in two major ways .", "ner": [["HED - fusion", "Method"], ["DSN", "Method"], ["HED", "Method"], ["BSDS 5 0 0", "Dataset"], ["BSDS 3 0 0", "Dataset"], ["HED", "Method"]], "rel": [["DSN", "Part-Of", "HED - fusion"], ["HED - fusion", "Evaluated-With", "BSDS 5 0 0"], ["HED", "Evaluated-With", "BSDS 3 0 0"]], "rel_plus": [["DSN:Method", "Part-Of", "HED - fusion:Method"], ["HED - fusion:Method", "Evaluated-With", "BSDS 5 0 0:Dataset"], ["HED:Method", "Evaluated-With", "BSDS 3 0 0:Dataset"]]}
{"doc_id": "6423078", "sentence": "We also enhance data augmentation by scaling the training images to 5 0 , 1 0 0 , and 1 5 0 % of the original BSDS 5 0 0 images , resulting in triple the training data compared to .", "ner": [["data augmentation", "Method"], ["BSDS 5 0 0", "Dataset"]], "rel": [["data augmentation", "Used-For", "BSDS 5 0 0"]], "rel_plus": [["data augmentation:Method", "Used-For", "BSDS 5 0 0:Dataset"]]}
{"doc_id": "6423078", "sentence": "Figure 5 shows the precision - recall of the previous edge detection methods and HED on the BSDS 5 0 0 dataset ; Table 5 shows a detailed quantitative measures between these competing approaches .", "ner": [["edge detection", "Task"], ["HED", "Method"], ["BSDS 5 0 0", "Dataset"]], "rel": [["HED", "Used-For", "edge detection"], ["BSDS 5 0 0", "Benchmark-For", "edge detection"], ["HED", "Evaluated-With", "BSDS 5 0 0"]], "rel_plus": [["HED:Method", "Used-For", "edge detection:Task"], ["BSDS 5 0 0:Dataset", "Benchmark-For", "edge detection:Task"], ["HED:Method", "Evaluated-With", "BSDS 5 0 0:Dataset"]]}
{"doc_id": "6423078", "sentence": "The NYU Depth ( NYUD ) dataset ( Silberman et al. 2 0 1 2 ) has 1 4 4 9 RGB - D images .", "ner": [["NYU Depth", "Dataset"], ["NYUD", "Dataset"]], "rel": [["NYUD", "Synonym-Of", "NYU Depth"]], "rel_plus": [["NYUD:Dataset", "Synonym-Of", "NYU Depth:Dataset"]]}
{"doc_id": "6423078", "sentence": "There are several notable differences between the Multicue dataset and the BSDS 5 0 0 dataset : ( 1 ) Multicue captures complex scenes whereas BSDS 5 0 0 contains objectcentric images of relatively lower complexity . ( 2 ) Each frame in Multicue is of size 1 2 8 0 \u00d7 7 2 0 which is much higher than the 4 8 0 \u00d7 3 2 0 size in BSDS 5 0 0 . ( 3 ) Two sets of ground - truth annotations are obtained in Multicue : one accounting for object boundaries and another focusing on low - level edges .", "ner": [["Multicue", "Dataset"], ["BSDS 5 0 0", "Dataset"], ["Multicue", "Dataset"], ["BSDS 5 0 0", "Dataset"], ["Multicue", "Dataset"], ["BSDS 5 0 0", "Dataset"], ["Multicue", "Dataset"]], "rel": [["Multicue", "Compare-With", "BSDS 5 0 0"]], "rel_plus": [["Multicue:Dataset", "Compare-With", "BSDS 5 0 0:Dataset"]]}
{"doc_id": "6423078", "sentence": "We keep the same algorithmic and network settings for HED when training on BSDS 5 0 0 and on Multicue .", "ner": [["HED", "Method"], ["BSDS 5 0 0", "Dataset"], ["Multicue", "Dataset"]], "rel": [["HED", "Trained-With", "BSDS 5 0 0"], ["HED", "Trained-With", "Multicue"]], "rel_plus": [["HED:Method", "Trained-With", "BSDS 5 0 0:Dataset"], ["HED:Method", "Trained-With", "Multicue:Dataset"]]}
{"doc_id": "6423078", "sentence": "Qualitative results on some Multicue images are shown in the third group ( Multicue - edge ) and the fourth group ( Multicue - boundary ) in Fig. 7 , where each group consists of three example images receiving relatively high F - scores and one example ( lower - right ) observing a relatively low F - score .", "ner": [["Multicue", "Dataset"], ["Multicue - edge", "Dataset"], ["Multicue - boundary", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "6423078", "sentence": "In this section , we validate HED on another widely used computer vision benchmark , PASCAL - Context ( Mottaghi et al. 2 0 1 4 ) , which is an extension to the PASCAL VOC - 2 0 1 0 image segmentation dataset ( Everingham et al. 2 0 1 4 ) , in which 1 1 , 5 3 0 images are composed of a wide variety of object categories beyond the original 2 0 object classes in VOC 2 0 1 0 .", "ner": [["HED", "Method"], ["computer vision", "Task"], ["PASCAL - Context", "Dataset"], ["PASCAL VOC - 2 0 1 0 image segmentation", "Dataset"], ["VOC 2 0 1 0", "Dataset"]], "rel": [["HED", "Used-For", "computer vision"], ["PASCAL - Context", "Benchmark-For", "computer vision"], ["PASCAL VOC - 2 0 1 0 image segmentation", "Benchmark-For", "computer vision"], ["HED", "Evaluated-With", "PASCAL - Context"], ["HED", "Evaluated-With", "PASCAL VOC - 2 0 1 0 image segmentation"], ["VOC 2 0 1 0", "Used-For", "PASCAL VOC - 2 0 1 0 image segmentation"]], "rel_plus": [["HED:Method", "Used-For", "computer vision:Task"], ["PASCAL - Context:Dataset", "Benchmark-For", "computer vision:Task"], ["PASCAL VOC - 2 0 1 0 image segmentation:Dataset", "Benchmark-For", "computer vision:Task"], ["HED:Method", "Evaluated-With", "PASCAL - Context:Dataset"], ["HED:Method", "Evaluated-With", "PASCAL VOC - 2 0 1 0 image segmentation:Dataset"], ["VOC 2 0 1 0:Dataset", "Used-For", "PASCAL VOC - 2 0 1 0 image segmentation:Dataset"]]}
{"doc_id": "6423078", "sentence": "Recent work ( Yang et al. 2 0 1 6 ; Maninis et al. 2 0 1 6 ) shows edge detection results on the original PASCAL VOC dataset .", "ner": [["edge detection", "Task"], ["PASCAL VOC", "Dataset"]], "rel": [["PASCAL VOC", "Benchmark-For", "edge detection"]], "rel_plus": [["PASCAL VOC:Dataset", "Benchmark-For", "edge detection:Task"]]}
{"doc_id": "6423078", "sentence": "Similar to the NYUD dataset , due to the inconsistency of the annotations , we increase the matching tolerance to 0.0 1 1 while evaluating on PASCAL - Context .", "ner": [["NYUD", "Dataset"], ["PASCAL - Context", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "6423078", "sentence": "We show the cross - dataset evaluation results with BSDS - trained model and PASCAL - Context trained model in Table 8 .", "ner": [["BSDS", "Dataset"], ["PASCAL - Context", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "6423078", "sentence": "Inspired by the cross - dataset validation in Zhu et al. ( 2 0 1 5 ) ; Premachandran et al. ( 2 0 1 5 ) , we investigate how HED trained on BSDS 5 0 0 generalizes for detecting boundaries on PASCAL - Context and vice versa .", "ner": [["HED", "Method"], ["BSDS 5 0 0", "Dataset"], ["PASCAL - Context", "Dataset"]], "rel": [["HED", "Trained-With", "BSDS 5 0 0"], ["HED", "Evaluated-With", "PASCAL - Context"]], "rel_plus": [["HED:Method", "Trained-With", "BSDS 5 0 0:Dataset"], ["HED:Method", "Evaluated-With", "PASCAL - Context:Dataset"]]}
{"doc_id": "6423078", "sentence": "For the PASCAL - Context on PASCAL - Context experiment , we train the model for 8 0 k iterations .", "ner": [["PASCAL - Context", "Dataset"], ["PASCAL - Context", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "6423078", "sentence": "For the PASCAL - Context on BSDS 5 0 0 experiment , the results are evaluated with a model trained for 5 k iterations to avoid severe over - fitting ( in a sense of cross - dataset generalization ) to the strong object boundaries in PASCAL - Context .", "ner": [["PASCAL - Context", "Dataset"], ["BSDS 5 0 0", "Dataset"], ["PASCAL - Context", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "6423078", "sentence": "Edge detection results on PASCAL - Context dataset are generally worse than those on BSDS 5 0 0 .", "ner": [["Edge detection", "Task"], ["PASCAL - Context", "Dataset"], ["BSDS 5 0 0", "Dataset"]], "rel": [["PASCAL - Context", "Benchmark-For", "Edge detection"]], "rel_plus": [["PASCAL - Context:Dataset", "Benchmark-For", "Edge detection:Task"]]}
{"doc_id": "6423078", "sentence": "This shows PASCAL - Context is a more challenging dataset as an edge detection benchmark , partially due to the inconsistent and noisy annotations .", "ner": [["PASCAL - Context", "Dataset"], ["edge detection", "Task"]], "rel": [["PASCAL - Context", "Benchmark-For", "edge detection"]], "rel_plus": [["PASCAL - Context:Dataset", "Benchmark-For", "edge detection:Task"]]}
{"doc_id": "6423078", "sentence": "HED model trained on PASCAL - Context works surprisingly well on BSDS 5 0 0 ( ODS score 0. 7 7 8) , which suggests that features learned on semantic segmentation datasets can generalize well to the Fig. 7 HED results on four benchmark datasets including BSDS 5 0 0 ( Arbelaez et al. 2 0 1 1 ) , NYUD ( Silberman et al. 2 0 1 2 ) , Multicueedge/boundary ( M\u00e9ly et al. 2 0 1 5 ) , and PASCAL - Context ( Everingham et al. 2 0 1 4 ) .", "ner": [["HED", "Method"], ["PASCAL - Context", "Dataset"], ["BSDS 5 0 0", "Dataset"], ["semantic segmentation", "Task"], ["HED", "Method"], ["BSDS 5 0 0", "Dataset"], ["NYUD", "Dataset"], ["Multicueedge/boundary", "Dataset"], ["PASCAL - Context", "Dataset"]], "rel": [["HED", "Trained-With", "PASCAL - Context"], ["HED", "Evaluated-With", "BSDS 5 0 0"], ["PASCAL - Context", "Benchmark-For", "semantic segmentation"], ["HED", "Used-For", "semantic segmentation"], ["HED", "Evaluated-With", "BSDS 5 0 0"], ["HED", "Evaluated-With", "NYUD"], ["HED", "Evaluated-With", "Multicueedge/boundary"], ["HED", "Evaluated-With", "PASCAL - Context"]], "rel_plus": [["HED:Method", "Trained-With", "PASCAL - Context:Dataset"], ["HED:Method", "Evaluated-With", "BSDS 5 0 0:Dataset"], ["PASCAL - Context:Dataset", "Benchmark-For", "semantic segmentation:Task"], ["HED:Method", "Used-For", "semantic segmentation:Task"], ["HED:Method", "Evaluated-With", "BSDS 5 0 0:Dataset"], ["HED:Method", "Evaluated-With", "NYUD:Dataset"], ["HED:Method", "Evaluated-With", "Multicueedge/boundary:Dataset"], ["HED:Method", "Evaluated-With", "PASCAL - Context:Dataset"]]}
{"doc_id": "6423078", "sentence": "Our algorithm builds on top of the ideas of fully convolutional neural networks and deeply - supervised nets .", "ner": [["fully convolutional neural networks", "Method"], ["deeply - supervised nets", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "211010520", "sentence": "Innovations in annotation methodology have been a catalyst for Reading Comprehension ( RC ) datasets and models .", "ner": [["Reading Comprehension", "Task"], ["RC", "Task"]], "rel": [["RC", "Synonym-Of", "Reading Comprehension"]], "rel_plus": [["RC:Task", "Synonym-Of", "Reading Comprehension:Task"]]}
{"doc_id": "211010520", "sentence": "When trained on data collected with a BiDAF model in the loop , RoBERTa achieves 3 9 . 9 F 1 on questions that it can not answer when trained on SQuAD - only marginally lower than when trained on data collected using RoBERTa itself ( 4 1 . 0 F 1 ) .", "ner": [["BiDAF model in the loop", "Method"], ["RoBERTa", "Method"], ["SQuAD", "Dataset"], ["RoBERTa", "Method"]], "rel": [["RoBERTa", "Trained-With", "BiDAF model in the loop"], ["RoBERTa", "Trained-With", "SQuAD"]], "rel_plus": [["RoBERTa:Method", "Trained-With", "BiDAF model in the loop:Method"], ["RoBERTa:Method", "Trained-With", "SQuAD:Dataset"]]}
{"doc_id": "211010520", "sentence": "Data collection is a fundamental prerequisite for Machine Learning - based approaches to Natural Language Processing ( NLP ) .", "ner": [["Machine Learning - based approaches", "Method"], ["Natural Language Processing", "Task"], ["NLP", "Task"]], "rel": [["NLP", "Synonym-Of", "Natural Language Processing"], ["Machine Learning - based approaches", "Used-For", "Natural Language Processing"]], "rel_plus": [["NLP:Task", "Synonym-Of", "Natural Language Processing:Task"], ["Machine Learning - based approaches:Method", "Used-For", "Natural Language Processing:Task"]]}
{"doc_id": "211010520", "sentence": "Examples of this method are realised in Build It Break It , The Language Edition ( Ettinger et al. , 2 0 1 7 ) , SWAG ( Zellers et al. , 2 0 1 8) , HotpotQA ( Yang et al. , 2 0 1 8) , DROP ( Dua et al. , 2 0 1 9 ) , CODAH ( Chen et al. , 2 0 1 9 ) , Quoref ( Dasigi et al. , 2 0 1 9 ) and Adversar - ialNLI ( Nie et al. , 2 0 1 9 ) . 1 The practice probes model robustness and ensures that the resulting datasets pose a challenge to current models , in turn driving research and modelling efforts to tackle the new problem set .", "ner": [["Build It Break It", "Dataset"], ["SWAG", "Dataset"], ["HotpotQA", "Dataset"], ["DROP", "Dataset"], ["CODAH", "Dataset"], ["Quoref", "Dataset"], ["Adversar - ialNLI", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "211010520", "sentence": "Compared to training on SQuAD , training on adversarially composed questions leads to a similar degree of generalisation to non - adversarially written questions , both for SQuAD and NaturalQuestions ( Kwiatkowski et al. , 2 0 1 9 ) .", "ner": [["SQuAD", "Dataset"], ["SQuAD", "Dataset"], ["NaturalQuestions", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "211010520", "sentence": "It furthermore leads to general improvements across the model - in - the - loop datasets we collected , as well as improvements of more than 2 0 . 0 F 1 for both BERT and RoBERTa on an extractive subset of DROP ( Dua et al. , 2 0 1 9 ) , another adversarially composed dataset .", "ner": [["model - in - the - loop", "Method"], ["BERT", "Method"], ["RoBERTa", "Method"], ["DROP", "Dataset"]], "rel": [["RoBERTa", "Evaluated-With", "DROP"], ["BERT", "Evaluated-With", "DROP"]], "rel_plus": [["RoBERTa:Method", "Evaluated-With", "DROP:Dataset"], ["BERT:Method", "Evaluated-With", "DROP:Dataset"]]}
{"doc_id": "211010520", "sentence": "An investigation into the model - in - the - loop approach to RC data collection based on three progressively stronger RC models . 2 .", "ner": [["model - in - the - loop", "Method"], ["RC", "Task"], ["RC", "Task"]], "rel": [["model - in - the - loop", "Used-For", "RC"]], "rel_plus": [["model - in - the - loop:Method", "Used-For", "RC:Task"]]}
{"doc_id": "211010520", "sentence": "It has thus become an increasingly important consideration to construct datasets which RC models find both challenging , and for which natural language understanding is a requisite for generalisation .", "ner": [["RC", "Task"], ["natural language understanding", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "211010520", "sentence": "While Dua et al. ( 2 0 1 9 ) and Dasigi et al. ( 2 0 1 9 ) make use of adversarial annotations for RC , both annotation setups limit the reach of the model - in - the - loop : in DROP , primarily due to the imposition of specific answer types , and in Quoref by focusing on co - reference , which is already a known RC model weakness .", "ner": [["RC", "Task"], ["model - in - the - loop", "Method"], ["DROP", "Dataset"], ["Quoref", "Dataset"], ["co - reference", "Task"], ["RC", "Task"]], "rel": [["Quoref", "Benchmark-For", "co - reference"]], "rel_plus": [["Quoref:Dataset", "Benchmark-For", "co - reference:Task"]]}
{"doc_id": "211010520", "sentence": "We will compare dataset annotation with a series of three progressively stronger models as adversary in the loop , namely BiDAF ( Seo et al. , 2 0 1 7 ) , BERT ( Devlin et al. , 2 0 1 9 ) and RoBERTa ( Liu et al. , 2 0 1 9 ) .", "ner": [["BiDAF", "Method"], ["BERT", "Method"], ["RoBERTa", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "211010520", "sentence": "Each of these will serve as a model adversary in a separate annotation experiment and result in separate datasets ; we will refer to these as D BiDAF , D BERT and D RoBERTa , respectively .", "ner": [["D BiDAF", "Dataset"], ["D BERT", "Dataset"], ["D RoBERTa", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "211010520", "sentence": "We rely on the AllenNLP ( Gardner et al. , 2 0 1 7 ) and Transformers ( Wolf et al. , 2 0 1 9 ) model implementations , and our models achieve EM/F 1 scores of 6 5 . 5 %/ 7 7 . 5 % , 8 2 . 7 %/ 9 0 . 3 % and 8 6 . 9 %/ 9 3 . 6 % for BiDAF , BERT and RoBERTa , respectively on the SQuAD 1 . 1 validation set .", "ner": [["AllenNLP", "Method"], ["Transformers", "Method"], ["BiDAF", "Method"], ["BERT", "Method"], ["RoBERTa", "Method"], ["SQuAD 1 . 1 validation", "Dataset"]], "rel": [["BiDAF", "Evaluated-With", "SQuAD 1 . 1 validation"], ["BERT", "Evaluated-With", "SQuAD 1 . 1 validation"], ["RoBERTa", "Evaluated-With", "SQuAD 1 . 1 validation"]], "rel_plus": [["BiDAF:Method", "Evaluated-With", "SQuAD 1 . 1 validation:Dataset"], ["BERT:Method", "Evaluated-With", "SQuAD 1 . 1 validation:Dataset"], ["RoBERTa:Method", "Evaluated-With", "SQuAD 1 . 1 validation:Dataset"]]}
{"doc_id": "211010520", "sentence": "Since SQuAD 1 . 1 validation set questions commonly have multiple answers and the standard SQuAD 1 . 1 evaluation method involves taking the maximum score over all possible answers , we enforce an additional evaluation constraint by taking the majority vote answer as ground truth for SQuAD 1 . 1 .", "ner": [["SQuAD 1 . 1 validation", "Dataset"], ["SQuAD 1 . 1", "Dataset"], ["SQuAD 1 . 1", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "211010520", "sentence": "For clarity , we will hereafter refer to this modified version of SQuAD 1 . 1 as D SQuAD .", "ner": [["SQuAD 1 . 1", "Dataset"], ["D SQuAD", "Dataset"]], "rel": [["D SQuAD", "Synonym-Of", "SQuAD 1 . 1"]], "rel_plus": [["D SQuAD:Dataset", "Synonym-Of", "SQuAD 1 . 1:Dataset"]]}
{"doc_id": "211010520", "sentence": "The mean HIT completion times for BiDAF , BERT and RoBERTa are 5 5 1 . 8 s , 7 2 2 . 4 s and 6 8 6 . 4 s respectively .", "ner": [["HIT completion", "Task"], ["BiDAF", "Method"], ["BERT", "Method"], ["RoBERTa", "Method"]], "rel": [["BiDAF", "Used-For", "HIT completion"], ["BERT", "Used-For", "HIT completion"], ["RoBERTa", "Used-For", "HIT completion"]], "rel_plus": [["BiDAF:Method", "Used-For", "HIT completion:Task"], ["BERT:Method", "Used-For", "HIT completion:Task"], ["RoBERTa:Method", "Used-For", "HIT completion:Task"]]}
{"doc_id": "211010520", "sentence": "Furthermore we find that human workers are able to generate questions which successfully \" beat \" the model in the loop 5 9 . 4 % of the time for BiDAF , 4 7 . 1 % for BERT and 4 4 . 0 % for RoBERTa .", "ner": [["model in the loop", "Method"], ["BiDAF", "Method"], ["BERT", "Method"], ["RoBERTa", "Method"]], "rel": [["BiDAF", "Part-Of", "model in the loop"], ["BERT", "Part-Of", "model in the loop"], ["RoBERTa", "Part-Of", "model in the loop"]], "rel_plus": [["BiDAF:Method", "Part-Of", "model in the loop:Method"], ["BERT:Method", "Part-Of", "model in the loop:Method"], ["RoBERTa:Method", "Part-Of", "model in the loop:Method"]]}
{"doc_id": "211010520", "sentence": "Workers are asked to highlight two answers for provided questions , generate two questions for provided answers , generate one full question - answer pair , and finally complete a question generation HIT with BiDAF as the model in the loop .", "ner": [["question generation HIT", "Task"], ["BiDAF", "Method"], ["model in the loop", "Method"]], "rel": [["model in the loop", "Used-For", "question generation HIT"], ["BiDAF", "Used-For", "question generation HIT"], ["BiDAF", "Part-Of", "model in the loop"]], "rel_plus": [["model in the loop:Method", "Used-For", "question generation HIT:Task"], ["BiDAF:Method", "Used-For", "question generation HIT:Task"], ["BiDAF:Method", "Part-Of", "model in the loop:Method"]]}
{"doc_id": "211010520", "sentence": "We conduct answerability checks on both the validation and test sets and achieve answerability scores of 8 7 . 9 5 % , 8 5 . 4 1 % and 8 2 . 6 3 % for D BiDAF , D BERT and D RoBERTa , respectively .", "ner": [["D BiDAF", "Dataset"], ["D BERT", "Dataset"], ["D RoBERTa", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "211010520", "sentence": "That is , on average there is a trigram overlap between the passage and question for D SQuAD , but only a bigram overlap for D RoBERTa ( Figure 3 This is in line with prior observations on lexical overlap as a predictive cue in SQuAD ( Weissenborn et al. , 2 0 1 7 ; Min et al. , 2 0 1 8) ; questions with less overlap are harder to answer for any of the three models .", "ner": [["D SQuAD", "Dataset"], ["D RoBERTa", "Dataset"], ["SQuAD", "Dataset"]], "rel": [["D SQuAD", "Compare-With", "D RoBERTa"]], "rel_plus": [["D SQuAD:Dataset", "Compare-With", "D RoBERTa:Dataset"]]}
{"doc_id": "211010520", "sentence": "For further dataset statistics on this , see Appendix A. While D BiDAF , D BERT and D RoBERTa were created for the investigation and analysis of human - sourced adversarial examples in a model - in - the - loop setting for RC , we recognise their potential value to the community and plan to release all three training and validation sets publicly .   We begin with an experiment about the consistency of the adversarial nature of the models in the annotation loop .", "ner": [["D BiDAF", "Dataset"], ["D BERT", "Dataset"], ["D RoBERTa", "Dataset"], ["model - in - the - loop", "Method"], ["RC", "Task"]], "rel": [["model - in - the - loop", "Evaluated-With", "D BiDAF"], ["model - in - the - loop", "Evaluated-With", "D BERT"], ["model - in - the - loop", "Evaluated-With", "D RoBERTa"], ["model - in - the - loop", "Used-For", "RC"]], "rel_plus": [["model - in - the - loop:Method", "Evaluated-With", "D BiDAF:Dataset"], ["model - in - the - loop:Method", "Evaluated-With", "D BERT:Dataset"], ["model - in - the - loop:Method", "Evaluated-With", "D RoBERTa:Dataset"], ["model - in - the - loop:Method", "Used-For", "RC:Task"]]}
{"doc_id": "211010520", "sentence": "We next conduct a series of experiments in which we train on D BiDAF , D BERT , and D RoBERTa , and observe how well models can then learn to generalise on the respective test portions of these datasets .", "ner": [["D BiDAF", "Dataset"], ["D BERT", "Dataset"], ["D RoBERTa", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "211010520", "sentence": "For example , RoBERTa trained on D RoBERTa achieves 7 1 . 4 , 5 3 . 5 , 4 8 . 6 and 3 8 . 9 F 1 when evaluated on D SQuAD , D BiDAF , D BERT and D RoBERTa , respectively .", "ner": [["RoBERTa", "Method"], ["D RoBERTa", "Dataset"], ["D SQuAD", "Dataset"], ["D BiDAF", "Dataset"], ["D BERT", "Dataset"], ["D RoBERTa", "Dataset"]], "rel": [["RoBERTa", "Trained-With", "D RoBERTa"], ["RoBERTa", "Evaluated-With", "D SQuAD"], ["RoBERTa", "Evaluated-With", "D BiDAF"], ["RoBERTa", "Evaluated-With", "D BERT"], ["RoBERTa", "Evaluated-With", "D RoBERTa"]], "rel_plus": [["RoBERTa:Method", "Trained-With", "D RoBERTa:Dataset"], ["RoBERTa:Method", "Evaluated-With", "D SQuAD:Dataset"], ["RoBERTa:Method", "Evaluated-With", "D BiDAF:Dataset"], ["RoBERTa:Method", "Evaluated-With", "D BERT:Dataset"], ["RoBERTa:Method", "Evaluated-With", "D RoBERTa:Dataset"]]}
{"doc_id": "211010520", "sentence": "Both when training only on D BiDAF , as well as when adding D SQuAD to D BiDAF during training ( cf .", "ner": [["D BiDAF", "Dataset"], ["D SQuAD", "Dataset"], ["D BiDAF", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "211010520", "sentence": "In contrast , BERT and RoBERTa are able to partially overcome their blind spots through training on data collected with a model in the annotation loop , and to a degree that far exceeds what one would expect from random retraining ( cf .", "ner": [["BERT", "Method"], ["RoBERTa", "Method"]], "rel": [["BERT", "Compare-With", "RoBERTa"]], "rel_plus": [["BERT:Method", "Compare-With", "RoBERTa:Method"]]}
{"doc_id": "211010520", "sentence": "For example , RoBERTa trained on D RoBERTa reaches 3 8 . 9 F 1 on D RoBERTa , and this Next , we observe that training on D S where S is a stronger model helps generalise to D W with a weaker RC model W , e.g. training on D RoBERTa and testing on D BERT .", "ner": [["RoBERTa", "Method"], ["D RoBERTa", "Dataset"], ["D RoBERTa", "Dataset"], ["RC", "Task"], ["D RoBERTa", "Dataset"], ["D BERT", "Dataset"]], "rel": [["RoBERTa", "Trained-With", "D RoBERTa"], ["RoBERTa", "Evaluated-With", "D RoBERTa"]], "rel_plus": [["RoBERTa:Method", "Trained-With", "D RoBERTa:Dataset"], ["RoBERTa:Method", "Evaluated-With", "D RoBERTa:Dataset"]]}
{"doc_id": "211010520", "sentence": "But on the other hand , training on D W also leads to generalisation towards D S : for example , the baseline of RoBERTa trained on 1 0 , 0 0 0 SQuAD samples reaches 2 2 . 1 F 1 on D RoBERTa ( D S ) , whereas training RoBERTa on D BiDAF and D BERT ( D W ) bumps this number to 3 6 . 0 F 1 and 3 4 . 6 F 1 , respectively .", "ner": [["RoBERTa", "Method"], ["SQuAD", "Dataset"], ["D RoBERTa", "Dataset"], ["RoBERTa", "Method"], ["D BiDAF", "Method"], ["D BERT", "Method"]], "rel": [["RoBERTa", "Trained-With", "SQuAD"], ["RoBERTa", "Evaluated-With", "D RoBERTa"], ["RoBERTa", "Trained-With", "D BiDAF"], ["RoBERTa", "Trained-With", "D BERT"]], "rel_plus": [["RoBERTa:Method", "Trained-With", "SQuAD:Dataset"], ["RoBERTa:Method", "Evaluated-With", "D RoBERTa:Dataset"], ["RoBERTa:Method", "Trained-With", "D BiDAF:Method"], ["RoBERTa:Method", "Trained-With", "D BERT:Method"]]}
{"doc_id": "211010520", "sentence": "In this training setup we generally see improved generalisation to D BiDAF , D BERT , and D RoBERTa .", "ner": [["D BiDAF", "Dataset"], ["D BERT", "Dataset"], ["D RoBERTa", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "211010520", "sentence": "Interestingly , the relative differences between D BiDAF , D BERT , and D RoBERTa as training set used in conjunction with SQuAD are now much diminished , and especially D RoBERTa as ( part of the ) training set now generalises substantially better .", "ner": [["D BiDAF", "Dataset"], ["D BERT", "Dataset"], ["D RoBERTa", "Dataset"], ["SQuAD", "Dataset"], ["D RoBERTa", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "211010520", "sentence": "RoBERTa achieves the strongest results on any of the D BiDAF , D BERT , and D RoBERTa evaluation sets , in particular when trained on D SQuAD + D RoBERTa .", "ner": [["RoBERTa", "Method"], ["D BiDAF", "Dataset"], ["D BERT", "Dataset"], ["D RoBERTa", "Dataset"], ["D SQuAD + D RoBERTa", "Dataset"]], "rel": [["RoBERTa", "Evaluated-With", "D BiDAF"], ["RoBERTa", "Evaluated-With", "D BERT"], ["RoBERTa", "Evaluated-With", "D RoBERTa"], ["RoBERTa", "Trained-With", "D SQuAD + D RoBERTa"]], "rel_plus": [["RoBERTa:Method", "Evaluated-With", "D BiDAF:Dataset"], ["RoBERTa:Method", "Evaluated-With", "D BERT:Dataset"], ["RoBERTa:Method", "Evaluated-With", "D RoBERTa:Dataset"], ["RoBERTa:Method", "Trained-With", "D SQuAD + D RoBERTa:Dataset"]]}
{"doc_id": "211010520", "sentence": "This stands in contrast to the previous results in Table 5 , where training on D BiDAF in several cases led to better generalisation than training on D RoBERTa .", "ner": [["D BiDAF", "Dataset"], ["D RoBERTa", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "211010520", "sentence": "A possible explanation for this observation is that training on D RoBERTa leads to a larger degree of adversarial overfitting than training on D BiDAF , and the inclusion of a large number of standard SQuAD training samples can mitigate this effect .", "ner": [["D RoBERTa", "Dataset"], ["D BiDAF", "Dataset"], ["SQuAD", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "211010520", "sentence": "For example , RoBERTa achieves 5 8 . 2 EM/ 7 3 . 2 F 1 on D BiDAF , in contrast to 0.0 EM/ 5 . 5 F 1 for BiDAF -which is not far from non - expert human performance of 6 2 . 6 EM/ 7 8 . 5 F 1 .", "ner": [["RoBERTa", "Method"], ["D BiDAF", "Dataset"], ["BiDAF", "Method"]], "rel": [["RoBERTa", "Evaluated-With", "D BiDAF"], ["BiDAF", "Evaluated-With", "D BiDAF"], ["RoBERTa", "Compare-With", "BiDAF"]], "rel_plus": [["RoBERTa:Method", "Evaluated-With", "D BiDAF:Dataset"], ["BiDAF:Method", "Evaluated-With", "D BiDAF:Dataset"], ["RoBERTa:Method", "Compare-With", "BiDAF:Method"]]}
{"doc_id": "211010520", "sentence": "To measure this we further train each of our three models on either D BiDAF , D BERT , or D RoBERTa and test on D SQuAD , with results in the D SQuAD columns of Table 5 .", "ner": [["D BiDAF", "Dataset"], ["D BERT", "Dataset"], ["D RoBERTa", "Dataset"], ["D SQuAD", "Dataset"], ["D SQuAD", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "211010520", "sentence": "For comparison , the models are also trained on 1 0 , 0 0 0 SQuAD 1 . 1 samples ( referred to as D SQuAD( 1 0 K ) ) chosen from the same passages as the adversarial datasets , thus eliminating size and paragraph choice as potential confounding factors .", "ner": [["SQuAD 1 . 1", "Dataset"], ["D SQuAD( 1 0 K )", "Dataset"]], "rel": [["SQuAD 1 . 1", "Synonym-Of", "D SQuAD( 1 0 K )"]], "rel_plus": [["SQuAD 1 . 1:Dataset", "Synonym-Of", "D SQuAD( 1 0 K ):Dataset"]]}
{"doc_id": "211010520", "sentence": "The models are tuned for Exact Match ( EM ) on our held - out validation data derived from the split SQuAD 1 . 1 validation set after applying majority vote ( D SQuAD -dev ) .", "ner": [["Exact Match", "Method"], ["EM", "Method"], ["SQuAD 1 . 1 validation", "Dataset"], ["D SQuAD -dev", "Dataset"]], "rel": [["EM", "Synonym-Of", "Exact Match"]], "rel_plus": [["EM:Method", "Synonym-Of", "Exact Match:Method"]]}
{"doc_id": "211010520", "sentence": "Remarkably , neither BERT or RoBERTa show a substantial drop when trained on D BiDAF compared to training on SQuAD data ( \u2212 2 . 0 F 1 , and \u2212 3 . 3 F 1 ): training these models on a dataset with a weaker model in the loop still leads to strong generalisation even to data from the original SQuAD distribution , which all models in the loop are trained on .", "ner": [["BERT", "Method"], ["RoBERTa", "Method"], ["D BiDAF", "Dataset"], ["SQuAD", "Dataset"], ["SQuAD", "Dataset"]], "rel": [["BERT", "Trained-With", "D BiDAF"], ["RoBERTa", "Trained-With", "D BiDAF"], ["BERT", "Trained-With", "SQuAD"], ["RoBERTa", "Trained-With", "SQuAD"], ["D BiDAF", "Compare-With", "SQuAD"]], "rel_plus": [["BERT:Method", "Trained-With", "D BiDAF:Dataset"], ["RoBERTa:Method", "Trained-With", "D BiDAF:Dataset"], ["BERT:Method", "Trained-With", "SQuAD:Dataset"], ["RoBERTa:Method", "Trained-With", "SQuAD:Dataset"], ["D BiDAF:Dataset", "Compare-With", "SQuAD:Dataset"]]}
{"doc_id": "211010520", "sentence": "BiDAF , on the other hand , fails to learn such information from the adversarially collected data , and drops > 3 0 F 1 for each of the new training sets , compared to training on SQuAD .", "ner": [["BiDAF", "Method"], ["SQuAD", "Dataset"]], "rel": [["BiDAF", "Trained-With", "SQuAD"]], "rel_plus": [["BiDAF:Method", "Trained-With", "SQuAD:Dataset"]]}
{"doc_id": "211010520", "sentence": "We furthermore observe a gradual decrease in generalisation to SQuAD when training on D BiDAF towards training on D RoBERTa .", "ner": [["SQuAD", "Dataset"], ["D BiDAF", "Dataset"], ["D RoBERTa", "Dataset"]], "rel": [["D BiDAF", "Trained-With", "D RoBERTa"]], "rel_plus": [["D BiDAF:Dataset", "Trained-With", "D RoBERTa:Dataset"]]}
{"doc_id": "211010520", "sentence": "It may however also be due to a limitation of BERT and RoBERTa -similar to BiDAF -in learning from a data distribution designed to beat these models ; an even stronger model might learn more e.g. from D RoBERTa .", "ner": [["BERT", "Method"], ["RoBERTa", "Method"], ["BiDAF", "Method"], ["D RoBERTa", "Dataset"]], "rel": [["BERT", "Compare-With", "BiDAF"], ["RoBERTa", "Compare-With", "BiDAF"]], "rel_plus": [["BERT:Method", "Compare-With", "BiDAF:Method"], ["RoBERTa:Method", "Compare-With", "BiDAF:Method"]]}
{"doc_id": "211010520", "sentence": "Finally , we investigate to what extent models can transfer skills learned on datasets created with a model in the loop to other datasets , concretely DROP and NaturalQuestions .", "ner": [["model in the loop", "Method"], ["DROP", "Dataset"], ["NaturalQuestions", "Dataset"]], "rel": [["model in the loop", "Evaluated-With", "DROP"], ["model in the loop", "Evaluated-With", "NaturalQuestions"]], "rel_plus": [["model in the loop:Method", "Evaluated-With", "DROP:Dataset"], ["model in the loop:Method", "Evaluated-With", "NaturalQuestions:Dataset"]]}
{"doc_id": "211010520", "sentence": "In this experiment we select the subsets of DROP and NaturalQuestions which align with the structural constraints of SQuAD to ensure a like - for - like analysis .", "ner": [["DROP", "Dataset"], ["NaturalQuestions", "Dataset"], ["SQuAD", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "211010520", "sentence": "Next we split it , stratifying by passage ( as we did for D SQuAD ) , which results in 1 4 0 9 / 1 4 1 8 validation and test set examples for DROP , and 9 6 4 / 9 8 2 for NaturalQuestions , respectively .", "ner": [["D SQuAD", "Dataset"], ["DROP", "Dataset"], ["NaturalQuestions", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "211010520", "sentence": "We denote these datasets as D DROP and D NQ for clarity and distinction from their unfiltered versions .", "ner": [["D DROP", "Dataset"], ["D NQ", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "211010520", "sentence": "We consider the same models and training datasets as before , but tune on the respective validation set portions of D DROP and D NQ .", "ner": [["D DROP", "Dataset"], ["D NQ", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "211010520", "sentence": "In Table 5 we can see the results of these experiments in the respective D DROP and D NQ columns .", "ner": [["D DROP", "Dataset"], ["D NQ", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "211010520", "sentence": "First , we observe clear generalisation improvements towards D DROP across all models compared to training on D SQuAD( 1 0 K ) when using any of the D BiDAF , D BERT , or D RoBERTa datasets for training .", "ner": [["D DROP", "Dataset"], ["D SQuAD( 1 0 K )", "Dataset"], ["D BiDAF", "Dataset"], ["D BERT", "Dataset"], ["D RoBERTa", "Dataset"]], "rel": [["D DROP", "Compare-With", "D SQuAD( 1 0 K )"]], "rel_plus": [["D DROP:Dataset", "Compare-With", "D SQuAD( 1 0 K ):Dataset"]]}
{"doc_id": "211010520", "sentence": "Note that the DROP dataset also makes use of a BiDAF model in the loop during annotation ; these results are in line with our prior observations when testing the same setups on D BiDAF , D BERT and D RoBERTa , compared to training on D SQuAD( 1 0 K ) .", "ner": [["DROP", "Dataset"], ["BiDAF model in the loop", "Method"], ["D BiDAF", "Dataset"], ["D BERT", "Dataset"], ["D RoBERTa", "Dataset"], ["D SQuAD( 1 0 K )", "Dataset"]], "rel": [["BiDAF model in the loop", "Used-For", "DROP"], ["D BiDAF", "Compare-With", "D SQuAD( 1 0 K )"], ["D BERT", "Compare-With", "D SQuAD( 1 0 K )"], ["D RoBERTa", "Compare-With", "D SQuAD( 1 0 K )"]], "rel_plus": [["BiDAF model in the loop:Method", "Used-For", "DROP:Dataset"], ["D BiDAF:Dataset", "Compare-With", "D SQuAD( 1 0 K ):Dataset"], ["D BERT:Dataset", "Compare-With", "D SQuAD( 1 0 K ):Dataset"], ["D RoBERTa:Dataset", "Compare-With", "D SQuAD( 1 0 K ):Dataset"]]}
{"doc_id": "211010520", "sentence": "Second , we observe overall strong transfer results towards D NQ : up to 7 1 . 0 F 1 for a BERT model trained on D BiDAF .", "ner": [["D NQ", "Dataset"], ["BERT", "Method"], ["D BiDAF", "Dataset"]], "rel": [["BERT", "Evaluated-With", "D NQ"], ["BERT", "Trained-With", "D BiDAF"]], "rel_plus": [["BERT:Method", "Evaluated-With", "D NQ:Dataset"], ["BERT:Method", "Trained-With", "D BiDAF:Dataset"]]}
{"doc_id": "211010520", "sentence": "That is , relative to training on SQuAD data , training on adversarially collected data D BiDAF does not impede generalisation to the D NQ dataset , which was created without a model in the annotation loop .", "ner": [["SQuAD", "Dataset"], ["D BiDAF", "Dataset"], ["D NQ", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "211010520", "sentence": "As reference points we also include the original SQuAD questions , as well as DROP and NaturalQuestions in this comparison : these datasets are both constructed to overcome limitations in SQuAD and have subsets which overlap sufficiently with SQuAD to make analysis possible .", "ner": [["SQuAD", "Dataset"], ["DROP", "Dataset"], ["NaturalQuestions", "Dataset"], ["SQuAD", "Dataset"], ["SQuAD", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "211010520", "sentence": "This number decreases substantially for any of the model - in - theloop datasets derived from SQuAD ( e.g. 8% for D BiDAF ) and also D DROP , yet 4 2 % of questions in D NQ share this property .", "ner": [["model - in - theloop", "Method"], ["SQuAD", "Dataset"], ["D BiDAF", "Dataset"], ["D DROP", "Dataset"], ["D NQ", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "211010520", "sentence": "Some of these more particular inference types are common features of the other two datasets , in particular comparative questions for DROP ( 6 0 % ) and to a small extent also NaturalQuestions .", "ner": [["DROP", "Dataset"], ["NaturalQuestions", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "211010520", "sentence": "Inter - estingly , D BiDAF possess the largest amount of comparison questions ( 1 1 % ) among our model - inthe - loop datasets , whereas D BERT and D RoBERTa only possess 1% and 3% , respectively .", "ner": [["D BiDAF", "Dataset"], ["model - inthe - loop", "Method"], ["D BERT", "Dataset"], ["D RoBERTa", "Dataset"]], "rel": [["D BiDAF", "Compare-With", "D BERT"], ["D BiDAF", "Compare-With", "D RoBERTa"]], "rel_plus": [["D BiDAF:Dataset", "Compare-With", "D BERT:Dataset"], ["D BiDAF:Dataset", "Compare-With", "D RoBERTa:Dataset"]]}
{"doc_id": "211010520", "sentence": "This offers an explanation for our previous observation in Table 5 , where models trained on D BiDAF outperformed those trained on D BERT or D RoBERTa when evaluated on D DROP .", "ner": [["D BiDAF", "Dataset"], ["D BERT", "Dataset"], ["D RoBERTa", "Dataset"]], "rel": [["D BiDAF", "Compare-With", "D BERT"], ["D BiDAF", "Compare-With", "D RoBERTa"]], "rel_plus": [["D BiDAF:Dataset", "Compare-With", "D BERT:Dataset"], ["D BiDAF:Dataset", "Compare-With", "D RoBERTa:Dataset"]]}
{"doc_id": "211010520", "sentence": "It is likely that BiDAF as a model in the loop is worse than BERT and RoBERTa at comparative questions , as evidenced by the results in Table 5 with BiDAF reaching 9. 3 F 1 and RoBERTa reaching 3 0 . 9 F 1 on D DROP ( when trained on D SQuAD( 1 0 K ) ) .", "ner": [["BiDAF", "Method"], ["model in the loop", "Method"], ["BERT", "Method"], ["RoBERTa", "Method"], ["BiDAF", "Method"], ["RoBERTa", "Method"], ["D DROP", "Dataset"], ["D SQuAD( 1 0 K )", "Dataset"]], "rel": [["BiDAF", "Part-Of", "model in the loop"], ["BERT", "Part-Of", "model in the loop"], ["RoBERTa", "Part-Of", "model in the loop"], ["BiDAF", "Compare-With", "BERT"], ["BiDAF", "Compare-With", "RoBERTa"], ["BiDAF", "Compare-With", "RoBERTa"], ["BiDAF", "Evaluated-With", "D DROP"], ["RoBERTa", "Evaluated-With", "D DROP"], ["BiDAF", "Trained-With", "D SQuAD( 1 0 K )"], ["RoBERTa", "Trained-With", "D SQuAD( 1 0 K )"]], "rel_plus": [["BiDAF:Method", "Part-Of", "model in the loop:Method"], ["BERT:Method", "Part-Of", "model in the loop:Method"], ["RoBERTa:Method", "Part-Of", "model in the loop:Method"], ["BiDAF:Method", "Compare-With", "BERT:Method"], ["BiDAF:Method", "Compare-With", "RoBERTa:Method"], ["BiDAF:Method", "Compare-With", "RoBERTa:Method"], ["BiDAF:Method", "Evaluated-With", "D DROP:Dataset"], ["RoBERTa:Method", "Evaluated-With", "D DROP:Dataset"], ["BiDAF:Method", "Trained-With", "D SQuAD( 1 0 K ):Dataset"], ["RoBERTa:Method", "Trained-With", "D SQuAD( 1 0 K ):Dataset"]]}
{"doc_id": "211010520", "sentence": "The distribution of NaturalQuestions contains elements of both the distribution of SQuAD and of D BiDAF , which offers a potential explanation for the strong performance of models trained on D SQuAD( 1 0 K ) and D BiDAF on D NQ .", "ner": [["NaturalQuestions", "Dataset"], ["SQuAD", "Dataset"], ["D BiDAF", "Dataset"], ["D SQuAD( 1 0 K )", "Dataset"], ["D BiDAF", "Dataset"], ["D NQ", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "211010520", "sentence": "Finally , the gradually shifting distribution away from both SQuAD and NaturalQuestions as the modelin - the - loop strength increases reflects our prior observations on the decreasing performance on SQuAD and NaturalQuestions of models trained on datasets with progressively stronger models in the annotation loop .", "ner": [["SQuAD", "Dataset"], ["NaturalQuestions", "Dataset"], ["modelin - the - loop", "Method"], ["SQuAD", "Dataset"], ["NaturalQuestions", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "211010520", "sentence": "Applying this approach with a series of progressively stronger RC models in the annotation loop , we arrived at three separate RC datasets , graduated by the difficulty of the model adversary .", "ner": [["RC", "Task"], ["RC", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "211010520", "sentence": "Models trained on data collected with a model in the loop furthermore generalise well towards nonadversarially collected data , both on SQuAD and on NaturalQuestions , yet we observe a slow deterioration with progressively stronger adversaries .", "ner": [["SQuAD", "Dataset"], ["NaturalQuestions", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "211010520", "sentence": "While the scope of this paper is focused on RC , with SQuAD as the original dataset used to train model adversaries , we see no reason in principle why similar findings would not be made for other tasks using the same annotation paradigm , when crowdsourcing the creation of challenging samples with a current model in the loop .", "ner": [["RC", "Task"], ["SQuAD", "Dataset"]], "rel": [["SQuAD", "Benchmark-For", "RC"]], "rel_plus": [["SQuAD:Dataset", "Benchmark-For", "RC:Task"]]}
{"doc_id": "211010520", "sentence": "In total , 1, 3 8 6 workers completed this task with 7 5 2 being assigned the qualification . \" Beat the AI Annotation \" The \" Beat the AI \" question generation HIT presents workers with a randomly selected passage from SQuAD 1 . 1 , about which workers are expected to generate questions and provide answers .", "ner": [["question generation HIT", "Task"], ["SQuAD 1 . 1", "Dataset"]], "rel": [["SQuAD 1 . 1", "Benchmark-For", "question generation HIT"]], "rel_plus": [["SQuAD 1 . 1:Dataset", "Benchmark-For", "question generation HIT:Task"]]}
{"doc_id": "208202241", "sentence": "Recently , graph convolution network ( GCN ) is leveraged to boost the performance of multi - label recognition .", "ner": [["graph convolution network", "Method"], ["GCN", "Method"], ["multi - label recognition", "Task"]], "rel": [["GCN", "Synonym-Of", "graph convolution network"], ["graph convolution network", "Used-For", "multi - label recognition"]], "rel_plus": [["GCN:Method", "Synonym-Of", "graph convolution network:Method"], ["graph convolution network:Method", "Used-For", "multi - label recognition:Task"]]}
{"doc_id": "208202241", "sentence": "In this paper , we propose a label graph superimposing framework to improve the conventional GCN+CNN framework developed for multi - label recognition in the following two aspects .", "ner": [["GCN+CNN", "Method"], ["multi - label recognition", "Task"]], "rel": [["GCN+CNN", "Used-For", "multi - label recognition"]], "rel_plus": [["GCN+CNN:Method", "Used-For", "multi - label recognition:Task"]]}
{"doc_id": "208202241", "sentence": "In detail , lateral connections between GCN and CNN are added at shallow , middle and deep layers to inject information of label system into backbone CNN for label - awareness in the feature learning process .", "ner": [["lateral connections", "Method"], ["GCN", "Method"], ["CNN", "Method"], ["CNN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "208202241", "sentence": "Extensive experiments are carried out on MS - COCO and Charades datasets , showing that our proposed solution can greatly improve the recognition performance and achieves new state - of - the - art recognition performance .", "ner": [["MS - COCO", "Dataset"], ["Charades", "Dataset"], ["recognition", "Task"], ["recognition", "Task"]], "rel": [["MS - COCO", "Benchmark-For", "recognition"], ["Charades", "Benchmark-For", "recognition"], ["MS - COCO", "Benchmark-For", "recognition"], ["Charades", "Benchmark-For", "recognition"]], "rel_plus": [["MS - COCO:Dataset", "Benchmark-For", "recognition:Task"], ["Charades:Dataset", "Benchmark-For", "recognition:Task"], ["MS - COCO:Dataset", "Benchmark-For", "recognition:Task"], ["Charades:Dataset", "Benchmark-For", "recognition:Task"]]}
{"doc_id": "208202241", "sentence": "In the computer vision community , multilabel recognition is a fundamental and practical task , and has attracted increasing research efforts .", "ner": [["computer vision", "Task"], ["multilabel recognition", "Task"]], "rel": [["multilabel recognition", "SubTask-Of", "computer vision"]], "rel_plus": [["multilabel recognition:Task", "SubTask-Of", "computer vision:Task"]]}
{"doc_id": "208202241", "sentence": "Given the great success of single label image/video classification brought by deep convolutional networks ( He et al. 2 0 1 5 ; Carreira and Zisserman 2 0 1 7 ; He et al. 2 0 1 6 a ; Feichtenhofer et al. 2 0 1 8 ; Wu et al. 2 0 1 9 ) , multi - label recognition can achieve pretty performance by naively treating each label as an independent individual and applying multiple binary classification ( a ) illustrates the co - occurrence of \" Sports Ball \" and \" Tennis Racket \" on the MS - COCO datasets , we can see the frequency that \" Tennis Racket \" co - occurs with \" Sports Ball \" is as high as 0. 4 2 .", "ner": [["image/video classification", "Task"], ["deep convolutional networks", "Method"], ["multi - label recognition", "Task"], ["multiple binary classification", "Task"], ["MS - COCO", "Dataset"]], "rel": [["deep convolutional networks", "Used-For", "image/video classification"], ["multiple binary classification", "SubTask-Of", "multi - label recognition"]], "rel_plus": [["deep convolutional networks:Method", "Used-For", "image/video classification:Task"], ["multiple binary classification:Task", "SubTask-Of", "multi - label recognition:Task"]]}
{"doc_id": "208202241", "sentence": "Secondly , given input X , the common practice for predicting its labels can be formulated as a two - stage mapping y = F 1 \u2022 F 0 ( X ) , where F 0 : X \u2192 f denotes the CNN feature extraction process and F 1 : f \u2192 y is the mapping from feature space to label space .", "ner": [["CNN feature extraction", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "208202241", "sentence": "Recently , graph convolutional network ( Kipf and Welling 2 0 1 6 ) , aka GCN , has witnessed prevailing success in modeling relationship among vertices of a graph .", "ner": [["graph convolutional network", "Method"], ["GCN", "Method"]], "rel": [["GCN", "Synonym-Of", "graph convolutional network"]], "rel_plus": [["GCN:Method", "Synonym-Of", "graph convolutional network:Method"]]}
{"doc_id": "208202241", "sentence": "The superimposing means the following two folds in our framework : ( 1 ) to model the priors of co - occurrence of labels following the GCN paradigm , instead of using statistics of label co - occurrence alone to build the relation graph of the label system , we propose to superimpose knowledge based graph into statistics based graph for constructing the final one . ( 2 ) In order to learn better feature representations for a specific multi - label recognition task anchored on its label structures , we design a novel superimposed CNN and GCN network to extract label structure aware descriptors .", "ner": [["GCN", "Method"], ["multi - label recognition", "Task"], ["CNN", "Method"], ["GCN", "Method"]], "rel": [["CNN", "Used-For", "multi - label recognition"], ["GCN", "Used-For", "multi - label recognition"]], "rel_plus": [["CNN:Method", "Used-For", "multi - label recognition:Task"], ["GCN:Method", "Used-For", "multi - label recognition:Task"]]}
{"doc_id": "208202241", "sentence": "Specifically , we first construct two adjacency matrices A S \u2208 R N \u00d7N and A K \u2208 R N \u00d7N to denote correlation graphs of labels , which is constructed by co - occurrence statistics and a knowledge graph named ConceptNet ( Speer , Chin , and Havasi 2 0 1 7 ) respectively .", "ner": [["knowledge graph", "Method"], ["ConceptNet", "Dataset"]], "rel": [["ConceptNet", "Used-For", "knowledge graph"]], "rel_plus": [["ConceptNet:Dataset", "Used-For", "knowledge graph:Method"]]}
{"doc_id": "208202241", "sentence": "Besides , different from conventional graph augmented CNN solutions which utilize information of label system at the final recognition stage , we add lateral connections between CNN and GCN at shallow , middle and deep layers to inject information of the label system into backbone CNN for the purpose of labels awareness in feature learning .", "ner": [["graph augmented CNN", "Method"], ["lateral connections", "Method"], ["CNN", "Method"], ["GCN", "Method"], ["CNN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "208202241", "sentence": "We have carried out extensive experiments on MS - COCO dataset ( Lin et al. 2 0 1 4 ) for multi - label image recognition and Charades ( Sigurdsson et al. 2 0 1 6 ) for multi - label video classification .", "ner": [["MS - COCO", "Dataset"], ["multi - label image recognition", "Task"], ["Charades", "Dataset"], ["multi - label video classification", "Task"]], "rel": [["MS - COCO", "Benchmark-For", "multi - label image recognition"], ["Charades", "Benchmark-For", "multi - label video classification"]], "rel_plus": [["MS - COCO:Dataset", "Benchmark-For", "multi - label image recognition:Task"], ["Charades:Dataset", "Benchmark-For", "multi - label video classification:Task"]]}
{"doc_id": "208202241", "sentence": "Results show that our solution obtains absolute mAP improvement of 6. 4 % and 1 2 . 0 % in MS - COCO and Charades with very limited computation cost overhead , when compared to its plain CNN counterpart .", "ner": [["MS - COCO", "Dataset"], ["Charades", "Dataset"], ["CNN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "208202241", "sentence": "Our model achieves new state - of - the - art and outperforms current state - of - the - art solution by 1. 3 % and 2. 4 % in mAP on MS - COCO and Charades , respectively .", "ner": [["MS - COCO", "Dataset"], ["Charades", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "208202241", "sentence": "State - of - the - art image or video classification frameworks ( He et al. 2 0 1 6 a ; Carreira and Zisserman 2 0 1 7 ; Feichtenhofer et al. 2 0 1 8 ; He et al. 2 0 1 9 ; Wu et al. 2 0 1 9 ) can be directly applied for multi - label classification by replacing the cross - entropy loss with multi - binary classification loss .", "ner": [["image or video classification frameworks", "Method"], ["multi - label classification", "Task"], ["cross - entropy loss", "Method"], ["multi - binary classification loss", "Method"]], "rel": [["multi - binary classification loss", "Part-Of", "image or video classification frameworks"], ["cross - entropy loss", "Part-Of", "image or video classification frameworks"], ["image or video classification frameworks", "Used-For", "multi - label classification"]], "rel_plus": [["multi - binary classification loss:Method", "Part-Of", "image or video classification frameworks:Method"], ["cross - entropy loss:Method", "Part-Of", "image or video classification frameworks:Method"], ["image or video classification frameworks:Method", "Used-For", "multi - label classification:Task"]]}
{"doc_id": "208202241", "sentence": "In ( Wang et al. 2 0 1 7 ) and ( Zhu et al. 2 0 1 7 a ) , either image region - level spatial attention map or attentive semantic - level label correlation modeling was used to boost the final recognition performance . ( Wang , Jia , and Breckon 2 0 1 9 ) proposed to improve the performance by model ensemble .", "ner": [["image region - level spatial attention map", "Method"], ["attentive semantic - level label correlation modeling", "Method"], ["recognition", "Task"]], "rel": [["image region - level spatial attention map", "Used-For", "recognition"], ["attentive semantic - level label correlation modeling", "Used-For", "recognition"]], "rel_plus": [["image region - level spatial attention map:Method", "Used-For", "recognition:Task"], ["attentive semantic - level label correlation modeling:Method", "Used-For", "recognition:Task"]]}
{"doc_id": "208202241", "sentence": "Researchers have leveraged GCN for many computer vision tasks and great performance was achieved .", "ner": [["GCN", "Method"], ["computer vision", "Task"]], "rel": [["GCN", "Used-For", "computer vision"]], "rel_plus": [["GCN:Method", "Used-For", "computer vision:Task"]]}
{"doc_id": "208202241", "sentence": "For instance , it was leveraged in ( Yan , Xiong , and Lin 2 0 1 8 ; Gao et al. 2 0 1 8) to model the relationship of skeletons of humans bodies for human action recognition and knowledgeaware GCN was applied for zero - shot video classification in ( Gao , Zhang , and Xu 2 0 1 9 ) .", "ner": [["human action recognition", "Task"], ["GCN", "Method"], ["zero - shot video classification", "Task"]], "rel": [["GCN", "Used-For", "zero - shot video classification"]], "rel_plus": [["GCN:Method", "Used-For", "zero - shot video classification:Task"]]}
{"doc_id": "208202241", "sentence": "Our work mostly relates to the one proposed in ( Chen et al. 2 0 1 9 ) , which used GCN to propagate information among labels and merges label information with CNN features at the final classification stage .", "ner": [["GCN", "Method"], ["CNN", "Method"], ["classification", "Task"]], "rel": [["GCN", "Used-For", "classification"], ["CNN", "Used-For", "classification"]], "rel_plus": [["GCN:Method", "Used-For", "classification:Task"], ["CNN:Method", "Used-For", "classification:Task"]]}
{"doc_id": "208202241", "sentence": "Differently , our work builds GCN by superimposing the Figure 2 : The overview of KSSNet with backbone of Inception - I 3 D . \" LC \" is our proposed lateral connection , 'S ' and ' L ' denote Sigmoid and LeakyReLU operations , respectively . \" Inc. \" is the Inception block in I 3 D ( Carreira and Zisserman 2 0 1 7 ) .", "ner": [["GCN", "Method"], ["KSSNet", "Method"], ["Inception - I 3 D", "Method"], ["LC", "Method"], ["lateral connection", "Method"], ["'S", "Method"], ["L", "Method"], ["Sigmoid", "Method"], ["LeakyReLU", "Method"], ["Inc.", "Method"], ["Inception block in I 3 D", "Method"]], "rel": [["Inception - I 3 D", "Part-Of", "KSSNet"], ["LC", "Synonym-Of", "lateral connection"], ["'S", "Synonym-Of", "Sigmoid"], ["L", "Synonym-Of", "LeakyReLU"], ["Inc.", "Synonym-Of", "Inception block in I 3 D"]], "rel_plus": [["Inception - I 3 D:Method", "Part-Of", "KSSNet:Method"], ["LC:Method", "Synonym-Of", "lateral connection:Method"], ["'S:Method", "Synonym-Of", "Sigmoid:Method"], ["L:Method", "Synonym-Of", "LeakyReLU:Method"], ["Inc.:Method", "Synonym-Of", "Inception block in I 3 D:Method"]]}
{"doc_id": "208202241", "sentence": "KSSNet takes videos and initial label embeddings as input , and outputs the predicted labels of these videos . \" GConv \" is the abbreviation of \" Graph Convolution \" . graph built from statistical co - occurrence information into the graph built with knowledge priors .", "ner": [["KSSNet", "Method"], ["GConv", "Method"], ["Graph Convolution", "Method"]], "rel": [["GConv", "Synonym-Of", "Graph Convolution"]], "rel_plus": [["GConv:Method", "Synonym-Of", "Graph Convolution:Method"]]}
{"doc_id": "208202241", "sentence": "Better feature learning network architecture by absorbing label structure information generated by GCN at shallow , middle and deep layers of backbone CNN is designed .", "ner": [["GCN", "Method"], ["CNN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "208202241", "sentence": "We call our model as KSSNet ( Knowledge and Statistics Superimposing Network ) .", "ner": [["KSSNet", "Method"], ["Knowledge and Statistics Superimposing Network", "Method"]], "rel": [["KSSNet", "Synonym-Of", "Knowledge and Statistics Superimposing Network"]], "rel_plus": [["KSSNet:Method", "Synonym-Of", "Knowledge and Statistics Superimposing Network:Method"]]}
{"doc_id": "208202241", "sentence": "Taking the KSSNet with backbone of Inception - I 3 D ( Carreira and Zisserman 2 0 1 7 ) designed for multi - label video classification as example , we show its block - diagram in Figure 2 .", "ner": [["KSSNet", "Method"], ["Inception - I 3 D", "Method"], ["multi - label video classification", "Task"]], "rel": [["Inception - I 3 D", "Synonym-Of", "KSSNet"], ["KSSNet", "Used-For", "multi - label video classification"]], "rel_plus": [["Inception - I 3 D:Method", "Synonym-Of", "KSSNet:Method"], ["KSSNet:Method", "Used-For", "multi - label video classification:Task"]]}
{"doc_id": "208202241", "sentence": "When it comes to multi - label image classification , the framework can be easily constructed by superimposing GCN with stateof - the - art 2D CNN such as ResNet ( He et al. 2 0 1 6 a ) .", "ner": [["multi - label image classification", "Task"], ["GCN", "Method"], ["2D CNN", "Method"], ["ResNet", "Method"]], "rel": [["GCN", "Used-For", "multi - label image classification"], ["ResNet", "SubClass-Of", "2D CNN"]], "rel_plus": [["GCN:Method", "Used-For", "multi - label image classification:Task"], ["ResNet:Method", "SubClass-Of", "2D CNN:Method"]]}
{"doc_id": "208202241", "sentence": "In the following subsections , we firstly introduce in detail how label graph are constructed and superimposed , and then we show what is our proposed GCN and CNN superimposing .", "ner": [["GCN", "Method"], ["CNN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "208202241", "sentence": "Meanwhile , knowledge graph , such as ConceptNet ( Speer , Chin , and Havasi 2 0 1 7 ) , is built with human knowledge by several methods , such as expert - created resources and games with a purpose .", "ner": [["knowledge graph", "Method"], ["ConceptNet", "Dataset"]], "rel": [["ConceptNet", "Used-For", "knowledge graph"]], "rel_plus": [["ConceptNet:Dataset", "Used-For", "knowledge graph:Method"]]}
{"doc_id": "208202241", "sentence": "We denote the statistical graph as G S = ( V , E S , A S ) , knowledge graph as G K = ( V , E K , A K ) , where A S and A K are adjacency matrices obtained with statistical information and knowledge priors respectively .", "ner": [["statistical graph", "Method"], ["knowledge graph", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "208202241", "sentence": "A K is obtained according to the human created knowledge graph ConceptNet ( Speer , Chin , and Havasi 2 0 1 7 ) .", "ner": [["knowledge graph", "Method"], ["ConceptNet", "Dataset"]], "rel": [["ConceptNet", "Used-For", "knowledge graph"]], "rel_plus": [["ConceptNet:Dataset", "Used-For", "knowledge graph:Method"]]}
{"doc_id": "208202241", "sentence": "Unlike conventional convolutions , GCN is designed for non - Euclidean topological structure .", "ner": [["convolutions", "Method"], ["GCN", "Method"], ["non - Euclidean topological structure", "Task"]], "rel": [["GCN", "Compare-With", "convolutions"], ["GCN", "Used-For", "non - Euclidean topological structure"]], "rel_plus": [["GCN:Method", "Compare-With", "convolutions:Method"], ["GCN:Method", "Used-For", "non - Euclidean topological structure:Task"]]}
{"doc_id": "208202241", "sentence": "Instead of only superimposing information of label relationship at the final recognition stage , we propose to inject label information into backbone 2D/ 3 D CNNs at different stages by lateral connection ( LC operation ) .", "ner": [["recognition", "Method"], ["2D/ 3 D CNNs", "Method"], ["lateral connection", "Method"], ["LC", "Method"]], "rel": [["2D/ 3 D CNNs", "Used-For", "recognition"], ["LC", "Synonym-Of", "lateral connection"]], "rel_plus": [["2D/ 3 D CNNs:Method", "Used-For", "recognition:Method"], ["LC:Method", "Synonym-Of", "lateral connection:Method"]]}
{"doc_id": "208202241", "sentence": "The motivation of LC is to push the CNN network to learn label - system anchored feature representations for better recognition .", "ner": [["LC", "Method"], ["CNN", "Method"], ["recognition", "Task"]], "rel": [["CNN", "Used-For", "recognition"]], "rel_plus": [["CNN:Method", "Used-For", "recognition:Task"]]}
{"doc_id": "208202241", "sentence": "As stated in ( 7 ) , it first calculates crosscorrelation of CNN features and label embeddings and outputs how each CNN feature point is correlated with a label embedding .", "ner": [["CNN", "Method"], ["CNN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "208202241", "sentence": "With the lateral connection , the relationship of label system and CNN feature maps is modeled and the learned CNN feature is kind of label - system anchored .", "ner": [["lateral connection", "Method"], ["CNN", "Method"], ["CNN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "208202241", "sentence": "Our KSSNet superimposes labels embeddings into CNN features not only in the classification layer but also in hidden layers .", "ner": [["KSSNet", "Method"], ["CNN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "208202241", "sentence": "There are several advantages of this strategy . ( 1 ) The hidden embeddings in GCN can help the feature learning process of CNN , making hidden CNN features aware of label relationship . ( 2 ) As for the learning process of hidden embeddings , the extra gradients from LC operation can Figure 4 : The block diagram of LC operation . ' R ' , \" ( \u00b7 ) T \" , ' \u00d7 ' and ' + ' denote matrix reshape , transpose , multiplication and sum operations respectively . x ( l ) and E ( l ) are CNN feature and GCN feature at the l th GCN layer .", "ner": [["GCN", "Method"], ["CNN", "Method"], ["CNN", "Method"], ["LC", "Method"], ["LC", "Method"], ["CNN", "Method"], ["GCN", "Method"], ["GCN", "Method"]], "rel": [["GCN", "Used-For", "CNN"]], "rel_plus": [["GCN:Method", "Used-For", "CNN:Method"]]}
{"doc_id": "208202241", "sentence": "Then , we carry out ablation studies to evaluate the effectiveness of the proposed graph construction method in our KSSNet .", "ner": [["graph construction", "Task"], ["KSSNet", "Method"]], "rel": [["KSSNet", "Used-For", "graph construction"]], "rel_plus": [["KSSNet:Method", "Used-For", "graph construction:Task"]]}
{"doc_id": "208202241", "sentence": "MS - COCO MS - COCO ( Lin et al. 2 0 1 4 ) is a static image dataset , which is widely used for many tasks , such as multilabel image recognition , object localization and semantic segmentation .", "ner": [["MS - COCO", "Dataset"], ["MS - COCO", "Dataset"], ["image recognition", "Task"], ["object localization", "Task"], ["semantic segmentation", "Task"]], "rel": [["MS - COCO", "Benchmark-For", "image recognition"], ["MS - COCO", "Benchmark-For", "object localization"], ["MS - COCO", "Benchmark-For", "semantic segmentation"]], "rel_plus": [["MS - COCO:Dataset", "Benchmark-For", "image recognition:Task"], ["MS - COCO:Dataset", "Benchmark-For", "object localization:Task"], ["MS - COCO:Dataset", "Benchmark-For", "semantic segmentation:Task"]]}
{"doc_id": "208202241", "sentence": "Experiment on MS - COCO For image recognition , we choose state - of - the - art ResNet 1 0 1 ( He et al. 2 0 1 6 b ) as the backbone of our KSSNet , which is pre - trained on ImageNet .", "ner": [["MS - COCO", "Dataset"], ["image recognition", "Task"], ["ResNet 1 0 1", "Method"], ["KSSNet", "Method"], ["ImageNet", "Dataset"]], "rel": [["MS - COCO", "Benchmark-For", "image recognition"], ["KSSNet", "Used-For", "image recognition"], ["ResNet 1 0 1", "Part-Of", "KSSNet"], ["ResNet 1 0 1", "Trained-With", "ImageNet"]], "rel_plus": [["MS - COCO:Dataset", "Benchmark-For", "image recognition:Task"], ["KSSNet:Method", "Used-For", "image recognition:Task"], ["ResNet 1 0 1:Method", "Part-Of", "KSSNet:Method"], ["ResNet 1 0 1:Method", "Trained-With", "ImageNet:Dataset"]]}
{"doc_id": "208202241", "sentence": "The GCN of KSSNet is built from four successive graph convolution layers and the number of channels of their outputs is 2 5 6 , 5 1 2 , 1 0 2 4 and 2 0 4 8 , respectively .", "ner": [["GCN", "Method"], ["KSSNet", "Method"], ["graph convolution", "Method"]], "rel": [["graph convolution", "Part-Of", "GCN"], ["GCN", "Part-Of", "KSSNet"]], "rel_plus": [["graph convolution:Method", "Part-Of", "GCN:Method"], ["GCN:Method", "Part-Of", "KSSNet:Method"]]}
{"doc_id": "208202241", "sentence": "In order to deal with the \" dead ReLU \" problem , we use LeakyReLU as activation operation for graph convolution layers , with negative slope of 0. 2 .", "ner": [["ReLU", "Method"], ["LeakyReLU", "Method"], ["graph convolution", "Method"]], "rel": [["LeakyReLU", "SubClass-Of", "ReLU"], ["LeakyReLU", "Part-Of", "graph convolution"]], "rel_plus": [["LeakyReLU:Method", "SubClass-Of", "ReLU:Method"], ["LeakyReLU:Method", "Part-Of", "graph convolution:Method"]]}
{"doc_id": "208202241", "sentence": "Three 2D version LC operations between GCN and the backbone ResNet 1 0 1 are used and the label embeddings of four graph convolution layers are injected to res 2 , res 3 , res 4 and res 5 of ResNet 1 0 1 .", "ner": [["LC", "Method"], ["GCN", "Method"], ["ResNet 1 0 1", "Method"], ["graph convolution", "Method"], ["ResNet 1 0 1", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "208202241", "sentence": "Adam is used as the optimizer with a momentum of 0. 9 , weight decay of 1 0 \u2212 4 and batch size of 8 0 .", "ner": [["Adam", "Method"], ["momentum", "Method"], ["weight decay", "Method"]], "rel": [["momentum", "Part-Of", "Adam"], ["weight decay", "Part-Of", "Adam"]], "rel_plus": [["momentum:Method", "Part-Of", "Adam:Method"], ["weight decay:Method", "Part-Of", "Adam:Method"]]}
{"doc_id": "208202241", "sentence": "Experiment on Charades Inception - I 3 D of KSSNet is initialized following the inflating mechanism proposed in I 3 D ( Carreira and Zisserman 2 0 1 7 ) with BN - Inception pretrained on ImageNet .", "ner": [["Charades", "Dataset"], ["Inception - I 3 D", "Method"], ["KSSNet", "Method"], ["I 3 D", "Method"], ["BN - Inception", "Method"], ["ImageNet", "Dataset"]], "rel": [["KSSNet", "Evaluated-With", "Charades"], ["Inception - I 3 D", "Part-Of", "KSSNet"], ["BN - Inception", "Part-Of", "I 3 D"], ["I 3 D", "Trained-With", "ImageNet"]], "rel_plus": [["KSSNet:Method", "Evaluated-With", "Charades:Dataset"], ["Inception - I 3 D:Method", "Part-Of", "KSSNet:Method"], ["BN - Inception:Method", "Part-Of", "I 3 D:Method"], ["I 3 D:Method", "Trained-With", "ImageNet:Dataset"]]}
{"doc_id": "208202241", "sentence": "Adam is used as the optimizer , starting with a momentum of 0. 9 and weight decay of 1 0 \u2212 4 .", "ner": [["Adam", "Method"], ["momentum", "Method"], ["weight decay", "Method"]], "rel": [["momentum", "Part-Of", "Adam"], ["weight decay", "Part-Of", "Adam"]], "rel_plus": [["momentum:Method", "Part-Of", "Adam:Method"], ["weight decay:Method", "Part-Of", "Adam:Method"]]}
{"doc_id": "208202241", "sentence": "Dropout ( Hinton et al. 2 0 1 2 ) with a ratio of 0. 5 is added after the average pooled CNN features .", "ner": [["Dropout", "Method"], ["CNN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "208202241", "sentence": "We use the strategy proposed in ( He et al. 2 0 1 5 ) to initialize the GCN and initial label embeddings are extracted with ConceptNet ( Speer , Chin , and Havasi 2 0 1 7 ) .", "ner": [["GCN", "Method"], ["ConceptNet", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "208202241", "sentence": "In this part , we present comparisons with several state - ofthe - arts on MS - COCO and Charades , respectively to show the effectiveness of our proposed solution .", "ner": [["MS - COCO", "Dataset"], ["Charades", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "208202241", "sentence": "Results on MS - COCO We compare our KSSNet with the state - of - the - art methods , including CNN - RNN ( Wang et al. 2 0 1 6 a ) , SRN ( Zhu et al. 2 0 1 7 b ) , ResNet 1 0 1 ( He et al. 2 0 1 6 b ) , Multi - Evidence ( Ge , Yang , and Yu 2 0 1 8) and ML - GCN ( Chen et al. 2 0 1 9 ) .", "ner": [["MS - COCO", "Dataset"], ["KSSNet", "Method"], ["CNN - RNN", "Method"], ["SRN", "Method"], ["ResNet 1 0 1", "Method"], ["Multi - Evidence", "Method"], ["ML - GCN", "Method"]], "rel": [["KSSNet", "Evaluated-With", "MS - COCO"], ["CNN - RNN", "Evaluated-With", "MS - COCO"], ["SRN", "Evaluated-With", "MS - COCO"], ["ResNet 1 0 1", "Evaluated-With", "MS - COCO"], ["Multi - Evidence", "Evaluated-With", "MS - COCO"], ["ML - GCN", "Evaluated-With", "MS - COCO"], ["KSSNet", "Compare-With", "CNN - RNN"], ["KSSNet", "Compare-With", "SRN"], ["KSSNet", "Compare-With", "ResNet 1 0 1"], ["KSSNet", "Compare-With", "Multi - Evidence"], ["KSSNet", "Compare-With", "ML - GCN"]], "rel_plus": [["KSSNet:Method", "Evaluated-With", "MS - COCO:Dataset"], ["CNN - RNN:Method", "Evaluated-With", "MS - COCO:Dataset"], ["SRN:Method", "Evaluated-With", "MS - COCO:Dataset"], ["ResNet 1 0 1:Method", "Evaluated-With", "MS - COCO:Dataset"], ["Multi - Evidence:Method", "Evaluated-With", "MS - COCO:Dataset"], ["ML - GCN:Method", "Evaluated-With", "MS - COCO:Dataset"], ["KSSNet:Method", "Compare-With", "CNN - RNN:Method"], ["KSSNet:Method", "Compare-With", "SRN:Method"], ["KSSNet:Method", "Compare-With", "ResNet 1 0 1:Method"], ["KSSNet:Method", "Compare-With", "Multi - Evidence:Method"], ["KSSNet:Method", "Compare-With", "ML - GCN:Method"]]}
{"doc_id": "208202241", "sentence": "ML - GCN is a GCN+CNN framework based on statistical label graph and it is the current state - of - the - art .", "ner": [["ML - GCN", "Method"], ["GCN+CNN", "Method"]], "rel": [["ML - GCN", "SubClass-Of", "GCN+CNN"]], "rel_plus": [["ML - GCN:Method", "SubClass-Of", "GCN+CNN:Method"]]}
{"doc_id": "208202241", "sentence": "The comparison of KSSNet and its backbone ResNet 1 0 1 shows that the absolute improvement in mAP is up to 6. 4 % and evidences that the label embeddings of GCN can explicitly take advantage of the label relationship , which is hard to be learned by plain CNN or even ignored by many frameworks .", "ner": [["KSSNet", "Method"], ["ResNet 1 0 1", "Method"], ["GCN", "Method"], ["CNN", "Method"]], "rel": [["ResNet 1 0 1", "Part-Of", "KSSNet"]], "rel_plus": [["ResNet 1 0 1:Method", "Part-Of", "KSSNet:Method"]]}
{"doc_id": "208202241", "sentence": "Results on Charades Table 2 shows the comparison with state - of - the - art models for our proposed KSSNet on Charades .", "ner": [["Charades", "Dataset"], ["KSSNet", "Method"], ["Charades", "Dataset"]], "rel": [["KSSNet", "Evaluated-With", "Charades"]], "rel_plus": [["KSSNet:Method", "Evaluated-With", "Charades:Dataset"]]}
{"doc_id": "208202241", "sentence": "Compared with backbone I 3 D model , KSSNet provides 1 2 . 0 % higher mAP at the cost of very little computation overhead ( from 1 0 8 GFLOPs to 1 2 7 GFLOPs ) .", "ner": [["I 3 D", "Method"], ["KSSNet", "Method"]], "rel": [["I 3 D", "Part-Of", "KSSNet"]], "rel_plus": [["I 3 D:Method", "Part-Of", "KSSNet:Method"]]}
{"doc_id": "208202241", "sentence": "It can be concluded from such observation that our proposed GCN and CNN superimposing framework can significantly improve baseline result , especially when the training data is not so sufficient .", "ner": [["GCN", "Method"], ["CNN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "208202241", "sentence": "We can also see that although no pretraining on extra large scale video dataset , KSSNet ( KS graph ) achieves the best performance , which is 2. 4 % higher than the current state - of - the - art method LFB and SlowFast(NL ) which are pretrained on Kinetics - 4 0 0 .", "ner": [["KSSNet", "Method"], ["KS graph", "Method"], ["LFB", "Method"], ["SlowFast(NL )", "Method"], ["Kinetics - 4 0 0", "Dataset"]], "rel": [["KS graph", "Part-Of", "KSSNet"], ["KSSNet", "Compare-With", "LFB"], ["KSSNet", "Compare-With", "SlowFast(NL )"], ["SlowFast(NL )", "Trained-With", "Kinetics - 4 0 0"]], "rel_plus": [["KS graph:Method", "Part-Of", "KSSNet:Method"], ["KSSNet:Method", "Compare-With", "LFB:Method"], ["KSSNet:Method", "Compare-With", "SlowFast(NL ):Method"], ["SlowFast(NL ):Method", "Trained-With", "Kinetics - 4 0 0:Dataset"]]}
{"doc_id": "208202241", "sentence": "It should be noted that the GFLOPs of our KSSNet is much smaller than SlowFast(NL ) , which means KSSNet has remarkable potential in fast multi - label video classification .", "ner": [["KSSNet", "Method"], ["SlowFast(NL )", "Method"], ["KSSNet", "Method"], ["multi - label video classification", "Task"]], "rel": [["KSSNet", "Compare-With", "SlowFast(NL )"], ["KSSNet", "Used-For", "multi - label video classification"]], "rel_plus": [["KSSNet:Method", "Compare-With", "SlowFast(NL ):Method"], ["KSSNet:Method", "Used-For", "multi - label video classification:Task"]]}
{"doc_id": "208202241", "sentence": "In this section , we perform ablation studies to evaluate the effectiveness of our KS graph and to analyze the influence of GCN depth in KSSNet framework .", "ner": [["KS graph", "Method"], ["GCN", "Method"], ["KSSNet", "Method"]], "rel": [["GCN", "Part-Of", "KSSNet"]], "rel_plus": [["GCN:Method", "Part-Of", "KSSNet:Method"]]}
{"doc_id": "208202241", "sentence": "Label graphs of KSSNet In order to evaluate the influence of different graph , we implement three versions of KSSNet with statistical graph , knowledge graph and our proposed KS graph .", "ner": [["KSSNet", "Method"], ["KSSNet", "Method"], ["statistical graph", "Method"], ["knowledge graph", "Method"], ["KS graph", "Method"]], "rel": [["KS graph", "Part-Of", "KSSNet"], ["knowledge graph", "Part-Of", "KSSNet"], ["statistical graph", "Part-Of", "KSSNet"]], "rel_plus": [["KS graph:Method", "Part-Of", "KSSNet:Method"], ["knowledge graph:Method", "Part-Of", "KSSNet:Method"], ["statistical graph:Method", "Part-Of", "KSSNet:Method"]]}
{"doc_id": "208202241", "sentence": "Table 3 summarizes the results of KSSNet ( statistical graph ) , KSSNet ( knowledge graph ) and KSSNet ( KS graph ) .", "ner": [["KSSNet", "Method"], ["statistical graph", "Method"], ["KSSNet", "Method"], ["knowledge graph", "Method"], ["KSSNet", "Method"], ["KS graph", "Method"]], "rel": [["statistical graph", "Part-Of", "KSSNet"], ["knowledge graph", "Part-Of", "KSSNet"], ["KS graph", "Part-Of", "KSSNet"]], "rel_plus": [["statistical graph:Method", "Part-Of", "KSSNet:Method"], ["knowledge graph:Method", "Part-Of", "KSSNet:Method"], ["KS graph:Method", "Part-Of", "KSSNet:Method"]]}
{"doc_id": "208202241", "sentence": "The experiment on MS - COCO shows that knowledge graph performs worse than statistical graph and KS graph , which is caused by the relationship missing of uncovered labels in knowledge graph and by the oversmoothing impact introduced by the presence of many trivial edges .", "ner": [["MS - COCO", "Dataset"], ["knowledge graph", "Method"], ["statistical graph", "Method"], ["KS graph", "Method"], ["knowledge graph", "Method"]], "rel": [["knowledge graph", "Evaluated-With", "MS - COCO"], ["statistical graph", "Evaluated-With", "MS - COCO"], ["statistical graph", "Evaluated-With", "MS - COCO"], ["KS graph", "Evaluated-With", "MS - COCO"], ["knowledge graph", "Compare-With", "statistical graph"], ["knowledge graph", "Compare-With", "KS graph"]], "rel_plus": [["knowledge graph:Method", "Evaluated-With", "MS - COCO:Dataset"], ["statistical graph:Method", "Evaluated-With", "MS - COCO:Dataset"], ["statistical graph:Method", "Evaluated-With", "MS - COCO:Dataset"], ["KS graph:Method", "Evaluated-With", "MS - COCO:Dataset"], ["knowledge graph:Method", "Compare-With", "statistical graph:Method"], ["knowledge graph:Method", "Compare-With", "KS graph:Method"]]}
{"doc_id": "208202241", "sentence": "However , the experiment on Charades exhibits a contrary result , KSSNet ( knowledge graph ) outperforms KSSNet ( statistical graph ) by a mAP of 0. 4 .", "ner": [["Charades", "Dataset"], ["KSSNet", "Method"], ["knowledge graph", "Method"], ["KSSNet", "Method"], ["statistical graph", "Method"]], "rel": [["KSSNet", "Evaluated-With", "Charades"], ["KSSNet", "Evaluated-With", "Charades"], ["knowledge graph", "Part-Of", "KSSNet"], ["statistical graph", "Part-Of", "KSSNet"]], "rel_plus": [["KSSNet:Method", "Evaluated-With", "Charades:Dataset"], ["KSSNet:Method", "Evaluated-With", "Charades:Dataset"], ["knowledge graph:Method", "Part-Of", "KSSNet:Method"], ["statistical graph:Method", "Part-Of", "KSSNet:Method"]]}
{"doc_id": "208202241", "sentence": "Graph constructed from co - occurrence information is not so reliable while knowledge priors are always valid , so the contradiction between complex label relationship modeling and the lack of samples in Charades makes knowledge graph more effective than statistical graph .", "ner": [["Charades", "Dataset"], ["knowledge graph", "Method"], ["statistical graph", "Method"]], "rel": [["knowledge graph", "Evaluated-With", "Charades"], ["statistical graph", "Evaluated-With", "Charades"], ["knowledge graph", "Compare-With", "statistical graph"]], "rel_plus": [["knowledge graph:Method", "Evaluated-With", "Charades:Dataset"], ["statistical graph:Method", "Evaluated-With", "Charades:Dataset"], ["knowledge graph:Method", "Compare-With", "statistical graph:Method"]]}
{"doc_id": "208202241", "sentence": "Both experiments show that our KS graph performs the best , which validates the effectiveness of superimposing statistical graph and knowledge graph .", "ner": [["KS graph", "Method"], ["statistical graph", "Method"], ["knowledge graph", "Method"]], "rel": [["KS graph", "Compare-With", "statistical graph"], ["KS graph", "Compare-With", "knowledge graph"]], "rel_plus": [["KS graph:Method", "Compare-With", "statistical graph:Method"], ["KS graph:Method", "Compare-With", "knowledge graph:Method"]]}
{"doc_id": "208202241", "sentence": "Influence of GCN Depth in KSSNet As we know , conventional GCN suffers from over - smoothing .", "ner": [["GCN", "Method"], ["KSSNet", "Method"], ["GCN", "Method"]], "rel": [["GCN", "Part-Of", "KSSNet"]], "rel_plus": [["GCN:Method", "Part-Of", "KSSNet:Method"]]}
{"doc_id": "208202241", "sentence": "In this part , we conduct multi - label image and video recognition experiments to demonstrate that our KSSNet framework can deal with this problem effectively .", "ner": [["multi - label image", "Task"], ["video recognition", "Task"], ["KSSNet", "Method"]], "rel": [["KSSNet", "Used-For", "multi - label image"], ["KSSNet", "Used-For", "video recognition"]], "rel_plus": [["KSSNet:Method", "Used-For", "multi - label image:Task"], ["KSSNet:Method", "Used-For", "video recognition:Task"]]}
{"doc_id": "208202241", "sentence": "The backbone of KSSNet is ResNet 1 0 1 and Inception - I 3 d for MS - COCO and Charades , respectively .", "ner": [["KSSNet", "Method"], ["ResNet 1 0 1", "Method"], ["Inception - I 3 d", "Method"], ["MS - COCO", "Dataset"], ["Charades", "Dataset"]], "rel": [["ResNet 1 0 1", "Part-Of", "KSSNet"], ["Inception - I 3 d", "Part-Of", "KSSNet"], ["KSSNet", "Trained-With", "MS - COCO"], ["KSSNet", "Evaluated-With", "Charades"]], "rel_plus": [["ResNet 1 0 1:Method", "Part-Of", "KSSNet:Method"], ["Inception - I 3 d:Method", "Part-Of", "KSSNet:Method"], ["KSSNet:Method", "Trained-With", "MS - COCO:Dataset"], ["KSSNet:Method", "Evaluated-With", "Charades:Dataset"]]}
{"doc_id": "208202241", "sentence": "In this experiment , we modify the GCN pathway of KSSNet to be with three and two graph convolution layers .", "ner": [["GCN", "Method"], ["KSSNet", "Method"], ["graph convolution", "Method"]], "rel": [["graph convolution", "Part-Of", "GCN"], ["GCN", "Part-Of", "KSSNet"]], "rel_plus": [["graph convolution:Method", "Part-Of", "GCN:Method"], ["GCN:Method", "Part-Of", "KSSNet:Method"]]}
{"doc_id": "208202241", "sentence": "The modification can be simply done in two steps : 1 ) delete the first one or two graph convolution layer(s ) and the corresponding LC operation(s ) from the GCN pathway ; 2 ) then the first graph convolution layer of the rest ones is adapted to take the initial label embeddings E ( 0 ) as input by adjusting its number of input channel C to the channel number of E ( 0 ) .", "ner": [["graph convolution", "Method"], ["LC", "Method"], ["GCN", "Method"], ["graph convolution layer", "Method"]], "rel": [["graph convolution", "Part-Of", "GCN"], ["LC", "Part-Of", "GCN"]], "rel_plus": [["graph convolution:Method", "Part-Of", "GCN:Method"], ["LC:Method", "Part-Of", "GCN:Method"]]}
{"doc_id": "208202241", "sentence": "It is obvious , with our KSSNet , more GCN layers lead to better classification results at the cost of small increase of computational cost and model size .", "ner": [["KSSNet", "Method"], ["GCN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "208202241", "sentence": "KSSNet ( 3 layers ) achieves better performance than KSSNet ( 2 layers ) by absolute mAP improvements of 0. 6 % and 1. 9 % in MS - COCO and Charades .", "ner": [["KSSNet ( 3 layers )", "Method"], ["KSSNet ( 2 layers )", "Method"], ["MS - COCO", "Dataset"], ["Charades", "Dataset"]], "rel": [["KSSNet ( 3 layers )", "Compare-With", "KSSNet ( 2 layers )"], ["KSSNet ( 3 layers )", "Evaluated-With", "MS - COCO"], ["KSSNet ( 2 layers )", "Evaluated-With", "Charades"]], "rel_plus": [["KSSNet ( 3 layers ):Method", "Compare-With", "KSSNet ( 2 layers ):Method"], ["KSSNet ( 3 layers ):Method", "Evaluated-With", "MS - COCO:Dataset"], ["KSSNet ( 2 layers ):Method", "Evaluated-With", "Charades:Dataset"]]}
{"doc_id": "208202241", "sentence": "KSS - Net ( 4 layers ) outperforms KSSNet ( 3 layers ) by 0. 2 % and 1. 1 % in mAP .", "ner": [["KSS - Net ( 4 layers )", "Method"], ["KSSNet ( 3 layers )", "Method"]], "rel": [["KSS - Net ( 4 layers )", "Compare-With", "KSSNet ( 3 layers )"]], "rel_plus": [["KSS - Net ( 4 layers ):Method", "Compare-With", "KSSNet ( 3 layers ):Method"]]}
{"doc_id": "208202241", "sentence": "As is reported in ML - GCN ( Chen et al. 2 0 1 9 ) , when GCN has no less than 2 layers , performance of conventional GCN+CNN solution degrades as long as the number of graph convolution layers gets larger .", "ner": [["ML - GCN", "Method"], ["GCN", "Method"], ["GCN+CNN", "Method"], ["graph convolution", "Method"]], "rel": [["GCN", "Part-Of", "ML - GCN"], ["graph convolution", "Part-Of", "GCN+CNN"]], "rel_plus": [["GCN:Method", "Part-Of", "ML - GCN:Method"], ["graph convolution:Method", "Part-Of", "GCN+CNN:Method"]]}
{"doc_id": "208202241", "sentence": "This is because that ( 1 ) more GCN layers bring more LC operations which guide CNN to learn better label structure aware features at shallow , middle and higher CNN layers . ( 2 ) The extra gradients from LC operation can regularize the learning of label embeddings in GCN . ( 3 ) We have proposed such strategies as redundant removal to tackle the over - smoothing issue of GCN .", "ner": [["GCN", "Method"], ["LC", "Method"], ["CNN", "Method"], ["CNN", "Method"], ["LC", "Method"], ["GCN", "Method"], ["GCN", "Method"]], "rel": [["LC", "Part-Of", "GCN"], ["LC", "Part-Of", "GCN"]], "rel_plus": [["LC:Method", "Part-Of", "GCN:Method"], ["LC:Method", "Part-Of", "GCN:Method"]]}
{"doc_id": "208202241", "sentence": "In order to better model this information , we propose to construct the KS graph for label correlation modeling by superimposing knowledge graph into statistical graph .", "ner": [["KS graph", "Method"], ["knowledge graph", "Method"], ["statistical graph", "Method"]], "rel": [["KS graph", "Compare-With", "knowledge graph"], ["KS graph", "Compare-With", "statistical graph"]], "rel_plus": [["KS graph:Method", "Compare-With", "knowledge graph:Method"], ["KS graph:Method", "Compare-With", "statistical graph:Method"]]}
{"doc_id": "208202241", "sentence": "Then the LC operation is presented for injecting GCN embeddings into CNN features , resulting in a novel neural network KSSNet .", "ner": [["LC", "Method"], ["GCN", "Method"], ["CNN", "Method"], ["neural network", "Method"], ["KSSNet", "Method"]], "rel": [["KSSNet", "SubClass-Of", "neural network"], ["GCN", "Part-Of", "KSSNet"], ["CNN", "Part-Of", "KSSNet"], ["LC", "Part-Of", "KSSNet"]], "rel_plus": [["KSSNet:Method", "SubClass-Of", "neural network:Method"], ["GCN:Method", "Part-Of", "KSSNet:Method"], ["CNN:Method", "Part-Of", "KSSNet:Method"], ["LC:Method", "Part-Of", "KSSNet:Method"]]}
{"doc_id": "208202241", "sentence": "The KSSNet is proven to be capable of learning better feature representations for a specific multi - label recognition task anchored on its label relationship .", "ner": [["KSSNet", "Method"], ["multi - label recognition", "Task"]], "rel": [["KSSNet", "Used-For", "multi - label recognition"]], "rel_plus": [["KSSNet:Method", "Used-For", "multi - label recognition:Task"]]}
{"doc_id": "208202241", "sentence": "Experiments on MS - COCO and Charades have demonstrated the effectiveness of our proposed KS graph and KSSNet for both multi - label image and video recognition tasks .", "ner": [["MS - COCO", "Dataset"], ["Charades", "Dataset"], ["KS graph", "Method"], ["KSSNet", "Method"], ["multi - label image", "Task"], ["video recognition", "Task"]], "rel": [["KS graph", "Evaluated-With", "MS - COCO"], ["KSSNet", "Evaluated-With", "MS - COCO"], ["KS graph", "Evaluated-With", "Charades"], ["KSSNet", "Evaluated-With", "Charades"], ["KS graph", "Used-For", "multi - label image"], ["KSSNet", "Used-For", "multi - label image"], ["MS - COCO", "Benchmark-For", "multi - label image"], ["KSSNet", "Used-For", "video recognition"], ["KS graph", "Used-For", "video recognition"], ["Charades", "Benchmark-For", "video recognition"]], "rel_plus": [["KS graph:Method", "Evaluated-With", "MS - COCO:Dataset"], ["KSSNet:Method", "Evaluated-With", "MS - COCO:Dataset"], ["KS graph:Method", "Evaluated-With", "Charades:Dataset"], ["KSSNet:Method", "Evaluated-With", "Charades:Dataset"], ["KS graph:Method", "Used-For", "multi - label image:Task"], ["KSSNet:Method", "Used-For", "multi - label image:Task"], ["MS - COCO:Dataset", "Benchmark-For", "multi - label image:Task"], ["KSSNet:Method", "Used-For", "video recognition:Task"], ["KS graph:Method", "Used-For", "video recognition:Task"], ["Charades:Dataset", "Benchmark-For", "video recognition:Task"]]}
{"doc_id": "203593581", "sentence": "We propose Additive Powers - of - Two~(APoT ) quantization , an efficient non - uniform quantization scheme for the bell - shaped and long - tailed distribution of weights and activations in neural networks .", "ner": [["Additive Powers - of - Two~(APoT ) quantization", "Method"], ["non - uniform quantization scheme", "Method"], ["neural networks", "Method"]], "rel": [["Additive Powers - of - Two~(APoT ) quantization", "SubClass-Of", "non - uniform quantization scheme"], ["non - uniform quantization scheme", "Used-For", "neural networks"], ["Additive Powers - of - Two~(APoT ) quantization", "Used-For", "neural networks"]], "rel_plus": [["Additive Powers - of - Two~(APoT ) quantization:Method", "SubClass-Of", "non - uniform quantization scheme:Method"], ["non - uniform quantization scheme:Method", "Used-For", "neural networks:Method"], ["Additive Powers - of - Two~(APoT ) quantization:Method", "Used-For", "neural networks:Method"]]}
{"doc_id": "203593581", "sentence": "By constraining all quantization levels as the sum of Powers - of - Two terms , APoT quantization enjoys high computational efficiency and a good match with the distribution of weights .", "ner": [["Powers - of - Two", "Method"], ["APoT quantization", "Method"]], "rel": [["Powers - of - Two", "Part-Of", "APoT quantization"]], "rel_plus": [["Powers - of - Two:Method", "Part-Of", "APoT quantization:Method"]]}
{"doc_id": "203593581", "sentence": "Experimental results show that our proposed method outperforms state - of - the - art methods , and is even competitive with the full - precision models , demonstrating the effectiveness of our proposed APoT quantization .", "ner": [["APoT quantization", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "203593581", "sentence": "For example , our 4 - bit quantized ResNet - 5 0 on ImageNet achieves 7 6 . 6 % top - 1 accuracy without bells and whistles ; meanwhile , our model reduces 2 2 % computational cost compared with the uniformly quantized counterpart .", "ner": [["4 - bit quantized ResNet - 5 0", "Method"], ["ImageNet", "Dataset"]], "rel": [["4 - bit quantized ResNet - 5 0", "Evaluated-With", "ImageNet"]], "rel_plus": [["4 - bit quantized ResNet - 5 0:Method", "Evaluated-With", "ImageNet:Dataset"]]}
{"doc_id": "203593581", "sentence": "Deep Neural Networks ( DNNs ) have made a significant improvement for various real - world applications .", "ner": [["Deep Neural Networks", "Method"], ["DNNs", "Method"]], "rel": [["DNNs", "Synonym-Of", "Deep Neural Networks"]], "rel_plus": [["DNNs:Method", "Synonym-Of", "Deep Neural Networks:Method"]]}
{"doc_id": "203593581", "sentence": "To the reduce memory footprint and computational burden , several model compression methods such as quantization ( Zhou et al. , 2 0 1 6 ) , pruning ( Han et al. , 2 0 1 5 ) and low - rank decomposition ( Denil et al. , 2 0 1 3 ) have been widely explored .", "ner": [["model compression", "Task"], ["quantization", "Method"], ["pruning", "Method"], ["low - rank decomposition", "Method"]], "rel": [["quantization", "Used-For", "model compression"], ["pruning", "Used-For", "model compression"], ["low - rank decomposition", "Used-For", "model compression"]], "rel_plus": [["quantization:Method", "Used-For", "model compression:Task"], ["pruning:Method", "Used-For", "model compression:Task"], ["low - rank decomposition:Method", "Used-For", "model compression:Task"]]}
{"doc_id": "203593581", "sentence": "Most of the existing quantization approaches ( Cai et al. , 2 0 1 7 ; Gong et al. , 2 0 1 9 ) , use uniform quantization although non - uniform quantization can usually achieve better accuracy ( Zhu et al. , 2 0 1 6 ) .", "ner": [["quantization", "Method"], ["uniform quantization", "Method"], ["non - uniform quantization", "Method"]], "rel": [["uniform quantization", "SubClass-Of", "quantization"], ["non - uniform quantization", "SubClass-Of", "quantization"]], "rel_plus": [["uniform quantization:Method", "SubClass-Of", "quantization:Method"], ["non - uniform quantization:Method", "SubClass-Of", "quantization:Method"]]}
{"doc_id": "203593581", "sentence": "The second contradiction is : considering the bell - shaped distribution of weight , it is well - motivated to assign higher resolution ( i.e. smaller quantization interval ) around the mean ; however , such non - uniform quantization levels will introduce high computational overhead .", "ner": [["non - uniform quantization", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "203593581", "sentence": "We introduce the Additive Powers - of - Two ( APoT ) quantization scheme for the weights of DNNs .", "ner": [["Additive Powers - of - Two", "Method"], ["APoT", "Method"], ["DNNs", "Method"]], "rel": [["APoT", "Synonym-Of", "Additive Powers - of - Two"], ["Additive Powers - of - Two", "Used-For", "DNNs"]], "rel_plus": [["APoT:Method", "Synonym-Of", "Additive Powers - of - Two:Method"], ["Additive Powers - of - Two:Method", "Used-For", "DNNs:Method"]]}
{"doc_id": "203593581", "sentence": "APoT is a non - uniform quantization scheme , in which the quantization levels is a sum of several PoT terms and can adapt well to the bell - shaped distribution of weights .", "ner": [["APoT", "Method"], ["non - uniform quantization", "Method"]], "rel": [["APoT", "SubClass-Of", "non - uniform quantization"]], "rel_plus": [["APoT:Method", "SubClass-Of", "non - uniform quantization:Method"]]}
{"doc_id": "203593581", "sentence": "APoT quantization enjoys a 2 \u00d7 speed - up compared with uniform quantization on both general and specific hardware . 2 .", "ner": [["APoT quantization", "Method"], ["uniform quantization", "Method"]], "rel": [["APoT quantization", "Compare-With", "uniform quantization"]], "rel_plus": [["APoT quantization:Method", "Compare-With", "uniform quantization:Method"]]}
{"doc_id": "203593581", "sentence": "We propose a Reparameterized Clipping Function ( RCF ) that can compute a more accurate gradient for the clipping threshold and thus facilitate the optimization of the clipping threshold . 3 .", "ner": [["Reparameterized Clipping Function", "Method"], ["RCF", "Method"]], "rel": [["RCF", "Synonym-Of", "Reparameterized Clipping Function"]], "rel_plus": [["RCF:Method", "Synonym-Of", "Reparameterized Clipping Function:Method"]]}
{"doc_id": "203593581", "sentence": "We introduce weight normalization for neural network quantization .", "ner": [["weight normalization", "Method"], ["neural network quantization", "Method"]], "rel": [["weight normalization", "Used-For", "neural network quantization"]], "rel_plus": [["weight normalization:Method", "Used-For", "neural network quantization:Method"]]}
{"doc_id": "203593581", "sentence": "Specifically , our 3 - bit quantized ResNet - 3 4 on ImageNet only drops 0. 3 % Top - 1 and 0. 2 % Top - 5 accuracy .", "ner": [["3 - bit quantized ResNet - 3 4", "Method"], ["ImageNet", "Dataset"]], "rel": [["3 - bit quantized ResNet - 3 4", "Evaluated-With", "ImageNet"]], "rel_plus": [["3 - bit quantized ResNet - 3 4:Method", "Evaluated-With", "ImageNet:Dataset"]]}
{"doc_id": "203593581", "sentence": "In this section , we first give some preliminaries in uniform quantization and PoT quantization .", "ner": [["uniform quantization", "Method"], ["PoT quantization", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "203593581", "sentence": "Then we introduce our APoT quantization and RCF .", "ner": [["APoT quantization", "Method"], ["RCF", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "203593581", "sentence": "For uniform quantization , the quantization levels are defined as For every floating - point number , uniform quantization maps it to a b - bit fixed - point representation ( quantization levels ) in Q u ( \u03b1 , b ) .", "ner": [["uniform quantization", "Method"], ["uniform quantization", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "203593581", "sentence": "Convolution is done against the quantization levels first and the results are then multiplied by \u03b1 .", "ner": [["Convolution", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "203593581", "sentence": "To solve the contradiction between non - uniform resolution and hardware efficiency , Powers - of - Two ( PoT ) quantization ( Miyashita et al. , 2 0 1 6 ; Zhou et al. , 2 0 1 7 ) is proposed by constraining quantization levels to be powers - of - two values or zero , i.e. , Apparently , as a non - uniform quantizer , PoT has a higher resolution for the value range with denser weights because of its exponential property .", "ner": [["Powers - of - Two", "Method"], ["PoT", "Method"], ["non - uniform quantizer", "Method"], ["PoT", "Method"]], "rel": [["PoT", "Synonym-Of", "Powers - of - Two"], ["PoT", "SubClass-Of", "non - uniform quantizer"]], "rel_plus": [["PoT:Method", "Synonym-Of", "Powers - of - Two:Method"], ["PoT:Method", "SubClass-Of", "non - uniform quantizer:Method"]]}
{"doc_id": "203593581", "sentence": "To tackle the rigid resolution problem , we propose Additive Powers - of - Two ( APoT ) quantization .", "ner": [["Additive Powers - of - Two", "Method"], ["APoT", "Method"]], "rel": [["APoT", "Synonym-Of", "Additive Powers - of - Two"]], "rel_plus": [["APoT:Method", "Synonym-Of", "Additive Powers - of - Two:Method"]]}
{"doc_id": "203593581", "sentence": "In APoT quantization , levels are viewed as a sum of several PoT terms as shown below , 1 To extend the solution for the signed number , we only need to add 1 more bit for the sign . where \u03b3 is a scaling coefficient to make sure the maximum level in Q a is \u03b1 . k is called the base bit - width , which is the bit - width for each additive term , and n is the number of additive terms .", "ner": [["APoT quantization", "Method"], ["PoT", "Method"]], "rel": [["PoT", "Part-Of", "APoT quantization"]], "rel_plus": [["PoT:Method", "Part-Of", "APoT quantization:Method"]]}
{"doc_id": "203593581", "sentence": "The number of additive terms in APoT quantization can increase with bit - width b , which provides flexible resolution for the non - uniform levels .", "ner": [["APoT quantization", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "203593581", "sentence": "Compared with the original PoT levels , APoT allocates quantization levels prudently for the central area .", "ner": [["PoT", "Method"], ["APoT", "Method"]], "rel": [["APoT", "Compare-With", "PoT"]], "rel_plus": [["APoT:Method", "Compare-With", "PoT:Method"]]}
{"doc_id": "203593581", "sentence": "Secondly , APoT generates 3 new quantization levels between 2 0 and 2 \u2212 1 , to properly increase the resolution .", "ner": [["APoT", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "203593581", "sentence": "APoT quantization has a reasonable distribution of quantization levels , with more levels in the peak area ( near 0 ) and relatively higher resolution than the vanilla PoT quantization at the tail ( near 1 ) .", "ner": [["APoT quantization", "Method"], ["PoT quantization", "Method"]], "rel": [["APoT quantization", "Compare-With", "PoT quantization"]], "rel_plus": [["APoT quantization:Method", "Compare-With", "PoT quantization:Method"]]}
{"doc_id": "203593581", "sentence": "On the one hand , the fixed - point number representations used in the uniform quantization is a special case of APoT. When k = 1 in Equation ( 5 ) , the quantization levels is a sum of b PoT terms or 0 .", "ner": [["uniform quantization", "Method"], ["APoT.", "Method"]], "rel": [["uniform quantization", "SubClass-Of", "APoT."]], "rel_plus": [["uniform quantization:Method", "SubClass-Of", "APoT.:Method"]]}
{"doc_id": "203593581", "sentence": "To jointly optimize the clipping threshold \u03b1 and weights via SGD during training , Choi et al. ( 2 0 1 8 b ) apply the Straight - Through Estimator ( STE ) ( Bengio et al. , 2 0 1 3 ) to do the backward propagation for the projection operation .", "ner": [["SGD", "Method"], ["Straight - Through Estimator", "Method"], ["STE", "Method"]], "rel": [["STE", "Synonym-Of", "Straight - Through Estimator"]], "rel_plus": [["STE:Method", "Synonym-Of", "Straight - Through Estimator:Method"]]}
{"doc_id": "203593581", "sentence": "To provide a refined gradient for the clipping threshold , we design a Reparameterized Clipping Function ( RCF ) a\u015d Instead of directly clipping them to [ \u2212\u03b1 , \u03b1 ] , RCF outputs a constant clipping range and rescales weights back after the projection , which is mathematically equivalent to Equation ( 1 ) during forward .", "ner": [["Reparameterized Clipping Function", "Method"], ["RCF", "Method"], ["RCF", "Method"]], "rel": [["RCF", "Synonym-Of", "Reparameterized Clipping Function"]], "rel_plus": [["RCF:Method", "Synonym-Of", "Reparameterized Clipping Function:Method"]]}
{"doc_id": "203593581", "sentence": "In backpropagation , STE is adopted for the projection operation and the gradients of \u03b1 are calculated by ( b ) Evolution of clipping ratio with fixed threshold Figure 2 : The evolution of clipping ratio of the first three layers in ResNet - 2 0 . ( a ) demonstrates clipping ratio is too sensitive to threshold to hurt its optimization without weights normalization . ( b ) shows that weights distribution after normalization is relatively more stable during training .", "ner": [["STE", "Method"], ["ResNet - 2 0", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "203593581", "sentence": "Inspired by the crucial role of batch normalization ( BN ) ( Ioffe & Szegedy , 2 0 1 5 ) in activation quantization ( Cai et al. , 2 0 1 7 ) , we propose weight normalization ( WN ) to refine the distribution of weights with zero mean and unit variance , where is a small number ( typically 1 0 \u2212 5 ) for numerical stability , and I denotes the number of weights in one layer .", "ner": [["batch normalization", "Method"], ["BN", "Method"], ["activation quantization", "Method"], ["weight normalization", "Method"], ["WN", "Method"]], "rel": [["BN", "Synonym-Of", "batch normalization"], ["batch normalization", "Part-Of", "activation quantization"], ["WN", "Synonym-Of", "weight normalization"]], "rel_plus": [["BN:Method", "Synonym-Of", "batch normalization:Method"], ["batch normalization:Method", "Part-Of", "activation quantization:Method"], ["WN:Method", "Synonym-Of", "weight normalization:Method"]]}
{"doc_id": "203593581", "sentence": "Here , we conduct a case study of ResNet - 2 0 on CIFAR 1 0 to illustrate how normalization for weights can help quantization .", "ner": [["ResNet - 2 0", "Method"], ["CIFAR 1 0", "Dataset"], ["quantization", "Method"]], "rel": [["ResNet - 2 0", "Evaluated-With", "CIFAR 1 0"]], "rel_plus": [["ResNet - 2 0:Method", "Evaluated-With", "CIFAR 1 0:Dataset"]]}
{"doc_id": "203593581", "sentence": "More experimental analysis demonstrating the effectiveness of the normalization on weights can be found in Appendix A. Algorithm 1 Forward and backward procedure for an APoT quantized convolutional layer   For activations , we use the uniform quantization as we observe APoT does not outperform uniform quantization significantly for activation .", "ner": [["normalization", "Method"], ["APoT quantized convolutional layer", "Method"], ["uniform quantization", "Method"], ["APoT", "Method"], ["uniform quantization", "Method"]], "rel": [["APoT", "Compare-With", "uniform quantization"]], "rel_plus": [["APoT:Method", "Compare-With", "uniform quantization:Method"]]}
{"doc_id": "203593581", "sentence": "Compared with other uniform quantization methods , APoT quantization is more efficient and effective during inference .", "ner": [["uniform quantization", "Method"], ["APoT quantization", "Method"]], "rel": [["APoT quantization", "Compare-With", "uniform quantization"]], "rel_plus": [["APoT quantization:Method", "Compare-With", "uniform quantization:Method"]]}
{"doc_id": "203593581", "sentence": "LQ - Nets(Zhang et al. , 2 0 1 8) learns quantization levels based on the quantization error minimization ( QEM ) algorithm .", "ner": [["LQ - Nets(Zhang", "Method"], ["quantization error minimization", "Method"], ["QEM", "Method"]], "rel": [["quantization error minimization", "Part-Of", "LQ - Nets(Zhang"], ["QEM", "Synonym-Of", "quantization error minimization"]], "rel_plus": [["quantization error minimization:Method", "Part-Of", "LQ - Nets(Zhang:Method"], ["QEM:Method", "Synonym-Of", "quantization error minimization:Method"]]}
{"doc_id": "203593581", "sentence": "LQ - Nets jointly train these parameters to minimize the quantization error .", "ner": [["LQ - Nets", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "203593581", "sentence": "QIL ( Jung et al. , 2 0 1 9 ) introduces a learnable transformer to change the quantization intervals and optimize them based on the task loss .", "ner": [["QIL", "Method"], ["transformer", "Method"]], "rel": [["transformer", "Part-Of", "QIL"]], "rel_plus": [["transformer:Method", "Part-Of", "QIL:Method"]]}
{"doc_id": "203593581", "sentence": "Salimans & Kingma ( 2 0 1 6 )   In this section , we validate our proposed method on ImageNet - ILSVRC 2 0 1 2 ( Russakovsky et al. , 2 0 1 5 ) and CIFAR 1 0 ( Krizhevsky et al. , 2 0 0 9 ) .", "ner": [["ImageNet - ILSVRC 2 0 1 2", "Dataset"], ["CIFAR 1 0", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "203593581", "sentence": "W 3 A 3 W 5 A 5   We compare our methods with several strong state - of - the - art methods on ResNet - 1 8 and ResNet - 3 4 ( He et al. , 2 0 1 6 ) , including DoReFa - Net ( Zhou et al. , 2 0 1 6 ) , PACT ( Choi et al. , 2 0 1 8 b ) , LQ - Net ( Zhang et al. , 2 0 1 8) , DSQ ( Gong et al. , 2 0 1 9 ) , QIL ( Jung et al. , 2 0 1 9 ) .", "ner": [["W 3 A 3 W 5 A 5", "Method"], ["ResNet - 1 8", "Method"], ["ResNet - 3 4", "Method"], ["DoReFa - Net", "Method"], ["PACT", "Method"], ["LQ - Net", "Method"], ["DSQ", "Method"], ["QIL", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "203593581", "sentence": "For our proposed APoT quantization algorithm , two configurations of the bit - width , i.e. , 3 and 5 ( k = 2 and b = 1 or 2 in Equation ( 5 ) ) 3 are tested .", "ner": [["APoT quantization", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "203593581", "sentence": "For the 2 - bit symmetric quantization levels for weights , Q(\u03b1 , 2 ) = { \u00b1\u03b1 , 0 } , therefore only RCF and WN are used in this setting .", "ner": [["RCF", "Method"], ["WN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "203593581", "sentence": "It can be observed that our 5 - bit quantized network achieves even higher accuracy than the full precision model ( 0. 7 % Top - 1 improvement on ResNet - 1 8 and 0. 2 % Top - 1 improvement on ResNet - 3 4 ) .", "ner": [["ResNet - 1 8", "Method"], ["ResNet - 3 4", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "203593581", "sentence": "Our 3 - bit quantized networks are also approaching full - precision accuracy and only drop 0. 5 % and 0. 3 % accuracy on ResNet - 1 8 and ResNet - 3 4 .", "ner": [["3 - bit quantized networks", "Method"], ["ResNet - 1 8", "Method"], ["ResNet - 3 4", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "203593581", "sentence": "When b is further reduced to 2 , our model still outperforms baselines , which demonstrates the effectiveness of RCF and WN .", "ner": [["RCF", "Method"], ["WN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "203593581", "sentence": "We quantize ResNet - 2 0 and ResNet - 5 6 ( He et al. , 2 0 1 6 ) on CIFAR 1 0 for evaluation .", "ner": [["ResNet - 2 0", "Method"], ["ResNet - 5 6", "Method"], ["CIFAR 1 0", "Dataset"]], "rel": [["ResNet - 2 0", "Evaluated-With", "CIFAR 1 0"], ["ResNet - 5 6", "Evaluated-With", "CIFAR 1 0"]], "rel_plus": [["ResNet - 2 0:Method", "Evaluated-With", "CIFAR 1 0:Dataset"], ["ResNet - 5 6:Method", "Evaluated-With", "CIFAR 1 0:Dataset"]]}
{"doc_id": "203593581", "sentence": "For 3 bit and 5 bit models , APoT quantization surpasses the full precision accuracy by 0. 6 to 0. 7 % .", "ner": [["APoT quantization", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "203593581", "sentence": "DoReFa - Net ( Zhou et al. , 2 0 1 6 ) 8 8 . 2 8 9 . 9 9 0 . 5 PACT ( Choi et al. , 2 0 1 8 b ) 8 9 . 7 9 1 . 1 9 1 . 7 LQ - Net ( Zhang et al. , 2 0 1 8) 9 0 . 2 9 1 . 6 -PACT+SAWB+fpsc ( Choi et al. , 2 0 1 8 a )   Typically , quantization error is defined as the mean squared error between weightsW and and\u0174 before and after quantization respectively .", "ner": [["DoReFa - Net", "Method"], ["PACT", "Method"], ["LQ - Net", "Method"], ["-PACT+SAWB+fpsc", "Method"], ["quantization", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "203593581", "sentence": "The quantization error can be viewed as a sum of clipping error and projection error , defined as Previous methods ( Zhang et al. , 2 0 1 8 ; Cai et al. , 2 0 1 7 ) seek to minimize the quantization error ( i.e. min(E clip + E proj ) ) to obtain the optimal clipping threshold , while RCF is directly optimized by the final training loss to balance projection error and clipping error .", "ner": [["RCF", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "203593581", "sentence": "We compare the Quantization Error Minimization ( QEM ) method with our RCF on the quantized ResNet - 1 8 model .", "ner": [["Quantization Error Minimization", "Method"], ["QEM", "Method"], ["RCF", "Method"], ["ResNet - 1 8", "Method"]], "rel": [["QEM", "Synonym-Of", "Quantization Error Minimization"], ["Quantization Error Minimization", "Compare-With", "RCF"], ["Quantization Error Minimization", "Used-For", "ResNet - 1 8"], ["RCF", "Used-For", "ResNet - 1 8"]], "rel_plus": [["QEM:Method", "Synonym-Of", "Quantization Error Minimization:Method"], ["Quantization Error Minimization:Method", "Compare-With", "RCF:Method"], ["Quantization Error Minimization:Method", "Used-For", "ResNet - 1 8:Method"], ["RCF:Method", "Used-For", "ResNet - 1 8:Method"]]}
{"doc_id": "203593581", "sentence": "Figure 4 gives an overview of the clipping error and projection error using RCF or QEM .", "ner": [["RCF", "Method"], ["QEM", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "203593581", "sentence": "For the 5 - bit quantized model , RCF has a much higher quantization error .", "ner": [["5 - bit quantized model", "Method"], ["RCF", "Method"]], "rel": [["RCF", "SubClass-Of", "5 - bit quantized model"]], "rel_plus": [["RCF:Method", "SubClass-Of", "5 - bit quantized model:Method"]]}
{"doc_id": "203593581", "sentence": "The projection error obtained by RCF is lower than QEM and QEM significantly reduces the clipping error .", "ner": [["RCF", "Method"], ["QEM", "Method"], ["QEM", "Method"]], "rel": [["RCF", "Compare-With", "QEM"]], "rel_plus": [["RCF:Method", "Compare-With", "QEM:Method"]]}
{"doc_id": "203593581", "sentence": "Based on the truth that RCF has better final accuracy , we can infer that projection error has a higher priority in RCF .", "ner": [["RCF", "Method"], ["RCF", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "203593581", "sentence": "When quantizing to 3 - bit , the clipping error in RCF still exceeds QEM except for the first quantized layer .", "ner": [["RCF", "Method"], ["QEM", "Method"]], "rel": [["RCF", "Compare-With", "QEM"]], "rel_plus": [["RCF:Method", "Compare-With", "QEM:Method"]]}
{"doc_id": "203593581", "sentence": "Since other quantization methods do not use RCF or normalization on weights , the effectiveness of our APoT quantization is not well evaluated .", "ner": [["quantization", "Method"], ["RCF", "Method"], ["normalization", "Method"], ["APoT quantization", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "203593581", "sentence": "For 5 bit quantized models , we evaluate uniform quantization , APoT quantization , and the vanilla PoT quantization .", "ner": [["5 bit quantized models", "Method"], ["uniform quantization", "Method"], ["APoT quantization", "Method"], ["PoT quantization", "Method"]], "rel": [["uniform quantization", "SubClass-Of", "5 bit quantized models"], ["APoT quantization", "SubClass-Of", "5 bit quantized models"], ["PoT quantization", "SubClass-Of", "5 bit quantized models"]], "rel_plus": [["uniform quantization:Method", "SubClass-Of", "5 bit quantized models:Method"], ["APoT quantization:Method", "SubClass-Of", "5 bit quantized models:Method"], ["PoT quantization:Method", "SubClass-Of", "5 bit quantized models:Method"]]}
{"doc_id": "203593581", "sentence": "As for 3 bit , APoT resembles PoT , therefore we evaluate APoT quantization and uniform quantization .", "ner": [["APoT", "Method"], ["PoT", "Method"], ["APoT quantization", "Method"], ["uniform quantization", "Method"]], "rel": [["PoT", "Part-Of", "APoT"]], "rel_plus": [["PoT:Method", "Part-Of", "APoT:Method"]]}
{"doc_id": "203593581", "sentence": "Table 2 shows the Top - 1 accuracy of ResNet - 1 8 on ImageNet with the above quantization methods .", "ner": [["ResNet - 1 8", "Method"], ["ImageNet", "Dataset"], ["quantization", "Method"]], "rel": [["quantization", "Used-For", "ResNet - 1 8"], ["ResNet - 1 8", "Evaluated-With", "ImageNet"]], "rel_plus": [["quantization:Method", "Used-For", "ResNet - 1 8:Method"], ["ResNet - 1 8:Method", "Evaluated-With", "ImageNet:Dataset"]]}
{"doc_id": "203593581", "sentence": "For both 3 - bit and 5 - bit setting , APoT achieves higher accuracy than uniform quantization as well as higher hardware efficiency .    In this section , we show some experimental results to illustrate the effect of our weights normalization in quantization neural networks .", "ner": [["APoT", "Method"], ["uniform quantization", "Method"], ["weights normalization", "Method"], ["quantization neural networks", "Method"]], "rel": [["APoT", "Compare-With", "uniform quantization"], ["weights normalization", "Part-Of", "quantization neural networks"]], "rel_plus": [["APoT:Method", "Compare-With", "uniform quantization:Method"], ["weights normalization:Method", "Part-Of", "quantization neural networks:Method"]]}
{"doc_id": "203593581", "sentence": "In this section , we compare the training of quantization neural networks with and without normalization to investigate the real effect of WN .", "ner": [["quantization neural networks", "Method"], ["normalization", "Method"], ["WN", "Method"]], "rel": [["normalization", "Part-Of", "quantization neural networks"]], "rel_plus": [["normalization:Method", "Part-Of", "quantization neural networks:Method"]]}
{"doc_id": "203593581", "sentence": "Based on the training behaviors , we can conclude that the learning for clipping threshold without WN in QNNs need a careful choice .", "ner": [["WN", "Method"], ["QNNs", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "203593581", "sentence": "We use the Pytorch official code 5 to construct ResNet - 1 8 and ResNet - 3 4 , and they are initialized from the released pretrained model .", "ner": [["ResNet - 1 8", "Method"], ["ResNet - 3 4", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "203593581", "sentence": "We use stochastic gradient descent ( SGD ) with the momentum of 0. 9 to optimize both weight parameters and the clipping threshold simultaneously .", "ner": [["stochastic gradient descent", "Method"], ["SGD", "Method"], ["momentum", "Method"]], "rel": [["SGD", "Synonym-Of", "stochastic gradient descent"], ["momentum", "Part-Of", "stochastic gradient descent"]], "rel_plus": [["SGD:Method", "Synonym-Of", "stochastic gradient descent:Method"], ["momentum:Method", "Part-Of", "stochastic gradient descent:Method"]]}
{"doc_id": "203593581", "sentence": "The network is trained up to 1 2 0 epochs and weight decay is set to 5 \u00d7 1 0 \u2212 5 for ResNet - 1 8 and 1 0 \u2212 4 for ResNet - 3 4 .", "ner": [["weight decay", "Method"], ["ResNet - 1 8", "Method"], ["ResNet - 3 4", "Method"]], "rel": [["weight decay", "Part-Of", "ResNet - 1 8"], ["weight decay", "Part-Of", "ResNet - 3 4"]], "rel_plus": [["weight decay:Method", "Part-Of", "ResNet - 1 8:Method"], ["weight decay:Method", "Part-Of", "ResNet - 3 4:Method"]]}
{"doc_id": "203593581", "sentence": "The ResNet architectures for CIFAR 1 0 ( He et al. , 2 0 1 6 ) contains a convolutional layer followed by 3 residual blocks and a final FC layer .", "ner": [["ResNet", "Method"], ["CIFAR 1 0", "Dataset"], ["convolutional layer", "Method"], ["residual blocks", "Method"], ["FC layer", "Method"]], "rel": [["convolutional layer", "Part-Of", "ResNet"], ["residual blocks", "Part-Of", "ResNet"], ["FC layer", "Part-Of", "ResNet"], ["ResNet", "Used-For", "CIFAR 1 0"]], "rel_plus": [["convolutional layer:Method", "Part-Of", "ResNet:Method"], ["residual blocks:Method", "Part-Of", "ResNet:Method"], ["FC layer:Method", "Part-Of", "ResNet:Method"], ["ResNet:Method", "Used-For", "CIFAR 1 0:Dataset"]]}
{"doc_id": "203593581", "sentence": "We train full precision ResNet - 2 0 and ResNet - 5 6 firstly and use them as initialization for quantized models .", "ner": [["ResNet - 2 0", "Method"], ["ResNet - 5 6", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "203593581", "sentence": "Different from PACT ( Choi et al. , 2 0 1 8 b ) , the update of \u03b1 in our works already consider the projection error , so we do not require a relatively large L 2 - regularization for it .", "ner": [["PACT", "Method"], ["L 2 - regularization", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "203593581", "sentence": "In this section , we apply APoT quantization to activations .", "ner": [["APoT quantization", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "203593581", "sentence": "For weights quantization , we use uniform quantization .", "ner": [["weights quantization", "Method"], ["uniform quantization", "Method"]], "rel": [["uniform quantization", "Used-For", "weights quantization"]], "rel_plus": [["uniform quantization:Method", "Used-For", "weights quantization:Method"]]}
{"doc_id": "203593581", "sentence": "Table 5 summarizes the top - 1 accuracy comparison of APoT and uniform quantization for activations .", "ner": [["APoT", "Method"], ["uniform quantization", "Method"]], "rel": [["APoT", "Compare-With", "uniform quantization"]], "rel_plus": [["APoT:Method", "Compare-With", "uniform quantization:Method"]]}
{"doc_id": "203593581", "sentence": "For 4 - bit quantization , APoT achieves the same accuracy with uniform quantization , while the uniform quantization can surpass APoT in the 2 - bit scenario .", "ner": [["4 - bit quantization", "Task"], ["APoT", "Method"], ["uniform quantization", "Method"], ["uniform quantization", "Method"], ["APoT", "Method"]], "rel": [["APoT", "Used-For", "4 - bit quantization"], ["uniform quantization", "Used-For", "4 - bit quantization"], ["APoT", "Compare-With", "uniform quantization"], ["uniform quantization", "Compare-With", "APoT"]], "rel_plus": [["APoT:Method", "Used-For", "4 - bit quantization:Task"], ["uniform quantization:Method", "Used-For", "4 - bit quantization:Task"], ["APoT:Method", "Compare-With", "uniform quantization:Method"], ["uniform quantization:Method", "Compare-With", "APoT:Method"]]}
{"doc_id": "203593581", "sentence": "Our proposed APoT quantization do not have significant advantages when quantizing activations .", "ner": [["APoT quantization", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "201070522", "sentence": "We believe that language features deserve more attention , and conduct experiments which compare different word embeddings , language models , and embedding augmentation steps on five common VL tasks : image - sentence retrieval , image captioning , visual question answering , phrase grounding , and text - to - clip retrieval .", "ner": [["VL", "Task"], ["image - sentence retrieval", "Task"], ["image captioning", "Task"], ["visual question answering", "Task"], ["phrase grounding", "Task"], ["text - to - clip retrieval", "Task"]], "rel": [["image - sentence retrieval", "SubTask-Of", "VL"], ["image captioning", "SubTask-Of", "VL"], ["visual question answering", "SubTask-Of", "VL"], ["phrase grounding", "SubTask-Of", "VL"], ["text - to - clip retrieval", "SubTask-Of", "VL"]], "rel_plus": [["image - sentence retrieval:Task", "SubTask-Of", "VL:Task"], ["image captioning:Task", "SubTask-Of", "VL:Task"], ["visual question answering:Task", "SubTask-Of", "VL:Task"], ["phrase grounding:Task", "SubTask-Of", "VL:Task"], ["text - to - clip retrieval:Task", "SubTask-Of", "VL:Task"]]}
{"doc_id": "201070522", "sentence": "Our experiments provide some striking results ; an average embedding language model outperforms an LSTM on retrieval - style tasks ; state - of - the - art representations such as BERT perform relatively poorly on vision - language tasks .", "ner": [["LSTM", "Method"], ["retrieval - style tasks", "Task"], ["BERT", "Method"], ["vision - language", "Task"]], "rel": [["LSTM", "Used-For", "retrieval - style tasks"], ["BERT", "Used-For", "vision - language"]], "rel_plus": [["LSTM:Method", "Used-For", "retrieval - style tasks:Task"], ["BERT:Method", "Used-For", "vision - language:Task"]]}
{"doc_id": "201070522", "sentence": "This multi - task training is applied to a new Graph Oriented Vision - Language Embedding ( GrOVLE ) , which we adapt from Word 2 Vec using WordNet and an original visual - language graph built from Visual Genome , providing a ready - to - use vision - language embedding : http://ai.bu.edu/grovle .", "ner": [["Graph Oriented Vision - Language Embedding", "Method"], ["GrOVLE", "Method"], ["Word 2 Vec", "Method"], ["WordNet", "Dataset"], ["visual - language graph", "Method"], ["Visual Genome", "Dataset"], ["vision - language embedding", "Method"]], "rel": [["GrOVLE", "Synonym-Of", "Graph Oriented Vision - Language Embedding"], ["Word 2 Vec", "Used-For", "Graph Oriented Vision - Language Embedding"], ["visual - language graph", "Used-For", "Graph Oriented Vision - Language Embedding"], ["Word 2 Vec", "Trained-With", "WordNet"], ["visual - language graph", "Trained-With", "Visual Genome"], ["Graph Oriented Vision - Language Embedding", "Used-For", "vision - language embedding"]], "rel_plus": [["GrOVLE:Method", "Synonym-Of", "Graph Oriented Vision - Language Embedding:Method"], ["Word 2 Vec:Method", "Used-For", "Graph Oriented Vision - Language Embedding:Method"], ["visual - language graph:Method", "Used-For", "Graph Oriented Vision - Language Embedding:Method"], ["Word 2 Vec:Method", "Trained-With", "WordNet:Dataset"], ["visual - language graph:Method", "Trained-With", "Visual Genome:Dataset"], ["Graph Oriented Vision - Language Embedding:Method", "Used-For", "vision - language embedding:Method"]]}
{"doc_id": "201070522", "sentence": "In recent years many methods have been proposed for vision - language tasks such as image and video captioning [ 1 2 , 2 7 , 4 7 , 4 8 , 5 2 ] , multimodal retrieval [ 1 6 , 2 4 , 2 0 , 4 9 , 3 7 , 4 6 , 5 1 ] , phrase grounding [ 4 2 , 1 9 , 4 1 , 4 3 ] , and visual question answering [ 1 4 , 2 , 5 6 , 4 4 , 5 4 ] .", "ner": [["vision - language", "Task"], ["image and video captioning", "Task"], ["multimodal retrieval", "Task"], ["phrase grounding", "Task"], ["visual question answering", "Task"]], "rel": [["image and video captioning", "SubTask-Of", "vision - language"], ["multimodal retrieval", "SubTask-Of", "vision - language"], ["phrase grounding", "SubTask-Of", "vision - language"], ["visual question answering", "SubTask-Of", "vision - language"]], "rel_plus": [["image and video captioning:Task", "SubTask-Of", "vision - language:Task"], ["multimodal retrieval:Task", "SubTask-Of", "vision - language:Task"], ["phrase grounding:Task", "SubTask-Of", "vision - language:Task"], ["visual question answering:Task", "SubTask-Of", "vision - language:Task"]]}
{"doc_id": "201070522", "sentence": "We provide a side by side comparison of how word - level and sentence - level embeddings , simple and more complex language models , and fine - tuning and post - processing vectors impact performance . these tasks include a simple one - hot encoding of each word in a vocabulary ( e.g. [ 1 4 , 4 8 , 4 9 ] ) , pretrained dense vector representations like Word 2 Vec [ 3 5 ] or GloVe [ 3 8 ] , and Fisher vectors built on top of these dense representations ( e.g. [ 2 4 , 4 0 , 4 9 ] ) .", "ner": [["sentence - level embeddings", "Method"], ["pretrained dense vector representations", "Method"], ["Word 2 Vec", "Method"], ["GloVe", "Method"], ["Fisher vectors", "Method"]], "rel": [["Word 2 Vec", "SubClass-Of", "pretrained dense vector representations"], ["GloVe", "SubClass-Of", "pretrained dense vector representations"], ["Fisher vectors", "SubClass-Of", "pretrained dense vector representations"]], "rel_plus": [["Word 2 Vec:Method", "SubClass-Of", "pretrained dense vector representations:Method"], ["GloVe:Method", "SubClass-Of", "pretrained dense vector representations:Method"], ["Fisher vectors:Method", "SubClass-Of", "pretrained dense vector representations:Method"]]}
{"doc_id": "201070522", "sentence": "Although there are more modern embeddings such as FastText [ 4 ] , ELMo [ 3 9 ] and BERT [ 9 ] that have shown significant performance improvements on language tasks such as sentiment analysis and question answering , many vision - language approaches still use the more dated feature representations .", "ner": [["FastText", "Method"], ["ELMo", "Method"], ["BERT", "Method"], ["sentiment analysis", "Task"], ["question answering", "Task"]], "rel": [["FastText", "Used-For", "sentiment analysis"], ["ELMo", "Used-For", "sentiment analysis"], ["BERT", "Used-For", "sentiment analysis"], ["FastText", "Used-For", "question answering"], ["ELMo", "Used-For", "question answering"], ["BERT", "Used-For", "question answering"]], "rel_plus": [["FastText:Method", "Used-For", "sentiment analysis:Task"], ["ELMo:Method", "Used-For", "sentiment analysis:Task"], ["BERT:Method", "Used-For", "sentiment analysis:Task"], ["FastText:Method", "Used-For", "question answering:Task"], ["ELMo:Method", "Used-For", "question answering:Task"], ["BERT:Method", "Used-For", "question answering:Task"]]}
{"doc_id": "201070522", "sentence": "We perform experiments using from - scratch , Word 2 Vec [ 3 5 ] , WordNet retrofitted Word 2 Vec [ 1 3 ] , Fast - Text [ 4 ] , Visual Word 2 Vec [ 2 6 ] , HGLMM ( 3 0 0 - D , 6K - D ) [ 2 4 ] , InferSent [ 8 ] , and BERT [ 9 ] representations in addition to a new embedding , GrOVLE , on five visionlanguage tasks : image - sentence retrieval , visual question answering , phrase grounding , image captioning , and textto - clip retrieval .", "ner": [["Word 2 Vec", "Method"], ["WordNet", "Dataset"], ["Word 2 Vec", "Method"], ["Fast - Text", "Method"], ["Visual Word 2 Vec", "Method"], ["HGLMM", "Method"], ["InferSent", "Method"], ["BERT", "Method"], ["GrOVLE", "Method"], ["image - sentence retrieval", "Task"], ["visual question answering", "Task"], ["phrase grounding", "Task"], ["image captioning", "Task"], ["textto - clip retrieval", "Task"]], "rel": [["Word 2 Vec", "Trained-With", "WordNet"], ["Word 2 Vec", "Used-For", "image - sentence retrieval"], ["Word 2 Vec", "Used-For", "image - sentence retrieval"], ["Fast - Text", "Used-For", "image - sentence retrieval"], ["InferSent", "Used-For", "image - sentence retrieval"], ["BERT", "Used-For", "image - sentence retrieval"], ["Visual Word 2 Vec", "Used-For", "image - sentence retrieval"], ["HGLMM", "Used-For", "image - sentence retrieval"], ["GrOVLE", "Used-For", "image - sentence retrieval"], ["Visual Word 2 Vec", "Used-For", "visual question answering"], ["Word 2 Vec", "Used-For", "visual question answering"], ["Word 2 Vec", "Used-For", "visual question answering"], ["Fast - Text", "Used-For", "visual question answering"], ["HGLMM", "Used-For", "visual question answering"], ["InferSent", "Used-For", "visual question answering"], ["BERT", "Used-For", "visual question answering"], ["HGLMM", "Used-For", "phrase grounding"], ["Word 2 Vec", "Used-For", "phrase grounding"], ["Word 2 Vec", "Used-For", "phrase grounding"], ["Fast - Text", "Used-For", "phrase grounding"], ["InferSent", "Used-For", "phrase grounding"], ["BERT", "Used-For", "phrase grounding"], ["Word 2 Vec", "Used-For", "image captioning"], ["Word 2 Vec", "Used-For", "image captioning"], ["Fast - Text", "Used-For", "image captioning"], ["Visual Word 2 Vec", "Used-For", "image captioning"], ["HGLMM", "Used-For", "image captioning"], ["InferSent", "Used-For", "image captioning"], ["BERT", "Used-For", "image captioning"], ["Word 2 Vec", "Used-For", "textto - clip retrieval"], ["Word 2 Vec", "Used-For", "textto - clip retrieval"], ["Fast - Text", "Used-For", "textto - clip retrieval"], ["Visual Word 2 Vec", "Used-For", "textto - clip retrieval"], ["HGLMM", "Used-For", "textto - clip retrieval"], ["InferSent", "Used-For", "textto - clip retrieval"], ["BERT", "Used-For", "textto - clip retrieval"]], "rel_plus": [["Word 2 Vec:Method", "Trained-With", "WordNet:Dataset"], ["Word 2 Vec:Method", "Used-For", "image - sentence retrieval:Task"], ["Word 2 Vec:Method", "Used-For", "image - sentence retrieval:Task"], ["Fast - Text:Method", "Used-For", "image - sentence retrieval:Task"], ["InferSent:Method", "Used-For", "image - sentence retrieval:Task"], ["BERT:Method", "Used-For", "image - sentence retrieval:Task"], ["Visual Word 2 Vec:Method", "Used-For", "image - sentence retrieval:Task"], ["HGLMM:Method", "Used-For", "image - sentence retrieval:Task"], ["GrOVLE:Method", "Used-For", "image - sentence retrieval:Task"], ["Visual Word 2 Vec:Method", "Used-For", "visual question answering:Task"], ["Word 2 Vec:Method", "Used-For", "visual question answering:Task"], ["Word 2 Vec:Method", "Used-For", "visual question answering:Task"], ["Fast - Text:Method", "Used-For", "visual question answering:Task"], ["HGLMM:Method", "Used-For", "visual question answering:Task"], ["InferSent:Method", "Used-For", "visual question answering:Task"], ["BERT:Method", "Used-For", "visual question answering:Task"], ["HGLMM:Method", "Used-For", "phrase grounding:Task"], ["Word 2 Vec:Method", "Used-For", "phrase grounding:Task"], ["Word 2 Vec:Method", "Used-For", "phrase grounding:Task"], ["Fast - Text:Method", "Used-For", "phrase grounding:Task"], ["InferSent:Method", "Used-For", "phrase grounding:Task"], ["BERT:Method", "Used-For", "phrase grounding:Task"], ["Word 2 Vec:Method", "Used-For", "image captioning:Task"], ["Word 2 Vec:Method", "Used-For", "image captioning:Task"], ["Fast - Text:Method", "Used-For", "image captioning:Task"], ["Visual Word 2 Vec:Method", "Used-For", "image captioning:Task"], ["HGLMM:Method", "Used-For", "image captioning:Task"], ["InferSent:Method", "Used-For", "image captioning:Task"], ["BERT:Method", "Used-For", "image captioning:Task"], ["Word 2 Vec:Method", "Used-For", "textto - clip retrieval:Task"], ["Word 2 Vec:Method", "Used-For", "textto - clip retrieval:Task"], ["Fast - Text:Method", "Used-For", "textto - clip retrieval:Task"], ["Visual Word 2 Vec:Method", "Used-For", "textto - clip retrieval:Task"], ["HGLMM:Method", "Used-For", "textto - clip retrieval:Task"], ["InferSent:Method", "Used-For", "textto - clip retrieval:Task"], ["BERT:Method", "Used-For", "textto - clip retrieval:Task"]]}
{"doc_id": "201070522", "sentence": "For example , we find that using an Average Embedding language model , which ignores word ordering , tends to perform better than a LSTM .", "ner": [["Average Embedding", "Method"], ["LSTM", "Method"]], "rel": [["Average Embedding", "Part-Of", "LSTM"]], "rel_plus": [["Average Embedding:Method", "Part-Of", "LSTM:Method"]]}
{"doc_id": "201070522", "sentence": "For example , in Word 2 Vec the words \" boy \" and \" girl \" have higher cosine similarity than either have to the word \" child . \" While this is a subtle difference , it can impact tasks such as image captioning where \" girl \" can be replaced by \" child \" when describing a visual scene , but not by \" boy . \" These nuances are not well captured when using text - only information .", "ner": [["Word 2 Vec", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "201070522", "sentence": "To address this , we introduce the Graph Oriented Vision - Language Embedding , GrOVLE , which has been learned for vision - language tasks specifically .", "ner": [["Graph Oriented Vision - Language Embedding", "Method"], ["GrOVLE", "Method"], ["vision - language", "Task"]], "rel": [["GrOVLE", "Synonym-Of", "Graph Oriented Vision - Language Embedding"]], "rel_plus": [["GrOVLE:Method", "Synonym-Of", "Graph Oriented Vision - Language Embedding:Method"]]}
{"doc_id": "201070522", "sentence": "Inspired by multi - task training strategies like PackNet [ 3 4 ] , we train the GrOVLE embedding on all the vision - language tasks in our experiments .", "ner": [["multi - task training strategies", "Method"], ["PackNet", "Method"], ["GrOVLE", "Method"], ["vision - language", "Task"]], "rel": [["GrOVLE", "Used-For", "vision - language"]], "rel_plus": [["GrOVLE:Method", "Used-For", "vision - language:Task"]]}
{"doc_id": "201070522", "sentence": "Note that unlike PackNet , GrOVLE operates directly on the word embeddings rather than model weights .", "ner": [["PackNet", "Method"], ["GrOVLE", "Method"], ["word embeddings", "Method"]], "rel": [["GrOVLE", "Compare-With", "PackNet"]], "rel_plus": [["GrOVLE:Method", "Compare-With", "PackNet:Method"]]}
{"doc_id": "201070522", "sentence": "Variance is defined as the average difference between the best and worst performance of the fine - tuned language model options ( e.g. Average Embedding + ft , Self - Attention + ft , LSTM + ft ) .", "ner": [["Average Embedding + ft", "Method"], ["Self - Attention + ft", "Method"], ["LSTM + ft", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "201070522", "sentence": "For the tasks InferSent and BERT operate on , they would land between 7th and 8th place for average rank ; average variance is N/A. Note that average variance is not provided for multi - task trained GrOVLE as it was created with the best model for each task . has been specially trained for vision - language tasks 1 . \u2022 Key insight into the transferability of word embeddings across the five vision - language tasks through the use of multi - task training .", "ner": [["InferSent", "Method"], ["BERT", "Method"], ["GrOVLE", "Method"], ["vision - language", "Task"], ["multi - task training", "Method"]], "rel": [["multi - task training", "Used-For", "vision - language"]], "rel_plus": [["multi - task training:Method", "Used-For", "vision - language:Task"]]}
{"doc_id": "201070522", "sentence": "Attention mechanisms have also become a popular way to improve performance : word - level attention has been used in image captioning by learning the weights of words using a LSTM [ 1 ] or a multi - layered perceptron [ 5 2 , 1 1 ] before being passed to a language generation model .", "ner": [["Attention mechanisms", "Method"], ["word - level attention", "Method"], ["image captioning", "Task"], ["LSTM", "Method"], ["multi - layered perceptron", "Method"]], "rel": [["word - level attention", "Used-For", "image captioning"], ["LSTM", "Used-For", "image captioning"], ["multi - layered perceptron", "Used-For", "image captioning"]], "rel_plus": [["word - level attention:Method", "Used-For", "image captioning:Task"], ["LSTM:Method", "Used-For", "image captioning:Task"], ["multi - layered perceptron:Method", "Used-For", "image captioning:Task"]]}
{"doc_id": "201070522", "sentence": "Dual attention [ 3 7 ] has also been used to attend to the question in VQA using feed - forward neural networks .", "ner": [["Dual attention", "Method"], ["VQA", "Task"], ["feed - forward neural networks", "Method"]], "rel": [["Dual attention", "Used-For", "VQA"], ["feed - forward neural networks", "Used-For", "VQA"]], "rel_plus": [["Dual attention:Method", "Used-For", "VQA:Task"], ["feed - forward neural networks:Method", "Used-For", "VQA:Task"]]}
{"doc_id": "201070522", "sentence": "In Figure 3 an Average Embedding , Self - Attention , and LSTM language architecture are shown .", "ner": [["Average Embedding", "Method"], ["Self - Attention", "Method"], ["LSTM", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "201070522", "sentence": "A more complex language architecture is a LSTM ; word representations are individually passed through a LSTM cell , each producing their own hidden state .", "ner": [["LSTM", "Method"], ["LSTM", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "201070522", "sentence": "Lastly , we compare a Self - Attention model that is closely related to the Average Embedding architecture .", "ner": [["Self - Attention", "Method"], ["Average Embedding", "Method"]], "rel": [["Self - Attention", "Compare-With", "Average Embedding"]], "rel_plus": [["Self - Attention:Method", "Compare-With", "Average Embedding:Method"]]}
{"doc_id": "201070522", "sentence": "It is passed through a fully connected layer which applies Softmax to give context \" scores \" for each word in a sentence .", "ner": [["fully connected layer", "Method"], ["Softmax", "Method"]], "rel": [["Softmax", "Part-Of", "fully connected layer"]], "rel_plus": [["Softmax:Method", "Part-Of", "fully connected layer:Method"]]}
{"doc_id": "201070522", "sentence": "Word 2 Vec , FastText , InferSent , and BERT are reviewed before results are discussed .", "ner": [["Word 2 Vec", "Method"], ["FastText", "Method"], ["InferSent", "Method"], ["BERT", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "201070522", "sentence": "Word 2 Vec introduced two variations of the NNLM model , with the primary distinction being that the nonlinear hidden layer is removed and the projection layer is shared amongst all words , i.e. the words are averaged .", "ner": [["Word 2 Vec", "Method"], ["NNLM", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "201070522", "sentence": "This leads to the first model , Continuous Bag of Words ( CBOW ) , in which given four previous and four future words , the current word is predicted .", "ner": [["Continuous Bag of Words", "Method"], ["CBOW", "Method"]], "rel": [["CBOW", "Synonym-Of", "Continuous Bag of Words"]], "rel_plus": [["CBOW:Method", "Synonym-Of", "Continuous Bag of Words:Method"]]}
{"doc_id": "201070522", "sentence": "FastText [ 4 ] is an extension of the Word 2 Vec model in which the atomic entities of the embeddings are no longer words , but are instead character n - grams .", "ner": [["FastText", "Method"], ["Word 2 Vec", "Method"]], "rel": [["FastText", "SubClass-Of", "Word 2 Vec"]], "rel_plus": [["FastText:Method", "SubClass-Of", "Word 2 Vec:Method"]]}
{"doc_id": "201070522", "sentence": "InferSent [ 8 ] uses a bi - directional LSTM with max - pooling to create a sentence - level embedding .", "ner": [["InferSent", "Method"], ["bi - directional LSTM", "Method"], ["max - pooling", "Method"], ["sentence - level embedding", "Method"]], "rel": [["bi - directional LSTM", "Part-Of", "InferSent"], ["max - pooling", "Part-Of", "bi - directional LSTM"], ["InferSent", "Used-For", "sentence - level embedding"]], "rel_plus": [["bi - directional LSTM:Method", "Part-Of", "InferSent:Method"], ["max - pooling:Method", "Part-Of", "bi - directional LSTM:Method"], ["InferSent:Method", "Used-For", "sentence - level embedding:Method"]]}
{"doc_id": "201070522", "sentence": "It is trained using the Natural Language Inference ( NLI ) task , in which the goal is to categorize natural language English sentence ( premise , hypothesis ) pairs into three classes : entailment , contradiction , and neutral .", "ner": [["Natural Language Inference", "Task"], ["NLI", "Task"]], "rel": [["NLI", "Synonym-Of", "Natural Language Inference"]], "rel_plus": [["NLI:Task", "Synonym-Of", "Natural Language Inference:Task"]]}
{"doc_id": "201070522", "sentence": "The NLI model architecture separately encodes each sentence of the input pair using a BiLSTM .", "ner": [["NLI", "Task"], ["BiLSTM", "Method"]], "rel": [["BiLSTM", "Used-For", "NLI"]], "rel_plus": [["BiLSTM:Method", "Used-For", "NLI:Task"]]}
{"doc_id": "201070522", "sentence": "This vector is then fed into a three - class classifier , defined by several FC layers and a Softmax .", "ner": [["FC", "Method"], ["Softmax", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "201070522", "sentence": "The embedding is trained on two tasks : Masked Language Modeling ( MLM ) and Next Sentence Prediction .", "ner": [["Masked Language Modeling", "Task"], ["MLM", "Task"], ["Next Sentence Prediction", "Task"]], "rel": [["MLM", "Synonym-Of", "Masked Language Modeling"]], "rel_plus": [["MLM:Task", "Synonym-Of", "Masked Language Modeling:Task"]]}
{"doc_id": "201070522", "sentence": "This is more important when considering a larger vocabulary as seen comparing phrase grounding experiments on DiDeMo and ReferIt , whose embeddings trained from scratch using their smaller vocabulary compare favorably to Word 2 Vec .", "ner": [["phrase grounding", "Task"], ["DiDeMo", "Dataset"], ["ReferIt", "Dataset"], ["Word 2 Vec", "Method"]], "rel": [["DiDeMo", "Benchmark-For", "phrase grounding"], ["ReferIt", "Benchmark-For", "phrase grounding"]], "rel_plus": [["DiDeMo:Dataset", "Benchmark-For", "phrase grounding:Task"], ["ReferIt:Dataset", "Benchmark-For", "phrase grounding:Task"]]}
{"doc_id": "201070522", "sentence": "The original Word 2 Vec embedding pretrained on Google News can be considered a second baseline .", "ner": [["Word 2 Vec", "Method"], ["Google News", "Dataset"]], "rel": [["Word 2 Vec", "Trained-With", "Google News"]], "rel_plus": [["Word 2 Vec:Method", "Trained-With", "Google News:Dataset"]]}
{"doc_id": "201070522", "sentence": "While Fast - Text is a more modern embedding , Word 2 Vec only falls behind within a point or two across all tasks , and even outperforms or performs equally as well as FastText for certain tasks ( e.g. text - to - clip , image captioning ) .", "ner": [["Fast - Text", "Method"], ["Word 2 Vec", "Method"], ["FastText", "Method"], ["text - to - clip", "Task"], ["image captioning", "Task"]], "rel": [["Word 2 Vec", "Compare-With", "FastText"], ["Word 2 Vec", "Used-For", "text - to - clip"], ["FastText", "Used-For", "text - to - clip"], ["Word 2 Vec", "Used-For", "image captioning"], ["FastText", "Used-For", "image captioning"]], "rel_plus": [["Word 2 Vec:Method", "Compare-With", "FastText:Method"], ["Word 2 Vec:Method", "Used-For", "text - to - clip:Task"], ["FastText:Method", "Used-For", "text - to - clip:Task"], ["Word 2 Vec:Method", "Used-For", "image captioning:Task"], ["FastText:Method", "Used-For", "image captioning:Task"]]}
{"doc_id": "201070522", "sentence": "This validates works which extend Word 2 Vec such as Retrofitting , HGLMM Fisher Vectors , and GrOVLE , as Word 2 Vec may still provide advantages with additional adaptations ; results for adapted embeddings follow in Section 6 .", "ner": [["Word 2 Vec", "Method"], ["Retrofitting", "Method"], ["HGLMM Fisher Vectors", "Method"], ["GrOVLE", "Method"], ["Word 2 Vec", "Method"]], "rel": [["Retrofitting", "SubClass-Of", "Word 2 Vec"], ["HGLMM Fisher Vectors", "SubClass-Of", "Word 2 Vec"], ["GrOVLE", "SubClass-Of", "Word 2 Vec"]], "rel_plus": [["Retrofitting:Method", "SubClass-Of", "Word 2 Vec:Method"], ["HGLMM Fisher Vectors:Method", "SubClass-Of", "Word 2 Vec:Method"], ["GrOVLE:Method", "SubClass-Of", "Word 2 Vec:Method"]]}
{"doc_id": "201070522", "sentence": "When comparing the architecture choices from Figure 3 we see that for retrieval - based tasks ( i.e. where the output is not free - form text ) the Average Embedding and Self - Attention models perform better than a simple LSTM - based approach , with Self - Attention being best on average .", "ner": [["Average Embedding", "Method"], ["Self - Attention", "Method"], ["LSTM - based approach", "Method"], ["Self - Attention", "Method"]], "rel": [["Average Embedding", "Compare-With", "LSTM - based approach"], ["Self - Attention", "Compare-With", "LSTM - based approach"]], "rel_plus": [["Average Embedding:Method", "Compare-With", "LSTM - based approach:Method"], ["Self - Attention:Method", "Compare-With", "LSTM - based approach:Method"]]}
{"doc_id": "201070522", "sentence": "While all language models perform closely on ReferIt phrase grounding , this still suggests that there is no need to use the more complex LSTM language model without additional modification .", "ner": [["ReferIt", "Dataset"], ["phrase grounding", "Task"], ["LSTM", "Method"]], "rel": [["ReferIt", "Benchmark-For", "phrase grounding"]], "rel_plus": [["ReferIt:Dataset", "Benchmark-For", "phrase grounding:Task"]]}
{"doc_id": "201070522", "sentence": "Lastly , sentence level embeddings InferSent and BERT are compared in Table 1 ( d ) ; results are without fine - tuning .", "ner": [["InferSent", "Method"], ["BERT", "Method"]], "rel": [["InferSent", "Compare-With", "BERT"]], "rel_plus": [["InferSent:Method", "Compare-With", "BERT:Method"]]}
{"doc_id": "201070522", "sentence": "Fine - tuning would likely improve performance , but is difficult to incorporate due to size ( e.g. the larger BERT model contains a total of 3 4 0 M parameters while the well - known VGG - 1 6 network uses 1 3 8 M ; fine - tuning the top layers of BERT still requires loading the full model ) .", "ner": [["BERT", "Method"], ["VGG - 1 6", "Method"], ["BERT", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "201070522", "sentence": "The two are comparable to each other with the exception of phrase grounding accuracy on Flickr 3 0 K Entities ; BERT surprisingly outperforms InferSent by 1 1 . 5 5 % .", "ner": [["phrase grounding", "Task"], ["Flickr 3 0 K Entities", "Dataset"], ["BERT", "Method"], ["InferSent", "Method"]], "rel": [["Flickr 3 0 K Entities", "Benchmark-For", "phrase grounding"], ["BERT", "Compare-With", "InferSent"]], "rel_plus": [["Flickr 3 0 K Entities:Dataset", "Benchmark-For", "phrase grounding:Task"], ["BERT:Method", "Compare-With", "InferSent:Method"]]}
{"doc_id": "201070522", "sentence": "Both InferSent and BERT do not provide the best results across any task , and thus are not a leading option for vision - language tasks .", "ner": [["InferSent", "Method"], ["BERT", "Method"], ["vision - language", "Task"]], "rel": [["InferSent", "Used-For", "vision - language"], ["BERT", "Used-For", "vision - language"]], "rel_plus": [["InferSent:Method", "Used-For", "vision - language:Task"], ["BERT:Method", "Used-For", "vision - language:Task"]]}
{"doc_id": "201070522", "sentence": "InferSent and BERT reach comparable values to the best Word 2 Vec models for image - sentence retrieval on Flickr 3 0 K , performing more poorly for the MSCOCO dataset .", "ner": [["InferSent", "Method"], ["BERT", "Method"], ["Word 2 Vec", "Method"], ["image - sentence retrieval", "Task"], ["Flickr 3 0 K", "Dataset"], ["MSCOCO", "Dataset"]], "rel": [["BERT", "Used-For", "image - sentence retrieval"], ["InferSent", "Used-For", "image - sentence retrieval"], ["Word 2 Vec", "Used-For", "image - sentence retrieval"], ["Flickr 3 0 K", "Benchmark-For", "image - sentence retrieval"], ["BERT", "Evaluated-With", "Flickr 3 0 K"], ["InferSent", "Evaluated-With", "Flickr 3 0 K"], ["Word 2 Vec", "Evaluated-With", "Flickr 3 0 K"], ["InferSent", "Evaluated-With", "MSCOCO"], ["BERT", "Evaluated-With", "MSCOCO"], ["Word 2 Vec", "Evaluated-With", "MSCOCO"]], "rel_plus": [["BERT:Method", "Used-For", "image - sentence retrieval:Task"], ["InferSent:Method", "Used-For", "image - sentence retrieval:Task"], ["Word 2 Vec:Method", "Used-For", "image - sentence retrieval:Task"], ["Flickr 3 0 K:Dataset", "Benchmark-For", "image - sentence retrieval:Task"], ["BERT:Method", "Evaluated-With", "Flickr 3 0 K:Dataset"], ["InferSent:Method", "Evaluated-With", "Flickr 3 0 K:Dataset"], ["Word 2 Vec:Method", "Evaluated-With", "Flickr 3 0 K:Dataset"], ["InferSent:Method", "Evaluated-With", "MSCOCO:Dataset"], ["BERT:Method", "Evaluated-With", "MSCOCO:Dataset"], ["Word 2 Vec:Method", "Evaluated-With", "MSCOCO:Dataset"]]}
{"doc_id": "201070522", "sentence": "For the remaining retrieval tasks , metrics are be - low the best performing model and embedding combination within 1 - 3 points , again noting the unusual exception of In - ferSent on phrase grounding of Flickr 3 0 K Entities , which significantly drops below scratch performance .", "ner": [["In - ferSent", "Method"], ["phrase grounding", "Task"], ["Flickr 3 0 K Entities", "Dataset"]], "rel": [["In - ferSent", "Used-For", "phrase grounding"], ["Flickr 3 0 K Entities", "Benchmark-For", "phrase grounding"], ["In - ferSent", "Evaluated-With", "Flickr 3 0 K Entities"]], "rel_plus": [["In - ferSent:Method", "Used-For", "phrase grounding:Task"], ["Flickr 3 0 K Entities:Dataset", "Benchmark-For", "phrase grounding:Task"], ["In - ferSent:Method", "Evaluated-With", "Flickr 3 0 K Entities:Dataset"]]}
{"doc_id": "201070522", "sentence": "Extensions either use language enhancements , visual enhancements , or both ( e.g. WordNet retrofitting , HGLMM vs. Visual Word 2 Vec vs. GrOVLE , respectively ) .", "ner": [["WordNet retrofitting", "Method"], ["HGLMM", "Method"], ["Visual Word 2 Vec", "Method"], ["GrOVLE", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "201070522", "sentence": "Visual Word 2 Vec [ 2 6 ] is a neural model designed to ground the original Word 2 Vec representation with visual semantics .", "ner": [["Visual Word 2 Vec", "Method"], ["Word 2 Vec", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "201070522", "sentence": "Another post - processed embedding we use for this set of experiments is the Hybrid Gaussian - Laplacian Mixture Model ( HGLMM ) representation built off of Fisher vectors for Word 2 Vec [ 2 4 ] .", "ner": [["Hybrid Gaussian - Laplacian Mixture Model", "Method"], ["HGLMM", "Method"], ["Fisher vectors", "Method"], ["Word 2 Vec", "Method"]], "rel": [["HGLMM", "Synonym-Of", "Hybrid Gaussian - Laplacian Mixture Model"], ["Fisher vectors", "Part-Of", "Word 2 Vec"]], "rel_plus": [["HGLMM:Method", "Synonym-Of", "Hybrid Gaussian - Laplacian Mixture Model:Method"], ["Fisher vectors:Method", "Part-Of", "Word 2 Vec:Method"]]}
{"doc_id": "201070522", "sentence": "Fisher vectors instead concatenate the gradients of the log - likelihood of local descriptors ( which in this case are the Word 2 Vec vectors ) with respect to the HGLMM parameters .", "ner": [["Fisher vectors", "Method"], ["log - likelihood", "Method"], ["Word 2 Vec", "Method"], ["HGLMM", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "201070522", "sentence": "Following [ 4 9 , 4 0 ] , we reduce the dimensions of the original encodings ( 1 8 K - D ) to 6K - D or 3 0 0 - D using PCA , as it has been found to improve numerical stability on VL tasks ( except for experiments on ReferIt which we reduce to 2K - D due to its small vocabulary size ) .", "ner": [["PCA", "Method"], ["VL", "Task"], ["ReferIt", "Dataset"]], "rel": [["PCA", "Used-For", "VL"]], "rel_plus": [["PCA:Method", "Used-For", "VL:Task"]]}
{"doc_id": "201070522", "sentence": "We provide a new embedding , GrOVLE , which adapts Word 2 Vec using two knowledge bases : WordNet and Visual Genome .", "ner": [["GrOVLE", "Method"], ["Word 2 Vec", "Method"], ["WordNet", "Dataset"], ["Visual Genome", "Dataset"]], "rel": [["GrOVLE", "SubClass-Of", "Word 2 Vec"], ["GrOVLE", "Trained-With", "WordNet"], ["GrOVLE", "Trained-With", "Visual Genome"]], "rel_plus": [["GrOVLE:Method", "SubClass-Of", "Word 2 Vec:Method"], ["GrOVLE:Method", "Trained-With", "WordNet:Dataset"], ["GrOVLE:Method", "Trained-With", "Visual Genome:Dataset"]]}
{"doc_id": "201070522", "sentence": "A joint lexicon is built with WordNet and Visual Genome as opposed to successively retrofitting the two ; this minimized forgetting of the first and thus improved performance .", "ner": [["WordNet", "Dataset"], ["Visual Genome", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "201070522", "sentence": "Visual Genome [ 2 8 ] contains a wealth of language annotations for 1 0 8 K images : descriptions of entities in an image , their attributes , relationships between multiple entities , and whole image and region - based QA pairs .", "ner": [["Visual Genome", "Dataset"], ["QA", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "201070522", "sentence": "For the generation - based tasks ( i.e. captioning and VQA ) , the benefits of using adapted embeddings are less clear .", "ner": [["generation - based tasks", "Task"], ["captioning", "Task"], ["VQA", "Task"]], "rel": [["captioning", "SubTask-Of", "generation - based tasks"], ["VQA", "SubTask-Of", "generation - based tasks"]], "rel_plus": [["captioning:Task", "SubTask-Of", "generation - based tasks:Task"], ["VQA:Task", "SubTask-Of", "generation - based tasks:Task"]]}
{"doc_id": "201070522", "sentence": "Visual Word 2 Vec performs comparably amongst results for generation tasks ( i.e. image captioning and VQA ) , but these tasks have little variance in results , with less than a point of difference across the adapted embeddings .", "ner": [["Visual Word 2 Vec", "Method"], ["generation tasks", "Task"], ["image captioning", "Task"], ["VQA", "Task"]], "rel": [["image captioning", "SubTask-Of", "generation tasks"], ["VQA", "SubTask-Of", "generation tasks"], ["Visual Word 2 Vec", "Used-For", "generation tasks"], ["Visual Word 2 Vec", "Used-For", "image captioning"], ["Visual Word 2 Vec", "Used-For", "VQA"]], "rel_plus": [["image captioning:Task", "SubTask-Of", "generation tasks:Task"], ["VQA:Task", "SubTask-Of", "generation tasks:Task"], ["Visual Word 2 Vec:Method", "Used-For", "generation tasks:Task"], ["Visual Word 2 Vec:Method", "Used-For", "image captioning:Task"], ["Visual Word 2 Vec:Method", "Used-For", "VQA:Task"]]}
{"doc_id": "201070522", "sentence": "The small gain provided in generation tasks by Visual Word 2 Vec does not out - weight the drops in performance across other tasks such as the significant mean recall drop of 6. 3 compared to HGLMM 's 6K - D Self - Attention result in line two of Table 2(c ) and Table 2 ( e ) for image - sentence retrieval of Flickr 3 0 K .", "ner": [["Visual Word 2 Vec", "Method"], ["HGLMM", "Method"], ["Self - Attention", "Method"], ["image - sentence retrieval", "Task"], ["Flickr 3 0 K", "Dataset"]], "rel": [["Flickr 3 0 K", "Benchmark-For", "image - sentence retrieval"]], "rel_plus": [["Flickr 3 0 K:Dataset", "Benchmark-For", "image - sentence retrieval:Task"]]}
{"doc_id": "201070522", "sentence": "For comparison , GrOVLE 's Self - Attention result in Table 2 ( b ) is only 3 points lower .", "ner": [["GrOVLE", "Method"], ["Self - Attention", "Method"]], "rel": [["Self - Attention", "Part-Of", "GrOVLE"]], "rel_plus": [["Self - Attention:Method", "Part-Of", "GrOVLE:Method"]]}
{"doc_id": "201070522", "sentence": "We use the best performing language model in our comparisons for each task , i.e. Self - Attention for image - sentence retrieval and phrase grounding , and the LSTM language model for text - to - clip , image captioning , and VQA .", "ner": [["Self - Attention", "Method"], ["image - sentence retrieval", "Task"], ["phrase grounding", "Task"], ["LSTM language model", "Method"], ["text - to - clip", "Task"], ["image captioning", "Task"], ["VQA", "Task"]], "rel": [["Self - Attention", "Used-For", "image - sentence retrieval"], ["Self - Attention", "Used-For", "phrase grounding"], ["LSTM language model", "Used-For", "text - to - clip"], ["LSTM language model", "Used-For", "image captioning"], ["LSTM language model", "Used-For", "VQA"]], "rel_plus": [["Self - Attention:Method", "Used-For", "image - sentence retrieval:Task"], ["Self - Attention:Method", "Used-For", "phrase grounding:Task"], ["LSTM language model:Method", "Used-For", "text - to - clip:Task"], ["LSTM language model:Method", "Used-For", "image captioning:Task"], ["LSTM language model:Method", "Used-For", "VQA:Task"]]}
{"doc_id": "201070522", "sentence": "We find similar trends in performance improvements across tasks : larger gains occur for image - sentence retrieval with + 7. 9 mean recall for the Flickr 3 0 K dataset and + 6. 3 for MSCOCO .", "ner": [["image - sentence retrieval", "Task"], ["Flickr 3 0 K", "Dataset"], ["MSCOCO", "Dataset"]], "rel": [["Flickr 3 0 K", "Benchmark-For", "image - sentence retrieval"], ["MSCOCO", "Benchmark-For", "image - sentence retrieval"]], "rel_plus": [["Flickr 3 0 K:Dataset", "Benchmark-For", "image - sentence retrieval:Task"], ["MSCOCO:Dataset", "Benchmark-For", "image - sentence retrieval:Task"]]}
{"doc_id": "201070522", "sentence": "All other tasks have performance improvements under one point , showing that while the vision - language tasks appear to transfer well without harming performance , they are leveraged most in image - sentence retrieval , with an exception of phrase grounding accuracy on ReferIt ( + 2. 3 6 % ) .", "ner": [["image - sentence retrieval", "Task"], ["phrase grounding", "Task"], ["ReferIt", "Dataset"]], "rel": [["ReferIt", "Benchmark-For", "phrase grounding"]], "rel_plus": [["ReferIt:Dataset", "Benchmark-For", "phrase grounding:Task"]]}
{"doc_id": "201070522", "sentence": "On retrieval - style tasks , the Average Embedding and Self - Attention language model tend to outperform a simple LSTM . 2 .", "ner": [["retrieval - style tasks", "Task"], ["Average Embedding", "Method"], ["Self - Attention language model", "Method"], ["LSTM", "Method"]], "rel": [["Average Embedding", "Used-For", "retrieval - style tasks"], ["Self - Attention language model", "Used-For", "retrieval - style tasks"], ["LSTM", "Used-For", "retrieval - style tasks"], ["Average Embedding", "Compare-With", "LSTM"], ["Self - Attention language model", "Compare-With", "LSTM"]], "rel_plus": [["Average Embedding:Method", "Used-For", "retrieval - style tasks:Task"], ["Self - Attention language model:Method", "Used-For", "retrieval - style tasks:Task"], ["LSTM:Method", "Used-For", "retrieval - style tasks:Task"], ["Average Embedding:Method", "Compare-With", "LSTM:Method"], ["Self - Attention language model:Method", "Compare-With", "LSTM:Method"]]}
{"doc_id": "201070522", "sentence": "Along with these findings , we have introduced GrOVLE , which incorporates hierarchical language relations from WordNet as well as language with visual context from Visual Genome .", "ner": [["GrOVLE", "Method"], ["WordNet", "Dataset"], ["Visual Genome", "Dataset"]], "rel": [["GrOVLE", "Trained-With", "WordNet"], ["GrOVLE", "Trained-With", "Visual Genome"]], "rel_plus": [["GrOVLE:Method", "Trained-With", "WordNet:Dataset"], ["GrOVLE:Method", "Trained-With", "Visual Genome:Dataset"]]}
{"doc_id": "211010758", "sentence": "We present a novel approach to perform the unsupervised domain adaptation for object detection through forward - backward cyclic ( FBC ) training .", "ner": [["unsupervised domain adaptation", "Task"], ["object detection", "Task"], ["forward - backward cyclic", "Method"], ["FBC", "Method"]], "rel": [["forward - backward cyclic", "Used-For", "unsupervised domain adaptation"], ["forward - backward cyclic", "Used-For", "object detection"], ["unsupervised domain adaptation", "Used-For", "object detection"], ["FBC", "Synonym-Of", "forward - backward cyclic"]], "rel_plus": [["forward - backward cyclic:Method", "Used-For", "unsupervised domain adaptation:Task"], ["forward - backward cyclic:Method", "Used-For", "object detection:Task"], ["unsupervised domain adaptation:Task", "Used-For", "object detection:Task"], ["FBC:Method", "Synonym-Of", "forward - backward cyclic:Method"]]}
{"doc_id": "211010758", "sentence": "As such , in each cycle , domain diversity is enforced by maximum entropy regularization on the source domain to penalize confident source - specific learning and minimum entropy regularization on target domain to intrigue target - specific learning .", "ner": [["maximum entropy regularization", "Method"], ["minimum entropy regularization", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "211010758", "sentence": "Object detection is a fundamental problem in computer vision [ 4 2 , 2 7 , 4 1 , 2 8 , 2 4 ] .", "ner": [["Object detection", "Task"], ["computer vision", "Task"]], "rel": [["Object detection", "SubTask-Of", "computer vision"]], "rel_plus": [["Object detection:Task", "SubTask-Of", "computer vision:Task"]]}
{"doc_id": "211010758", "sentence": "Unsupervised domain adaptation methods [ 2 9 , 3 0 , 1 1 , 5 4 , 4 5 , 4 7 ] for image classification are developed to learn domain - invariant features by minimizing the source error and simultaneously the domain discrepancy through feature distribution alignment .", "ner": [["Unsupervised domain adaptation methods", "Method"], ["image classification", "Task"]], "rel": [["Unsupervised domain adaptation methods", "Used-For", "image classification"]], "rel_plus": [["Unsupervised domain adaptation methods:Method", "Used-For", "image classification:Task"]]}
{"doc_id": "211010758", "sentence": "In object detection , performing domain alignment is more challenging compared to alignment in the image classification task in the following two aspects : ( 1 ) The input image may contain multiple objects , while there is only one centered object in the classification task ; ( 2 ) The images in object detection are dominated by background and nonobjects .", "ner": [["object detection", "Task"], ["image classification", "Task"], ["classification", "Task"], ["object detection", "Task"]], "rel": [["object detection", "Compare-With", "image classification"]], "rel_plus": [["object detection:Task", "Compare-With", "image classification:Task"]]}
{"doc_id": "211010758", "sentence": "To achieve the goals , we propose a Forward - Backward Cyclic Adaptation ( FBC ) approach to learn adaptive object detectors .", "ner": [["Forward - Backward Cyclic Adaptation", "Method"], ["FBC", "Method"]], "rel": [["FBC", "Synonym-Of", "Forward - Backward Cyclic Adaptation"]], "rel_plus": [["FBC:Method", "Synonym-Of", "Forward - Backward Cyclic Adaptation:Method"]]}
{"doc_id": "211010758", "sentence": "We provide theoretical analysis to show that by computing the forward and backward adaptation sequentially via Stochastic Gradient Descent ( SGD ) , gradient alignment can be achieved .", "ner": [["Stochastic Gradient Descent", "Method"], ["SGD", "Method"]], "rel": [["SGD", "Synonym-Of", "Stochastic Gradient Descent"]], "rel_plus": [["SGD:Method", "Synonym-Of", "Stochastic Gradient Descent:Method"]]}
{"doc_id": "211010758", "sentence": "Our proposed approach is related to the cycle consistency utilized in machine translation [ 1 4 ] , image - toimage translation [ 5 9 , 5 8 ] and unsupervised domain adaptation [ 5 0 ] with a similar intuition that the mappings of an example transferred from source to target and then back to the source domain should have same results .", "ner": [["machine translation", "Task"], ["image - toimage translation", "Task"], ["unsupervised domain adaptation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "211010758", "sentence": "A representative of two - stage framework is the Faster R - CNN proposed by Ren et al. [ 4 2 ] , which consists of two subnetworks : a region proposal network that generates region proposals and a R - CNN that classifies the categories of the proposals .", "ner": [["Faster R - CNN", "Method"], ["region proposal network", "Method"], ["region proposals", "Task"], ["R - CNN", "Method"]], "rel": [["region proposal network", "Part-Of", "Faster R - CNN"], ["R - CNN", "Part-Of", "Faster R - CNN"], ["region proposal network", "Used-For", "region proposals"]], "rel_plus": [["region proposal network:Method", "Part-Of", "Faster R - CNN:Method"], ["R - CNN:Method", "Part-Of", "Faster R - CNN:Method"], ["region proposal network:Method", "Used-For", "region proposals:Task"]]}
{"doc_id": "211010758", "sentence": "Single - stage detectors , e.g. SSD [ 2 7 ] and YOLO [ 4 1 ] , have demonstrated high efficiency in object detection , where the networks perform object classification and localization simultaneously .", "ner": [["SSD", "Method"], ["YOLO", "Method"], ["object detection", "Task"], ["object classification", "Task"], ["localization", "Task"]], "rel": [["YOLO", "Used-For", "object detection"], ["SSD", "Used-For", "object detection"], ["YOLO", "Used-For", "object classification"], ["SSD", "Used-For", "object classification"], ["YOLO", "Used-For", "localization"], ["SSD", "Used-For", "localization"]], "rel_plus": [["YOLO:Method", "Used-For", "object detection:Task"], ["SSD:Method", "Used-For", "object detection:Task"], ["YOLO:Method", "Used-For", "object classification:Task"], ["SSD:Method", "Used-For", "object classification:Task"], ["YOLO:Method", "Used-For", "localization:Task"], ["SSD:Method", "Used-For", "localization:Task"]]}
{"doc_id": "211010758", "sentence": "Other methods like FPN [ 2 4 ] and RetinaNet [ 2 5 ] propose to leverage a combination of features from different levels to improve the feature representations .", "ner": [["FPN", "Method"], ["RetinaNet", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "211010758", "sentence": "Unsupervised Domain Adaptation for Image Classification .", "ner": [["Unsupervised Domain Adaptation", "Method"], ["Image Classification", "Task"]], "rel": [["Unsupervised Domain Adaptation", "Used-For", "Image Classification"]], "rel_plus": [["Unsupervised Domain Adaptation:Method", "Used-For", "Image Classification:Task"]]}
{"doc_id": "211010758", "sentence": "Unsupervised Domain Adaptation for Object Detection .", "ner": [["Unsupervised Domain Adaptation", "Method"], ["Object Detection", "Task"]], "rel": [["Unsupervised Domain Adaptation", "Used-For", "Object Detection"]], "rel_plus": [["Unsupervised Domain Adaptation:Method", "Used-For", "Object Detection:Task"]]}
{"doc_id": "211010758", "sentence": "Fewer works are available in the unsupervised domain adaptation for object detection .", "ner": [["unsupervised domain adaptation", "Method"], ["object detection", "Task"]], "rel": [["unsupervised domain adaptation", "Used-For", "object detection"]], "rel_plus": [["unsupervised domain adaptation:Method", "Used-For", "object detection:Task"]]}
{"doc_id": "211010758", "sentence": "To our knowledge , there are only three works , Domain Adaptive Faster R - CNN ( DA - Faster ) [ 5 ] , Selective Domain Alignment ( SDA ) [ 6 0 ] and Strong - Weak Domain Alignment ( SWDA ) [ 4 6 ] .", "ner": [["Domain Adaptive Faster R - CNN", "Method"], ["DA - Faster", "Method"], ["Selective Domain Alignment", "Method"], ["SDA", "Method"], ["Strong - Weak Domain Alignment", "Method"], ["SWDA", "Method"]], "rel": [["DA - Faster", "Synonym-Of", "Domain Adaptive Faster R - CNN"], ["SDA", "Synonym-Of", "Selective Domain Alignment"], ["SWDA", "Synonym-Of", "Strong - Weak Domain Alignment"]], "rel_plus": [["DA - Faster:Method", "Synonym-Of", "Domain Adaptive Faster R - CNN:Method"], ["SDA:Method", "Synonym-Of", "Selective Domain Alignment:Method"], ["SWDA:Method", "Synonym-Of", "Strong - Weak Domain Alignment:Method"]]}
{"doc_id": "211010758", "sentence": "The weak alignment is achieved by replacing the cross - entropy loss of domain classifier with focal loss [ 2 5 ] .", "ner": [["cross - entropy loss", "Method"], ["domain classifier", "Method"], ["focal loss", "Method"]], "rel": [["cross - entropy loss", "Part-Of", "domain classifier"], ["focal loss", "Part-Of", "domain classifier"]], "rel_plus": [["cross - entropy loss:Method", "Part-Of", "domain classifier:Method"], ["focal loss:Method", "Part-Of", "domain classifier:Method"]]}
{"doc_id": "211010758", "sentence": "Our method is also related to recent gradient - based meta learning methods : MAML [ 1 0 ] and Reptile [ 3 7 ] , which are designed to learn a good initialization for few shot learning and have demonstrated good within - task generalization .", "ner": [["gradient - based meta learning methods", "Method"], ["MAML", "Method"], ["Reptile", "Method"], ["few shot learning", "Task"], ["within - task generalization", "Task"]], "rel": [["MAML", "Part-Of", "gradient - based meta learning methods"], ["Reptile", "Part-Of", "gradient - based meta learning methods"], ["gradient - based meta learning methods", "Used-For", "few shot learning"], ["MAML", "Used-For", "few shot learning"], ["Reptile", "Used-For", "few shot learning"], ["gradient - based meta learning methods", "Used-For", "within - task generalization"], ["MAML", "Used-For", "within - task generalization"], ["Reptile", "Used-For", "within - task generalization"]], "rel_plus": [["MAML:Method", "Part-Of", "gradient - based meta learning methods:Method"], ["Reptile:Method", "Part-Of", "gradient - based meta learning methods:Method"], ["gradient - based meta learning methods:Method", "Used-For", "few shot learning:Task"], ["MAML:Method", "Used-For", "few shot learning:Task"], ["Reptile:Method", "Used-For", "few shot learning:Task"], ["gradient - based meta learning methods:Method", "Used-For", "within - task generalization:Task"], ["MAML:Method", "Used-For", "within - task generalization:Task"], ["Reptile:Method", "Used-For", "within - task generalization:Task"]]}
{"doc_id": "211010758", "sentence": "Riemer et al. [ 4 3 ] integrates the Reptile algorithm with an experience replay module for the task of continual learning , where the transfer between examples is maximized via meta - learning .", "ner": [["Reptile", "Method"], ["experience replay", "Method"], ["continual learning", "Task"], ["meta - learning", "Task"]], "rel": [["experience replay", "Part-Of", "Reptile"], ["Reptile", "Used-For", "continual learning"], ["meta - learning", "Used-For", "continual learning"]], "rel_plus": [["experience replay:Method", "Part-Of", "Reptile:Method"], ["Reptile:Method", "Used-For", "continual learning:Task"], ["meta - learning:Task", "Used-For", "continual learning:Task"]]}
{"doc_id": "211010758", "sentence": "Inspired by these methods , we leverage the generalization ability of Reptile [ 3 7 ] to improve the generalization across domains for unsupervised domain adaptation via gradient alignment .   In unsupervised domain adaptation , are not accessible during training .", "ner": [["Reptile", "Method"], ["unsupervised domain adaptation", "Task"], ["gradient alignment", "Method"], ["unsupervised domain adaptation", "Method"]], "rel": [["gradient alignment", "Used-For", "Reptile"], ["Reptile", "Used-For", "unsupervised domain adaptation"], ["gradient alignment", "Used-For", "unsupervised domain adaptation"]], "rel_plus": [["gradient alignment:Method", "Used-For", "Reptile:Method"], ["Reptile:Method", "Used-For", "unsupervised domain adaptation:Task"], ["gradient alignment:Method", "Used-For", "unsupervised domain adaptation:Task"]]}
{"doc_id": "211010758", "sentence": "As the ultimate goal of domain adaptation is to achieve good performance on the target domain , we further introduce domain - diversity into training to boost the detection performance in the target space .", "ner": [["domain adaptation", "Method"], ["detection", "Task"]], "rel": [["domain adaptation", "Used-For", "detection"]], "rel_plus": [["domain adaptation:Method", "Used-For", "detection:Task"]]}
{"doc_id": "211010758", "sentence": "The proposed training strategy is relevant to the meta - learning approaches [ 1 0 , 3 7 , 2 3 ] that are initially designed for few shot learning .", "ner": [["meta - learning", "Task"], ["few shot learning", "Task"]], "rel": [["meta - learning", "Used-For", "few shot learning"]], "rel_plus": [["meta - learning:Task", "Used-For", "few shot learning:Task"]]}
{"doc_id": "211010758", "sentence": "We utilize the gradient reversal layer ( GRL ) proposed by Ganin and Lempitsky [ 1 1 ] for domain adversarial training , where the gradients of the domain classifier are reversed for domain confusion .", "ner": [["gradient reversal layer", "Method"], ["GRL", "Method"]], "rel": [["GRL", "Synonym-Of", "gradient reversal layer"]], "rel_plus": [["GRL:Method", "Synonym-Of", "gradient reversal layer:Method"]]}
{"doc_id": "211010758", "sentence": "In this section , we evaluate the proposed forwardbackward cycling adaptation approach ( FBC ) on four crossdomain detection datasets .", "ner": [["forwardbackward cycling", "Method"], ["FBC", "Method"]], "rel": [["FBC", "Synonym-Of", "forwardbackward cycling"]], "rel_plus": [["FBC:Method", "Synonym-Of", "forwardbackward cycling:Method"]]}
{"doc_id": "211010758", "sentence": "Following DA - Faster [ 5 ] and SWDA [ 4 6 ] , we use the Faster - RCNN [ 4 2 ] as our detection framework .", "ner": [["DA - Faster", "Method"], ["SWDA", "Method"], ["Faster - RCNN", "Method"], ["detection", "Task"]], "rel": [["Faster - RCNN", "Used-For", "detection"]], "rel_plus": [["Faster - RCNN:Method", "Used-For", "detection:Task"]]}
{"doc_id": "211010758", "sentence": "We compare our method with three baselines : ( 1 ) Source Only : A Faster R - CNN detector that is finetuned on the pre - trained ImageNet [ 7 ] model with labeled source samples without adaptation ; ( 2 ) DA - Faster [ 5 ] ; ( 3 ) SWDA [ 4 6 ] ; ( 4 ) Zhu et al. [ 6 0 ] .", "ner": [["Faster R - CNN", "Method"], ["ImageNet", "Dataset"], ["DA - Faster", "Method"], ["SWDA", "Method"]], "rel": [["Faster R - CNN", "Trained-With", "ImageNet"]], "rel_plus": [["Faster R - CNN:Method", "Trained-With", "ImageNet:Dataset"]]}
{"doc_id": "211010758", "sentence": "We evaluate the adaptation performance on two pairs of dissimilar domains : PASCAL [ 9 ] to Clipart [ 1 7 ] , and PAS - CAL [ 9 ] to Watercolor [ 1 7 ] .", "ner": [["PASCAL", "Dataset"], ["Clipart", "Dataset"], ["PAS - CAL", "Dataset"], ["Watercolor", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "211010758", "sentence": "Following SWDA [ 4 6 ] , we use ResNet 1 0 1 [ 1 5 ] as the backbone network for Faster R - CNN detector and the settings of training and test sets are the same .", "ner": [["SWDA", "Method"], ["ResNet 1 0 1", "Method"], ["Faster R - CNN", "Method"]], "rel": [["ResNet 1 0 1", "Part-Of", "Faster R - CNN"]], "rel_plus": [["ResNet 1 0 1:Method", "Part-Of", "Faster R - CNN:Method"]]}
{"doc_id": "211010758", "sentence": "The two dissimilar target domains are the Clipart dataset [ 1 7 ] with comic images and the Watercolor dataset [ 1 7 ] with artistic images .", "ner": [["Clipart", "Dataset"], ["Watercolor", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "211010758", "sentence": "Clipart dataset has the same 2 0 object classes as the PASCAL , while Watercolor only has six .", "ner": [["Clipart", "Dataset"], ["PASCAL", "Dataset"], ["Watercolor", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "211010758", "sentence": "In the original paper of DA - Faster [ 5 ] , they do not evaluate on the Clipart and Watercolor datasets .", "ner": [["DA - Faster", "Method"], ["Clipart", "Dataset"], ["Watercolor", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "211010758", "sentence": "Thus , we follow with the results of DA - Faster [ 5 ] reported in SWDA [ 4 6 ] .", "ner": [["DA - Faster", "Method"], ["SWDA", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "211010758", "sentence": "As shown in Table 5 , in comparison to the source only model , DA - Faster [ 5 ] degrades the detection performance significantly , with a drop of 8 percentage points in mAP .", "ner": [["DA - Faster", "Method"], ["detection", "Task"]], "rel": [["DA - Faster", "Used-For", "detection"]], "rel_plus": [["DA - Faster:Method", "Used-For", "detection:Task"]]}
{"doc_id": "211010758", "sentence": "The problem is more challenging when domain shift in object detection is large , i.e. PASCAL [ 9 ] to Clipart [ 1 7 ] .", "ner": [["object detection", "Task"], ["PASCAL", "Dataset"], ["Clipart", "Dataset"]], "rel": [["PASCAL", "Benchmark-For", "object detection"], ["Clipart", "Benchmark-For", "object detection"]], "rel_plus": [["PASCAL:Dataset", "Benchmark-For", "object detection:Task"], ["Clipart:Dataset", "Benchmark-For", "object detection:Task"]]}
{"doc_id": "211010758", "sentence": "In Clipart [ 1 7 ] , the comic images contain objects that are far different from those in PASCAL [ 9 ] w.r.t . the shapes and appearance , such as sketches .", "ner": [["Clipart", "Dataset"], ["PASCAL", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "211010758", "sentence": "To address this , the SWDA [ 4 6 ] conducts a weak alignment on the image - level features by training the domain classifier with a focal loss .", "ner": [["SWDA", "Method"], ["focal loss", "Method"]], "rel": [["focal loss", "Part-Of", "SWDA"]], "rel_plus": [["focal loss:Method", "Part-Of", "SWDA:Method"]]}
{"doc_id": "211010758", "sentence": "With the additional help of a domain classifier on lower level features and context regularization , the SWDA [ 4 6 ] can boost the mAP of detection from 2 7 . 8 % to 3 8 . 1 % with an increase of 1 0 . 3 points .", "ner": [["SWDA", "Method"], ["detection", "Task"]], "rel": [["SWDA", "Used-For", "detection"]], "rel_plus": [["SWDA:Method", "Used-For", "detection:Task"]]}
{"doc_id": "211010758", "sentence": "Feature visualization for showing the evidence for classifiers before and after domain adaptation using Grad - cam [ 4 9 ] . objects with less variations of shape and appearance compared with the Clipart dataset .", "ner": [["domain adaptation", "Task"], ["Grad - cam", "Method"], ["Clipart", "Dataset"]], "rel": [["Grad - cam", "Used-For", "domain adaptation"]], "rel_plus": [["Grad - cam:Method", "Used-For", "domain adaptation:Task"]]}
{"doc_id": "211010758", "sentence": "As reported in SWDA [ 4 6 ] , the source only model can achieve quite good results with a mAP of 4 4 . 6 % and DA - Faster [ 5 ] can improve it slightly by only 1. 4 points .", "ner": [["SWDA", "Method"], ["DA - Faster", "Method"]], "rel": [["SWDA", "Compare-With", "DA - Faster"]], "rel_plus": [["SWDA:Method", "Compare-With", "DA - Faster:Method"]]}
{"doc_id": "211010758", "sentence": "SWDA [ 4 6 ] performs much better than DA - Faster [ 5 ] and obtain a high mAP of 5 3 . 5 % .", "ner": [["SWDA", "Method"], ["DA - Faster", "Method"]], "rel": [["SWDA", "Compare-With", "DA - Faster"]], "rel_plus": [["SWDA:Method", "Compare-With", "DA - Faster:Method"]]}
{"doc_id": "211010758", "sentence": "The mAP of our proposed FBC is 5 3 . 6 % , which is 0. 3 % higher than that of SWDA .", "ner": [["FBC", "Method"], ["SWDA", "Method"]], "rel": [["FBC", "Compare-With", "SWDA"]], "rel_plus": [["FBC:Method", "Compare-With", "SWDA:Method"]]}
{"doc_id": "211010758", "sentence": "To visualize the adaptability of our method , we use the Grad - cam [ 4 9 ] to show the evidence ( heatmap ) for the last fully connected layer in the object detectors .", "ner": [["Grad - cam", "Method"], ["fully connected layer", "Method"], ["object detectors", "Method"]], "rel": [["Grad - cam", "Part-Of", "fully connected layer"], ["fully connected layer", "Part-Of", "object detectors"]], "rel_plus": [["Grad - cam:Method", "Part-Of", "fully connected layer:Method"], ["fully connected layer:Method", "Part-Of", "object detectors:Method"]]}
{"doc_id": "211010758", "sentence": "The high value in the heatmap indicates the evidence that why the classifiers make the classification .", "ner": [["classification", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "211010758", "sentence": "As the adaptation from the synthetic images to the real images can potentially reduce the efforts of collecting the real data and labels , we evaluate the adaptation performance in the scenario of Sim 1 0 k [ 1 9 ] to Cityscapes [ 6 ] .", "ner": [["Sim 1 0 k", "Dataset"], ["Cityscapes", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "211010758", "sentence": "The reported mAP gain of DA - Faster [ 5 ] in its original report ( 7. 8 points ) is significantly different from its reproduced gain ( - 0 . 4 points ) in SWDA [ 4 6 ] .", "ner": [["DA - Faster", "Method"], ["SWDA", "Method"]], "rel": [["DA - Faster", "Compare-With", "SWDA"]], "rel_plus": [["DA - Faster:Method", "Compare-With", "SWDA:Method"]]}
{"doc_id": "211010758", "sentence": "Our proposed FBC have a competitive result of mAP , 4 2 . 7 % , which is 0. 4 % higher than that of SWDA .", "ner": [["FBC", "Method"], ["SWDA", "Method"]], "rel": [["FBC", "Compare-With", "SWDA"]], "rel_plus": [["FBC:Method", "Compare-With", "SWDA:Method"]]}
{"doc_id": "211010758", "sentence": "Results ( % ) on the adaptation from PASCAL [ 9 ] to Clipart Dataset [ 1 7 ] .", "ner": [["PASCAL", "Dataset"], ["Clipart Dataset", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "211010758", "sentence": "The DA - Faster \u2020result is the reported in SWDA [ 4 6 ] .", "ner": [["DA - Faster", "Method"], ["SWDA", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "211010758", "sentence": "Results ( % ) on the adaptation from PASCAL [ 9 ] to Watercolor [ 1 7 ] .", "ner": [["PASCAL", "Dataset"], ["Watercolor", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "211010758", "sentence": "The DA - Faster \u2020is the reproduced in SWDA [ 5 ] .", "ner": [["DA - Faster", "Method"], ["SWDA", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "211010758", "sentence": "Method AP on Car Source Only [ 5 ] 3 1 . 2 DA - Faster [ 5 ] 3 9 . 0 Source Only [ 4 6 ] 3 4 . 6 DA - Faster \u2020 [ 5 ] 3 4 .", "ner": [["DA - Faster", "Method"], ["DA - Faster", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "211010758", "sentence": "Results ( % ) on the adaptation from Sim 1 0 k [ 1 9 ] to Cityscapes [ 6 ] .", "ner": [["Sim 1 0 k", "Dataset"], ["Cityscapes", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "211010758", "sentence": "The DA - Faster \u2020is the reproduced in SWDA [ 5 ] .", "ner": [["DA - Faster", "Method"], ["SWDA", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "211010758", "sentence": "Results ( % ) on the adaptation from Cityscapes [ 6 ] to Fog - gyCityscapes Dataset [ 4 8 ] . with the method by Zhu et al. [ 6 0 ] , our proposed method has a much simpler network architecture and training scheme .", "ner": [["Cityscapes", "Dataset"], ["Fog - gyCityscapes", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "211010758", "sentence": "The target dataset , FoggyCityscapes [ 4 8 ] , is a synthetic foggy dataset where images are rendered from the Cityscapes [ 6 ] .", "ner": [["FoggyCityscapes", "Dataset"], ["Cityscapes", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "211010758", "sentence": "We addressed the unsupervised domain adaptation for object detection task where the target domain does not have labels .", "ner": [["unsupervised domain adaptation", "Task"], ["object detection", "Task"]], "rel": [["unsupervised domain adaptation", "Used-For", "object detection"]], "rel_plus": [["unsupervised domain adaptation:Task", "Used-For", "object detection:Task"]]}
{"doc_id": "211010758", "sentence": "According to the Taylor 's theorem , we have the SGD gradients as follows : If we consider there are two steps of parameter updates with stochastic gradient descent ( SGD ) , where the gradient of the first step is g 1 and the one of second step is g 2 .", "ner": [["SGD", "Method"], ["stochastic gradient descent", "Method"], ["SGD", "Method"]], "rel": [["SGD", "Synonym-Of", "stochastic gradient descent"]], "rel_plus": [["SGD:Method", "Synonym-Of", "stochastic gradient descent:Method"]]}
{"doc_id": "211010758", "sentence": "According to the Eq. 2 2 , we have Then , the overall gradient of the two SGD steps is In Reptile [ 3 7 ] , they noted that where the is the expected loss .", "ner": [["SGD", "Method"], ["Reptile", "Method"]], "rel": [["SGD", "Part-Of", "Reptile"]], "rel_plus": [["SGD:Method", "Part-Of", "Reptile:Method"]]}
{"doc_id": "211010758", "sentence": "Therefore , the overall expected loss is In our work , we aim to address the domain adaptation problem for object detection .", "ner": [["domain adaptation", "Task"], ["object detection", "Task"]], "rel": [["domain adaptation", "Used-For", "object detection"]], "rel_plus": [["domain adaptation:Task", "Used-For", "object detection:Task"]]}
{"doc_id": "211010758", "sentence": "To evaluate the effects of gradient alignment , we perform the forward - backward cyclic method ( FBC ) on four different cross - domain scenarios with gradient alignment only .", "ner": [["forward - backward cyclic", "Method"], ["FBC", "Method"]], "rel": [["FBC", "Synonym-Of", "forward - backward cyclic"]], "rel_plus": [["FBC:Method", "Synonym-Of", "forward - backward cyclic:Method"]]}
{"doc_id": "211010758", "sentence": "In the adaptation scenarios , PASCAL [ 9 ] -to - Clipart [ 1 7 ] ( in Table 5 ) and PASCAL - to - Watercolor [ 1 7 ] ( in Table 6 ) , the FBC with gradient alignment can achieve better adaptation results than the FBC with local feature alignment only .", "ner": [["PASCAL", "Dataset"], ["Clipart", "Dataset"], ["PASCAL", "Dataset"], ["Watercolor", "Dataset"], ["FBC", "Method"], ["FBC", "Method"]], "rel": [["FBC", "Evaluated-With", "PASCAL"], ["FBC", "Evaluated-With", "Clipart"], ["FBC", "Evaluated-With", "PASCAL"], ["FBC", "Evaluated-With", "Watercolor"]], "rel_plus": [["FBC:Method", "Evaluated-With", "PASCAL:Dataset"], ["FBC:Method", "Evaluated-With", "Clipart:Dataset"], ["FBC:Method", "Evaluated-With", "PASCAL:Dataset"], ["FBC:Method", "Evaluated-With", "Watercolor:Dataset"]]}
{"doc_id": "211010758", "sentence": "However , in the adaptation scenarios , Sim 1 0 k [ 1 9 ] -to - Cityscapes [ 6 ] ( in Table 7 and Cityscapes [ 6 ] -to - FoggyCityscapes [ 4 8 ] ( in Table 8 , the domain discrepancy between two domain are mainly in the low - level features , e.g. textures and colors .", "ner": [["Sim 1 0 k", "Dataset"], ["Cityscapes", "Dataset"], ["Cityscapes", "Dataset"], ["FoggyCityscapes", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "211010758", "sentence": "Therefore , in these scenarios , the FBC with gradient alignment only can achieve limited gain on mAP , compared witht he FBC with local feature alignment .", "ner": [["FBC", "Method"], ["FBC", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "211010758", "sentence": "It is more evident in Cityscapes - to - FoggyCityscapes , where the foggy images are rendered from the real images .", "ner": [["Cityscapes - to", "Dataset"], ["FoggyCityscapes", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "211010758", "sentence": "The results ( % ) on the adaptation from PASCAL [ 9 ] to Clipart Dataset [ 1 7 ] .", "ner": [["PASCAL", "Dataset"], ["Clipart Dataset", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "211010758", "sentence": "Results ( % ) on the adaptation from Cityscapes [ 6 ] to FoggyCityscapes Dataset [ 4 8 ] .", "ner": [["Cityscapes", "Dataset"], ["FoggyCityscapes", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "211010758", "sentence": "In this work , we utilize the Gradient Reversal Layer ( GRL ) proposed by Ganin and Lempitsky [ 1 1 ] for adversarial training .", "ner": [["Gradient Reversal Layer", "Method"], ["GRL", "Method"]], "rel": [["GRL", "Synonym-Of", "Gradient Reversal Layer"]], "rel_plus": [["GRL:Method", "Synonym-Of", "Gradient Reversal Layer:Method"]]}
{"doc_id": "211010758", "sentence": "Feature visualization shows the evidence for improvements in classifiers before and after domain adaptation using Grad - cam [ 4 9 ] on the Watercolor dataset [ 1 7 ] .", "ner": [["domain adaptation", "Method"], ["Grad - cam", "Method"], ["Watercolor dataset", "Dataset"]], "rel": [["Grad - cam", "Part-Of", "domain adaptation"], ["domain adaptation", "Evaluated-With", "Watercolor dataset"]], "rel_plus": [["Grad - cam:Method", "Part-Of", "domain adaptation:Method"], ["domain adaptation:Method", "Evaluated-With", "Watercolor dataset:Dataset"]]}
{"doc_id": "202734316", "sentence": "Recent works have resorted to synthetic data generation , but the inferior performance of models trained on synthetic data when applied to the real world , introduced the challenge of unsupervised domain adaptation .", "ner": [["synthetic data generation", "Method"], ["unsupervised domain adaptation", "Task"]], "rel": [["synthetic data generation", "Used-For", "unsupervised domain adaptation"]], "rel_plus": [["synthetic data generation:Method", "Used-For", "unsupervised domain adaptation:Task"]]}
{"doc_id": "202734316", "sentence": "While this concept is already implicitly formulated within the intricate objectives of domain adaptation GANs , we take an explicit approach and apply it directly as data pre - processing .", "ner": [["domain adaptation GANs", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202734316", "sentence": "The proven capability of deep convolutional neural networks ( CNNs ) to represent complex functions has been steadily pushing their adoption in many computer vision tasks .", "ner": [["convolutional neural networks", "Method"], ["CNNs", "Method"], ["computer vision", "Task"]], "rel": [["CNNs", "Synonym-Of", "convolutional neural networks"], ["convolutional neural networks", "Used-For", "computer vision"]], "rel_plus": [["CNNs:Method", "Synonym-Of", "convolutional neural networks:Method"], ["convolutional neural networks:Method", "Used-For", "computer vision:Task"]]}
{"doc_id": "202734316", "sentence": "Nevertheless , besides the inherent dataset bias [ 4 4 ] , such automatic data generation methods suffer from the synthetic - to - real ( S 2 R ) domain discrepancy [ 4 9 ] that deteriorates the performance of models trained using rendered content when applied to real world acquired inputs .", "ner": [["synthetic - to - real", "Task"], ["S 2 R", "Task"]], "rel": [["S 2 R", "Synonym-Of", "synthetic - to - real"]], "rel_plus": [["S 2 R:Task", "Synonym-Of", "synthetic - to - real:Task"]]}
{"doc_id": "202734316", "sentence": "State - of - the - art S 2 R domain adaptation methods typically utilize Generative Adversarial Networks ( GANs ) [ 1 5 ] to align the source ( synthetic ) and target ( real ) domains under an adversarial framework .", "ner": [["S 2 R domain adaptation", "Method"], ["Generative Adversarial Networks", "Method"], ["GANs", "Method"]], "rel": [["Generative Adversarial Networks", "Used-For", "S 2 R domain adaptation"], ["GANs", "Synonym-Of", "Generative Adversarial Networks"]], "rel_plus": [["Generative Adversarial Networks:Method", "Used-For", "S 2 R domain adaptation:Method"], ["GANs:Method", "Synonym-Of", "Generative Adversarial Networks:Method"]]}
{"doc_id": "202734316", "sentence": "Other approaches rely on image - to - image translation to create a mapping between the source and target domains [ 4 2 ] or employ GANs to transfer the style of the target domain to the labelled source images [ 3 ] .", "ner": [["image - to - image translation", "Task"], ["GANs", "Method"]], "rel": [["GANs", "Used-For", "image - to - image translation"]], "rel_plus": [["GANs:Method", "Used-For", "image - to - image translation:Task"]]}
{"doc_id": "202734316", "sentence": "In summary our contributions are : i ) We demonstrate a straightforward and effective technique for unsupervised domain adaptation ( UDA ) that offers competitive performance against the state - of - the - art , but without its complexity . ii ) We present a sample selection process that is suitable for appropriately matching labelled samples of the source data distribution with unlabelled ones from the target data distribution , in order to improve the performance of our proposed technique . iii ) We extensively evaluate our approach and demonstrate its generality across different tasks and datasets . iv ) Unlike most similar approaches , we take a step beyond and consider target domains thatalbeit contextually similar -are different from both the labelled ( source ) , as well as the unlabelled ( style ) domains .", "ner": [["unsupervised domain adaptation", "Task"], ["UDA", "Task"]], "rel": [["UDA", "Synonym-Of", "unsupervised domain adaptation"]], "rel_plus": [["UDA:Task", "Synonym-Of", "unsupervised domain adaptation:Task"]]}
{"doc_id": "202734316", "sentence": "Vision related machine learning tasks mostly rely on CNNs , and consequently , preliminary works introduced domain classifiers along with gradient reversal on their results , adding another objective for the CNN to optimize for , which ensured domain invariant feature learning [ 1 2 ] .", "ner": [["machine learning", "Method"], ["CNNs", "Method"], ["CNN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202734316", "sentence": "With the introduction of the highly performing GANs [ 1 5 ] , most of the recent UDA methods rely on adversarial objectives between the source and target data distributions .", "ner": [["GANs", "Method"], ["UDA", "Task"]], "rel": [["GANs", "Used-For", "UDA"]], "rel_plus": [["GANs:Method", "Used-For", "UDA:Task"]]}
{"doc_id": "202734316", "sentence": "As this approach is driven by the assumption that both domains contain similar distribu - tions of the categories , it is limited only to dense segmentation scenarios While quantitative results were presented using SYNTHIA [ 3 5 ] and GTA [ 3 3 ] as source domains and Cityscapes ( CS ) [ 8 ] as the target domain , an in - the - wild dataset was also collected for assessing adaptation between real world environments as well .", "ner": [["SYNTHIA", "Dataset"], ["GTA", "Dataset"], ["Cityscapes", "Dataset"], ["CS", "Dataset"]], "rel": [["CS", "Synonym-Of", "Cityscapes"]], "rel_plus": [["CS:Dataset", "Synonym-Of", "Cityscapes:Dataset"]]}
{"doc_id": "202734316", "sentence": "In [ 5 0 ] , focusing on semantic segmentation , a CNN is trained with direct supervision on the source domain , as well as properties consistent with the target domain like global and local label distributions , with the latter enforced via superpixel landmarks .", "ner": [["semantic segmentation", "Task"], ["CNN", "Method"]], "rel": [["CNN", "Used-For", "semantic segmentation"]], "rel_plus": [["CNN:Method", "Used-For", "semantic segmentation:Task"]]}
{"doc_id": "202734316", "sentence": "Results were presented for SYN - THIA and Cityscapes as the source and target domains respectively .", "ner": [["SYN - THIA", "Dataset"], ["Cityscapes", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "202734316", "sentence": "Other works [ 3 0 , 1 7 ] approached the problem of UDA under an unpaired image - to - image translation framework .", "ner": [["UDA", "Task"], ["image - to - image translation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "202734316", "sentence": "Overall , inter domain translations were used , along with cycle consistencies between them and translation classification losses .", "ner": [["translation classification", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "202734316", "sentence": "As both of these frameworks are generic , they were applied to the digit classification and semantic segmentation tasks .", "ner": [["digit classification", "Task"], ["semantic segmentation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "202734316", "sentence": "SG - GAN [ 2 5 ] complements cycle consistencies and adversarial discriminators with a soft - gradient objective that aims to align texture edges with semantic boundaries , and presents results for the GTA to CS semantic segmentation task .", "ner": [["SG - GAN", "Method"], ["adversarial discriminators", "Method"], ["soft - gradient objective", "Method"], ["GTA", "Dataset"], ["CS", "Dataset"], ["semantic segmentation", "Task"]], "rel": [["soft - gradient objective", "Part-Of", "SG - GAN"], ["adversarial discriminators", "Part-Of", "SG - GAN"], ["SG - GAN", "Evaluated-With", "GTA"], ["SG - GAN", "Evaluated-With", "CS"], ["GTA", "Benchmark-For", "semantic segmentation"], ["CS", "Benchmark-For", "semantic segmentation"], ["SG - GAN", "Used-For", "semantic segmentation"]], "rel_plus": [["soft - gradient objective:Method", "Part-Of", "SG - GAN:Method"], ["adversarial discriminators:Method", "Part-Of", "SG - GAN:Method"], ["SG - GAN:Method", "Evaluated-With", "GTA:Dataset"], ["SG - GAN:Method", "Evaluated-With", "CS:Dataset"], ["GTA:Dataset", "Benchmark-For", "semantic segmentation:Task"], ["CS:Dataset", "Benchmark-For", "semantic segmentation:Task"], ["SG - GAN:Method", "Used-For", "semantic segmentation:Task"]]}
{"doc_id": "202734316", "sentence": "Their provided results focus on virtual to real adaptation using the SYNTHIA and GTA datasets that are evaluated on Cityscapes .", "ner": [["SYNTHIA", "Dataset"], ["GTA datasets", "Dataset"], ["Cityscapes", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "202734316", "sentence": "Taking a slightly more original approach , [ 4 5 ] chose to discriminate based on the output semantic segmentation map on multiple scales and demonstrate their method under a S 2 R theme using GTA and SYNTHIA adapted to CS , and also under a real - to - real scheme , using CS and a Cross - City dataset [ 7 ] .", "ner": [["semantic segmentation", "Task"], ["S 2 R", "Task"], ["GTA", "Dataset"], ["SYNTHIA", "Dataset"], ["CS", "Dataset"], ["CS", "Dataset"], ["Cross - City dataset", "Dataset"]], "rel": [["GTA", "Benchmark-For", "semantic segmentation"], ["SYNTHIA", "Benchmark-For", "semantic segmentation"], ["CS", "Benchmark-For", "semantic segmentation"], ["Cross - City dataset", "Benchmark-For", "semantic segmentation"]], "rel_plus": [["GTA:Dataset", "Benchmark-For", "semantic segmentation:Task"], ["SYNTHIA:Dataset", "Benchmark-For", "semantic segmentation:Task"], ["CS:Dataset", "Benchmark-For", "semantic segmentation:Task"], ["Cross - City dataset:Dataset", "Benchmark-For", "semantic segmentation:Task"]]}
{"doc_id": "202734316", "sentence": "More recently , the issues that traditional batch normalization creates during domain adaptation were acknowledged by [ 3 4 ] , and then adapted this technique appropriately for domain adaptation .", "ner": [["batch normalization", "Method"], ["domain adaptation", "Method"], ["domain adaptation", "Method"]], "rel": [["batch normalization", "Used-For", "domain adaptation"]], "rel_plus": [["batch normalization:Method", "Used-For", "domain adaptation:Method"]]}
{"doc_id": "202734316", "sentence": "Besides offering results for the GTA to CS task , they also reported performance on unseen real data , and more specifically , in Mapillary Vistas ( Map ) [ 3 1 ] and ApolloScape ( AS ) [ 2 1 ] .", "ner": [["GTA", "Dataset"], ["CS", "Dataset"], ["Mapillary Vistas", "Dataset"], ["Map", "Dataset"], ["ApolloScape", "Dataset"], ["AS", "Dataset"]], "rel": [["Map", "Synonym-Of", "Mapillary Vistas"], ["AS", "Synonym-Of", "ApolloScape"]], "rel_plus": [["Map:Dataset", "Synonym-Of", "Mapillary Vistas:Dataset"], ["AS:Dataset", "Synonym-Of", "ApolloScape:Dataset"]]}
{"doc_id": "202734316", "sentence": "In [ 5 3 ] the novel concept of self - training is introduced , where target domain pseudo - labels are estimated under an iterative self - training framework with spatial priors , achieving state - of - the - art results on the S 2 R adaptation task on SYNTHIA and GTA to CS experiments .", "ner": [["self - training", "Method"], ["self - training", "Method"], ["S 2 R", "Task"], ["SYNTHIA", "Dataset"], ["GTA", "Dataset"], ["CS", "Dataset"]], "rel": [["SYNTHIA", "Benchmark-For", "S 2 R"], ["GTA", "Benchmark-For", "S 2 R"], ["CS", "Benchmark-For", "S 2 R"], ["self - training", "Used-For", "S 2 R"], ["self - training", "Evaluated-With", "SYNTHIA"], ["self - training", "Evaluated-With", "GTA"], ["self - training", "Evaluated-With", "CS"]], "rel_plus": [["SYNTHIA:Dataset", "Benchmark-For", "S 2 R:Task"], ["GTA:Dataset", "Benchmark-For", "S 2 R:Task"], ["CS:Dataset", "Benchmark-For", "S 2 R:Task"], ["self - training:Method", "Used-For", "S 2 R:Task"], ["self - training:Method", "Evaluated-With", "SYNTHIA:Dataset"], ["self - training:Method", "Evaluated-With", "GTA:Dataset"], ["self - training:Method", "Evaluated-With", "CS:Dataset"]]}
{"doc_id": "202734316", "sentence": "An orthogonal task to UDA is style transfer [ 1 4 ] , as the transfer of the style of a target domain to a source one , aligns with the concept of adapting a model from one domain to another .", "ner": [["UDA", "Task"], ["style transfer", "Task"]], "rel": [["style transfer", "Compare-With", "UDA"]], "rel_plus": [["style transfer:Task", "Compare-With", "UDA:Task"]]}
{"doc_id": "202734316", "sentence": "This was the driving idea behind [ 3 ] which used a GAN to learn the task of monocular depth estimation in the target domain and showcased their concept in a S 2 R adaptation task using GTA and KITTI [ 2 9 ] .", "ner": [["GAN", "Method"], ["monocular depth estimation", "Task"], ["S 2 R", "Task"], ["GTA", "Dataset"], ["KITTI", "Dataset"]], "rel": [["GAN", "Used-For", "monocular depth estimation"], ["GTA", "Benchmark-For", "S 2 R"], ["KITTI", "Benchmark-For", "S 2 R"]], "rel_plus": [["GAN:Method", "Used-For", "monocular depth estimation:Task"], ["GTA:Dataset", "Benchmark-For", "S 2 R:Task"], ["KITTI:Dataset", "Benchmark-For", "S 2 R:Task"]]}
{"doc_id": "202734316", "sentence": "Nevertheless , transferring the style of one domain to another has only been an implicit goal of some approaches while it has been explicitly used as data augmentation and partly for domain adaptation by [ 2 3 , 4 3 ] .", "ner": [["data augmentation", "Method"], ["domain adaptation", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202734316", "sentence": "The former [ 2 3 ] investigate the effect of style augmentation on three tasks ( image classification , cross - domain classification and depth estimation ) whereas the latter [ 4 3 ] enforces the network to learn domain invariant features between source and restyled samples .", "ner": [["image classification", "Task"], ["cross - domain classification", "Task"], ["depth estimation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "202734316", "sentence": "Overall , we observe a trend in using adversarial learning and cycle consistencies for UDA , as well as a focus on classification , either image - wide or at the pixel level .", "ner": [["UDA", "Task"], ["classification", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "202734316", "sentence": "Driven by this , we identify a new solution to the UDA problem , based on the orthogonal style transfer concept .", "ner": [["UDA", "Task"], ["style transfer", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "202734316", "sentence": "We transform samples from X s and X t using discrete cosine transform ( DCT ) into the frequency domain : where T represents the resulting DCT spectrum , A is a quadratic , real - valued , orthonormal transformation matrix , and g is a low - resolution grayscale transformed sample x. Then , we extract the low frequencies ( 8 \u00d7 8 top left block ) and calculate the median value after excluding the high amplitude DC term .", "ner": [["discrete cosine transform", "Method"], ["DCT", "Method"], ["DCT", "Method"]], "rel": [["DCT", "Synonym-Of", "discrete cosine transform"]], "rel_plus": [["DCT:Method", "Synonym-Of", "discrete cosine transform:Method"]]}
{"doc_id": "202734316", "sentence": "We conduct a series of experiments in two different vision tasks to validate this , mostly focusing on S 2 R UDA .", "ner": [["S 2 R UDA", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "202734316", "sentence": "Setting out to Table 3 , the same applies regarding metrics and results presentation , except from ours , where the second row shows results for the configuration GTA(RS - Map ) \u2192 Map , and the third for GTA(PH - Map ) \u2192 Map . validate and assess our proposed technique , we fix K to 5 for all conducted experiments , which is a good compromise between efficiency and effectiveness .    In this section , we present results for data restyling on the semantic segmentation task in a synthetic to - real adaptation setting .", "ner": [["GTA(RS - Map )", "Dataset"], ["Map", "Dataset"], ["GTA(PH - Map", "Dataset"], ["Map", "Dataset"], ["semantic segmentation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "202734316", "sentence": "The dataset is rendered from the video - game Grand Theft Auto V and is based on the city of Los Angeles with labels fully compatible with those of Cityscapes . 3 ) Mapillary Vistas , a large - scale street - level image dataset containing 2 5 , 0 0 0 high - resolution images ( split into 1 8 , 0 0 0 for training , 2 , 0 0 0 for validation , 5 , 0 0 0 for testing ; at an average resolution of 9 megapixels ) .", "ner": [["Cityscapes", "Dataset"], ["Mapillary Vistas", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "202734316", "sentence": "We use GTA as our labelled synthetic source data distribution , Mapillary , Apolloscape and Cityscapes for the real domain data distributions , and use the overlapping labels among them .", "ner": [["GTA", "Dataset"], ["Mapillary", "Dataset"], ["Apolloscape", "Dataset"], ["Cityscapes", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "202734316", "sentence": "Our experiments are based on DeepLab - v 2 [ 6 ] with ResNet - 1 0 1 [ 1 6 ] and VGG - 1 6 [ 3 9 ] encoders , both initialized with pre - trained weights on ImageNet [ 1 0 ] .", "ner": [["DeepLab - v 2", "Method"], ["ResNet - 1 0 1", "Method"], ["VGG - 1 6", "Method"], ["ImageNet", "Dataset"]], "rel": [["ResNet - 1 0 1", "Part-Of", "DeepLab - v 2"], ["VGG - 1 6", "Part-Of", "DeepLab - v 2"], ["ResNet - 1 0 1", "Trained-With", "ImageNet"], ["VGG - 1 6", "Trained-With", "ImageNet"]], "rel_plus": [["ResNet - 1 0 1:Method", "Part-Of", "DeepLab - v 2:Method"], ["VGG - 1 6:Method", "Part-Of", "DeepLab - v 2:Method"], ["ResNet - 1 0 1:Method", "Trained-With", "ImageNet:Dataset"], ["VGG - 1 6:Method", "Trained-With", "ImageNet:Dataset"]]}
{"doc_id": "202734316", "sentence": "All models are trained for 2 7 epochs using SGD with a learning rate of 2. 5 \u00d7 1 0 \u2212 4 , supported by a 0. 9 momentum , a weight decay of 5 \u00d7 1 0 \u2212 4 and a polynomial learning rate decay policy with \u03b3 = 0. 9 .", "ner": [["SGD", "Method"], ["momentum", "Method"], ["weight decay", "Method"]], "rel": [["momentum", "Part-Of", "SGD"], ["weight decay", "Part-Of", "SGD"]], "rel_plus": [["momentum:Method", "Part-Of", "SGD:Method"], ["weight decay:Method", "Part-Of", "SGD:Method"]]}
{"doc_id": "202734316", "sentence": "Table 1 presents results for GTA(Mapillary ) \u2192 Mapillary in comparison to [ 3 4 ] , the only method to presents results in Map , albeit in their case it is considered as unseen . \" Unseen \" refers to the scenario where a domain adaptation model was trained using a specific target dataset to align its source domain to , but applied to another one , which is similar in context and represents the same domain as the target .", "ner": [["GTA(Mapillary )", "Dataset"], ["Mapillary", "Dataset"], ["Map", "Dataset"], ["domain adaptation", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202734316", "sentence": "We observe that data restyling offers competitive performance and a much larger increase compared to the baseline , with matched restyling ( PH ) additionally offering a higher boost compared to random selection ( RS ) .", "ner": [["PH", "Method"], ["random selection", "Method"], ["RS", "Method"]], "rel": [["RS", "Synonym-Of", "random selection"], ["PH", "Compare-With", "random selection"]], "rel_plus": [["RS:Method", "Synonym-Of", "random selection:Method"], ["PH:Method", "Compare-With", "random selection:Method"]]}
{"doc_id": "202734316", "sentence": "Table 2 presents results for the conducted GTA(Map ) \u2192 AS experiment .", "ner": [["GTA(Map )", "Dataset"], ["AS", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "202734316", "sentence": "While the synthetic data were restyled using Map 's data distribution , the model 's performance on the AS real -but unseen -data distribution is preserved and surpasses that reported in [ 3 4 ] .", "ner": [["Map", "Dataset"], ["AS", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "202734316", "sentence": "As with GTA(Map ) \u2192 Map , a performance increase is observed for PH compared to RS .", "ner": [["GTA(Map )", "Dataset"], ["Map", "Dataset"], ["PH", "Method"], ["RS", "Method"]], "rel": [["PH", "Compare-With", "RS"]], "rel_plus": [["PH:Method", "Compare-With", "RS:Method"]]}
{"doc_id": "202734316", "sentence": "Finally , we perform the GTA(CS ) \u2192 CS experiment and present our results in Table 3 which are consistent for both backends used .", "ner": [["GTA(CS )", "Dataset"], ["CS", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "202734316", "sentence": "Curiously , unlike all other experiments , RS performance is higher than PH in CS , which can be attributed to CS 's lack of diversity compared to the other datasets [ 2 1 ] .", "ner": [["RS", "Method"], ["PH", "Method"], ["CS", "Dataset"], ["CS", "Dataset"]], "rel": [["RS", "Compare-With", "PH"], ["PH", "Evaluated-With", "CS"], ["RS", "Evaluated-With", "CS"]], "rel_plus": [["RS:Method", "Compare-With", "PH:Method"], ["PH:Method", "Evaluated-With", "CS:Dataset"], ["RS:Method", "Evaluated-With", "CS:Dataset"]]}
{"doc_id": "202734316", "sentence": "Notably , the discrepancy be - tween RS and PH is still different from all other experiments in that no significant increase is observed for PH when targeting CS .", "ner": [["RS", "Method"], ["PH", "Method"], ["PH", "Method"], ["CS", "Dataset"]], "rel": [["RS", "Compare-With", "PH"]], "rel_plus": [["RS:Method", "Compare-With", "PH:Method"]]}
{"doc_id": "202734316", "sentence": "Apart from the quantitative evaluation , we also offer a set of exemplary qualitative results for both Map and CS in Figure 3 .", "ner": [["Map", "Dataset"], ["CS", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "202734316", "sentence": "Overall , we observe : i ) that data restyling is competitive to most other compared UDA methods for semantic segmentation , ii ) an increased generalization performance to unseen distributions , and iii ) that matching styles helps in addressing the performance deterioration that uncanny restyles via random matching would introduce .", "ner": [["UDA methods", "Method"], ["semantic segmentation", "Task"]], "rel": [["UDA methods", "Used-For", "semantic segmentation"]], "rel_plus": [["UDA methods:Method", "Used-For", "semantic segmentation:Task"]]}
{"doc_id": "202734316", "sentence": "Hence , using synthetic data generation is a very promising approach , constituting synthetic - to - real adaptation very relevant .", "ner": [["synthetic data generation", "Method"], ["synthetic - to - real adaptation", "Task"]], "rel": [["synthetic data generation", "Used-For", "synthetic - to - real adaptation"]], "rel_plus": [["synthetic data generation:Method", "Used-For", "synthetic - to - real adaptation:Task"]]}
{"doc_id": "202734316", "sentence": "It is composed of omnidirectional renders from Matterport 3 D ( M 3 D ) [ 5 ] , Stanford 2 D 3 D ( S 2 D 3 D ) [ 2 ] and SunCG [ 4 0 ] ( SunCG ) .", "ner": [["Matterport 3 D", "Dataset"], ["M 3 D", "Dataset"], ["Stanford 2 D 3 D", "Dataset"], ["S 2 D 3 D", "Dataset"], ["SunCG", "Dataset"], ["SunCG", "Dataset"]], "rel": [["M 3 D", "Synonym-Of", "Matterport 3 D"], ["S 2 D 3 D", "Synonym-Of", "Stanford 2 D 3 D"], ["SunCG", "Synonym-Of", "SunCG"]], "rel_plus": [["M 3 D:Dataset", "Synonym-Of", "Matterport 3 D:Dataset"], ["S 2 D 3 D:Dataset", "Synonym-Of", "Stanford 2 D 3 D:Dataset"], ["SunCG:Dataset", "Synonym-Of", "SunCG:Dataset"]]}
{"doc_id": "202734316", "sentence": "Given that the context of 3 6 0 D is indoors scenes , we only use that subset of Sun 3 6 0 for deriving realistic styles that consists of 3 3 5 1 2 samples .", "ner": [["3 6 0 D", "Dataset"], ["Sun 3 6 0", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "202734316", "sentence": "We use the synthetic SunCG dataset as our source domain and the realistic Sun 3 6 0 dataset as our style pool .", "ner": [["SunCG", "Dataset"], ["Sun 3 6 0", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "202734316", "sentence": "We train RectNet [ 5 2 ] only on the valid subset of SunCG ( 4 7 1 6 samples ) and consider it as our baseline .", "ner": [["RectNet", "Method"], ["SunCG", "Dataset"]], "rel": [["RectNet", "Trained-With", "SunCG"]], "rel_plus": [["RectNet:Method", "Trained-With", "SunCG:Dataset"]]}
{"doc_id": "202734316", "sentence": "In order to present quantitative results we also evaluate our trained models in an unseen task , i.e. SunCG(Sun 3 6 0 ) \u2192 S 2 D 3 D - M 3 D using the realistic S 2 D 3 D - M 3 D data .", "ner": [["SunCG(Sun 3 6 0", "Dataset"], ["S 2 D 3 D - M 3 D", "Dataset"], ["S 2 D 3 D - M 3 D", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "202734316", "sentence": "In addition , we restyle the S 2 D 3 D - M 3 D data using the Sun 3 6 0 style pool in order to infuse its style into the ground truth paired S 2 D 3 D - M 3 D data and be able to calculate quantifiable results .", "ner": [["S 2 D 3 D - M 3 D", "Dataset"], ["Sun 3 6 0", "Dataset"], ["S 2 D 3 D - M 3 D", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "202734316", "sentence": "We refer to this task as SunCG(Sun 3 6 0 ) \u2192 RS - S 2 D 3 D - M 3 D .", "ner": [["SunCG(Sun 3 6 0 )", "Dataset"], ["RS - S 2 D 3 D - M 3 D", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "202734316", "sentence": "Quantitative results are presented in Table 5 , for which Input RGB Source PH Figure 5 : Qualitative results on omnidirectional dense depth estimation on samples of SUN 3 6 0 where there is no ground truth depth map available .", "ner": [["PH", "Method"], ["depth estimation", "Task"], ["SUN 3 6 0", "Dataset"]], "rel": [["SUN 3 6 0", "Benchmark-For", "depth estimation"]], "rel_plus": [["SUN 3 6 0:Dataset", "Benchmark-For", "depth estimation:Task"]]}
{"doc_id": "202734316", "sentence": "Table 5 : Results of omnidirectional depth estimation for the different domain adaptation scenarios ( \u2193 means lower is better , and \u2191 means higher is better ) .", "ner": [["depth estimation", "Task"], ["domain adaptation", "Task"]], "rel": [["depth estimation", "Used-For", "domain adaptation"]], "rel_plus": [["depth estimation:Task", "Used-For", "domain adaptation:Task"]]}
{"doc_id": "202734316", "sentence": "The first three rows represent evaluating on M 3 D and S 2 D 3 D datasets , while the latter two correspond to evaluations on the RS - M 3 D - S 2 D 3 D with styles derived form SUN 3 6 0 .", "ner": [["M 3 D", "Dataset"], ["S 2 D 3 D", "Dataset"], ["RS - M 3 D - S 2 D 3 D", "Dataset"], ["SUN 3 6 0", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "202734316", "sentence": "Baseline implies that RectNet was trained on SunCG only , ours - RS that RectNet was trained on RS - SunCG , while ours - PH that RectNet was trained with PH - SunCG with styles from SUN 3 6 0 . we used typical metrics as reported in [ 5 2 ] .", "ner": [["RectNet", "Method"], ["SunCG", "Dataset"], ["RS", "Method"], ["RectNet", "Method"], ["RS - SunCG", "Dataset"], ["PH", "Method"], ["RectNet", "Method"], ["PH - SunCG", "Dataset"], ["SUN 3 6 0", "Dataset"]], "rel": [["RectNet", "Trained-With", "SunCG"], ["RectNet", "Trained-With", "RS - SunCG"], ["RectNet", "Trained-With", "PH - SunCG"]], "rel_plus": [["RectNet:Method", "Trained-With", "SunCG:Dataset"], ["RectNet:Method", "Trained-With", "RS - SunCG:Dataset"], ["RectNet:Method", "Trained-With", "PH - SunCG:Dataset"]]}
{"doc_id": "202734316", "sentence": "The results further support previous findings in that restyling the data increases performance to both the targeted domain ( RS - S 2 D 3 D - M 3 D ) , as well as similar domains ( S 2 D 3 D - M 3 D ) , with the largest performance gained obtained on the domain that the styles were picked from .", "ner": [["RS - S 2 D 3 D - M 3 D", "Dataset"], ["S 2 D 3 D - M 3 D", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "3920676", "sentence": "The experimental protocol incorporates the most recent advances in both feature extraction and metric learning .", "ner": [["feature extraction", "Method"], ["metric learning", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "3920676", "sentence": "To ensure a fair comparison , all of the approaches were implemented using a unified code library that includes 1 1 feature extraction algorithms and 2 2 metric learning and ranking techniques .", "ner": [["feature extraction algorithms", "Method"], ["metric learning", "Method"], ["ranking techniques", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "3920676", "sentence": "All approaches were evaluated using a new large - scale dataset that closely mimics a real - world problem setting , in addition to 1 6 other publicly available datasets : VIPeR , GRID , CAVIAR , DukeMTMC 4 ReID , 3DPeS , PRID , V 4 7 , WARD , SAIVT - SoftBio , CUHK 0 1 , CHUK 0 2 , CUHK 0 3 , RAiD , iLIDSVID , HDA+ and Market 1 5 0 1 .", "ner": [["VIPeR", "Dataset"], ["GRID", "Dataset"], ["CAVIAR", "Dataset"], ["DukeMTMC 4 ReID", "Dataset"], ["3DPeS", "Dataset"], ["PRID", "Dataset"], ["V 4 7", "Dataset"], ["WARD", "Dataset"], ["SAIVT - SoftBio", "Dataset"], ["CUHK 0 1", "Dataset"], ["CHUK 0 2", "Dataset"], ["CUHK 0 3", "Dataset"], ["RAiD", "Dataset"], ["iLIDSVID", "Dataset"], ["HDA+", "Dataset"], ["Market 1 5 0 1", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "3920676", "sentence": "P ERSON re - identification , or re - id , is a critical task in most surveillance and security applications [ 1 ] , [ 2 ] , [ 3 ] and has increasingly attracted attention from the computer vision community [ 4 ] , [ 5 ] , [ 6 ] , [ 7 ] , [ 8 ] , [ 9 ] , [ 1 0 ] , [ 1 1 ] , [ 1 2 ] , [ 1 3 ] , [ 1 4 ] , [ 1 5 ] , [ 1 6 ] , [ 1 7 ] , [ 1 8 ] , [ 1 9 ] , [ 2 0 ] , [ 2 1 ] , [ 2 2 ] , [ 2 3 ] .", "ner": [["P ERSON re - identification", "Task"], ["re - id", "Task"], ["computer vision", "Task"]], "rel": [["re - id", "Synonym-Of", "P ERSON re - identification"], ["P ERSON re - identification", "SubTask-Of", "computer vision"]], "rel_plus": [["re - id:Task", "Synonym-Of", "P ERSON re - identification:Task"], ["P ERSON re - identification:Task", "SubTask-Of", "computer vision:Task"]]}
{"doc_id": "3920676", "sentence": "In this paper , we present a careful , fair , and systematic evaluation of feature extraction , metric learning , and multi - shot ranking algorithms proposed for re - id on a wide variety of benchmark datasets .", "ner": [["feature extraction", "Method"], ["metric learning", "Method"], ["multi - shot ranking", "Method"], ["re - id", "Task"]], "rel": [["multi - shot ranking", "Used-For", "re - id"], ["feature extraction", "Used-For", "re - id"], ["metric learning", "Used-For", "re - id"]], "rel_plus": [["multi - shot ranking:Method", "Used-For", "re - id:Task"], ["feature extraction:Method", "Used-For", "re - id:Task"], ["metric learning:Method", "Used-For", "re - id:Task"]]}
{"doc_id": "3920676", "sentence": "Our general evaluation framework is to consider all possible combinations of feature extraction and metric learning algorithms for single - shot datasets and all possible combinations of feature extraction , metric learning , and multi - shot ranking algorithms for multi - shot datasets .", "ner": [["feature extraction", "Method"], ["metric learning", "Method"], ["feature extraction", "Method"], ["metric learning", "Method"], ["multi - shot ranking", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "3920676", "sentence": "As part of the evaluation , we built a public code library with an easy - to - use input/output code structure and uniform algorithm parameters that includes 1 1 contemporary feature extraction and 2 2 metric learning and ranking algorithms .", "ner": [["feature extraction", "Method"], ["metric learning", "Method"], ["ranking algorithms", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "3920676", "sentence": "In this section , we summarize the feature extraction , metric learning , and multi - shot ranking techniques that are evaluated as part of the proposed re - id benchmark , which include algorithms published through ECCV 2 0 1 6 .", "ner": [["feature extraction", "Method"], ["metric learning", "Method"], ["multi - shot ranking", "Method"], ["re - id", "Task"]], "rel": [["feature extraction", "Used-For", "re - id"], ["metric learning", "Used-For", "re - id"], ["multi - shot ranking", "Used-For", "re - id"]], "rel_plus": [["feature extraction:Method", "Used-For", "re - id:Task"], ["metric learning:Method", "Used-For", "re - id:Task"], ["multi - shot ranking:Method", "Used-For", "re - id:Task"]]}
{"doc_id": "3920676", "sentence": "In IDE - CaffeNet , IDE - ResNet , and IDE - VGGNet , we use the idea first presented in the DeepFace paper [ 4 0 ] and applied to reid by Zheng et al. [ 4 1 ] , in which every person is treated as a separate class and a convolutional neural network is trained for a classification objective .", "ner": [["IDE - CaffeNet", "Method"], ["IDE - ResNet", "Method"], ["IDE - VGGNet", "Method"], ["DeepFace", "Method"], ["convolutional neural network", "Method"], ["classification", "Task"]], "rel": [["convolutional neural network", "Used-For", "classification"]], "rel_plus": [["convolutional neural network:Method", "Used-For", "classification:Task"]]}
{"doc_id": "3920676", "sentence": "AlexNet [ 4 2 ] , ResNet [ 4 3 ] , and VGGNet [ 4 4 ] architectures are employed in IDE - CaffeNet , IDE - ResNet and IDE - VGGNet respectively .", "ner": [["AlexNet", "Method"], ["ResNet", "Method"], ["VGGNet", "Method"], ["IDE - CaffeNet", "Method"], ["IDE - ResNet", "Method"], ["IDE - VGGNet", "Method"]], "rel": [["AlexNet", "Part-Of", "IDE - CaffeNet"], ["ResNet", "Part-Of", "IDE - ResNet"], ["VGGNet", "Part-Of", "IDE - VGGNet"]], "rel_plus": [["AlexNet:Method", "Part-Of", "IDE - CaffeNet:Method"], ["ResNet:Method", "Part-Of", "IDE - ResNet:Method"], ["VGGNet:Method", "Part-Of", "IDE - VGGNet:Method"]]}
{"doc_id": "3920676", "sentence": "In DenseColorSIFT [ 9 ] , each image is densely divided into patches , and color histograms and SIFT features are extracted from each patch .", "ner": [["DenseColorSIFT", "Method"], ["color histograms", "Method"], ["SIFT features", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "3920676", "sentence": "In HistLBP [ 1 3 ] , color histograms in the RGB , YCbCr , and HS color spaces and texture histograms from local binary patterns ( LBP ) [ 4 5 ] features are computed .", "ner": [["HistLBP", "Method"], ["local binary patterns", "Method"], ["LBP", "Method"]], "rel": [["local binary patterns", "Part-Of", "HistLBP"], ["LBP", "Synonym-Of", "local binary patterns"]], "rel_plus": [["local binary patterns:Method", "Part-Of", "HistLBP:Method"], ["LBP:Method", "Synonym-Of", "local binary patterns:Method"]]}
{"doc_id": "3920676", "sentence": "In LOMO [ 1 8 ] , HSV color histograms and scale - invariant LBP [ 4 6 ] features are extracted from the image processed by a multi - scale Retinex algorithm [ 4 7 ] , and maximally - pooled along the same horizontal strip .", "ner": [["LOMO", "Method"], ["HSV color histograms", "Method"], ["scale - invariant LBP", "Method"], ["multi - scale Retinex algorithm", "Method"]], "rel": [["multi - scale Retinex algorithm", "Part-Of", "LOMO"], ["multi - scale Retinex algorithm", "Used-For", "HSV color histograms"], ["multi - scale Retinex algorithm", "Used-For", "scale - invariant LBP"]], "rel_plus": [["multi - scale Retinex algorithm:Method", "Part-Of", "LOMO:Method"], ["multi - scale Retinex algorithm:Method", "Used-For", "HSV color histograms:Method"], ["multi - scale Retinex algorithm:Method", "Used-For", "scale - invariant LBP:Method"]]}
{"doc_id": "3920676", "sentence": "Fisher discriminant analysis ( FDA ) [ 5 1 ] , local Fisher discriminant analysis ( LFDA ) [ 1 1 ] , marginal Fisher analysis ( MFA ) [ 5 4 ] , cross - view quadratic discriminant analysis ( XQDA ) [ 1 8 ] , and discriminative null space learning ( NFST ) [ 5 7 ] all formulate a Fisher - type optimization problem that seeks to minimize the within - class data scatter while maximizing between - class data scatter .", "ner": [["Fisher discriminant analysis", "Method"], ["FDA", "Method"], ["local Fisher discriminant analysis", "Method"], ["LFDA", "Method"], ["marginal Fisher analysis", "Method"], ["MFA", "Method"], ["cross - view quadratic discriminant analysis", "Method"], ["XQDA", "Method"], ["discriminative null space learning", "Method"], ["NFST", "Method"]], "rel": [["FDA", "Synonym-Of", "Fisher discriminant analysis"], ["LFDA", "Synonym-Of", "local Fisher discriminant analysis"], ["MFA", "Synonym-Of", "marginal Fisher analysis"], ["XQDA", "Synonym-Of", "cross - view quadratic discriminant analysis"], ["NFST", "Synonym-Of", "discriminative null space learning"]], "rel_plus": [["FDA:Method", "Synonym-Of", "Fisher discriminant analysis:Method"], ["LFDA:Method", "Synonym-Of", "local Fisher discriminant analysis:Method"], ["MFA:Method", "Synonym-Of", "marginal Fisher analysis:Method"], ["XQDA:Method", "Synonym-Of", "cross - view quadratic discriminant analysis:Method"], ["NFST:Method", "Synonym-Of", "discriminative null space learning:Method"]]}
{"doc_id": "3920676", "sentence": "Information - theoretic metric learning ( ITML ) [ 5 2 ] , large - margin nearest neighbor ( LMNN ) [ 5 5 ] , relative distance comparison ( PRDC ) [ 6 ] , keep - it - simple - and - straightforward metric ( KISSME ) [ 7 ] , and pairwise constrained component analysis ( PCCA ) [ 8 ] all learn Mahalanobis - type distance functions using variants of the basic pairwise constraints principle . kPCCA [ 8 ] , kLFDA [ 1 3 ] , and kMFA [ 1 3 ] kernelize PCCA , LFDA , and MFA , [ 5 6 ] adopts canonical correlation analysis to map the kernelized features into a common subspace .", "ner": [["Information - theoretic metric learning", "Method"], ["ITML", "Method"], ["large - margin nearest neighbor", "Method"], ["LMNN", "Method"], ["relative distance comparison", "Method"], ["PRDC", "Method"], ["keep - it - simple - and - straightforward metric", "Method"], ["KISSME", "Method"], ["pairwise constrained component analysis", "Method"], ["PCCA", "Method"], ["Mahalanobis - type distance functions", "Method"], ["kPCCA", "Method"], ["kLFDA", "Method"], ["kMFA", "Method"], ["PCCA", "Method"], ["LFDA", "Method"], ["MFA", "Method"]], "rel": [["ITML", "Synonym-Of", "Information - theoretic metric learning"], ["LMNN", "Synonym-Of", "large - margin nearest neighbor"], ["PRDC", "Synonym-Of", "relative distance comparison"], ["KISSME", "Synonym-Of", "keep - it - simple - and - straightforward metric"], ["PCCA", "Synonym-Of", "pairwise constrained component analysis"]], "rel_plus": [["ITML:Method", "Synonym-Of", "Information - theoretic metric learning:Method"], ["LMNN:Method", "Synonym-Of", "large - margin nearest neighbor:Method"], ["PRDC:Method", "Synonym-Of", "relative distance comparison:Method"], ["KISSME:Method", "Synonym-Of", "keep - it - simple - and - straightforward metric:Method"], ["PCCA:Method", "Synonym-Of", "pairwise constrained component analysis:Method"]]}
{"doc_id": "3920676", "sentence": "In RankSVM [ 5 ] , a weight vector that weights the different features appropriately is learned using a soft - margin SVM formulation .", "ner": [["RankSVM", "Method"], ["soft - margin SVM", "Method"]], "rel": [["soft - margin SVM", "Part-Of", "RankSVM"]], "rel_plus": [["soft - margin SVM:Method", "Part-Of", "RankSVM:Method"]]}
{"doc_id": "3920676", "sentence": "In SVMML [ 5 3 ] , locally adaptive decision functions are learned in a large - margin SVM framework .", "ner": [["SVMML", "Method"], ["large - margin SVM", "Method"]], "rel": [["large - margin SVM", "Part-Of", "SVMML"]], "rel_plus": [["large - margin SVM:Method", "Part-Of", "SVMML:Method"]]}
{"doc_id": "3920676", "sentence": "Specifically , we considered the AHISD [ 5 8 ] and RNP [ 5 9 ] algorithms .", "ner": [["AHISD", "Method"], ["RNP", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "3920676", "sentence": "While these methods were proposed in the context of face recognition , the basic notion of image set matching applies to re - id as well .", "ner": [["face recognition", "Task"], ["image set matching", "Task"], ["re - id", "Task"]], "rel": [["image set matching", "Used-For", "re - id"], ["face recognition", "Used-For", "re - id"]], "rel_plus": [["image set matching:Task", "Used-For", "re - id:Task"], ["face recognition:Task", "Used-For", "re - id:Task"]]}
{"doc_id": "3920676", "sentence": "Specifically , we consider SRID [ 6 0 ] , where a block sparse recovery problem is solved to retrieve the identity of a probe person , and ISR [ 5 0 ] , where the recovered sparse coefficient vector is re - weighted using an iterative scheme to rank gallery candidates and re - identify the person of interest .", "ner": [["SRID", "Method"], ["ISR", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "3920676", "sentence": "We also indicate the number of bounding boxes ( BBox ) and cameras ( cam ) in each dataset , and the means by which the bounding boxes were obtained : using hand - labeling ( hand ) , aggregated channel features [ 7 4 ] ( ACF ) , or the deformable parts model detector [ 7 5 ] ( DPM ) .", "ner": [["hand - labeling", "Method"], ["hand", "Method"], ["aggregated channel features", "Method"], ["ACF", "Method"], ["deformable parts model detector", "Method"], ["DPM", "Method"]], "rel": [["hand", "Synonym-Of", "hand - labeling"], ["ACF", "Synonym-Of", "aggregated channel features"], ["DPM", "Synonym-Of", "deformable parts model detector"]], "rel_plus": [["hand:Method", "Synonym-Of", "hand - labeling:Method"], ["ACF:Method", "Synonym-Of", "aggregated channel features:Method"], ["DPM:Method", "Synonym-Of", "deformable parts model detector:Method"]]}
{"doc_id": "3920676", "sentence": "Market 1 5 0 1 [ 8 6 ] has 1, 5 0 1 people with 3 2 , 6 4 3 images and 2, 7 9 3 false alarms from the DPM person detector [ 7 5 ] .", "ner": [["Market 1 5 0 1", "Dataset"], ["DPM person detector", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "3920676", "sentence": "Unlike other datasets that primarily capture images of people in a university setup ( e.g. , Market 1 5 0 1 , CUHK , DukeMTMC 4 ReID ) , the Airport dataset captures images of people from an eclectic mix of professions , leading to a richer , more diversified set of images .", "ner": [["Market 1 5 0 1", "Dataset"], ["CUHK", "Dataset"], ["DukeMTMC 4 ReID", "Dataset"], ["Airport", "Dataset"]], "rel": [["Airport", "Compare-With", "Market 1 5 0 1"], ["Airport", "Compare-With", "CUHK"], ["Airport", "Compare-With", "DukeMTMC 4 ReID"]], "rel_plus": [["Airport:Dataset", "Compare-With", "Market 1 5 0 1:Dataset"], ["Airport:Dataset", "Compare-With", "CUHK:Dataset"], ["Airport:Dataset", "Compare-With", "DukeMTMC 4 ReID:Dataset"]]}
{"doc_id": "3920676", "sentence": "We employ the single - shot evaluation protocol for VIPeR , GRID , 3DPeS , DukeMTMC 4 ReID , CUHK 0 1 , CUHK 0 2 , CUHK 0 3 , HDA+ , Market 1 5 0 1 , and Airport .", "ner": [["VIPeR", "Dataset"], ["GRID", "Dataset"], ["3DPeS", "Dataset"], ["DukeMTMC 4 ReID", "Dataset"], ["CUHK 0 1", "Dataset"], ["CUHK 0 2", "Dataset"], ["CUHK 0 3", "Dataset"], ["HDA+", "Dataset"], ["Market 1 5 0 1", "Dataset"], ["Airport", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "3920676", "sentence": "In the case of CUHK 0 3 , DukeMTMC 4 ReID , GRID , HDA+ , and Market 1 5 0 1 , we use the partition files provided by the respective authors .", "ner": [["CUHK 0 3", "Dataset"], ["DukeMTMC 4 ReID", "Dataset"], ["GRID", "Dataset"], ["HDA+", "Dataset"], ["Market 1 5 0 1", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "3920676", "sentence": "In RAiD , we fix camera 1 as the probe view , resulting in three sub - datasets , RAiD - 1 2 , RAiD - 1 3 , and RAiD - 1 4 , corresponding to the 3 possible gallery views .", "ner": [["RAiD", "Dataset"], ["RAiD - 1 2", "Dataset"], ["RAiD - 1 3", "Dataset"], ["RAiD - 1 4", "Dataset"]], "rel": [["RAiD - 1 2", "SubClass-Of", "RAiD"], ["RAiD - 1 3", "SubClass-Of", "RAiD"], ["RAiD - 1 4", "SubClass-Of", "RAiD"]], "rel_plus": [["RAiD - 1 2:Dataset", "SubClass-Of", "RAiD:Dataset"], ["RAiD - 1 3:Dataset", "SubClass-Of", "RAiD:Dataset"], ["RAiD - 1 4:Dataset", "SubClass-Of", "RAiD:Dataset"]]}
{"doc_id": "3920676", "sentence": "In WARD , we fix camera 1 as the probe view , resulting in two sub - datasets , WARD - 1 2 and WARD - 1 3 , corresponding to the 2 possible gallery views .", "ner": [["WARD", "Dataset"], ["WARD - 1 2", "Dataset"], ["WARD - 1 3", "Dataset"]], "rel": [["WARD - 1 2", "SubClass-Of", "WARD"], ["WARD - 1 3", "SubClass-Of", "WARD"]], "rel_plus": [["WARD - 1 2:Dataset", "SubClass-Of", "WARD:Dataset"], ["WARD - 1 3:Dataset", "SubClass-Of", "WARD:Dataset"]]}
{"doc_id": "3920676", "sentence": "We split VIPeR , CUHK 0 1 , CUHK 0 2 , GRID , CAVIAR , 3DPeS , PRID , WARD - 1 2 , WARD - 1 3 and iLIDSVID into equal - sized training and testing sets .", "ner": [["VIPeR", "Dataset"], ["CUHK 0 1", "Dataset"], ["CUHK 0 2", "Dataset"], ["GRID", "Dataset"], ["CAVIAR", "Dataset"], ["3DPeS", "Dataset"], ["PRID", "Dataset"], ["WARD - 1 2", "Dataset"], ["WARD - 1 3", "Dataset"], ["iLIDSVID", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "3920676", "sentence": "We note that in the cases of iLIDSVID , PRID , and SAIVT , the split protocol used here is the same as in previous works that propose multi - shot re - id algorithms [ 2 8 ] , [ 9 0 ] , [ 9 1 ] , [ 9 2 ] , [ 9 3 ] to ensure evaluation consistency .", "ner": [["iLIDSVID", "Dataset"], ["PRID", "Dataset"], ["SAIVT", "Dataset"], ["multi - shot re - id", "Method"]], "rel": [["multi - shot re - id", "Evaluated-With", "iLIDSVID"], ["multi - shot re - id", "Evaluated-With", "PRID"], ["multi - shot re - id", "Evaluated-With", "SAIVT"]], "rel_plus": [["multi - shot re - id:Method", "Evaluated-With", "iLIDSVID:Dataset"], ["multi - shot re - id:Method", "Evaluated-With", "PRID:Dataset"], ["multi - shot re - id:Method", "Evaluated-With", "SAIVT:Dataset"]]}
{"doc_id": "3920676", "sentence": "However , in the case of CLUST , we do not consider kernelized metric learning algorithms and other non - typical algorithms such as RankSVM and SVMML because only AVER can be employed to rank gallery candidates .", "ner": [["CLUST", "Task"], ["kernelized metric learning", "Method"], ["RankSVM", "Method"], ["SVMML", "Method"]], "rel": [["kernelized metric learning", "Used-For", "CLUST"], ["RankSVM", "Used-For", "CLUST"], ["SVMML", "Used-For", "CLUST"]], "rel_plus": [["kernelized metric learning:Method", "Used-For", "CLUST:Task"], ["RankSVM:Method", "Used-For", "CLUST:Task"], ["SVMML:Method", "Used-For", "CLUST:Task"]]}
{"doc_id": "3920676", "sentence": "We normalize all images of a particular dataset to the same size , which is set to 1 2 8 \u00c2 4 8 for VIPeR , GRID , CAVIAR and 3DPeS and 1 2 8 \u00c2 6 4 for all other datasets .", "ner": [["VIPeR", "Dataset"], ["GRID", "Dataset"], ["CAVIAR", "Dataset"], ["3DPeS", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "3920676", "sentence": "In the case of IDE - ResNet and IDE - VGGNet , we resize each image to 2 2 4 \u00c2 2 2 4 pixels following [ 4 4 ] .", "ner": [["IDE - ResNet", "Method"], ["IDE - VGGNet", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "3920676", "sentence": "We start training with a model pre - trained on the ImageNet dataset and train the fully connected layers fc 7 and fc 8 from scratch . 1 .", "ner": [["ImageNet", "Dataset"], ["fully connected layers", "Method"], ["fc 7", "Method"], ["fc 8", "Method"]], "rel": [["fc 8", "Trained-With", "ImageNet"], ["fc 7", "Trained-With", "ImageNet"], ["fc 7", "SubClass-Of", "fully connected layers"], ["fc 8", "SubClass-Of", "fully connected layers"]], "rel_plus": [["fc 8:Method", "Trained-With", "ImageNet:Dataset"], ["fc 7:Method", "Trained-With", "ImageNet:Dataset"], ["fc 7:Method", "SubClass-Of", "fully connected layers:Method"], ["fc 8:Method", "SubClass-Of", "fully connected layers:Method"]]}
{"doc_id": "3920676", "sentence": "We evaluate only linear and exp kernels for LDFV , GOG , IDE - CaffeNet , IDE - ResNet , and IDE - VGGNet .", "ner": [["LDFV", "Method"], ["GOG", "Method"], ["IDE - CaffeNet", "Method"], ["IDE - ResNet", "Method"], ["IDE - VGGNet", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "3920676", "sentence": "The number of output units in the fc 7 layer is set to 4, 0 9 6 for IDE - VGGNet and IDE - CaffeNet , and 2, 0 4 8 for IDE - ResNet .", "ner": [["fc 7", "Method"], ["IDE - VGGNet", "Method"], ["IDE - CaffeNet", "Method"], ["IDE - ResNet", "Method"]], "rel": [["fc 7", "Part-Of", "IDE - VGGNet"], ["fc 7", "Part-Of", "IDE - CaffeNet"], ["fc 7", "Part-Of", "IDE - ResNet"]], "rel_plus": [["fc 7:Method", "Part-Of", "IDE - VGGNet:Method"], ["fc 7:Method", "Part-Of", "IDE - CaffeNet:Method"], ["fc 7:Method", "Part-Of", "IDE - ResNet:Method"]]}
{"doc_id": "3920676", "sentence": "Once the model is trained , we use the output of the fc 7 layer as the image descriptor , giving a 4 0 9 6 - dimensional feature vector in the case of IDE - VGGNet and IDE - CaffeNet , and a 2 0 4 8 dimensional feature vector in the case of IDE - ResNet .", "ner": [["fc 7", "Method"], ["IDE - VGGNet", "Method"], ["IDE - CaffeNet", "Method"], ["IDE - ResNet", "Method"]], "rel": [["fc 7", "Part-Of", "IDE - VGGNet"], ["fc 7", "Part-Of", "IDE - CaffeNet"], ["fc 7", "Part-Of", "IDE - ResNet"]], "rel_plus": [["fc 7:Method", "Part-Of", "IDE - VGGNet:Method"], ["fc 7:Method", "Part-Of", "IDE - CaffeNet:Method"], ["fc 7:Method", "Part-Of", "IDE - ResNet:Method"]]}
{"doc_id": "3920676", "sentence": "We note that IDE - ResNet [ 4 3 ] and GOG [ 4 8 ] perform the best among the 1 1 evaluated feature extraction algorithms , with them being a part of the best performing algorithm combination in 6 of the 1 0 single - shot and all the 1 1 multi - shot datasets respectively .", "ner": [["IDE - ResNet", "Method"], ["GOG", "Method"], ["feature extraction", "Method"]], "rel": [["GOG", "SubClass-Of", "feature extraction"], ["IDE - ResNet", "SubClass-Of", "feature extraction"]], "rel_plus": [["GOG:Method", "SubClass-Of", "feature extraction:Method"], ["IDE - ResNet:Method", "SubClass-Of", "feature extraction:Method"]]}
{"doc_id": "3920676", "sentence": "CMC curves for the single - shot dataset VIPeR and the multi - shot dataset CAVIAR .", "ner": [["VIPeR", "Dataset"], ["CAVIAR", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "3920676", "sentence": "This is set to 1 for kPCCA and rPCCA on Market 1 5 0 1 due to system memory issues . performing algorithm in these two areas .", "ner": [["kPCCA", "Method"], ["rPCCA", "Method"], ["Market 1 5 0 1", "Dataset"]], "rel": [["rPCCA", "Evaluated-With", "Market 1 5 0 1"], ["kPCCA", "Evaluated-With", "Market 1 5 0 1"]], "rel_plus": [["rPCCA:Method", "Evaluated-With", "Market 1 5 0 1:Dataset"], ["kPCCA:Method", "Evaluated-With", "Market 1 5 0 1:Dataset"]]}
{"doc_id": "3920676", "sentence": "First , we note that IDE - ResNet is the best performing feature extraction algorithm in our evaluation .", "ner": [["IDE - ResNet", "Method"], ["feature extraction", "Method"]], "rel": [["IDE - ResNet", "SubClass-Of", "feature extraction"]], "rel_plus": [["IDE - ResNet:Method", "SubClass-Of", "feature extraction:Method"]]}
{"doc_id": "3920676", "sentence": "To corroborate this observation , we study the impact of IDE - ResNet in comparison with other feature extraction algorithms both in the presence as well as the absence of any metric learning .", "ner": [["IDE - ResNet", "Method"], ["feature extraction", "Method"], ["metric learning", "Method"]], "rel": [["IDE - ResNet", "SubClass-Of", "feature extraction"]], "rel_plus": [["IDE - ResNet:Method", "SubClass-Of", "feature extraction:Method"]]}
{"doc_id": "3920676", "sentence": "Next , we study how IDE - ResNet performs in comparison with other features in the presence of metric learning .", "ner": [["IDE - ResNet", "Method"], ["metric learning", "Method"]], "rel": [["metric learning", "Used-For", "IDE - ResNet"]], "rel_plus": [["metric learning:Method", "Used-For", "IDE - ResNet:Method"]]}
{"doc_id": "3920676", "sentence": "In this experiment , we fix NFST exp as our metric learning algorithm and rank gallery candidates using all the 1 1 evaluated feature extraction algorithms .", "ner": [["NFST", "Method"], ["metric learning", "Method"], ["feature extraction", "Method"]], "rel": [["NFST", "SubClass-Of", "metric learning"]], "rel_plus": [["NFST:Method", "SubClass-Of", "metric learning:Method"]]}
{"doc_id": "3920676", "sentence": "As can be noted from the graph , IDE - ResNet gives the best performance on 4 of the 7 datasets shown in the figure , with GOG giving the best performance on the remaining 3 datasets .", "ner": [["IDE - ResNet", "Method"], ["GOG", "Method"]], "rel": [["GOG", "Part-Of", "IDE - ResNet"]], "rel_plus": [["GOG:Method", "Part-Of", "IDE - ResNet:Method"]]}
{"doc_id": "3920676", "sentence": "These experiments show that IDE - ResNet is indeed the best performing feature extraction algorithm .", "ner": [["IDE - ResNet", "Method"], ["feature extraction", "Method"]], "rel": [["IDE - ResNet", "SubClass-Of", "feature extraction"]], "rel_plus": [["IDE - ResNet:Method", "SubClass-Of", "feature extraction:Method"]]}
{"doc_id": "3920676", "sentence": "This should not be surprising given the powerful modeling and generalization ability of the ResNet architecture , which is also evidenced by its strong performance in other computer vision domains and applications [ 4 3 ] .", "ner": [["ResNet", "Method"], ["computer vision", "Task"]], "rel": [["ResNet", "Used-For", "computer vision"]], "rel_plus": [["ResNet:Method", "Used-For", "computer vision:Task"]]}
{"doc_id": "3920676", "sentence": "Next , we analyze the performance of different metric learning algorithms , 5 in the context of IDE - ResNet , the best performing feature extraction algorithm .", "ner": [["metric learning", "Method"], ["IDE - ResNet", "Method"], ["feature extraction", "Method"]], "rel": [["metric learning", "Used-For", "IDE - ResNet"], ["IDE - ResNet", "SubClass-Of", "feature extraction"]], "rel_plus": [["metric learning:Method", "Used-For", "IDE - ResNet:Method"], ["IDE - ResNet:Method", "SubClass-Of", "feature extraction:Method"]]}
{"doc_id": "3920676", "sentence": "In this experiment , we fix IDE - ResNet as the feature extraction algorithm and study how different metric learning algorithms perform .", "ner": [["IDE - ResNet", "Method"], ["feature extraction", "Method"], ["metric learning", "Method"]], "rel": [["IDE - ResNet", "SubClass-Of", "feature extraction"]], "rel_plus": [["IDE - ResNet:Method", "SubClass-Of", "feature extraction:Method"]]}
{"doc_id": "3920676", "sentence": "The results of this experiment are shown in Fig. 4c , from which we can note that NFST exp gives the best performance on Market 1 5 0 1 , DukeMTMC , 3DPeS , and Airport , with XQDA and kLFDA not being too far behind .", "ner": [["NFST", "Method"], ["Market 1 5 0 1", "Dataset"], ["DukeMTMC", "Dataset"], ["3DPeS", "Dataset"], ["Airport", "Dataset"], ["XQDA", "Method"], ["kLFDA", "Method"]], "rel": [["NFST", "Evaluated-With", "Market 1 5 0 1"], ["XQDA", "Evaluated-With", "Market 1 5 0 1"], ["kLFDA", "Evaluated-With", "Market 1 5 0 1"], ["NFST", "Evaluated-With", "DukeMTMC"], ["XQDA", "Evaluated-With", "DukeMTMC"], ["kLFDA", "Evaluated-With", "DukeMTMC"], ["NFST", "Evaluated-With", "3DPeS"], ["XQDA", "Evaluated-With", "3DPeS"], ["kLFDA", "Evaluated-With", "3DPeS"], ["NFST", "Evaluated-With", "Airport"], ["XQDA", "Evaluated-With", "Airport"], ["kLFDA", "Evaluated-With", "Airport"], ["NFST", "Compare-With", "XQDA"], ["NFST", "Compare-With", "kLFDA"], ["XQDA", "Compare-With", "kLFDA"]], "rel_plus": [["NFST:Method", "Evaluated-With", "Market 1 5 0 1:Dataset"], ["XQDA:Method", "Evaluated-With", "Market 1 5 0 1:Dataset"], ["kLFDA:Method", "Evaluated-With", "Market 1 5 0 1:Dataset"], ["NFST:Method", "Evaluated-With", "DukeMTMC:Dataset"], ["XQDA:Method", "Evaluated-With", "DukeMTMC:Dataset"], ["kLFDA:Method", "Evaluated-With", "DukeMTMC:Dataset"], ["NFST:Method", "Evaluated-With", "3DPeS:Dataset"], ["XQDA:Method", "Evaluated-With", "3DPeS:Dataset"], ["kLFDA:Method", "Evaluated-With", "3DPeS:Dataset"], ["NFST:Method", "Evaluated-With", "Airport:Dataset"], ["XQDA:Method", "Evaluated-With", "Airport:Dataset"], ["kLFDA:Method", "Evaluated-With", "Airport:Dataset"], ["NFST:Method", "Compare-With", "XQDA:Method"], ["NFST:Method", "Compare-With", "kLFDA:Method"], ["XQDA:Method", "Compare-With", "kLFDA:Method"]]}
{"doc_id": "3920676", "sentence": "These results further corroborate what we observe in Table 3 , with NFST , kLFDA , and XQDA being among the best performing metric learning algorithms .", "ner": [["NFST", "Method"], ["kLFDA", "Method"], ["XQDA", "Method"], ["metric learning", "Method"]], "rel": [["metric learning", "SubClass-Of", "NFST"], ["metric learning", "SubClass-Of", "kLFDA"], ["metric learning", "SubClass-Of", "XQDA"]], "rel_plus": [["metric learning:Method", "SubClass-Of", "NFST:Method"], ["metric learning:Method", "SubClass-Of", "kLFDA:Method"], ["metric learning:Method", "SubClass-Of", "XQDA:Method"]]}
{"doc_id": "3920676", "sentence": "From the above discussion , we can infer the following : while NFST exp gives the best overall performance , kLFDA and XQDA also emerge as strong and competitive metric learning algorithms .", "ner": [["NFST", "Method"], ["kLFDA", "Method"], ["XQDA", "Method"], ["metric learning", "Method"]], "rel": [["NFST", "Compare-With", "kLFDA"], ["NFST", "Compare-With", "XQDA"], ["XQDA", "SubClass-Of", "metric learning"], ["kLFDA", "SubClass-Of", "metric learning"], ["NFST", "SubClass-Of", "metric learning"]], "rel_plus": [["NFST:Method", "Compare-With", "kLFDA:Method"], ["NFST:Method", "Compare-With", "XQDA:Method"], ["XQDA:Method", "SubClass-Of", "metric learning:Method"], ["kLFDA:Method", "SubClass-Of", "metric learning:Method"], ["NFST:Method", "SubClass-Of", "metric learning:Method"]]}
{"doc_id": "3920676", "sentence": "While kLFDA and XQDA directly employ Fisher - type objective functions , NFST uses the Foley - Shannon transform [ 9 4 ] , which is very closely related to the Fisher discriminant analysis .", "ner": [["kLFDA", "Method"], ["XQDA", "Method"], ["Fisher - type objective functions", "Method"], ["NFST", "Method"], ["Foley - Shannon transform", "Method"], ["Fisher discriminant analysis", "Method"]], "rel": [["Fisher - type objective functions", "Part-Of", "kLFDA"], ["Fisher - type objective functions", "Part-Of", "XQDA"], ["XQDA", "Compare-With", "NFST"], ["kLFDA", "Compare-With", "NFST"], ["Foley - Shannon transform", "Part-Of", "NFST"], ["Foley - Shannon transform", "Compare-With", "Fisher discriminant analysis"]], "rel_plus": [["Fisher - type objective functions:Method", "Part-Of", "kLFDA:Method"], ["Fisher - type objective functions:Method", "Part-Of", "XQDA:Method"], ["XQDA:Method", "Compare-With", "NFST:Method"], ["kLFDA:Method", "Compare-With", "NFST:Method"], ["Foley - Shannon transform:Method", "Part-Of", "NFST:Method"], ["Foley - Shannon transform:Method", "Compare-With", "Fisher discriminant analysis:Method"]]}
{"doc_id": "3920676", "sentence": "Here , we compare the performance of GOG - KISSME - AVER and GOG - KISSME - SRID .", "ner": [["GOG - KISSME - AVER", "Method"], ["GOG - KISSME - SRID", "Method"]], "rel": [["GOG - KISSME - AVER", "Compare-With", "GOG - KISSME - SRID"]], "rel_plus": [["GOG - KISSME - AVER:Method", "Compare-With", "GOG - KISSME - SRID:Method"]]}
{"doc_id": "3920676", "sentence": "Next , we analyze the performance of the feature extraction and metric learning algorithms and compare the observed trends with those in the single - shot case .", "ner": [["feature extraction", "Method"], ["metric learning", "Method"]], "rel": [["feature extraction", "Compare-With", "metric learning"]], "rel_plus": [["feature extraction:Method", "Compare-With", "metric learning:Method"]]}
{"doc_id": "3920676", "sentence": "In the metric learning case , we fix SRID as the ranking algorithm and GOG as the feature extraction algorithm , with Fig. 5d showing the rank - 1 results .", "ner": [["metric learning", "Method"], ["SRID", "Method"], ["ranking algorithm", "Method"], ["GOG", "Method"], ["feature extraction", "Method"]], "rel": [["SRID", "SubClass-Of", "ranking algorithm"], ["GOG", "SubClass-Of", "feature extraction"]], "rel_plus": [["SRID:Method", "SubClass-Of", "ranking algorithm:Method"], ["GOG:Method", "SubClass-Of", "feature extraction:Method"]]}
{"doc_id": "3920676", "sentence": "To this end , we analyze the impact of an unsupervised dimensionality reduction scheme , principal component analysis ( PCA ) .", "ner": [["unsupervised dimensionality reduction", "Method"], ["principal component analysis", "Method"], ["PCA", "Method"]], "rel": [["principal component analysis", "SubClass-Of", "unsupervised dimensionality reduction"], ["PCA", "Synonym-Of", "principal component analysis"]], "rel_plus": [["principal component analysis:Method", "SubClass-Of", "unsupervised dimensionality reduction:Method"], ["PCA:Method", "Synonym-Of", "principal component analysis:Method"]]}
{"doc_id": "3920676", "sentence": "We fix GOG features with the highest dimensionality in our evaluation framework as the feature extraction scheme and perform experiments with and without PCA .", "ner": [["GOG", "Method"], ["feature extraction", "Method"], ["PCA", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "3920676", "sentence": "The results without PCA are better than those with PCA on all the datasets shown in the graphs .", "ner": [["PCA", "Method"], ["PCA", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "3920676", "sentence": "To this end , we perform experiments with 6 , 9 , and 1 5 horizontal strips in the best handcrafted feature extraction algorithm , GOG , with Euclidean distance as the metric in the single - shot case and Euclidean distance as the metric and AVER as the ranking strategy in the multi - shot case .", "ner": [["handcrafted feature extraction algorithm", "Method"], ["GOG", "Method"], ["Euclidean distance", "Method"], ["Euclidean distance", "Method"]], "rel": [["GOG", "SubClass-Of", "handcrafted feature extraction algorithm"], ["Euclidean distance", "Part-Of", "GOG"]], "rel_plus": [["GOG:Method", "SubClass-Of", "handcrafted feature extraction algorithm:Method"], ["Euclidean distance:Method", "Part-Of", "GOG:Method"]]}
{"doc_id": "3920676", "sentence": "In Fig. 6b , we show results of this experiment for values of PCA dimension ranging from 5 0 to 5 0 0 for a small - scale dataset , VIPeR , and a large - scale dataset , Market 1 5 0 1 .", "ner": [["PCA", "Method"], ["VIPeR", "Dataset"], ["Market 1 5 0 1", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "3920676", "sentence": "As can be noted from the results , as we increase the PCA dimension , the training time increases ( quite substantially for the large - scale dataset ) , while the accuracy saturates beyond a certain value of the PCA dimension .", "ner": [["PCA", "Method"], ["PCA", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "3920676", "sentence": "This empirically substantiates the sufficiency of a relatively small value for performing dimensionality reduction using algorithms such as PCA .", "ner": [["dimensionality reduction", "Method"], ["PCA", "Method"]], "rel": [["PCA", "SubClass-Of", "dimensionality reduction"]], "rel_plus": [["PCA:Method", "SubClass-Of", "dimensionality reduction:Method"]]}
{"doc_id": "3920676", "sentence": "In all the scenarios , IDE - ResNet resulted in the best performance , with IDE - VGGNet and IDE - CaffeNet following closely behind .", "ner": [["IDE - ResNet", "Method"], ["IDE - VGGNet", "Method"], ["IDE - CaffeNet", "Method"]], "rel": [["IDE - ResNet", "Compare-With", "IDE - VGGNet"], ["IDE - ResNet", "Compare-With", "IDE - CaffeNet"], ["IDE - VGGNet", "Compare-With", "IDE - CaffeNet"]], "rel_plus": [["IDE - ResNet:Method", "Compare-With", "IDE - VGGNet:Method"], ["IDE - ResNet:Method", "Compare-With", "IDE - CaffeNet:Method"], ["IDE - VGGNet:Method", "Compare-With", "IDE - CaffeNet:Method"]]}
{"doc_id": "3920676", "sentence": "While we only discuss the best performing hand - crafted algorithms - GOG , LDFV , and LOMO - these insights can be quite useful across the spectrum as researchers think about designing feature learning architectures .", "ner": [["hand - crafted algorithms", "Method"], ["GOG", "Method"], ["LDFV", "Method"], ["LOMO", "Method"]], "rel": [["GOG", "SubClass-Of", "hand - crafted algorithms"], ["LDFV", "SubClass-Of", "hand - crafted algorithms"], ["LOMO", "SubClass-Of", "hand - crafted algorithms"]], "rel_plus": [["GOG:Method", "SubClass-Of", "hand - crafted algorithms:Method"], ["LDFV:Method", "SubClass-Of", "hand - crafted algorithms:Method"], ["LOMO:Method", "SubClass-Of", "hand - crafted algorithms:Method"]]}
{"doc_id": "3920676", "sentence": "Additionally , since viewpoint invariance is an extremely important attribute for any re - id descriptor , these results suggest that incorporating local region information and horizontal strip pooling , as is done explicitly in both GOG and LOMO , is critical to achieve viewpoint invariant descriptors .", "ner": [["re - id", "Task"], ["horizontal strip pooling", "Method"], ["GOG", "Method"], ["LOMO", "Method"]], "rel": [["horizontal strip pooling", "Part-Of", "GOG"], ["horizontal strip pooling", "Part-Of", "LOMO"]], "rel_plus": [["horizontal strip pooling:Method", "Part-Of", "GOG:Method"], ["horizontal strip pooling:Method", "Part-Of", "LOMO:Method"]]}
{"doc_id": "3920676", "sentence": "While MARS [ 4 1 ] , a recently proposed dataset , has a large number of images , constructing datasets that are of the size of ImageNet , in terms of the number of people , positive examples , and under an eclectic mix of conditions as noted above , will assist in the application of some of the recent algorithmic advances in feature learning using CNNs [ 4 3 ] , [ 9 5 ] .", "ner": [["MARS", "Dataset"], ["ImageNet", "Dataset"], ["feature learning", "Method"], ["CNNs", "Method"]], "rel": [["CNNs", "Used-For", "feature learning"]], "rel_plus": [["CNNs:Method", "Used-For", "feature learning:Method"]]}
{"doc_id": "3920676", "sentence": "We noted that IDE - ResNet resulted in the best performance among all the evaluated feature extraction methods , with IDE - VGGNet and IDE - CaffeNet close behind .", "ner": [["IDE - ResNet", "Method"], ["feature extraction", "Method"], ["IDE - VGGNet", "Method"], ["IDE - CaffeNet", "Method"]], "rel": [["IDE - VGGNet", "SubClass-Of", "feature extraction"], ["IDE - CaffeNet", "SubClass-Of", "feature extraction"], ["IDE - ResNet", "SubClass-Of", "feature extraction"]], "rel_plus": [["IDE - VGGNet:Method", "SubClass-Of", "feature extraction:Method"], ["IDE - CaffeNet:Method", "SubClass-Of", "feature extraction:Method"], ["IDE - ResNet:Method", "SubClass-Of", "feature extraction:Method"]]}
{"doc_id": "3920676", "sentence": "The region proposal network of Faster R - CNN [ 9 9 ] is a potential candidate to generate local patch proposals and learn representations , which can be trained in an end - to - end fashion .", "ner": [["region proposal network", "Method"], ["Faster R - CNN", "Method"]], "rel": [["region proposal network", "Part-Of", "Faster R - CNN"]], "rel_plus": [["region proposal network:Method", "Part-Of", "Faster R - CNN:Method"]]}
{"doc_id": "3920676", "sentence": "Consequently , recent advances in learning translation - invariant local features such as bilinear CNNs [ 1 0 2 ] or gated CNNs with specially designed convolution operations [ 1 0 3 ] would be particularly relevant .", "ner": [["bilinear CNNs", "Method"], ["gated CNNs", "Method"], ["convolution operations", "Method"]], "rel": [["convolution operations", "Part-Of", "bilinear CNNs"], ["convolution operations", "Part-Of", "gated CNNs"]], "rel_plus": [["convolution operations:Method", "Part-Of", "bilinear CNNs:Method"], ["convolution operations:Method", "Part-Of", "gated CNNs:Method"]]}
{"doc_id": "3920676", "sentence": "While existing Siamese network frameworks learn to tell pairs of images apart , we can extend them , in conjunction with C 3 D - like networks , to tell pairs of video sequences apart .", "ner": [["Siamese network", "Method"], ["C 3 D", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "3920676", "sentence": "While common strategies such as random 2 - D translations can be readily applied [ 7 3 ] , we can use sophisticated methods such as CycleGAN [ 1 0 8 ] and LSRO [ 1 0 9 ] to generate realisitic and meaningful images .", "ner": [["CycleGAN", "Method"], ["LSRO", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210920315", "sentence": "This paper introduces a novel approach , termed as PSC - Net , for occluded pedestrian detection .", "ner": [["PSC - Net", "Method"], ["pedestrian detection", "Task"]], "rel": [["PSC - Net", "Used-For", "pedestrian detection"]], "rel_plus": [["PSC - Net:Method", "Used-For", "pedestrian detection:Task"]]}
{"doc_id": "210920315", "sentence": "The proposed PSC - Net contains a dedicated module that is designed to explicitly capture both inter and intra - part co - occurrence information of different pedestrian body parts through a Graph Convolutional Network ( GCN ) .", "ner": [["PSC - Net", "Method"], ["Graph Convolutional Network", "Method"], ["GCN", "Method"]], "rel": [["Graph Convolutional Network", "Part-Of", "PSC - Net"], ["GCN", "Synonym-Of", "Graph Convolutional Network"]], "rel_plus": [["Graph Convolutional Network:Method", "Part-Of", "PSC - Net:Method"], ["GCN:Method", "Synonym-Of", "Graph Convolutional Network:Method"]]}
{"doc_id": "210920315", "sentence": "Comprehensive experiments are performed on two challenging datasets : CityPersons and Caltech datasets .", "ner": [["CityPersons", "Dataset"], ["Caltech datasets", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "210920315", "sentence": "The proposed PSC - Net achieves state - of - the - art detection performance on both .", "ner": [["PSC - Net", "Method"], ["detection", "Task"]], "rel": [["PSC - Net", "Used-For", "detection"]], "rel_plus": [["PSC - Net:Method", "Used-For", "detection:Task"]]}
{"doc_id": "210920315", "sentence": "On the heavy occluded ( \\textbf{HO } ) set of CityPerosns test set , our PSC - Net obtains an absolute gain of 4. 0 \\% in terms of log - average miss rate over the state - of - the - art with same backbone , input scale and without using additional VBB supervision .", "ner": [["CityPerosns", "Dataset"], ["PSC - Net", "Method"]], "rel": [["PSC - Net", "Evaluated-With", "CityPerosns"]], "rel_plus": [["PSC - Net:Method", "Evaluated-With", "CityPerosns:Dataset"]]}
{"doc_id": "210920315", "sentence": "Further , PSC - Net improves the state - of - the - art from 3 7 . 9 to 3 4 . 8 in terms of log - average miss rate on Caltech ( \\textbf{HO } ) test set .", "ner": [["PSC - Net", "Method"], ["Caltech", "Dataset"]], "rel": [["PSC - Net", "Evaluated-With", "Caltech"]], "rel_plus": [["PSC - Net:Method", "Evaluated-With", "Caltech:Dataset"]]}
{"doc_id": "210920315", "sentence": "P EDESTRIAN detection is a challenging problem in computer vision with various real - application applications , e.g. , robotics , autonomous driving and visual surveillance .", "ner": [["P EDESTRIAN detection", "Task"], ["computer vision", "Task"], ["robotics", "Task"], ["autonomous driving", "Task"], ["visual surveillance", "Task"]], "rel": [["P EDESTRIAN detection", "SubTask-Of", "computer vision"], ["robotics", "SubTask-Of", "computer vision"], ["autonomous driving", "SubTask-Of", "computer vision"], ["visual surveillance", "SubTask-Of", "computer vision"]], "rel_plus": [["P EDESTRIAN detection:Task", "SubTask-Of", "computer vision:Task"], ["robotics:Task", "SubTask-Of", "computer vision:Task"], ["autonomous driving:Task", "SubTask-Of", "computer vision:Task"], ["visual surveillance:Task", "SubTask-Of", "computer vision:Task"]]}
{"doc_id": "210920315", "sentence": "Recent years have witnessed significant progress in the field of pedestrian detection , mainly due to the advances in deep convolutional neural networks ( CNNs ) .", "ner": [["pedestrian detection", "Task"], ["convolutional neural networks", "Method"], ["CNNs", "Method"]], "rel": [["convolutional neural networks", "Used-For", "pedestrian detection"], ["CNNs", "Synonym-Of", "convolutional neural networks"]], "rel_plus": [["convolutional neural networks:Method", "Used-For", "pedestrian detection:Task"], ["CNNs:Method", "Synonym-Of", "convolutional neural networks:Method"]]}
{"doc_id": "210920315", "sentence": "Most existing two - stage pedestrian detectors [ 5 2 ] , [ 3 3 ] , [ 1 9 ] , [ 5 1 ] , [ 4 5 ] , [ 2 3 ] are based on the popular Faster R - CNN detection framework [ 3 6 ] that is adapted from generic object detection .", "ner": [["pedestrian detectors", "Method"], ["Faster R - CNN detection", "Method"], ["object detection", "Task"]], "rel": [["Faster R - CNN detection", "Used-For", "object detection"]], "rel_plus": [["Faster R - CNN detection:Method", "Used-For", "object detection:Task"]]}
{"doc_id": "210920315", "sentence": "Contributions : We propose a two - stage approach , termed as PSC - Net , to address the problem of occluded pedestrian detection .", "ner": [["PSC - Net", "Method"], ["occluded pedestrian detection", "Task"]], "rel": [["PSC - Net", "Used-For", "occluded pedestrian detection"]], "rel_plus": [["PSC - Net:Method", "Used-For", "occluded pedestrian detection:Task"]]}
{"doc_id": "210920315", "sentence": "Our main contribution is the introduction of a novel part spatial co - occurrence ( PSC ) module designed to explicitly capture both inter and intra - part co - occurrence information of different pedestrian body - parts through a Graph Convolutional ( a ) MGAN [ 3 3 ] ( b ) Baseline ( c ) PSC - Net Network ( GCN ) [ 1 4 ] .", "ner": [["part spatial co - occurrence", "Method"], ["PSC", "Method"], ["Graph Convolutional", "Method"], ["MGAN", "Method"], ["PSC - Net", "Method"], ["GCN", "Method"]], "rel": [["PSC", "Synonym-Of", "part spatial co - occurrence"], ["Graph Convolutional", "Part-Of", "part spatial co - occurrence"], ["GCN", "Part-Of", "PSC - Net"]], "rel_plus": [["PSC:Method", "Synonym-Of", "part spatial co - occurrence:Method"], ["Graph Convolutional:Method", "Part-Of", "part spatial co - occurrence:Method"], ["GCN:Method", "Part-Of", "PSC - Net:Method"]]}
{"doc_id": "210920315", "sentence": "To the best of our knowledge , we are the first to propose an approach based on GCN to capture both inter and intra - part spatial co - occurrence for occluded pedestrian detection .", "ner": [["GCN", "Method"], ["occluded pedestrian detection", "Task"]], "rel": [["GCN", "Used-For", "occluded pedestrian detection"]], "rel_plus": [["GCN:Method", "Used-For", "occluded pedestrian detection:Task"]]}
{"doc_id": "210920315", "sentence": "We validate our PSC - Net by performing comprehensive experiments on two standard pedestrian detection datasets : CityPersons [ 5 0 ] and Caltech [ 1 0 ] .", "ner": [["PSC - Net", "Method"], ["pedestrian detection", "Task"], ["CityPersons", "Dataset"], ["Caltech", "Dataset"]], "rel": [["PSC - Net", "Used-For", "pedestrian detection"], ["CityPersons", "Benchmark-For", "pedestrian detection"], ["Caltech", "Benchmark-For", "pedestrian detection"], ["PSC - Net", "Evaluated-With", "CityPersons"], ["PSC - Net", "Evaluated-With", "Caltech"]], "rel_plus": [["PSC - Net:Method", "Used-For", "pedestrian detection:Task"], ["CityPersons:Dataset", "Benchmark-For", "pedestrian detection:Task"], ["Caltech:Dataset", "Benchmark-For", "pedestrian detection:Task"], ["PSC - Net:Method", "Evaluated-With", "CityPersons:Dataset"], ["PSC - Net:Method", "Evaluated-With", "Caltech:Dataset"]]}
{"doc_id": "210920315", "sentence": "On the heavy occluded ( HO ) set of CityPersons , our PSC - Net achieves an absolute gain of 5. 5 % in terms of log - average miss rate , compared to the baseline .", "ner": [["CityPersons", "Dataset"], ["PSC - Net", "Method"]], "rel": [["PSC - Net", "Evaluated-With", "CityPersons"]], "rel_plus": [["PSC - Net:Method", "Evaluated-With", "CityPersons:Dataset"]]}
{"doc_id": "210920315", "sentence": "On the CityPerson ( HO ) test set , PSC - Net achieves a log - average miss rate of 3 7 . 4 with a significant gain of 3. 6 % over the best reported results in literature [ 3 3 ] .", "ner": [["CityPerson", "Dataset"], ["PSC - Net", "Method"]], "rel": [["PSC - Net", "Evaluated-With", "CityPerson"]], "rel_plus": [["PSC - Net:Method", "Evaluated-With", "CityPerson:Dataset"]]}
{"doc_id": "210920315", "sentence": "Our PSC - Net is able to accurately detect pedestrians , compared to both the baseline and the state - ofthe - art MGAN [ 3 3 ] .", "ner": [["PSC - Net", "Method"], ["MGAN", "Method"]], "rel": [["PSC - Net", "Compare-With", "MGAN"]], "rel_plus": [["PSC - Net:Method", "Compare-With", "MGAN:Method"]]}
{"doc_id": "210920315", "sentence": "Convolutional neural networks ( CNNs ) have significantly advanced the state - of - the - art in numerous computer vision applications , such as image classification [ 1 2 ] , [ 4 0 ] , [ 3 8 ] , [ 4 2 ] , object detection [ 3 6 ] , [ 3 2 ] , [ 4 4 ] , [ 2 8 ] , [ 2 0 ] , object counting [ 4 7 ] , [ 9 ] , [ 8 ] , [ 1 6 ] , image retrieval [ 3 4 ] , [ 3 5 ] , [ 2 2 ] , [ 4 8 ] , action recognition [ 3 9 ] , [ 3 7 ] , [ 1 8 ] , [ 2 7 ] , and pedestrian detection [ 5 1 ] , [ 3 3 ] , [ 5 2 ] , [ 4 5 ] .", "ner": [["Convolutional neural networks", "Method"], ["CNNs", "Method"], ["computer vision", "Task"], ["image classification", "Task"], ["object detection", "Task"], ["object counting", "Task"], ["image retrieval", "Task"], ["action recognition", "Task"], ["pedestrian detection", "Task"]], "rel": [["CNNs", "Synonym-Of", "Convolutional neural networks"], ["Convolutional neural networks", "Used-For", "computer vision"], ["image classification", "SubTask-Of", "computer vision"], ["object detection", "SubTask-Of", "computer vision"], ["object counting", "SubTask-Of", "computer vision"], ["image retrieval", "SubTask-Of", "computer vision"], ["action recognition", "SubTask-Of", "computer vision"], ["pedestrian detection", "SubTask-Of", "computer vision"], ["Convolutional neural networks", "Used-For", "image classification"], ["Convolutional neural networks", "Used-For", "object detection"], ["Convolutional neural networks", "Used-For", "object counting"], ["Convolutional neural networks", "Used-For", "image retrieval"], ["Convolutional neural networks", "Used-For", "action recognition"], ["Convolutional neural networks", "Used-For", "pedestrian detection"]], "rel_plus": [["CNNs:Method", "Synonym-Of", "Convolutional neural networks:Method"], ["Convolutional neural networks:Method", "Used-For", "computer vision:Task"], ["image classification:Task", "SubTask-Of", "computer vision:Task"], ["object detection:Task", "SubTask-Of", "computer vision:Task"], ["object counting:Task", "SubTask-Of", "computer vision:Task"], ["image retrieval:Task", "SubTask-Of", "computer vision:Task"], ["action recognition:Task", "SubTask-Of", "computer vision:Task"], ["pedestrian detection:Task", "SubTask-Of", "computer vision:Task"], ["Convolutional neural networks:Method", "Used-For", "image classification:Task"], ["Convolutional neural networks:Method", "Used-For", "object detection:Task"], ["Convolutional neural networks:Method", "Used-For", "object counting:Task"], ["Convolutional neural networks:Method", "Used-For", "image retrieval:Task"], ["Convolutional neural networks:Method", "Used-For", "action recognition:Task"], ["Convolutional neural networks:Method", "Used-For", "pedestrian detection:Task"]]}
{"doc_id": "210920315", "sentence": "Zhang et al. , [ 5 0 ] propose key adaptations in the popular Faster R - CNN [ 3 6 ] for pedestrian detection .", "ner": [["Faster R - CNN", "Method"], ["pedestrian detection", "Task"]], "rel": [["Faster R - CNN", "Used-For", "pedestrian detection"]], "rel_plus": [["Faster R - CNN:Method", "Used-For", "pedestrian detection:Task"]]}
{"doc_id": "210920315", "sentence": "Towards Occluded Pedestrian Detection : The problem of occluded pedestrian detection is well studied in literature [ 4 3 ] , [ 5 5 ] , [ 5 6 ] , [ 5 1 ] , [ 4 5 ] , [ 5 3 ] , [ 5 2 ] , [ 3 3 ] .", "ner": [["Occluded Pedestrian Detection", "Task"], ["occluded pedestrian detection", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "210920315", "sentence": "Our Approach : Contrary to above mentioned recent approaches that rely on additional visible bounding - box ( VBB ) annotations , our proposed PSC - Net only requires the standard full body supervision to handle occluded pedestrian detection .", "ner": [["PSC - Net", "Method"], ["occluded pedestrian detection", "Task"]], "rel": [["PSC - Net", "Used-For", "occluded pedestrian detection"]], "rel_plus": [["PSC - Net:Method", "Used-For", "occluded pedestrian detection:Task"]]}
{"doc_id": "210920315", "sentence": "The focus of our design is the introduction of a part spatial co - occurrence ( PSC ) module that explicitly captures both inter and intra - part co - occurrence information of different body parts through a Graph Convolutional Network ( GCN ) [ 1 4 ] .", "ner": [["part spatial co - occurrence", "Method"], ["PSC", "Method"], ["Graph Convolutional Network", "Method"], ["GCN", "Method"]], "rel": [["PSC", "Synonym-Of", "part spatial co - occurrence"], ["Graph Convolutional Network", "Part-Of", "part spatial co - occurrence"], ["GCN", "Synonym-Of", "Graph Convolutional Network"]], "rel_plus": [["PSC:Method", "Synonym-Of", "part spatial co - occurrence:Method"], ["Graph Convolutional Network:Method", "Part-Of", "part spatial co - occurrence:Method"], ["GCN:Method", "Synonym-Of", "Graph Convolutional Network:Method"]]}
{"doc_id": "210920315", "sentence": "To the best of our knowledge , the proposed approach is the first to capture both inter and intra - part co - occurrence information through a GCN to address the problem of occluded pedestrian detection .", "ner": [["GCN", "Method"], ["occluded pedestrian detection", "Task"]], "rel": [["GCN", "Used-For", "occluded pedestrian detection"]], "rel_plus": [["GCN:Method", "Used-For", "occluded pedestrian detection:Task"]]}
{"doc_id": "210920315", "sentence": "It consists of a standard pedestrian detection ( PD ) branch ( Sec. III - A ) and a part spatial co - occurrence ( PSC ) module ( Sec. III - B ) .", "ner": [["pedestrian detection", "Task"], ["PD", "Task"], ["part spatial co - occurrence", "Method"], ["PSC", "Method"]], "rel": [["PD", "Synonym-Of", "pedestrian detection"], ["PSC", "Synonym-Of", "part spatial co - occurrence"]], "rel_plus": [["PD:Task", "Synonym-Of", "pedestrian detection:Task"], ["PSC:Method", "Synonym-Of", "part spatial co - occurrence:Method"]]}
{"doc_id": "210920315", "sentence": "The standard pedestrian detection ( PD ) branch is based on Faster R - CNN [ 3 6 ] typically employed in existing pedestrian detection works [ 5 0 ] , [ 3 3 ] .", "ner": [["pedestrian detection", "Task"], ["PD", "Task"], ["Faster R - CNN", "Method"], ["pedestrian detection", "Task"]], "rel": [["PD", "Synonym-Of", "pedestrian detection"], ["Faster R - CNN", "Used-For", "pedestrian detection"]], "rel_plus": [["PD:Task", "Synonym-Of", "pedestrian detection:Task"], ["Faster R - CNN:Method", "Used-For", "pedestrian detection:Task"]]}
{"doc_id": "210920315", "sentence": "The part spatial co - occurrence ( PSC ) module encodes both inter and intra - part co - occurrence information of different body parts .", "ner": [["part spatial co - occurrence", "Method"], ["PSC", "Method"]], "rel": [["PSC", "Synonym-Of", "part spatial co - occurrence"]], "rel_plus": [["PSC:Method", "Synonym-Of", "part spatial co - occurrence:Method"]]}
{"doc_id": "210920315", "sentence": "This final enhanced feature representation of a candidate proposal is then deployed as an input to the later part of the pedestrian detection ( PD ) branch which performs final bounding - box regression and classification .", "ner": [["pedestrian detection", "Task"], ["PD", "Task"], ["bounding - box regression", "Task"], ["classification", "Task"]], "rel": [["PD", "Synonym-Of", "pedestrian detection"]], "rel_plus": [["PD:Task", "Synonym-Of", "pedestrian detection:Task"]]}
{"doc_id": "210920315", "sentence": "Next , we describe the standard pedestrian detection ( PD ) branch , followed by a detailed presentation of our part cooccurrence ( PSC ) module ( Sec. III - B ) .", "ner": [["pedestrian detection", "Task"], ["PD", "Task"], ["PSC", "Method"]], "rel": [["PD", "Synonym-Of", "pedestrian detection"]], "rel_plus": [["PD:Task", "Synonym-Of", "pedestrian detection:Task"]]}
{"doc_id": "210920315", "sentence": "Here , we describe the standard pedestrian detection ( PD ) branch that is based on the popular Faster R - CNN framework [ 3 6 ] and typically employed in several pedestrian detection methods [ 5 0 ] , [ 3 3 ] .", "ner": [["pedestrian detection", "Task"], ["PD", "Task"], ["Faster R - CNN", "Method"], ["pedestrian detection", "Task"]], "rel": [["PD", "Synonym-Of", "pedestrian detection"], ["Faster R - CNN", "Used-For", "pedestrian detection"], ["Faster R - CNN", "Used-For", "pedestrian detection"]], "rel_plus": [["PD:Task", "Synonym-Of", "pedestrian detection:Task"], ["Faster R - CNN:Method", "Used-For", "pedestrian detection:Task"], ["Faster R - CNN:Method", "Used-For", "pedestrian detection:Task"]]}
{"doc_id": "210920315", "sentence": "The PD branch consists of a backbone network , a region proposal network ( RPN ) , region - of - interest ( RoI ) pooling layer and a classification network for final bounding - box regression and classification .", "ner": [["PD branch", "Method"], ["region proposal network", "Method"], ["RPN", "Method"], ["region - of - interest ( RoI ) pooling layer", "Method"], ["classification network", "Method"], ["bounding - box regression", "Task"], ["classification", "Task"]], "rel": [["region proposal network", "Part-Of", "PD branch"], ["region - of - interest ( RoI ) pooling layer", "Part-Of", "PD branch"], ["classification network", "Part-Of", "PD branch"], ["RPN", "Synonym-Of", "region proposal network"], ["classification network", "Used-For", "bounding - box regression"], ["classification network", "Used-For", "classification"]], "rel_plus": [["region proposal network:Method", "Part-Of", "PD branch:Method"], ["region - of - interest ( RoI ) pooling layer:Method", "Part-Of", "PD branch:Method"], ["classification network:Method", "Part-Of", "PD branch:Method"], ["RPN:Method", "Synonym-Of", "region proposal network:Method"], ["classification network:Method", "Used-For", "bounding - box regression:Task"], ["classification network:Method", "Used-For", "classification:Task"]]}
{"doc_id": "210920315", "sentence": "In PD branch , the image is first feed into the backbone network and the RPN generates a set of candidate proposals for the input image .", "ner": [["PD branch", "Method"], ["RPN", "Method"]], "rel": [["RPN", "Part-Of", "PD branch"]], "rel_plus": [["RPN:Method", "Part-Of", "PD branch:Method"]]}
{"doc_id": "210920315", "sentence": "Finally , this fixed - sized feature representation is passed through a classification network that outputs the classification score and the regressed bounding box locations for the corresponding proposal .", "ner": [["classification network", "Method"], ["classification", "Task"]], "rel": [["classification network", "Used-For", "classification"]], "rel_plus": [["classification network:Method", "Used-For", "classification:Task"]]}
{"doc_id": "210920315", "sentence": "The loss function L f of the standard pedestrian detection ( PD ) branch is given as follows : where L rpn cls and L rpn reg are the classification loss and bounding box regression loss of RPN , respectively , and L rcnn cls and L rcnn reg are the classification and bounding box regression loss of the classification network .", "ner": [["pedestrian detection", "Task"], ["PD", "Task"], ["L rpn cls", "Method"], ["L rpn reg", "Method"], ["classification loss", "Method"], ["bounding box regression loss", "Method"], ["RPN", "Method"], ["L rcnn cls", "Method"], ["L rcnn reg", "Method"], ["bounding box regression loss", "Method"], ["classification network", "Method"]], "rel": [["PD", "Synonym-Of", "pedestrian detection"], ["L rpn cls", "SubClass-Of", "classification loss"], ["L rpn reg", "Part-Of", "bounding box regression loss"], ["L rpn cls", "Part-Of", "RPN"], ["L rpn reg", "Part-Of", "RPN"], ["L rcnn cls", "Part-Of", "classification network"], ["L rcnn reg", "Part-Of", "classification network"]], "rel_plus": [["PD:Task", "Synonym-Of", "pedestrian detection:Task"], ["L rpn cls:Method", "SubClass-Of", "classification loss:Method"], ["L rpn reg:Method", "Part-Of", "bounding box regression loss:Method"], ["L rpn cls:Method", "Part-Of", "RPN:Method"], ["L rpn reg:Method", "Part-Of", "RPN:Method"], ["L rcnn cls:Method", "Part-Of", "classification network:Method"], ["L rcnn reg:Method", "Part-Of", "classification network:Method"]]}
{"doc_id": "210920315", "sentence": "In PD branch , Cross - Entropy loss is used as classification loss , and Smooth - L 1 loss as bounding - box regression loss .", "ner": [["PD branch", "Method"], ["Cross - Entropy loss", "Method"], ["classification loss", "Method"], ["Smooth - L 1 loss", "Method"], ["bounding - box regression loss", "Method"]], "rel": [["Cross - Entropy loss", "Part-Of", "PD branch"], ["Smooth - L 1 loss", "Part-Of", "PD branch"], ["Cross - Entropy loss", "SubClass-Of", "classification loss"], ["Smooth - L 1 loss", "SubClass-Of", "bounding - box regression loss"]], "rel_plus": [["Cross - Entropy loss:Method", "Part-Of", "PD branch:Method"], ["Smooth - L 1 loss:Method", "Part-Of", "PD branch:Method"], ["Cross - Entropy loss:Method", "SubClass-Of", "classification loss:Method"], ["Smooth - L 1 loss:Method", "SubClass-Of", "bounding - box regression loss:Method"]]}
{"doc_id": "210920315", "sentence": "Limitations : To handle heavy occlusions , several recent twostage pedestrian detection approaches [ 5 1 ] , [ 5 2 ] , [ 3 3 ] extend the PD branch by exploiting additional visible bounding - box ( VBB ) annotations along with the standard full body information .", "ner": [["pedestrian detection approaches", "Method"], ["PD branch", "Method"]], "rel": [["PD branch", "SubClass-Of", "pedestrian detection approaches"]], "rel_plus": [["PD branch:Method", "SubClass-Of", "pedestrian detection approaches:Method"]]}
{"doc_id": "210920315", "sentence": "Further , these approaches obtain a fixedsized proposal representation by performing a pooling operation ( e.g. , RoI Pool or RoiAlign ) on the high - level features from the later layer of the backbone network ( e.g. , conv 5 of VGG ) .", "ner": [["pooling operation", "Method"], ["RoI Pool", "Method"], ["RoiAlign", "Method"], ["conv 5", "Method"], ["VGG", "Method"]], "rel": [["RoI Pool", "SubClass-Of", "pooling operation"], ["RoiAlign", "SubClass-Of", "pooling operation"]], "rel_plus": [["RoI Pool:Method", "SubClass-Of", "pooling operation:Method"], ["RoiAlign:Method", "SubClass-Of", "pooling operation:Method"]]}
{"doc_id": "210920315", "sentence": "Instead of performing an RoI pooling operation only on the high - level features ( e. g. , conv 5 of VGG ) , as in [ 5 1 ] , [ 5 2 ] , [ 3 3 ] , the PD branch in our PSC - Net utilizes a multi - level RoI pooling ( mRoI ) that concatenates features ( shallow and deep ) from all conv layers of the backbone .", "ner": [["RoI pooling operation", "Method"], ["conv 5", "Method"], ["VGG", "Method"], ["PD branch", "Method"], ["PSC - Net", "Method"], ["multi - level RoI pooling", "Method"], ["mRoI", "Method"], ["conv layers", "Method"]], "rel": [["PD branch", "Part-Of", "PSC - Net"], ["conv layers", "Part-Of", "PSC - Net"], ["multi - level RoI pooling", "Part-Of", "PSC - Net"], ["mRoI", "Synonym-Of", "multi - level RoI pooling"]], "rel_plus": [["PD branch:Method", "Part-Of", "PSC - Net:Method"], ["conv layers:Method", "Part-Of", "PSC - Net:Method"], ["multi - level RoI pooling:Method", "Part-Of", "PSC - Net:Method"], ["mRoI:Method", "Synonym-Of", "multi - level RoI pooling:Method"]]}
{"doc_id": "210920315", "sentence": "Our main contribution is the introduction of a part spatial co - occurrence ( PSC ) module that only requires standard full body supervision and explicitly captures both inter and intra - part spatial co - occurrence information of different body parts .", "ner": [["part spatial co - occurrence", "Method"], ["PSC", "Method"]], "rel": [["PSC", "Synonym-Of", "part spatial co - occurrence"]], "rel_plus": [["PSC:Method", "Synonym-Of", "part spatial co - occurrence:Method"]]}
{"doc_id": "210920315", "sentence": "Here , we introduce a part spatial co - occurrence ( PSC ) module that utilizes spatial co - occurrence of different body parts captured through a Graph Convolutional Network ( GCN ) [ 1 4 ] .", "ner": [["part spatial co - occurrence", "Method"], ["PSC", "Method"], ["Graph Convolutional Network", "Method"], ["GCN", "Method"]], "rel": [["PSC", "Synonym-Of", "part spatial co - occurrence"], ["Graph Convolutional Network", "Part-Of", "part spatial co - occurrence"], ["GCN", "Synonym-Of", "Graph Convolutional Network"]], "rel_plus": [["PSC:Method", "Synonym-Of", "part spatial co - occurrence:Method"], ["Graph Convolutional Network:Method", "Part-Of", "part spatial co - occurrence:Method"], ["GCN:Method", "Synonym-Of", "Graph Convolutional Network:Method"]]}
{"doc_id": "210920315", "sentence": "In PSC module , the GCN is employed to capture intra - part and inter - part spatial co - occurrence by exploiting the topological structure of pedestrian .", "ner": [["PSC", "Method"], ["GCN", "Method"]], "rel": [["GCN", "Part-Of", "PSC"]], "rel_plus": [["GCN:Method", "Part-Of", "PSC:Method"]]}
{"doc_id": "210920315", "sentence": "Each body part mRoI feature F m is first passed through two parallel convolutional layers that are cascaded by ReLU .", "ner": [["mRoI", "Method"], ["convolutional layers", "Method"], ["ReLU", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210920315", "sentence": "Then , a multi - level RoI pooling ( mRoI ) is performed by concatenating the RoI Pooled features from multiple layers ( shallow and deep ) of the VGG backbone .", "ner": [["multi - level RoI pooling", "Method"], ["mRoI", "Method"], ["VGG", "Method"]], "rel": [["mRoI", "Synonym-Of", "multi - level RoI pooling"], ["multi - level RoI pooling", "Part-Of", "VGG"]], "rel_plus": [["mRoI:Method", "Synonym-Of", "multi - level RoI pooling:Method"], ["multi - level RoI pooling:Method", "Part-Of", "VGG:Method"]]}
{"doc_id": "210920315", "sentence": "The mRoI pooling operation is performed on each body part ( five ) as well as the full body ( F D ) , resulting in six mRoI pooled features for each proposal .", "ner": [["mRoI pooling operation", "Method"], ["mRoI", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210920315", "sentence": "Note that the Faster R - CNN and its pedestrian detection adaptations [ 5 0 ] , [ 3 3 ] , [ 5 2 ] commonly use a single RoI pooling only on the conv 5 features of VGG , resulting in 5 1 2 channels .", "ner": [["Faster R - CNN", "Method"], ["pedestrian detection", "Task"], ["RoI pooling", "Method"], ["conv 5", "Method"], ["VGG", "Method"]], "rel": [["Faster R - CNN", "Used-For", "pedestrian detection"], ["conv 5", "Part-Of", "VGG"], ["RoI pooling", "Part-Of", "VGG"]], "rel_plus": [["Faster R - CNN:Method", "Used-For", "pedestrian detection:Task"], ["conv 5:Method", "Part-Of", "VGG:Method"], ["RoI pooling:Method", "Part-Of", "VGG:Method"]]}
{"doc_id": "210920315", "sentence": "To maintain a similar number of channels as in Faster R - CNN and its pedestrian detection adaptations [ 5 1 ] , [ 5 2 ] , [ 3 3 ] , we introduce an additional 1 \u00d7 1 convolution in mRoI pooling strategy that significantly reduces the number of channels ( 5 7 2 in total ) .", "ner": [["Faster R - CNN", "Method"], ["pedestrian detection", "Task"], ["1 \u00d7 1 convolution", "Method"], ["mRoI pooling", "Method"]], "rel": [["Faster R - CNN", "Used-For", "pedestrian detection"], ["1 \u00d7 1 convolution", "Part-Of", "mRoI pooling"]], "rel_plus": [["Faster R - CNN:Method", "Used-For", "pedestrian detection:Task"], ["1 \u00d7 1 convolution:Method", "Part-Of", "mRoI pooling:Method"]]}
{"doc_id": "210920315", "sentence": "Consequently , mRoI pooled features of each body part and the full body has only 6 4 and 2 5 6 channels , respectively . 1 ) Intra - part Co - occurrence : Here , we enhance mRoI pooled feature representation of each body part by considering their intra - part co - occurrence .", "ner": [["mRoI", "Method"], ["mRoI", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210920315", "sentence": "We first pass the mRoI feature F m \u2208 R H \u00d7 W \u00d7C through two parallel 1 \u00d7 1 convolution layers that are cascaded by ReLU activation .", "ner": [["mRoI", "Method"], ["1 \u00d7 1 convolution layers", "Method"], ["ReLU", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210920315", "sentence": "This outputF m is first added to its input F m ( original mRoI feature ) , followed by passing it through a fully connected layer to obtain a d dimensional enhanced part feature .", "ner": [["mRoI", "Method"], ["fully connected layer", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210920315", "sentence": "To address this issue , we introduce an additional graph convolutional layer in our PSC module , that captures interpart relationship of different body parts and the full body of a pedestrian .", "ner": [["graph convolutional layer", "Method"], ["PSC", "Method"]], "rel": [["graph convolutional layer", "Part-Of", "PSC"]], "rel_plus": [["graph convolutional layer:Method", "Part-Of", "PSC:Method"]]}
{"doc_id": "210920315", "sentence": "Here , the three convolution layers are separated by ReLU activation .", "ner": [["convolution layers", "Method"], ["ReLU", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210920315", "sentence": "Heavily occluded parts are indicated in gray . an input to the classification network which predicts the final classification score and bounding - box regression .   We perform experiments on two datasets : CityPersons [ 5 0 ] and Caltech [ 1 0 ] .", "ner": [["classification network", "Method"], ["classification", "Task"], ["bounding - box regression", "Task"], ["CityPersons", "Dataset"], ["Caltech", "Dataset"]], "rel": [["classification network", "Used-For", "classification"], ["classification network", "Used-For", "bounding - box regression"]], "rel_plus": [["classification network:Method", "Used-For", "classification:Task"], ["classification network:Method", "Used-For", "bounding - box regression:Task"]]}
{"doc_id": "210920315", "sentence": "In case of CityPersons , we fine - tune pre - trained ImageNet VGG model [ 4 0 ] on the trainset of the CityPersons .", "ner": [["CityPersons", "Dataset"], ["ImageNet", "Dataset"], ["VGG", "Method"], ["CityPersons", "Dataset"]], "rel": [["VGG", "Trained-With", "ImageNet"], ["VGG", "Trained-With", "CityPersons"]], "rel_plus": [["VGG:Method", "Trained-With", "ImageNet:Dataset"], ["VGG:Method", "Trained-With", "CityPersons:Dataset"]]}
{"doc_id": "210920315", "sentence": "II shows the impact of integrating our intra - part and inter - part co - occurrence components in the baseline ( PD with mRoI ) .", "ner": [["PD with mRoI", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210920315", "sentence": "We also conduct an experiment by replacing our PSC module with part occlusion - aware RoI ( PoROI ) pooling unit of [ 5 2 ] in our framework .", "ner": [["PSC", "Method"], ["part occlusion - aware RoI", "Method"], ["PoROI", "Method"]], "rel": [["part occlusion - aware RoI", "Part-Of", "PSC"], ["PoROI", "Synonym-Of", "part occlusion - aware RoI"]], "rel_plus": [["part occlusion - aware RoI:Method", "Part-Of", "PSC:Method"], ["PoROI:Method", "Synonym-Of", "part occlusion - aware RoI:Method"]]}
{"doc_id": "210920315", "sentence": "Our PSC - Net achieves improved results ( 1 0 . 5 on R and 5 0 .   R HO Adaptive Faster RCNN [ 5 0 ] 1 3 . 0 5 0 . 5 MS - CNN [ 7 ] 1 3 . 3 5 1 . 9 Rep. Loss [ 4 5 ] 1 1 . 5 5 2 . 6 OR - CNN [ 5 2 ] 1 1 . 3 5 1 . 4 Cascade MS - CNN [ 7 ] 1 1 . 6 4 7 . 1 Adaptive - NMS [ 1 9 ] 1 1 . 4 -MGAN [ 3 3 ] 9. 3 4 1 . 0 PSC - Net ( Ours ) 9. 0 3 7 . 4 sets , compared to the state - of - the - art methods .", "ner": [["PSC - Net", "Method"], ["Faster RCNN", "Method"], ["MS - CNN", "Method"], ["OR - CNN", "Method"], ["Cascade MS - CNN", "Method"], ["Adaptive - NMS", "Method"], ["-MGAN", "Method"], ["PSC - Net", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210920315", "sentence": "When using an input scale of 1 \u00d7 and data visibility ( \u2265 6 5 % ) , the attention - based approaches , F.RCNN+ATT - part [ 5 1 ] and F.RCNN+ATT - vbb [ 5 1 ] , obtain log - average miss rates of 1 6 . 0 , 5 6 . 7 and 1 6 . 4 , 5 7 . 3 on the R and HO sets .", "ner": [["attention - based approaches", "Method"], ["F.RCNN+ATT - part", "Method"], ["F.RCNN+ATT - vbb", "Method"]], "rel": [["F.RCNN+ATT - part", "SubClass-Of", "attention - based approaches"], ["F.RCNN+ATT - vbb", "SubClass-Of", "attention - based approaches"]], "rel_plus": [["F.RCNN+ATT - part:Method", "SubClass-Of", "attention - based approaches:Method"], ["F.RCNN+ATT - vbb:Method", "SubClass-Of", "attention - based approaches:Method"]]}
{"doc_id": "210920315", "sentence": "MGAN [ 3 3 ] learns a spatial attention mask using VBB information to modulate full body features and achieves log - average miss rates of 1 1 . 5 and 5 1 . 7 on the R and HO sets , respectively .", "ner": [["MGAN", "Method"], ["spatial attention", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210920315", "sentence": "Our PSC - Net outperforms MGAN , without using VBB supervision , on both sets with log - average miss rates of 1 0 . 5 and 5 0 . 5 .", "ner": [["PSC - Net", "Method"], ["MGAN", "Method"]], "rel": [["PSC - Net", "Compare-With", "MGAN"]], "rel_plus": [["PSC - Net:Method", "Compare-With", "MGAN:Method"]]}
{"doc_id": "210920315", "sentence": "When using same data visibility but 1. 3 \u00d7 input scale , Adaptive - NMS [ 1 9 ] and MGAN [ 3 3 ] achieve log - average miss rates of 5 4 . 0 and 4 9 . 6 , respectively on the HO set .", "ner": [["Adaptive - NMS", "Method"], ["MGAN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210920315", "sentence": "Our PSC - Net significantly reduces the error by 3. 6 % over MGAN on the HO set .", "ner": [["PSC - Net", "Method"], ["MGAN", "Method"]], "rel": [["PSC - Net", "Compare-With", "MGAN"]], "rel_plus": [["PSC - Net:Method", "Compare-With", "MGAN:Method"]]}
{"doc_id": "210920315", "sentence": "Fig. 6 ( a ) AR - Ped [ 3 ] ( b ) MGAN [ 3 3 ] ( c ) PSC - Net Fig. 7 : Qualitative detection comparison of ( c ) PSC - Net with ( a ) AR - Ped [ 3 ] and ( b ) MGAN [ 3 3 ] under occlusions on caltech test images .", "ner": [["AR - Ped", "Method"], ["MGAN", "Method"], ["PSC - Net", "Method"], ["detection", "Task"], ["PSC - Net", "Method"], ["AR - Ped", "Method"], ["MGAN", "Method"], ["caltech", "Dataset"]], "rel": [["PSC - Net", "Used-For", "detection"], ["AR - Ped", "Used-For", "detection"], ["MGAN", "Used-For", "detection"], ["MGAN", "Evaluated-With", "caltech"], ["AR - Ped", "Evaluated-With", "caltech"], ["PSC - Net", "Evaluated-With", "caltech"]], "rel_plus": [["PSC - Net:Method", "Used-For", "detection:Task"], ["AR - Ped:Method", "Used-For", "detection:Task"], ["MGAN:Method", "Used-For", "detection:Task"], ["MGAN:Method", "Evaluated-With", "caltech:Dataset"], ["AR - Ped:Method", "Evaluated-With", "caltech:Dataset"], ["PSC - Net:Method", "Evaluated-With", "caltech:Dataset"]]}
{"doc_id": "210920315", "sentence": "Fig. 7 displays example detections showing a visual comparison of our PSC - Net with recently introduced AR - Ped [ 3 ] and MGAN [ 3 3 ] under occlusions .", "ner": [["PSC - Net", "Method"], ["AR - Ped", "Method"], ["MGAN", "Method"]], "rel": [["PSC - Net", "Compare-With", "AR - Ped"], ["PSC - Net", "Compare-With", "MGAN"]], "rel_plus": [["PSC - Net:Method", "Compare-With", "AR - Ped:Method"], ["PSC - Net:Method", "Compare-With", "MGAN:Method"]]}
{"doc_id": "210920315", "sentence": "We proposed a two - stage approach , PSC - Net , for occluded pedestrian detection .", "ner": [["PSC - Net", "Method"], ["occluded pedestrian detection", "Task"]], "rel": [["PSC - Net", "Used-For", "occluded pedestrian detection"]], "rel_plus": [["PSC - Net:Method", "Used-For", "occluded pedestrian detection:Task"]]}
{"doc_id": "210920315", "sentence": "Our PSC - Net consists of a standard pedestrian detection branch and a part spatial co - occurrence ( PSC ) module .", "ner": [["PSC - Net", "Method"], ["pedestrian detection branch", "Method"], ["part spatial co - occurrence", "Method"], ["PSC", "Method"]], "rel": [["pedestrian detection branch", "Part-Of", "PSC - Net"], ["part spatial co - occurrence", "Part-Of", "PSC - Net"], ["PSC", "Synonym-Of", "part spatial co - occurrence"]], "rel_plus": [["pedestrian detection branch:Method", "Part-Of", "PSC - Net:Method"], ["part spatial co - occurrence:Method", "Part-Of", "PSC - Net:Method"], ["PSC:Method", "Synonym-Of", "part spatial co - occurrence:Method"]]}
{"doc_id": "210920315", "sentence": "The focus of our design is the PSC module that is designed to capture intra - part and inter - part spatial co - occurrence of different body parts through a Graph Convolutional Network ( GCN ) .", "ner": [["PSC", "Method"], ["Graph Convolutional Network", "Method"], ["GCN", "Method"]], "rel": [["Graph Convolutional Network", "Part-Of", "PSC"], ["GCN", "Synonym-Of", "Graph Convolutional Network"]], "rel_plus": [["Graph Convolutional Network:Method", "Part-Of", "PSC:Method"], ["GCN:Method", "Synonym-Of", "Graph Convolutional Network:Method"]]}
{"doc_id": "210920315", "sentence": "Experiments are performed on two popular datasets : CityPersons and Caltech .", "ner": [["CityPersons", "Dataset"], ["Caltech", "Dataset"]], "rel": [], "rel_plus": []}
